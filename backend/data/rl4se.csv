project	key	title	abstract	doi	decision	mode	exclusion_criteria	reviewer_count
RL4SE	Abdelmoaty2022	Resilient Topology Design for Wireless Backhaul: A Deep Reinforcement Learning Approach	Ultra-dense 5G and beyond deployments are setting significant burden on cellular networks, especially for wireless backhauls. Today, a careful planning for wireless backhaul is more critical than ever. In this letter, we study the hierarchical wireless backhaul topology design problem. We introduce a Deep Reinforcement Learning (DRL) based algorithm that can solve the problem efficiently. We compare the quality of the solutions derived by our DRL approach to the optimal solution, derived according to the Integer Linear Program (ILP) formulation in our previous work. A simulation using practical channel propagation scenarios and different network densities proves that our DRL-based algorithm is providing a sub-optimal solution with different levels of resiliency. Our DRL algorithm is further shown to scale for larger instances of the problem.	https://dx.doi.org/10.1109/LWC.2022.3207358	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Abdulrahman2022	Reinforcement-learning-based damping control scheme of a PV plant in wide-area measurement system		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135556791&doi=10.1007\%2fs00202-022-01615-3&partnerID=40&md5=2a201de2174f125867277d60badb5e45	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Abichandani2022	Design and Implementation of Closed Loop Control of PSFB Topology Using Artificial Intelligence		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141558490&doi=10.4271\%2f2022-28-0121&partnerID=40&md5=2b8c8cfc78f84c5d35f722203ce067c8	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Abichandani2021	Implementation of Decentralized Reinforcement Learning-Based Multi-Quadrotor Flocking	Enabling coordinated motion of multiple quadrotors is an active area of research in the field of small unmanned aerial vehicles (sUAVs). While there are many techniques found in the literature that address the problem, these studies are limited to simulation results and seldom account for wind disturbances. This paper presents the experimental validation of a decentralized planner based on multi-objective reinforcement learning (RL) that achieves waypoint-based flocking (separation, velocity alignment, and cohesion) for multiple quadrotors in the presence of wind gusts. The planner is learned using an object-focused, greatest mass, state-action-reward-state-action (OF-GM-SARSA) approach. The Dryden wind gust model is used to simulate wind gusts during hardware-in-the-loop (HWIL) tests. The hardware and software architecture developed for the multi-quadrotor flocking controller is described in detail. HWIL and outdoor flight tests results show that the trained RL planner can generalize the flocking behaviors learned in training to the real-world flight dynamics of the DJI M100 quadrotor in windy conditions.	https://dx.doi.org/10.1109/ACCESS.2021.3115711	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Abirami2022	Crypto-Deep Reinforcement Learning Based Cloud Security For Trusted Communication	One of the most essential business models in modern information technology is cloud computing. It offers a variety of services for user interaction as well as low-cost hardware and software. Cloud services are built on newline virtualization designs that use multi-tenancy for improved resource management and newline strong isolation across several Virtual Machines (VMs). Despite advancements in virtualization, data security and isolation assurances continue to be the major difficulties faced by cloud providers using existing approaches. To overcome this problem, Deep Reinforcement Learning is applied to offload the task and also to detect the generalized attackers in the cloud network. This proposed solution enables remote data monitoring approaches such as identity-based linear classification algorithms for VM attack classification channels. It can minimise data secrecy and increase communication by using a reinforcement learning technique. The attacker channel is identified using identity-based linear classification when data is transferred/retrieved from the VM cloud server. When the classifier finds channel misbehaviour, the port or channel may be blocked, and the communication of other accessible ports may be modified maintaining the end to end communication secrecy using the improved Multi Agent Deep Reinforcement Learning (MADRL). The service verification is done to ensure that users have secure access to the cloud server. When an unknown request to the cloud server runs the key authentication to check the user authorization, this linear classification trains the existingside-channel attack datasets to the classifier and detects the VM cloud's attack channel. In terms of overall performance, the proposed methodology is investigated and compared to the existing approaches.	https://dx.doi.org/10.1109/ICSSIT53264.2022.9716429	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Abrazeh2023	Virtual Hardware-in-the-Loop FMU Co-Simulation Based Digital Twins for Heating, Ventilation, and Air-Conditioning (HVAC) Systems	In this paper, a novel self-adaptive control method based on a digital twin is developed and investigated for a multi-input multi-output (MIMO) nonlinear system, which is a heating, ventilation, and air-conditioning system. For this purpose, hardware-in-loop (HIL) and software-in-loop (SIL) are integrated to develop the digital twin control concept in a straightforward manner. A nonlinear integral backstepping (NIB) model-free control technique is integrated with the HIL (implemented as a physical controller) and SIL (implemented as a virtual controller) controllers to control the HVAC system without the need for dynamic feature identification. The main goal is to design the virtual controller to minimize the distinction between system outputs in the SIL and HIL setups. For this purpose, Deep Reinforcement Learning (DRL) is applied to update the NIB controller coefficients of the virtual controller based on the measured data of the physical controller. Since the temperature and humidity of HVAC systems should be regulated, the NIB controllers in the HIL and SIL are designed by the DRL algorithm in a multi-objective scheme (MO). In particular, the simulations of the HIL and SIL environments are coupled by a new advanced tool: function mockup interface (FMI) standard. The Functional Mock-up Unit (FMU) is adopted into the FMI interface for data exchange. The extensive research of HIL and SIL controllers shows that the system outputs of the virtual controller are controlled exactly according to the physical controller.	https://dx.doi.org/10.1109/TETCI.2022.3168507	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Achamrah2022	Bi-level programming for modeling inventory sharing in decentralized supply chains		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127480149&doi=10.1016\%2fj.trpro.2022.02.064&partnerID=40&md5=abb94485282c37def68c3e904694b8de	Excluded	new_screen	E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Adel2021	A Multi-agent Reinforcement Learning Risk Management Model for Distributed Agile Software Projects	Nowadays, due to the benefits of the agile method, such as changing rapidly and fast delivery of working software, most software projects are naturally distributed agile projects (DAD). DAD team members work from various remote sites. This leads to the emergence of many significant challenges in risk management. Hence, it requires risk safety techniques to be implemented to mitigate the occurrence of risks. There is no standardized process for teams to deal with DAD risks. In this paper, a multi-agent reinforcement learning risk management model for distributed agile software projects is proposed. The proposed model is implemented to apply a dynamic policy. The model is applied as an experiment to definite numbers for a set of risk factors such as communication and coordination risk, project management risk, and SDLC risk. The proposed model is developed using multiple individual learning using the Q- learning algorithm. A DAD project with two projects is used to evaluate the proposed model. The proposed model was assessed and analyzed for its effectiveness. The results of the evaluation are given.	https://dx.doi.org/10.1109/ICICIS52592.2021.9694252	Included	new_screen		6
RL4SE	Adriaenssens2022	Learning to Ascend Stairs and Ramps: Deep Reinforcement Learning for a Physics-Based Human Musculoskeletal Model	This paper proposes to use deep reinforcement learning to teach a physics-based human musculoskeletal model to ascend stairs and ramps. The deep reinforcement learning architecture employs the proximal policy optimization algorithm combined with imitation learning and is trained with experimental data of a public dataset. The human model is developed in the open-source simulation software OpenSim, together with two objects (i.e., the stairs and ramp) and the elastic foundation contact dynamics. The model can learn to ascend stairs and ramps with muscle forces comparable to healthy subjects and with a forward dynamics comparable to the experimental training data, achieving an average correlation of 0.82 during stair ascent and of 0.58 during ramp ascent across both the knee and ankle joints.	https://www.ncbi.nlm.nih.gov/pubmed/36366177	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Aghli2018	A reinforcement learning approach to autonomous speed control in robotic systems			Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Agrawal2021	A multi-agent reinforcement learning framework for intelligent manufacturing with autonomous mobile robots		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117824775&doi=10.1017\%2fpds.2021.17&partnerID=40&md5=158b6e89fc56ce84df98a40148f321f1	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Aguirre2021	A pre-expectation calculus for probabilistic sensitivity		https://doi.org/10.1145/3434333	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method,E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Aguzzi2022	Addressing Collective Computations Efficiency: Towards a Platform-level Reinforcement Learning Approach	Aggregate Computing is a macro-level approach for programming collective intelligence and self-organisation in distributed systems. In this paradigm, system behaviour unfolds as a combination of a system-wide program, functionally manipulating distributed data structures called computational fields, and a distributed protocol where devices work at asynchronous rounds comprising sense-compute-interact steps. Interestingly, there exists a large amount of flexibility in how aggregate systems could actually execute while preserving the desired functionality. The ideal place for making choices about execution is the aggregate computing platform (or middleware), which can be engineered with the goal of promoting efficiency and other non-functional goals. In this work, we explore the possibility of applying Reinforcement Learning at the platform level in order to optimise aspects of a collective computation while achieving coherent functional goals. This idea is substantiated through synthetic experiments of data propagation and collection, where we show how Q-Learning could reduce the power consumption of aggregate computations.	https://dx.doi.org/10.1109/ACSOS55765.2022.00019	Included	conflict_resolution		6
RL4SE	Ahmad2020	Using Deep Reinforcement Learning for Exploratory Performance Testing of Software Systems With Multi-Dimensional Input Spaces	During exploratory performance testing, software testers evaluate the performance of a software system with different input combinations in order to identify combinations that cause performance problems in the system under test. Performance problems such as low throughput, high response times, hangs, or crashes in software applications have an adverse effect on the customer's satisfaction. Since many of today's large-scale, complex software systems (e.g., eCommerce applications, databases, web servers) exhibit very large multi-dimensional input spaces with many input parameters and large ranges, it has become costly and inefficient to explore all possible combinations of inputs in order to detect performance problems. In order to address this issue, we introduce a method for identifying input combinations that trigger performance problems in the software system under test. Our method, under the name of iPerfXRL, employs deep reinforcement learning in order to explore a given large multi-dimensional input space efficiently. The main benefit of the approach is that, during the exploration process, it learns and recognizes the problematic regions of the input space that have a higher chance of triggering performance problems. It concentrates the search in those problematic regions to find as many input combinations as possible that can trigger performance problems while executing a limited number of input combinations against the system. In addition, our approach does not require prior domain knowledge or access to the source code of the system. Therefore, it can be applied to any software system where we can interactively execute different input combinations while monitoring their performance impact on the system. We implement iPerfXRL on top of the Soft Actor-Critic algorithm. We evaluate empirically the efficiency and effectiveness of our approach against alternative state-of-the-art approaches. Our results show that iPerfXRL accurately identifies the problematic regions of the input space and finds up to 9 times more input combinations that trigger performance problems on the system under test than the alternative approaches.	https://dx.doi.org/10.1109/ACCESS.2020.3033888	Included	new_screen		6
RL4SE	Ahmad2019	Exploratory Performance Testing Using Reinforcement Learning	Performance bottlenecks resulting in high response times and low throughput of software systems can ruin the reputation of the companies that rely on them. Almost two-thirds of performance bottlenecks are triggered on specific input values. However, finding the input values for performance test cases that can identify performance bottlenecks in a large-scale complex system within a reasonable amount of time is a cumbersome, cost-intensive, and time-consuming task. The reason is that there can be numerous combinations of test input values to explore in a limited amount of time. This paper presents PerfXRL, a novel approach for finding those combinations of input values that can reveal performance bottlenecks in the system under test. Our approach uses reinforcement learning to explore a large input space comprising combinations of input values and to learn to focus on those areas of the input space which trigger performance bottlenecks. The experimental results show that PerfxRL can detect 72\% more performance bottlenecks than random testing by only exploring the 25\% of the input space.	https://dx.doi.org/10.1109/SEAA.2019.00032	Included	new_screen		6
RL4SE	Ahmadi2022	A DQN-based agent for automatic software refactoring	Context: Nowadays, technical debt has become a very important issue in software project management. The main mechanism to repay this debt is through refactoring. Refactoring software projects usually comes at a high cost. As a result, researchers have always looked for ways to minimize this cost, and a good potential candidate to reduce the cost of a process is to automate it. Objective: One of the automatic software refactoring methods that recently has received a lot of attention is based on search-based software engineering (SBSE) methods. Although because of comprehensiveness and versatility SBSE is considered an appropriate method for automatic refactoring, it has its downsides, the most important of which are the uncertainty of the results and the exponential execution time. Method: In this research, a solution is proposed inspired by search-based refactoring while taking advantage of exploitation in reinforcement learning techniques. This work aims to solve the uncertainty problems and execution time for large programs. In the proposed approach, the problem of uncertainty is solved by targeting the selection of refactoring actions used in the search-based approach. Also, due to the reduction of the dependency between the choice of the appropriate refactoring and its execution time, the time problem in large software refactoring has been greatly improved. Results: Amongst the performed evaluations and specifically for the refactoring of the largest case study, the proposed approach managed to increase the accuracy to more than twice of the SBSE refactoring approaches, while reducing the execution time of refactoring by more than 98\%. Conclusion: The results of the tests show that with increasing the volume and size of the software, the performance of the proposed approach also improves compared to the methods based on SBSE, both in terms of reducing technical debt and speeding up the refactoring process.	https://dx.doi.org/10.1016/j.infsof.2022.106893	Included	new_screen		6
RL4SE	Ahmed2020	An evaluation of Monte Carlo-based hyper-heuristic for interaction testing of industrial embedded software applications		https://doi.org/10.1007/s00500-020-04769-z	Included	new_screen		6
RL4SE	Ahmed2015	Memory modelling schemes in neuromorphic VLSI chips using reinforcement learning based on cognition a computational cognititve neuroscience approach			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Ahmed2017	Building load management clusters using reinforcement learning	In this paper we introduce a customer selection model for decision making in a Demand response program. In particular, we focus on modelling demand response as a reinforcement learning problem that decomposes the customers into clusters based on their ability to provide curtailments at time of Demand response signal. The reinforcement learning approach allows the retailer to make fast informed decision on the customers reliable to provide demand management capabilities without the need of exploring the entire set of customers when needed in real-time and allow for classification of future customers in appropriate clusters. We demonstrate using this approach on a representative example to create clusters based on provided customer profiles for varying DR signals.	https://dx.doi.org/10.1109/IEMCON.2017.8117147	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Ahrarinouri2021	Multiagent Reinforcement Learning for Energy Management in Residential Buildings		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096037876&doi=10.1109\%2fTII.2020.2977104&partnerID=40&md5=22e7ee052871d2f4b8c4ff135bad87e9	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Ahuja2021	Learning to Optimize Molecular Geometries Using Reinforcement Learning	Though quasi-Newton methods have been widely adopted in computational chemistry software for molecular geometry optimization, it is well known that these methods might not perform well for initial guess geometries far away from the local minima, where the quadratic approximation might be inaccurate. We propose a reinforcement learning approach to develop a model that produces a correction term for the quasi-Newton step calculated with the BFGS algorithm to improve the overall optimization performance. Our model is able to complete the optimization in about 30\% fewer steps than pure BFGS for molecules starting from perturbed geometries. The new method has similar convergence to BFGS when complemented with a line search procedure, but it is much faster since it avoids the multiple gradient evaluations associated with line searches.	https://www.ncbi.nlm.nih.gov/pubmed/33470813	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Aiba1996	A computational model for distributed knowledge systems with learning mechanisms	This paper addresses the issues of machine learning in distributed knowledge systems, which will consist of distributed software agents with problem solving, communication and learning functions. To develop such systems, we must analyze the roles of problem-solving and communication capabilities among knowledge systems. To facilitate the analyses, we propose a computational model: LPC. The model consists of a set of agents with (a) a knowledge base for learned concepts, (b) a knowledge base for problem solving, (c) prolog-based inference mechanisms and (d) a set of beliefs on the reliability of the other agents. Each agent can improve its own problem-solving capabilities by deductive learning from the given problems, by memory-based learning from communications between the agents and by reinforcement learning from the reliability of communications between the other agents. An experimental system of the model has been implemented in prolog language on a Window-based personal computer Intensive experiments have been carried out to examine the feasibility of the machine learning mechanisms of agents for problem-solving and communication capabilities. The experimental results have shown that the multiagent system improves the performance of the whole system in problem solving, when each agent has a higher learning ability or when an agent with a very high ability for problem solving joins the organization to cooperate with the other agents in problem solving. These results suggest that the proposed model is useful in analyzing the learning mechanisms applicable to distributed knowledge systems. Copyright (C) 1996 Elsevier Science Ltd	https://dx.doi.org/10.1016/0957-4174(96)00020-6	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Aissani2012	Dynamic scheduling for multi-site companies: a decisional approach based on reinforcement multi-agent learning		https://doi.org/10.1007/s10845-011-0580-y	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Akbarinasaji2018	Prioritizing lingering bugs		https://doi.org/10.1145/3178315.3178326	Included	new_screen		6
RL4SE	Akther2022	Interest forwarding strategy in Named Data Networks (NDN) using Thompson Sampling	Optimizing Interest forwarding and Data delivery has been among the top dissected problems in NDN for the last decade; however, only a few contributions thrive to minimize communication cost and delay concurrently. In NDN, a receiver-driven forwarding strategy is considered resource-consuming as the routers incur computation to find the best path to the desired item, specified by an Interest's name. On the other hand, a source-driven forwarding strategy, a scheme that suppresses the sub-optimal sources, experiences increased delay when no source answers in the exploration phase. The confluence of the two strategies can counteract the drawbacks of each one, which, however, has never been investigated. In this work, a reinforcement learning -based, namely Thompson Sampling, strategy is proposed that operates in a receiver-cum-source-driven fashion to optimize Interest forwarding and answering. The proposed method introduces a 'Beam' concept coupled with adaptive scoped-flooding to optimize Interest forwarding, and the sources adopt Thompson Sampling to suppress the sub-optimal responses. When hit by an Interest, an optimal source sends back the desired Data to the consumer whereas a sub-optimal source remains Silent. Together, the 'Beam' and the scoped-flooding adapt the Interest forwarding range based on cache hit/miss ratio. The adaptation optimizes communication cost and delay, and contributes to scheming the proposed strategy resource-savvy. The proof-of-concept implementation in software (simulation) reveals that the proposed system outperforms the counterpart benchmarks by reducing the communication costs and delay in NDN (by around 350\% and 10\%, respectively) without negotiating packet delivery ratio.	https://dx.doi.org/10.1016/j.jnca.2022.103458	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Alawsi2022	Quality of service system that is self-updating by intrusion detection systems using reinforcement learning	Machine learning techniques are widely used in many areas and have also been used in intrusion detection systems (IDS) to identify hard-to-recognize patterns that can distinguish benign traffic from attack. However, these types of relationships and patterns are discovered through training, and there are several types of methods used to train. First, supervised methods, through their application to samples of benign and offensive traffic, secondly, unsupervised methods. Either way, human interaction is necessary to update the IDS, so that a new type of attack cannot be accepted in the protected network. In this study, a new method based on reinforcement learning is proposed. The proposed method has the potential to adapt to new types of attacks based on the quality of service of the network. The proposed method in this paper showed the best performance in terms of filtering the attack traffic that was not included in the training of the neural network used by the agent to choose the appropriate action for each packet. The program was evaluated using the CICIDS2017 dataset and achieved a F1 score of 0.96, compared to only 0.51 achieved using a classification-based deep learning approach.	https://dx.doi.org/10.1007/s13204-021-02172-0	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Albahli2019	A Deep Ensemble Learning Method for Effort-Aware Just-In-Time Defect Prediction	Since the introduction of just-in-time effort aware defect prediction, many researchers are focusing on evaluating the different learning methods, which can predict the defect inducing changes in a software product. In order to predict these changes, it is important for a learning model to consider the nature of the dataset, its unbalancing properties and the correlation between different attributes. In this paper, we evaluated the importance of these properties for a specific dataset and proposed a novel methodology for learning the effort aware just-in-time prediction of defect inducing changes. Moreover, we devised an ensemble classifier, which fuses the output of three individual classifiers (Random forest, XGBoost, Multi-layer perceptron) to build an efficient state-of-the-art prediction model. The experimental analysis of the proposed methodology showed significant performance with 77\% accuracy on the sample dataset and 81\% accuracy on different datasets. Furthermore, we proposed a highly competent reinforcement learning technique to avoid false alarms in real time predictions.	https://dx.doi.org/10.3390/fi11120246	Included	new_screen		6
RL4SE	Albeaik2019	Deep Truck : A deep neural network model for longitudinal dynamics of heavy duty trucks	This article demonstrates the use of deep neural networks (NN) and deep reinforcement learning (deep-RL) for modeling and control of longitudinal heavy duty truck dynamics. Instead of explicit use of analytical model derived information or parameters about the truck, the deep NN model is fitted to data using a brief set of historical data collected from an arbitrary driving cycle. The deep model is used in this article to design a cruise controller for the truck using model-free deep-RL. The deep model and the control loop performances are demonstrated both using state-of-the-art commercial simulation software, and using a real-physical truck. Model and control performances are compared to classical physics-based modeling and control design approaches. The deep NN model is shown to capture latent nonlinear state dynamics and the deep-RL cruise controller is shown to achieve comparable results to a carefully designed and calibrated controller.	https://dx.doi.org/10.1109/ITSC.2019.8917038	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Albeaik2019a	Deep Truck	This article demonstrates the use of deep neural networks (NN) and deep reinforcement learning (deep-RL) for modeling and control of longitudinal heavy duty truck dynamics. Instead of explicit use of analytical model derived information or parameters about the truck, the deep NN model is fitted to data using a brief set of historical data collected from an arbitrary driving cycle. The deep model is used in this article to design a cruise controller for the truck using model-free deep-RL. The deep model and the control loop performances are demonstrated both using state-of-the-art commercial simulation software, and using a real-physical truck. Model and control performances are compared to classical physics-based modeling and control design approaches. The deep NN model is shown to capture latent nonlinear state dynamics and the deep-RL cruise controller is shown to achieve comparable results to a carefully designed and calibrated controller.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Albeaik2019b	Deep Truck : A deep neural network model for longitudinal dynamics of heavy duty trucks		https://doi.org/10.1109/ITSC.2019.8917038	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Albericci2021	A curriculum-based reinforcement learninig approach to pedestrian simulation			Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Al-Dayaa2012	Towards a Multiple-Lookahead-Levels agent reinforcement-learning technique and its implementation in integrated circuits		https://doi.org/10.1007/s11227-011-0738-6	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Alfaverh2020	Demand Response Strategy Based on Reinforcement Learning and Fuzzy Reasoning for Home Energy Management		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081669247&doi=10.1109\%2fACCESS.2020.2974286&partnerID=40&md5=7aee2012044e2087f1eefb60b9864780	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Al-Gabalawy2019	Machine learning for aircraft control			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Al-Hefnawi2021	Reinforcement Learning Method for Autonomous UAVs Monitoring an Uncertain Target	Autonomous unmanned aerial vehicles are able to sense their surrounding environments, and fly safely with little or no human intervention. Autonomous unmanned aerial vehicles are characterized by their ability to make decisions based on predicting future possible situations and learning from previous experiences. In this paper, we aim at developing algorithms that enable unmanned aerial vehicles to monitor and detect a dynamic uncertain target autonomously. This work considers a real monitoring system consists of a mission area, an autonomous unmanned aerial vehicle, a charging station, and a dynamic uncertain target. The mission area consists of two main areas, which are the area where the charging station is placed and the area where the target moves. The target area is divided to a number of subareas. We also adopt a time slotted system that has M equal-duration slots. The unmanned aerial vehicle is equipped with a battery of finite energy that can be recharged from the charging station. It can fly from one subarea to another during one time slot. The target moves from one subarea to another according to an unknown Markov process. In this context, we propose to using reinforcement learning algorithms that enables autonomous unmanned aerial vehicles to learn the movement of a dynamic uncertain target autonomously. Simulation results show that reinforcement learning algorithms outperform the performance of random and circular algorithms.11This work was supported by the ASPIRE Award for Research Excellence Program 2020 (Abu Dhabi, UAE) under grant AARE20-161.	https://dx.doi.org/10.1109/SNAMS53716.2021.9732147	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Al-Hilo2022	Reconfigurable Intelligent Surface Enabled Vehicular Communication: Joint User Scheduling and Passive Beamforming	Given its ability to control and manipulate wireless environments, reconfigurable intelligent surface (RIS), also known as intelligent reflecting surface (IRS), has emerged as a key enabler technology for the six-generation (6G) cellular networks. In the meantime, vehicular environment radio propagation is negatively influenced by a large set of objects that cause transmission distortion such as high buildings. Therefore, this work is devoted to explore the area of RIS technology integration with vehicular communications while considering the dynamic nature of such communication environment. Specifically, we provide a system model where RoadSide Unit (RSU) leverages RIS to provide indirect wireless transmissions to disconnected areas, known as dark zones. Dark zones are spots within RSU coverage where the communication links are blocked due to the existence of blockages. In details, a discrete RIS is utilized to provide communication links between the RSU and the vehicles passing through out-of-service zones. Therefore, the joint problem of RSU resource scheduling and RIS passive beamforming or phase-shift matrix is formulated as an optimization problem with the objective of maximizing the minimum average bit rate. The formulated problem is mixed integer non-convex program which is difficult to be solved and does not account for the uncertain dynamic environment in vehicular networks. Thereby, we resort to alternative methods based on Deep Reinforcement Learning to determine RSU wireless scheduling and Block Coordinate Descent (BCD) to solve for the phase-shift matrix, i.e., passive beamforming, of the RIS. The Markov Decision Process (MDP) is defined and the complexity of the solution approach is discussed. Our numerical results demonstrate the superiority of our proposed approach over baseline techniques.	https://dx.doi.org/10.1109/TVT.2022.3141935	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for	6
RL4SE	Alhussein2022	Dynamic Topology Design of NFV-Enabled Services Using Deep Reinforcement Learning	Next-generation networks are endowed with enhanced capabilities thanks to software-defined networking and network function virtualization (NFV). There is a radical shift from device-centric to experience-driven environments of which data is the primary driver behind its running engines. In this paper, we consider joint topology design, traffic routing and NF placement for unicast NFV-enabled services. We develop an end-to-end model-free deep reinforcement learning (RL) framework to dynamically allocate processing and transmission resources, while considering time-varying network traffic patterns. First, we provide a flexible pre-processing technique that represents and reduces the state space and action space of the considered joint problem for the deep RL algorithm. Second, we present a deep deterministic policy gradient (DDPG) algorithm that is enhanced with a model-assisted exploration procedure. Due to the multiple resource types with strongly adverse effects, the existing vanilla DDPG algorithm cannot achieve consistent performance. The model-assisted exploration procedure, which utilizes a perturbed step-wise sub-optimal integer linear program, bootstraps and stabilizes the vanilla DDPG algorithm and finds optimal solutions efficiently.	https://dx.doi.org/10.1109/TCCN.2021.3139632	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Alonso2003	Agency, learning and animal-based reinforcement learning		https://doi.org/10.1007/978-3-540-25928-2_1	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E3: Only conceptual results are reported,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E3: Only conceptual results are reported	6
RL4SE	Alonso-Lupez2022	Level of Trust and Privacy Management in 6G Intent-based Networks for Vertical Scenarios	Security is one of the main concerns when designing future 6G networks. Most of the components of service chains are software-based and their security must be ensured. In this work we propose a system that will check the level of trust of 6G services following 3 complementary approaches: attestation, generation of proofs of transit and usage of smart contracts whose execution will be published via Distributed Ledger Technology in order to enable their traceability. The service orchestration function will use reinforcement learning to formulate optimised service orchestration decisions. Services will be specified in a declarative model, following a privacy-aware flavour of intent-based networking. The proposed architecture will take into account privacy of every stakeholder: final users, service providers and infrastructure providers.	https://dx.doi.org/10.1109/6GNet54646.2022.9830323	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E5: Other not a paper	6
RL4SE	Alroobaea2022	Markov decision process with deep reinforcement learning for robotics data offloading in cloud network		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147505356&doi=10.1117\%2f1.JEI.31.6.061809&partnerID=40&md5=a36f8a9e7dcc6742b5a7049503bb5995	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Alrubyli2022	Using Q-learning to Automatically Tune Quadcopter PID Controller Online for Fast Altitude Stabilization	Unmanned Arial Vehicles (UAVs), and more specifically, quadcopters need to be stable during their flights. Altitude stability is usually achieved by using a PID controller that is built into the flight controller software. Furthermore, the PID controller has gains that need to be tuned to reach optimal altitude stabilization during the quadcopter's flight. For that, control system engineers need to tune those gains by using extensive modeling of the environment, which might change from one environment and condition to another. As quadcopters penetrate more sectors from the military to the consumer sectors, they have been put into complex and challenging environments more than ever before. Hence, intelligent self-stabilizing quadcopters are needed to maneuver through those complex environments and situations. Here we show that by using online reinforcement learning with minimal background knowledge, the altitude stability of the quadcopter can be achieved using a model-free approach. We found that by using background knowledge and an activation function like Sigmoid, altitude stabilization can be achieved faster with a small memory footprint. In addition, using this approach will accelerate development by avoiding extensive simulations before applying the PID gains to the real-world quadcopter. Our results demonstrate the possibility of using the trial and error approach of reinforcement learning combined with activation function and background knowledge to achieve faster quadcopter altitude stabilization in different environments and conditions.	https://dx.doi.org/10.1109/ICMA54519.2022.9856292	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	AL-sarray2020	Securing 5G Network using low power wireless personal area network	The purpose of this work was to get acquainted with wireless communication technologies and the information security challenges they create from the viewpoint of 5G and its preceding technologies. 5th Generation (5G) is becoming a global phenomenon and it is currently being implemented in dozens of countries around the globe with it comes new information security challenges. Potential solutions for the challenges are also offered. The outcome of this research is an overview of information security challenges in 5G using Low-power wireless personal area Network (LPWAN) and in the technologies preceding 5G. Possible information security solutions are presented in this work for the new technologies coming with 5G. This work showed that the new technologies coming with 5G, such as the virtualization of hardware and services as well as the utilization of cloud computing, create completely new areas of attack for networks. With this knowledge, Labelled and Freely Available Dataset from Open-Source Repository will be used and it is possible to prevent attacks targeting networks by implementing necessary information security elements. For the training, testing and validation of our dataset which is an IoT and cyber-security based dataset, a well-known MATLAB R2019a software was used for this purpose. The proposed reinforcement learning algorithm for Securing 5G network is designed for mesh topology from the ground up by the model of the network itself using low power personal area networks. We model the network operating in a finite area with a finite number of nodes distributed inside the area randomly in this algorithm. Hence, we defined the service area of the target network by assuming the finiteness of the network in the model.	https://dx.doi.org/10.1109/ISMSIT50672.2020.9255054	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for	6
RL4SE	Altin2020	Evolutionary Reinforcement Learning for the Coordination of Swarm UAVs	Deep Reinforcement Learning (DRL) algorithms are used in many challenging tasks and their usage areas are rapidly increasing. One of these areas is the formation flights of Unmanned Aerial Vehicles (UAVs). The rising of Reinforcement Learning (RL) algorithms performances is directly proportional to the development of environments. This paper presents a new environment developed through software (Ardupilot, Mavlink, drone-kit) that is frequently used in open source UAV simulation and programming, and the performance of the Evolutionary Reinforcement Learning (ERL) agent in this environment. The difference of this environment is that, unlike other environments, the model can be operated directly on a drone-kit supported vehicle and is specifically defined on the centralised formation task. The aim of this study is; in order to question the performance of the Evolutionary Reinforcement Learning (ERL) algorithm which has better results than other algorithms in DRL training environments,in this environment, and increasing the usage of the algorithm in this direction.	https://dx.doi.org/10.1109/SIU49456.2020.9302227	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Amaral2022	Deep Reinforcement Learning Based Routing in IP Media Broadcast Networks: Feasibility and Performance	The media broadcast industry has evolved from Serial Digital Interface (SDI) based infrastructures to IP networks. While IP based video broadcast is well established in the data plane, the use of IP networks to transport media flows still poses challenges in terms of resource management and orchestration. Software Defined Networking (SDN) based orchestration architectures have emerged in the industry that use SDN to route the media flows of a broadcast service across the provider IP network. Several approaches to multimedia flow routing in IP based SDN networks have been proposed in the context of streaming applications over the Internet. These range from model based linear optimization solutions that have high complexity to simple shortest path based routing with either Static Link Costs (SLC) or Dynamic Link Costs (DLC). More recently model-free optimization methods such as Deep Reinforcement Learning (DRL) have been proposed for routing and Traffic Engineering (TE) of multimedia flows in SDN networks. The media broadcast scenario however has specific requirements, with services like Master Control Room (MCR) operation and live broadcasting of events, and it has been rarely addressed in the literature. In this work we propose a DRL based routing method for this scenario and compare it to SLC and DLC algorithms based on Dijkstra shortest paths. This is, to our knowledge, the first work to follow this approach in the context of media broadcast services in IP infrastructures. The algorithm is designed considering the specifications and capabilities of one of the leading SDN orchestrators in the market and considers the more common Service Level Agreement (SLA) requirements in the industry. Three different DRL algorithms are implemented and compared and we evaluate them using a real service provider network topology. The results indicate that DRL based routing is applicable in real production scenarios and that it achieves considerable performance gains when compared to the SLC and DLC shortest path algorithms commonly used today.	https://dx.doi.org/10.1109/ACCESS.2022.3182009	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Amoui2008	Adaptive Action Selection in Autonomic Software Using Reinforcement Learning	"The planning process in autonomic software aims at selecting an action from a finite set of alternatives for adaptation. This is an abstruse problem due to the fact that software behaviour is usually very complex with numerous number of control variables. This research work focuses on proposing a planning process and specifically an action selection technique based on ""Reinforcement Learning"" (RL). We argue why, how, and when RL can be beneficial for an autonomic software system. The proposed approach is applied to a simulated model of a news web application. Evaluation results show that this approach can learn to select appropriate actions in a highly dynamic environment. Furthermore, we compare this approach with another technique from the literature, and the results suggest that it can achieve similar performance in spite of no expert involvement."	https://dx.doi.org/10.1109/ICAS.2008.35	Included	conflict_resolution		6
RL4SE	An2022	Traffic Signal Control Method Based on Modified Proximal Policy Optimization	For the traffic congestion problem at intersections, this paper proposed a modified proximal policy optimization algorithm, which can adaptively regulate traffic signals to alleviate intersection congestion. The algorithm made further theoretical corrections to the unbiasedness of the advantage function in it through the importance sampling method. The algorithm was also modified to better adapt to traffic signal control and thus increase the sensitivity of the algorithm to control actions. In order to fully extract road state information and reduce the complexity of the state space, the experiment used images as state inputs, which are real-time snapshots of road conditions at intersections. To verify the performance of the proposed algorithm, it was compared with other deep reinforcement learning algorithms in the traffic simulation software SUMO. The results show that the cumulative reward of the model constructed by the proposed algorithm is increased by 68.3\% and the average waiting time is reduced by 65.3\% compared to the baseline DQN model. Experimental results demonstrate that the algorithm can effectively regulate traffic congestion.	https://dx.doi.org/10.1109/ICTLE55577.2022.9901894	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Andersson1999	Reactive and Memory-Based Genetic Programming for Robot Control			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Andre2002	State abstraction for programmable reinforcement learning agents	Safe state abstraction in reinforcement learning allows an agent to ignore aspects of its current,state that are irrelevant to its current decision, and therefore speeds up dynamic programming and learning. This paper explores safe state abstraction in hierarchical reinforcement learning, where learned behaviors must conform to a given partial, hierarchical program. Unlike previous approaches to this problem, our methods yield significant state abstraction while maintaining hierarchical optimality, i.e., optimality among all policies consistent with the partial program. We show how to achieve this for a partial programming language that is. essentially Lisp augmented with nondeterministic constructs. We demonstrate our methods on two variants of Dietterich's taxi domain, showing how state abstraction and hierarchical optimality result in faster learning of better policies and enable the transfer of learned skills from one problem to another.		Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Andrews2020	Tracking the State of Large Dynamic Networks via Reinforcement Learning	A Network Inventory Manager (NIM) is a software solution that scans, processes and records data about all devices in a network. We consider the problem faced by a NIM that can send out a limited number of probes to track changes in a large, dynamic network. The underlying change rate for the Network Elements (NEs) is unknown and may be highly non-uniform. The NIM should concentrate its probe budget on the NEs that change most frequently with the ultimate goal of minimizing the weighted Fraction of Stale Time (wFOST) of the inventory. However, the NIM cannot discover the change rate of a NE unless the NE is repeatedly probed.We develop and analyze two algorithms based on Reinforcement Learning to solve this exploration-vs-exploitation problem. The first is motivated by the Thompson Sampling method and the second is derived from the Robbins-Monro stochastic learning paradigm. We show that for a fixed probe budget, both of these algorithms produce a potentially unbounded improvement in terms of wFOST compared to the baseline algorithm that divides the probe budget equally between all NEs. Our simulations of practical scenarios show optimal performance in minimizing wFOST while discovering the change rate of the NEs.	https://dx.doi.org/10.1109/INFOCOM41043.2020.9155484	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Angles2021	Improving the Teaching-Learning Process in Engineering Through a Game-Based Web Support System: Edutrivias	"In this article, the authors propose the use of trivia games as a tool to support the teaching-learning process in engineering, and present results of the use of trivia games in the Chilean university context. Specifically, we describe a web system called Edutrivias (https://edutrivias.cl) which allows the interaction between teacher and engineering students through game-based learning. Edutrivias allows teachers to create trivia games that can be played by students, as many times as they want, within determined goals and a period of time defined by the teacher. Every time a student plays a trivia, he/she receives information regarding his/her level of progress. At the same time, the teacher can verify the participation of the students, as well as the level of the group and individual advancement. From a pedagogical point of view, teachers deliver knowledge through the trivia games, while the students ""play"" trivia games to acquire, increase and apply their knowledge through an intermittent reinforcement learning program. The main goals of this article are: (1) to present the foundations of the use of game-based learning in the competence development of engineering students, (2) to describe the main components and functionalities of Edutrivias, and (3) to present the results of an exploratory case study of Edutrivias with information of Chilean students of an engineering school at the University of Talca."	https://dx.doi.org/10.1007/978-3-030-68198-2_45	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Aoki2020	Cooperative Perception with Deep Reinforcement Learning for Connected Vehicles	Sensor-based perception on vehicles are becoming prevalent and important to enhance road safety. Autonomous driving systems use cameras, LiDAR and radar to detect surrounding objects, while human-driven vehicles use them to assist the driver. However, the environmental perception by individual vehicles has the limitations on coverage and/or detection accuracy. For example, a vehicle cannot detect objects occluded by other moving/static obstacles. In this paper, we present a cooperative perception scheme with deep reinforcement learning to enhance the detection accuracy for the surrounding objects. By using deep reinforcement learning to select the data to transmit, our scheme mitigates the network load in vehicular networks and enhances the communication reliability. To design, test and verify the practical and resource-efficient cooperative perception framework, we develop a Cooperative & Intelligent Vehicle Simulation (CIVS) Platform where we integrate three software components: a traffic simulator, a vehicle simulator, and an object classifier. The simulation platform constitutes a unified framework to evaluate a traffic model, vehicle model, communication model, and object classification model. Simulation results show that our scheme decreases packet loss and thereby increases the detection accuracy by up to 12\%, compared to the baseline protocol.	https://dx.doi.org/10.1109/IV47402.2020.9304570	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Araiza-Illan2016	Intelligent Agent-Based Stimulation for Testing Robotic Software in Human-Robot Interactions		https://doi.org/10.1145/3022099.3022101	Included	conflict_resolution		6
RL4SE	Araujo2020	URNAI: A Multi-Game Toolkit for Experimenting Deep Reinforcement Learning Algorithms	In the last decade, several game environments have been popularized as testbeds for experimenting reinforcement learning algorithms, an area of research that has shown great potential for artificial intelligence based solutions. These game environments range from the simplest ones like CartPole to the most complex ones such as StarCraft II. However, in order to experiment an algorithm in each of these environments, researchers need to prepare all the settings for each one, a task that is very time consuming since it entails integrating the game environment to their software and treating the game environment variables. So, this paper introduces URNAI, a new multi-game toolkit that enables researchers to easily experiment with deep reinforcement learning algorithms in several game environments. To do this, URNAI implements layers that integrate existing reinforcement learning libraries and existing game environments, simplifying the setup and management of several reinforcement learning components, such as algorithms, state spaces, action spaces, reward functions, and so on. Moreover, URNAI provides a framework prepared for GPU supercomputing, which allows much faster experiment cycles. The first toolkit results are very promising.	https://dx.doi.org/10.1109/SBGames51465.2020.00032	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	6
RL4SE	Araujo1998	Connectionist reinforcement learning of robot control skills	Many robot manipulator tasks are difficult to model explicitly and it is difficult to design and program automatic control algorithms for them. The development, improvement, and application of learning techniques taking advantage of sensory information would enable the acquisition of new robot skills and avoid some of the difficulties of explicit programming. In this paper we use a reinforcement learning approach far on-line generation of skills for control of robot manipulator systems. Instead of generating skills by explicit programming of a perception to action mapping they are generated by trial and error learning, guided by a performance evaluation feedback function. The resulting system may be seen as an anticipatory system that constructs an internal representation model of itself and of its environment. This enables it to identify its current situation and to generate corresponding appropriate commands to the system in order to perform the required skill. The method was applied to the problem of learning a force control skill in which the tool-tip of a robot manipulator must be moved from a free space situation, to a contact state with a compliant surface and having a constant interaction force.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Arend2022	MLPro - An integrative middleware framework for standardized machine learning tasks in Python	In recent years, many powerful software packages have been released on various aspects of machine learning (ML). However, there is still a lack of holistic development environments for the standardized creation of ML applications. The current practice is that researchers, developers, engineers and students have to piece together functionalities from several packages in their own applications. This prompted us to develop the integrative middleware framework MLPro that embeds flexible and recombinable ML models into standardized processes for training and real operations. In addition, it integrates numerous common open source frameworks and thus standardizes their use. A meticulously designed architecture combined with a powerful foundation of overarching basic functionalities ensures maximum recombinability and extensibility. In the first version of MLPro, we provide sub-frameworks for reinforcement learning (RL) and game theory (GT).	https://dx.doi.org/10.1016/j.simpa.2022.100421	Excluded	conflict_resolution	E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method,E3: Only conceptual results are reported,E2: Software engineering is not the problem RL is used for	6
RL4SE	Arend2022a	MLPro emdash An integrative middleware framework for standardized machine learning tasks in Python[Formula presented]		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138322283&doi=10.1016\%2fj.simpa.2022.100421&partnerID=40&md5=d1e8b02f34263a10e0f48c7b0576fa79	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E5: Other not a paper,E3: Only conceptual results are reported,E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	6
RL4SE	Arif2022	A smart reactive jamming approach to counter reinforcement learning-based antijamming strategies in GEO SATCOM scenario	Reinforcement learning (RL) is being considered for future SATCOM systems due to its inherent capability to self-learn the optimum decision-making policy under different scenarios. This capability enables SATCOM systems to manage their resources judiciously and mitigate jamming attacks autonomously without prior jammer type classification. We propose a novel smart reactive SATCOM jamming approach that would not only counter these RL based anti-jamming strategies but would also be effective against conventional anti-jamming schemes, that is, FHSS and DSSS. The proposed jamming approach exploits the limitations in learning patterns of Q-learning-based RL agent and achieves effective jamming while conserving considerable amount of jamming power. To achieve this, we propose an intelligent jamming engine (IJE) along with few potent jamming algorithms and evaluate their performance in terms of throughput degradation of victim SATCOM link, jamming power conservation, and design complexity of the jammer. Software simulations successfully demonstrate the effectiveness of our proposed smart reactive jamming approach which outperforms the standard reactive jammer against RL-based antijamming schemes.	https://dx.doi.org/10.1002/sat.1418	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Arinze2020	Novel Scheme For Congestion Control In Cellular Networks Using Deep Reinforcement Learning And Markov Decision Process Models	This research deals with the general issue of quality of service (QoS) provisioning and resource utilization in telecommunication networks. The issue requires that mobile network income be optimized while simultaneously satisfying QoS constraints that prevent getting into specific states and utilization of specific actions. However, supporting QoS requirements of different traffic types is more complicated due to the need to minimize two performance indicators - the probability of discarding a handover call and the probability of hindering a new call. Several approaches proposed recently try to provide efficient model-based solution to the problem by formulating it as an average reward neuro-dynamic programming (NDP) optimization problem together with decomposition function, but this is limited by Bellman's curse of dimensionality. In this paper, we proposed a novel hybrid optimization scheme to address the problem using Deep Reinforcement Learning (DRL), Markov Decision Process (MDP) and adaptive joint call admission control (AJCAC) respectively. In the proposed scheme, two classes of arrival traffic at the base station (BS) are considered; voice (real-time) and data (non-real-time) calls. Furthermore, traffic is classified as new and handoff according to the type of request. The scheme introduces an adaptive threshold value, which dynamically adjusts the network resources under high traffic intensity. In addition, the scheme introduces a learning agent whose state is described by an MDP. The MATLAB version 2010 software, OMNET++ simulator and SPSS will be used for data, numerical, algorithm simulation and modeling. Data analysis and simulation results will be carried out for performance evaluation of the proposed DQL-AJCAC scheme against existing models.	https://dx.doi.org/10.1109/ICMCECS47690.2020.247001	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Armstrong2006	Dynamic Algorithm Selection Using Reinforcement Learning	It is often the case that many algorithms exist to solve a single problem, each possessing different performance characteristics. The usual approach in this situation is to manually select the algorithm which has the best average performance. However, this strategy has drawbacks in cases where the optimal algorithm changes during an invocation of the program, in response to changes in the program's state and the computational environment. This paper presents a prototype tool that uses reinforcement learning to guide algorithm selection at runtime, matching the algorithm used to the current state of the computation. The tool is applied to a simulation similar to those used in some computational chemistry problems. It is shown that the low dimensionality of the problem enables the optimal choice of algorithm to be determined quickly, and that the learning system can react rapidly to phase changes in the target program	https://dx.doi.org/10.1109/AIDM.2006.4	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Armstrong2008	Reinforcement learning for automated performance tuning: Initial evaluation for sparse matrix format selection	The field of reinforcement learning has developed techniques for choosing beneficial actions within a dynamic environment. Such techniques learn from experience and do not require teaching. This paper explores how reinforcement learning techniques might be used to determine efficient storage formats for sparse matrices. Three different storage formats are considered: coordinate, compressed sparse row, and blocked compressed sparse row. Which format performs best depends heavily on the nature of the matrix and the computer system being used. To test the above a program has been written to generate a series of sparse matrices, where any given matrix performs optimally using one of the three different storage types. For each matrix several sparse matrix vector products are performed. The goal of the learning agent is to predict the optimal sparse matrix storage format for that matrix. The proposed agent uses five attributes of the sparse matrix: the number of rows, the number of columns, the number of non-zero elements, the standard deviation of non-zeroes per row and the mean number of neighbours. The agent is characterized by two parameters: an exploration rate and a parameter that determines how the state space is partitioned. The ability of the agent to successfully predict the optimal storage format is analyzed for a series of 1,000 automatically generated test matrices.	https://dx.doi.org/10.1109/CLUSTR.2008.4663802	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Arslan2022	Actor-critic reinforcement learning for bidding in bilateral negotiation		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139306597&doi=10.55730\%2f1300-0632.3899&partnerID=40&md5=b0cc57d23699d598e930ebe41693636b	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Arza2022	Comparing Two Samples Through Stochastic Dominance: A Graphical Approach	Nondeterministic measurements are common in real-world scenarios: the performance of a stochastic optimization algorithm or the total reward of a reinforcement learning agent in a chaotic environment are just two examples in which unpredictable outcomes are common. These measures can be modeled as random variables and compared among each other via their expected values or more sophisticated tools such as null hypothesis statistical tests. In this article, we propose an alternative framework to visually compare two samples according to their estimated cumulative distribution functions. First, we introduce a dominance measure for two random variables that quantifies the proportion in which the cumulative distribution function of one of the random variables scholastically dominates the other one. Then, we present a graphical method that decomposes in quantiles (i) the proposed dominance measure and (ii) the probability that one of the random variables takes lower values than the other. With illustrative purposes, we reevaluate the experimentation of an already published work with the proposed methodology and we show that additional conclusions-missed by the rest of the methods-can be inferred. Additionally, the software package RVCompare was created as a convenient way of applying and experimenting with the proposed framework.	https://dx.doi.org/10.1080/10618600.2022.2084405	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	6
RL4SE	Arzymatov2020	SANgo: a storage infrastructure simulator with reinforcement learning support	We introduce SANgo (Storage Area Network in the Go language)-a Go-based package for simulating the behavior of modern storage infrastructure. The software is based on the discrete-event modeling paradigm and captures the structure and dynamics of high-level storage system building blocks. The flexible structure of the package allows us to create a model of a real storage system with a configurable number of components. The granularity of the simulated system can be defined depending on the replicated patterns of actual system behavior. Accurate replication enables us to reach the primary goal of our simulator-to explore the stability boundaries of real storage systems. To meet this goal, SANgo offers a variety of interfaces for easy monitoring and tuning of the simulated model. These interfaces allow us to track the number of metrics of such components as storage controllers, network connections, and harddrives. Other interfaces allow altering the parameter values of the simulated system effectively in real-time, thus providing the possibility for training a realistic digital twin using, for example, the reinforcement learning (RL) approach. One can train an RL model to reduce discrepancies between simulated and real SAN data. The external control algorithm can adjust the simulator parameters to make the difference as small as possible. SANgo supports the standard OpenAI gym interface; thus, the software can serve as a benchmark for comparison of different learning algorithms.	https://www.ncbi.nlm.nih.gov/pubmed/33816922	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Asadpour2004	Compact Q-learning optimized for micro-robots with processing and memory constraints	Scaling down robots to miniature size introduces many new challenges including memory and program size limitations, low processor performance and low power autonomy. In this paper we describe the concept and implementation of learning of a safe-wandering task with the autonomous micro-robots, Alice. We propose a simplified reinforcement learning algorithm based on one-step Q-learning that is optimized in speed and memory consumption. This algorithm uses only integer-based sum operators and avoids floating-point and multiplication operators. Finally, quality of learning is compared to a floating-point based algorithm. (C) 2004 Elsevier B.V. All rights reserved.	https://dx.doi.org/10.1016/j.robot.2004.05.006	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Asteroth1992	TRACKING AND GRASPING OF MOVING-OBJECTS - A BEHAVIOR-BASED APPROACH	Behaviour-based robotics (cf. Brooks [2]) has mainly been applied to the domain of autonomous systems and mobile robots. In this paper we show how this approach to robot programming can be used to design a flexible and robust controller for a five degrees of freedom (DOF) robot arm. The implementation of the robot controller to be presented features the sensor and motor patterns necessary to tackle a problem we consider to be hard to solve for traditional controllers. These sensor and motor patterns are linked together forming various behaviours. The global control structure based on Brooks' subsumption architecture will be outlined. It coordinates the individual behaviours into goal-directed behaviour of the robot without the necessity to program this emerging global behaviour explicitly and in advance. To conclude, some shortcomings of the current implementation are discussed and future work, especially in the field of reinforcement learning of individual behaviours, is sketched.		Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Asteroth1992a	Tracking and grasping of moving objects endash a behaviour-based approach endash		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029539075&doi=10.1007\%2fbfb0024971&partnerID=40&md5=386c16b30eb7eb419da2415c58ce8653	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E5: Other not a paper,E2: Software engineering is not the problem RL is used for	6
RL4SE	Auer2011	Invited talk: UCRL and autonomous exploration		https://doi.org/10.1007/978-3-642-29946-9_1	Excluded	new_screen	E5: Other not a paper,E5: Other not a paper,E5: Other not a paper,E5: Other not a paper,E5: Other not a paper,E5: Other not a paper	6
RL4SE	Aumjaud2021	rl_reach: Reproducible reinforcement learning experiments for robotic tasks	Training reinforcement learning agents at solving a given task is highly dependent on identifying optimal sets of hyperparameters and selecting suitable environment input/output configurations. This tedious process could be eased with a straightforward toolbox allowing its user to quickly compare different training parameter sets. We present rl_reach, a self-contained, open-source and easy-to-use software package designed to run reproducible reinforcement learning experiments for customisable robotic reaching tasks. rl_reach packs together training environments, agents, hyperparameter optimisation tools and policy evaluation scripts, allowing its users to quickly investigate and identify optimal training configurations. rl_reach is publicly available at this URL: https://github.com/PierreExeter/rl_reach.	https://dx.doi.org/10.1016/j.simpa.2021.100061	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Aumjaud2021a	rl_reach: Reproducible reinforcement learning experiments for robotic reaching tasks[Formula presented]		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115865247&doi=10.1016\%2fj.simpa.2021.100061&partnerID=40&md5=32582555c93ccc79f4052767a1241dc0	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E5: Other not a paper,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Aung2010	Reinforcement learning using voronoi space division			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Aunger2016	Behaviour Centred Design: towards an applied science of behaviour change	Behaviour change has become a hot topic. We describe a new approach, Behaviour Centred Design (BCD), which encompasses a theory of change, a suite of behavioural determinants and a programme design process. The theory of change is generic, assuming that successful interventions must create a cascade of effects via environments, through brains, to behaviour and hence to the desired impact, such as improved health. Changes in behaviour are viewed as the consequence of a reinforcement learning process involving the targeting of evolved motives and changes to behaviour settings, and are produced by three types of behavioural control mechanism (automatic, motivated and executive). The implications are that interventions must create surprise, revalue behaviour and disrupt performance in target behaviour settings. We then describe a sequence of five steps required to design an intervention to change specific behaviours: Assess, Build, Create, Deliver and Evaluate. The BCD approach has been shown to change hygiene, nutrition and exercise-related behaviours and has the advantages of being applicable to product, service or institutional design, as well as being able to incorporate future developments in behaviour science. We therefore argue that BCD can become the foundation for an applied science of behaviour change.	https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982279417&doi=10.1080\%2f17437199.2016.1219673&partnerID=40&md5=1675458abbfb3683d537a51f5f46c0d7	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Ayimba2021	SQLR: Short-Term Memory Q-Learning for Elastic Provisioning	As a growing number of service and application providers choose cloud networks to deliver their services on a software-as-a-service (SaaS) basis, cloud providers need to make their provisioning systems agile enough to meet service level agreements (SLAs). At the same time, they should guard against over-provisioning, which limits their capacity to accommodate more tenants. To this end, we propose Shortterm memory Q-Learning pRovisioning (SQLR, pronounced as ``scaler''), a system employing a customized variant of the modelfree reinforcement learning algorithm. It can reuse contextual knowledge learned from one workload to optimize the number of virtual machines (resources) allocated to serve other workload patterns. With minimal overhead, SQLR achieves comparable results to systems where resources are unconstrained. Our experiments show that we can reduce the amount of provisioned resources by about 20\% with less than 1\% overall service unavailability (due to blocking), while delivering similar response times to those of an over-provisioned system.	https://dx.doi.org/10.1109/TNSM.2021.3075619	Included	conflict_resolution		6
RL4SE	Baccour2020	RL-OPRA: Reinforcement Learning for Online and Proactive Resource Allocation of crowdsourced live videos	With the advancement of rich media generating devices, the proliferation of live Content Providers (CP), and the availability of convenient internet access, crowdsourced live streaming services have witnessed unexpected growth. To ensure a better Quality of Experience (QoE), higher availability, and lower costs, large live streaming CPs are migrating their services to geo-distributed cloud infrastructure. However, because of the dynamics of live broadcasting and the wide geo-distribution of viewers and broadcasters, it is still challenging to satisfy all requests with reasonable resources. To overcome this challenge, we introduce in this paper a prediction driven approach that estimates the potential number of viewers near different cloud sites at the instant of broadcasting. This online and instant prediction of distributed popularity distinguishes our work from previous efforts that provision constant resources or alter their allocation as the popularity of the content changes. Based on the derived predictions, we formulate an Integer-Linear Program (ILP) to proactively and dynamically choose the right data center to allocate exact resources and serve potential viewers, while minimizing the perceived delays. As the optimization is not adequate for online serving, we propose a real-time approach based on Reinforcement Learning (RL), namely RL-OPRA, which adaptively learns to optimize the allocation and serving decisions by interacting with the network environment. Extensive simulation and comparison with the ILP have shown that our RL-based approach is able to present optimal results compared to heuristic-based approaches. (c) 2020 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).	https://dx.doi.org/10.1016/j.future.2020.06.038	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Backeus2005	Impact of climate change uncertainty on optimal forest management policies at stand level			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Badica2017	Integration of Jason Reinforcement Learning Agents into an Interactive Application	The context of this paper is research on the application of agent-oriented programming to experiment with reinforcement learning algorithms. Traditionally, reinforcement learning fits well within the theory of agent systems. Nevertheless, most experimental approaches employ standard software engineering tools, rather than specific agent-oriented technologies developed within the agent community. The goal of our work is to stimulate synergies and development of agent-oriented programming technologies to better fit the context of agent systems research. In particular in this paper we focus on the development of an experimental interactive application that incorporates reinforcement learning agents created using the Jason agent-oriented programming language.	https://dx.doi.org/10.1109/SYNASC.2017.00065	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Baek2019	Managing Fog Networks using Reinforcement Learning Based Load Balancing Algorithm	The powerful paradigm of Fog computing is currently receiving major interest, as it provides the possibility to integrate virtualized servers into networks and brings cloud service closer to end devices. To support this distributed intelligent platform, Software-Defined Network (SDN) has emerged as a viable network technology in the Fog computing environment. However, uncertainties related to task demands and the different computing capacities of Fog nodes, inquire an effective load balancing algorithm. In this paper, the load balancing problem has been addressed under the constraint of achieving the minimum latency in Fog networks. To handle this problem, a reinforcement learning based decision-making process has been proposed to find the optimal offloading decision with unknown reward and transition functions. The proposed process allows Fog nodes to offload an optimal number of tasks among incoming tasks by selecting an available neighboring Fog node under their respective resource capabilities with the aim to minimize the processing time and the overall overloading probability. Compared with the traditional approaches, the proposed scheme not only simplifies the algorithmic framework without imposing any specific assumption on the network model but also guarantees convergence in polynomial time. The results show that, during average delays, the proposed reinforcement learning-based offloading method achieves significant performance improvements over the variation of service rate and traffic arrival rate. The proposed algorithm achieves 1.17\%, 1.02\%, and 3.21\% lower overload probability relative to random, least-queue and nearest offloading selection schemes, respectively.	https://dx.doi.org/10.1109/WCNC.2019.8885745	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Baez2013	Application of an educational strategy based on a soccer robotic platform	In this paper we describe the design and implementation of an educational methodology based on a robotic platform used for the small size league (SSL) challenge of the RoboCup initiative. The methodology is based on three main aspects of the learning process, namely classical conditioning, reinforcement learning and cognitive learning. This is achieved through the combination of robotic concepts applied to the soccer problem; a highly interesting topic to the students, together with skill oriented modules. We show practical results achieved after applying this methodology in specific courses in an undergraduate electrical engineering program. Our initial results demonstrate that it is possible to attain significantly better results in terms of learnt concepts and motivation when using our robotic soccer based strategy.	https://dx.doi.org/10.1109/ICAR.2013.6766533	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Bag2019	A combined reinforcement learning and sliding mode control scheme for grid integration of a PV system	The paper presents development of a reinforcement learning (RL) and sliding mode control (SMC) algorithm for a 3-phase PV system integrated to a grid. The PV system is integrated to grid through a voltage source inverter (VSI), in which PV-VSI combination supplies active power and compensates reactive power of the local non-linear load connected to the point of common coupling (PCC). For extraction of maximum power from the PV panel, we develop a RL based maximum power point tracking (MPPT) algorithm. The instantaneous power theory (IPT) is adopted for generation reference inverter current (RIC). An SMC algorithm has been developed for injecting current to the local non-linear load at a reference value. The RL-SMC scheme is implemented in both simulation using MATLAB/SIMULINK software and on a prototype PV experimental. The performance of the proposed RL-SMC scheme is compared with that of fuzzy logic-sliding mode control (FL-SMC) and incremental conductance-sliding mode control (IC-SMC) algorithms. From the obtained results, it is observed that the proposed RL-SMC scheme provides better maximum power extraction and active power control than the FL-SMC and IC-SMC schemes.	https://dx.doi.org/10.17775/CSEEJPES.2017.01000	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Bahamid2022	A review on crowd analysis of evacuation and abnormality detection based on machine learning systems	Human crowds have become hotspot research, particularly in crowd analysis to ensure human safety. Adaptations of machine learning (ML) approaches, especially deep learning, play a vital role in the applications of evacuation, detection, and prediction pertaining to crowd analysis. Further development in the analysis of crowd is needed to understand human behaviors due to the fast growth of crowd in urban megacities. This article presents a comprehensive review of crowd analysis ML-based systems, where it is categorized with respect to its purposes, viz. crowd evacuation that provides efficient evacuation routes, abnormality detection that could detect the occurrence of any irregular movement or behavior, and crowd prediction that could foresee the occurrence of any possible disasters or predict pedestrian trajectory. Moreover, this article reviews the applied techniques of machine learning with a brief discussion on the used software and simulation platforms. This work also classifies crowd evacuation into data-driven methods and goal-driven learning methods that have attracted significant attention due to their potential to adopt virtual agents with learning capabilities. This review finds that convolutional neural networks and recurrent neural networks have shown superiority in abnormality detection and prediction, whereas deep reinforcement learning has shown potential performance in the development of human level capacities of reasoning. These three methods contribute to the modeling and understanding of pedestrian behavior and will enhance further development in crowd analysis to ensure human safety.	https://dx.doi.org/10.1007/s00521-022-07758-5	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Bahr2019	Development and Validation of Active Roll Control based on Actor-critic Neural Network Reinforcement Learning		https://doi.org/10.5220/0007787400360046	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Bahrami2021	Deep Reinforcement Learning for Demand Response in Distribution Networks		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096865050&doi=10.1109\%2fTSG.2020.3037066&partnerID=40&md5=c248d562004386f75d41fa72995c8f3d	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Bai2022	Lamarckian Platform: Pushing the Boundaries of Evolutionary Reinforcement Learning Towards Asynchronous Commercial Games	Despite the emerging progress of integrating evolutionary computation into reinforcement learning, the absence of a high-performance platform endowing composability and massive parallelism causes non-trivial difficulties for research and applications related to asynchronous commercial games. Here we introduce Lamarckian11The code and demonstrational setup of Lamarckian are publicly available at https://github. com/lamarckian/lamarckian. \endash an open-source platform featuring support for evolutionary reinforcement learning scalable to distributed computing resources. To improve the training speed and data efficiency, Lamarckian adopts optimized communication methods and an asynchronous evolutionary reinforcement learning workflow. To meet the demand for an asynchronous interface by commercial games and various methods, Lamarckian tailors an asynchronous Markov Decision Process interface and designs an object-oriented software architecture with decoupled modules. In comparison with the state-of-the-art RLlib, we empirically demonstrate the unique advantages of Lamarckian on benchmark tests with up to 6000 CPU cores: i) both the sampling efficiency and training speed are doubled when running PPO on Google football game; ii) the training speed is 13 times faster when running PBT+PPO on Pong game. Moreover, we also present two use cases: i) how Lamarckian is applied to generating behavior-diverse game AI; ii) how Lamarckian is applied to game balancing tests for an asynchronous commercial game.	https://dx.doi.org/10.1109/TG.2022.3208324	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Bai2006	A Mobile Agents-Based Real-time Mechanism for Wireless Sensor Network Access on the Internet	Due to the variety of applications and their importance, wireless sensor networks (WSNs) would need to be connected to the Internet. Some approaches have been proposed to connect wireless sensor networks to the existing TCP/IP networks, such as the application-level gateways or overlay networks. However, most existing approaches have to consume network bandwidth and node energy to maintain the static network structure which is neither scalable nor reliable. In this paper, we describe the mobile agent based real-time (MBR) mechanism which use of the mobile software agent (MSA) paradigm to design a dynamic infrastructure for WSNs access on the Internet. We present a agent migration protocol based on reinforcement learning method to reduce the query delay and improve the total performance	https://dx.doi.org/10.1109/ICIA.2006.306016	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Baker2011	Individual differences in substance dependence: At the intersection of brain, behaviour and cognition	Recent theories of drug dependence propose that the transition from occasional recreational substance use to harmful use and dependence results from the impact of disrupted midbrain dopamine signals for reinforcement learning on frontal brain areas that implement cognitive control and decision-making. We investigated this hypothesis in humans using electrophysiological and behavioral measures believed to assay the integrity of midbrain dopamine system and its neural targets. Our investigation revealed two groups of dependent individuals, one characterized by disrupted dopamine-dependent reward learning and the other by disrupted error learning associated with depression-proneness. These results highlight important neurobiological and behavioral differences between two classes of dependent users that can inform the development of individually tailored treatment programs.	https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959823204&doi=10.1111\%2fj.1369-1600.2010.00243.x&partnerID=40&md5=e52c842a99576e3ddaf9e327c0554916	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Bakshi2018	Considerations for artificial intelligence and machine learning: Approaches and use cases	As data sets grow, leveraging machines to learn valuable patterns from structured data can be extremely powerful. The volume of data is too large for comprehensive analysis, and the range of potential correlations and relationships between disparate data sources are too great for any analyst to test all hypotheses and derive all the value buried in the data. Machine learning (ML) is ideal for exploiting the opportunities hidden in big data. Machine learning is a type of artificial intelligence (AI) that allows software applications to become more accurate in predicting outcomes without being explicitly programmed. The basics of machine learning is to build algorithms that can take input data and use statistical analysis to predict an output value within an acceptable range. This paper explores the basics of machine learning, discussing concepts and topics like supervised, unsupervised and reinforcement learning, regression, classification, model evaluation metrics, overfitting, variance versus bias, linear regression, ensemble methods, model selection, Decision Trees, Random Forests. The paper then will review several several use cases, where machine learning can be applied, including but not limited to Aerospace, Internet of Things (IoT) and Computer Network Analytics use cases. The applicability of AI and ML will be reviewed in these use cases. Finally, the latest trends in machine learning will be discussed.	https://dx.doi.org/10.1109/AERO.2018.8396488	Excluded	new_screen	E3: Only conceptual results are reported,E3: Only conceptual results are reported,E2: Software engineering is not the problem RL is used for,E3: Only conceptual results are reported,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Balaji2010	Urban traffic signal control using reinforcement learning agents	This study presents a distributed multi-agent-based traffic signal control for optimising green timing in an urban arterial road network to reduce the total travel time and delay experienced by vehicles. The proposed multi-agent architecture uses traffic data collected by sensors at each intersection, stored historical traffic patterns and data communicated from agents in adjacent intersections to compute green time for a phase. The parameters like weights, threshold values used in computing the green time is fine tuned by online reinforcement learning with an objective to reduce overall delay. PARAMICS software was used as a platform to simulate 29 signalised intersection at Central Business District of Singapore and test the performance of proposed multi-agent traffic signal control for different traffic scenarios. The proposed multi-agent reinforcement learning (RLA) signal control showed significant improvement in mean time delay and speed in comparison to other traffic control system like hierarchical multi-agent system (HMS), cooperative ensemble (CE) and actuated control.	https://dx.doi.org/10.1049/iet-its.2009.0096	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Balakiruthiga2021	(ITMP) endash Intelligent Traffic Management Prototype using Reinforcement Learning approach for Software Defined Data Center (SDDC)		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116381356&doi=10.1016\%2fj.suscom.2021.100610&partnerID=40&md5=5f4775ffcdd88f000db68647c34bf778	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Balakiruthiga2021a	(ITMP)-Intelligent Traffic Management Prototype using Reinforcement Learning approach for Software Defined Data Center (SDDC)	Software defined network architecture offers scalability and resilience as the significant advantages to data center networks. This increases the fault tolerance ability of traditional data center network architectures. Massive amounts of mobile network data as well as e-commerce application data requests are the key sources for data centers which recurrently desire attention. Researchers are yet to design a suitable prototype with functional intelligence to support traffic optimization techniques in SDDC. In this research work, we are proposing an intelligent traffic management prototype for software defined data center by means of reinforcement learning approach through the integration of the functionalities such as controller positioning, traffic load balancing, routing and energy efficiency. These are the key areas where traffic optimization becomes essential to improve network performance. The proposed prototype provides a complete framework for enterprises to deploy applications in an efficient manner. We model the prototype to handle dynamic network data applications such as information retrieval, communication and banking applications. We focus in this article on how communication happens among the data center nodes as an inter-data center communication process upon receiving requests from the applications considered. To further enhance the novelty and efficiency of our research work, we adopt multiple reinforcement learning agents to lever load balancing and routing functionalities. Moreover, to assess and ensure the optimized network performance, we evaluate the energy consumption of the network achieved through our proposed prototype.	https://dx.doi.org/10.1016/j.suscom.2021.100610	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Balakrishnan2021	Transfer Reinforcement Learning for Autonomous Driving: From WiseMove to WiseSim		https://doi.org/10.1145/3449356	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Banarse2019	The Body is Not a Given: Joint Agent Policy Learning and Morphology Evolution			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Baranwal2021	ReLAccS: A Multilevel Approach to Accelerator Design for Reinforcement Learning on FPGA-Based Systems	Reinforcement learning (RL), specifically Q-learning, with human-like learning abilities to learn from experience without any a priori data, is being increasingly used in embedded systems in the field of control and navigation. However, finding the optimal policy in this approach can be highly compute-intensive, and a software-only implementation may not satisfy the application's timing constraints. To this end, we propose optimization methods at multiple levels of accelerator design for RL. Specifically, at the architecture-level, we exploit the instruction-level parallelism and the spatial parallelism in FPGAs to improve the throughput over state-of-the-art designs by up to 34\%. Further, we propose lookup table-level optimizations to reduce the resource utilization and power dissipation of the accelerator. Finally, we propose algorithm-level approximation that can be used for acceleration of Q-learning problems with more states and for reducing the peak power dissipation. We report up to 10$\times$ reduction in power dissipation with marginal degradation in quality of results.	https://dx.doi.org/10.1109/TCAD.2020.3028350	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Barash2019	Reports of the Workshops Held at the 2019 AAAI Conference on Artificial Intelligence	The workshop program of the Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19) was held in Honolulu, Hawaii, on Sunday and Monday, January 27 and 28, 2019. There were 16 workshops in the program: Affective Content Analysis: Modeling Affect-in-Action; Agile Robotics for Industrial Automation Competition; Artificial Intelligence for Cyber Security; Artificial Intelligence Safety; Dialog System Technology Challenge; Engineering Dependable and Secure Machine Learning Systems; Games and Simulations for Artificial Intelligence; Health Intelligence; Knowledge Extraction from Games; Network Interpretability for Deep Learning; Plan, Activity, and Intent Recognition; Reasoning and Learning for Human-Machine Dialogues; Reasoning for Complex Question Answering; Recommender Systems Meet Natural Language Processing; Reinforcement Learning in Games; and Reproducible AI. This report contains brief summaries of all the workshops that were held.	https://dx.doi.org/10.1609/aimag.v40i3.4981	Excluded	new_screen	E5: Other not a paper,E5: Other not a paper,E5: Other not a paper,E5: Other not a paper,E5: Other not a paper,E5: Other not a paper	6
RL4SE	Barbieri2018	DEVS Modeling and Simulation of Financial Leverage Effect Based on Markov Decision Process	Decision making during a financial asset optimization process leading to a potential leverage effect is a major issue in the management of an investment program such as European development programs. Modeling and simulation based on reinforcement learning can propose a decision-making policy in this kind of process. This paper presents a DEVS discrete-event modeling and simulation approach from Markov decision-making processes applied to the search for maximum leverage on self-financing capabilities in grant application instruction phase. The application of the approach presented in this paper is made on the search for the leverage effect linked to the price volatility of the main stock market indices (CAC40, NasDaq, etc.).	https://dx.doi.org/10.1109/UV.2018.8642121	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Barbucha2012	Search modes for the cooperative multi-agent system solving the vehicle routing problem		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860242268&doi=10.1016\%2fj.neucom.2011.07.032&partnerID=40&md5=2aae3d810ed1539355ec489c563e1b44	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Bargiacchi2020	AI-Toolbox: A C plus plus library for Reinforcement Learning and Planning (with Python Bindings)	This paper describes AI-Toolbox, a C++ software library that contains reinforcement learning and planning algorithms, and supports both single and multi agent problems, as well as partial observability. It is designed for simplicity and clarity, and contains extensive documentation of its API and code. It supports Python to enable users not comfortable with C++ to take advantage of the library's speed and functionality.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Bargiacchi2022	AI-Toolbox: a C++ library for reinforcement learning and planning (with python bindings)			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Barrett2010	A comparison of learning approaches to support the adaptive provision of distributed services			Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E5: Other not a paper,E5: Other not a paper,E2: Software engineering is not the problem RL is used for	6
RL4SE	Barriga2021	Addressing the trade off between smells and quality when refactoring class diagrams	Models are core artifacts of modern software engineering processes, and they are subject to evolution throughout their life cycle due to maintenance and to comply with new requirements as any other software artifact. Smells in modeling are indicators that something may be wrong within the model design. Removing the smells using refactoring usually has a positive effect on the general quality of the model. However, it could have a negative impact in some cases since it could destroy the quality wanted by stakeholders. PARMOREL is a framework that, using reinforcement learning, can automatically refactor models to comply with user preferences. The work presented in this paper extends PARMOREL to support smells detection and selective refactoring based on quality characteristics to assure only the refactoring with a positive impact is applied. We evaluated the approach on a large available public dataset to show that PARMOREL can decide which smells should be refactored to maintain and, even improve, the quality characteristics selected by the user.	https://dx.doi.org/10.5381/jot.2021.20.3.a1	Included	new_screen		6
RL4SE	Barriga2022	PARMOREL: a framework for customizable model repair		https://doi.org/10.1007/s10270-022-01005-0	Included	new_screen		6
RL4SE	Barriga2020	A comparative study of reinforcement learning techniques to repair models		https://doi.org/10.1145/3417990.3421395	Included	new_screen		6
RL4SE	Barriga2018	Automatic model repair using reinforcement learning			Included	new_screen		4
RL4SE	Barriga2019	Personalized and Automatic Model Repairing using Reinforcement Learning	When performing modeling activities, the chances of breaking a model increase together with the size of development teams and number of changes in software specifications. Model repair research mostly proposes two different solutions to this issue: fully automatic, non-interactive model repairing tools or support systems where the repairing choice is left to the developer's criteria. In this paper, we propose the use of reinforcement learning algorithms to achieve the repair of broken models allowing both automation and personalization. We validate our proposal by repairing a large set of broken models randomly generated with a mutation tool.	https://dx.doi.org/10.1109/MODELS-C.2019.00030	Included	new_screen		4
RL4SE	Barriga2019a	Towards quality assurance in repaired models with PARMOREL			Included	new_screen		4
RL4SE	Barriga2020_1	Improving model repair through experience sharing	In model-driven software engineering, models are used in all phases of the development process. These models may get broken due to various editions throughout their life-cycle. There are already approaches that provide an automatic repair of models, however, the same issues might not have the same solutions in all contexts due to different user preferences and business policies. Personalization would enhance the usability of automatic repairs in different contexts, and by reusing the experience from previous repairs we would avoid duplicated calculations when facing similar issues. By using reinforcement learning we have achieved the repair of broken models allowing both automation and personalization of results. In this paper, we propose transfer learning to reuse the experience learned from each model repair. We have validated our approach by repairing models using different sets of personalization preferences and studying how the repair time improved when reusing the experience from each repair.	https://dx.doi.org/10.5381/jot.2020.19.2.a13	Included	new_screen		4
RL4SE	Bartin2019	Use of learning classifier systems in microscopic toll plaza simulation models	This study presents the application of XCS learning algorithm in simulating drivers' lane selection behaviour in microscopic simulation models of toll plazas. XCS is a special case of learning classifier systems, a machine learning technique that ties reinforcement learning (RL) and genetic algorithm. The proposed formulation translates an agent's lane selection decision into a learning problem, assuming that its ultimate objective is to reduce delay and crash risk. The agent is simulated without any notion of the outcome of its decisions. Through multiple learning episodes and the outcome of its actions, it learns the best policy to implement in a given network setting. A hypothetical toll plaza simulation network developed in Paramics simulation software is used to conduct experimental analyses. The results demonstrate that the agent's toll lane selection decision yields superior results in terms of delay and crash risk compared with those of minimum queue lane selection, minimum risk lane selection, random lane selection and multinomial probit model-based lane selection behaviours. Although both the delay and crash risk objectives are not used simultaneously during learning, this study is intended as a proof of concept to demonstrate the feasibility of implementing RL algorithms in microscopic traffic simulation models.	https://dx.doi.org/10.1049/iet-its.2018.5121	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Barzilay2012	Learning to behave by reading			Excluded	new_screen	E5: Other not a paper,E5: Other not a paper	4
RL4SE	Batista2019	Utterance Copy in Formant-Based Speech Synthesizers Using LSTM Neural Networks	Utterance copy, also known as speech imitation, is the task of estimating the parameters of an input, target speech signal in order to artificially reconstruct another signal with the same properties at the output. This can be considered a difficult inverse problem, since the input-output relationship is often non-linear, apart from having several parameters to be estimated and adjusted. This work describes the development of an application that uses a long short-term memory neural network (LSTM) to learn how to estimate the input parameters of thel formant-based Klatt speech synthesizer. Formant-based synthesizers do not reach state-of-art performance for text-to-speech (TTS) applications, but are an important tool for linguists studies due to the high interpretability of its input parameters. The proposed system was compared to the WinSnoori baseline software on both artificially-produced target utterances, generated by the DECtalk TTS system; and natural target ones. Results show that our system outperforms the baseline for synthetic voices on the metrics of PESQ, SNR, RMSE and LSD. For natural voices, the experiments indicate the need for an architecture that does not depend on labeled data, such as reinforcement learning.	https://dx.doi.org/10.1109/BRACIS.2019.00025	Excluded	new_screen	E1: Does not define or use a RL method,E1: Does not define or use a RL method	4
RL4SE	Bauer2011	Adaptation-based programming in Haskell		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954472193&doi=10.4204\%2fEPTCS.66.1&partnerID=40&md5=4fd3ea18bfd14ecb74c5eae8a33a23f6	Excluded	new_screen	E3: Only conceptual results are reported,E2: Software engineering is not the problem RL is used for	4
RL4SE	Baum2002	Toward code evolution by artificial economies	We have begun exploring code evolution by artificial economies. We implemented a reinforcement learning machine called Hayek2 consisting of agents, written in a machine language inspired by Ray's Tierra, that interact economically. The economic structure of Hayek2 addresses credit assignment at both the agent and meta levels. Hayek2 succeeds in evolving code to solve Blocks World problems, and has been more effective at this than our hillclimbing program and our genetic program (GP). Our hillclimber and our GP also performed well, learning algorithms as strong as a simple search program that incorporates hand-coded domain knowledge. We made efforts to optimize our hillclimbing program and it has features that may be of independent interest. Our GP using crossover performed far better than a version utilizing other macro-mutations or our hillclimber, bearing on a controversy in the genetic programming literature.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Baumgart2021	A Reinforcement Learning Approach for Traffic Control	Intelligent traffic control is a key tool to achieve and to realize resource-efficient and sustainable mobility solutions. In this contribution, we study a promising data-based control approach, reinforcement learning (RL), and its applicability to traffic flow problems in a virtual environment. We model different traffic networks using the microscopic traffic simulation software SUMO. RL-methods are used to teach controllers, so called RL agents, to guide certain vehicles or to control a traffic light system. The agents obtain real-time information from other vehicles and learn to improve the traffic flow by repetitive observation and algorithmic optimization. As controller models, we consider both simple linear models and non-linear radial basis function networks. The latter allow to include prior knowledge from the training data and a two-step training procedure leading to an efficient controller training.	https://dx.doi.org/10.5220/0010448501330141	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Beck2009	Reinforcement learning for Golog programs			Included	new_screen		4
RL4SE	Beck2012	Reinforcement learning for Golog programs with first-order state-abstraction	A special feature of programs in the action language Golog are non-deterministic constructs such as non-deterministic choice of actions or arguments. It has been shown that in the presence of stochastic actions and rewards reinforcement learning techniques can be applied to obtain optimal choices for those choice-points. In order to avoid an explosion of the state space, an abstraction mechanism is employed that computes first-order state descriptions for the given program. Intuitively, the idea is to generate abstract descriptions that group together states for which the expected reward of executing the program is the same. A current limitation is that a non-deterministic choice of arguments can be handled only if the possible candidates are known in advance. In this article we show how this restriction can be lifted. We also show how a first-order variant of binary decision diagrams can be used to efficiently compute first-order state abstractions. Moreover, we give a completely declarative specification of a learning Golog interpreter that incorporates the presented state-abstraction mechanisms.	https://dx.doi.org/10.1093/jigpal/jzs011	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Becker2010	CONTROLLING A SIMULATED ROBOT USING MACHINE LEARNING TECHNIQUES	USARSim group at NIST developed a simulated robot that operated in the Unreal Tournament 3 (UT3) gaming environment. They used a software PID controller to control the robot in UT3 worlds. Unfortunately, the PID controller did not work well, so NIST asked us to develop a better controller using machine learning techniques. In the process, we characterized the software PID controller and the robot's behavior in UT3 worlds. Using data collected from our simulations, we compared different machine learning techniques including linear regression and reinforcement learning (RL). Finally, we implemented a RL based controller in Mat lab and ran it in the UT3 environment via a TCP/IP link between Mat lab and UT3.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Begashaw2016	Enhancing Blind Interference Alignment with Reinforcement Learning	Blind interference alignment (IA) is a signaling scheme that suppresses interference in multi-user systems, without the knowledge of channel state information at the transmitter (CSIT). The key to performing IA without CSIT is the use of reconfigurable antennas (RA) that are capable of dynamically switching among a fixed number of radiation patterns to introduce artificial fluctuations in the channel. The radiation patterns used to realize blind IA have significant impacts on the overall performance of the system. Hence, an intelligent antenna pattern selection strategy is a crucial component of any practical RA-based blind IA implementation. In this work, we propose two reinforcement learning algorithms for selecting the optimal antenna configuration for blind IA. Furthermore, we evaluate the performance of these antenna mode selection techniques using over the air measurements on our software defined radio implementation of blind IA using a Reconfigurable Alford Loop Antenna that is capable of generating multiple radiation patterns. We quantify the performance of the algorithms in terms of received signal to interference and noise ratio (SINR) and show that our learning-based mode selection strategies are capable of choosing the highest performing mode 90\% of the time and attain over 2 dB gain in SINR over other selection approaches.	https://dx.doi.org/10.1109/GLOCOM.2016.7841815	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bekar2020	High fidelity progressive reinforcement learning for agile maneuvering uavs		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091908935&doi=10.2514\%2f6.2020-0898&partnerID=40&md5=d9fae619af21a737c069dead8fdb7fe9	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bellemare2013	The arcade learning environment: an evaluation platform for general agents			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	BenBraiek2020	On testing machine learning programs	Nowadays, we are witnessing a wide adoption of Machine learning (ML) models in many software systems. They are even being tested in safety-critical systems, thanks to recent breakthroughs in deep learning and reinforcement learning. Many people are now interacting with systems based on ML every day, e.g., voice recognition systems used by virtual personal assistants like Amazon Alexa or Google Home. As the field of ML continues to grow, we are likely to witness transformative advances in a wide range of areas, from finance, energy, to health and transportation. Given this growing importance of ML-based systems in our daily life, it is becoming utterly important to ensure their reliability. Recently, software researchers have started adapting concepts from the software testing domain (e.g., code coverage, mutation testing, or property-based testing) to help ML engineers detect and correct faults in ML programs. This paper reviews current existing testing practices for ML programs. First, we identify and explain challenges that should be addressed when testing ML programs. Next, we report existing solutions found in the literature for testing ML programs. Finally, we identify gaps in the literature related to the testing of ML programs and make recommendations of future research directions for the scientific community. We hope that this comprehensive review of software testing practices will help ML engineers identify the right approach to improve the reliability of their ML-based systems. We also hope that the research community will act on our proposed research directions to advance the state of the art of testing for ML programs. (C) 2020 Published by Elsevier Inc.	https://dx.doi.org/10.1016/j.jss.2020.110542	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Benatti2019	A Modular Simulation Platform for Training Robots via Deep Reinforcement Learning and Multibody Dynamics		https://doi.org/10.1145/3365265.3365274	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bera2021	Pythia: A Customizable Hardware Prefetching Framework Using Online Reinforcement Learning		https://doi.org/10.1145/3466752.3480114	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Berghout2022	Machine learning for cybersecurity in smart grids: A comprehensive review-based study on methods, solutions, and prospects		https://doi.org/10.1016/j.ijcip.2022.100547	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bermudez-Contreras2020	The Neuroscience of Spatial Navigation and the Relationship to Artificial Intelligence	Recent advances in artificial intelligence (AI) and neuroscience are impressive. In AI, this includes the development of computer programs that can beat a grandmaster at GO or outperform human radiologists at cancer detection. A great deal of these technological developments are directly related to progress in artificial neural networks-initially inspired by our knowledge about how the brain carries out computation. In parallel, neuroscience has also experienced significant advances in understanding the brain. For example, in the field of spatial navigation, knowledge about the mechanisms and brain regions involved in neural computations of cognitive maps-an internal representation of space-recently received the Nobel Prize in medicine. Much of the recent progress in neuroscience has partly been due to the development of technology used to record from very large populations of neurons in multiple regions of the brain with exquisite temporal and spatial resolution in behaving animals. With the advent of the vast quantities of data that these techniques allow us to collect there has been an increased interest in the intersection between AI and neuroscience, many of these intersections involve using AI as a novel tool to explore and analyze these large data sets. However, given the common initial motivation point-to understand the brain-these disciplines could be more strongly linked. Currently much of this potential synergy is not being realized. We propose that spatial navigation is an excellent area in which these two disciplines can converge to help advance what we know about the brain. In this review, we first summarize progress in the neuroscience of spatial navigation and reinforcement learning. We then turn our attention to discuss how spatial navigation has been modeled using descriptive, mechanistic, and normative approaches and the use of AI in such models. Next, we discuss how AI can advance neuroscience, how neuroscience can advance AI, and the limitations of these approaches. We finally conclude by highlighting promising lines of research in which spatial navigation can be the point of intersection between neuroscience and AI and how this can contribute to the advancement of the understanding of intelligent behavior.	https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089315185&doi=10.3389\%2ffncom.2020.00063&partnerID=40&md5=a08d6811f61738bf6a0d4ec96eaac616	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Berthier2022	Infinite-Dimensional Sums-of-Squares for Optimal Control	In this paper, we introduce an approximation method to solve an optimal control problem via the Lagrange dual of its weak formulation, which applies to problems with an unknown, non-necessarily polynomial, dynamics accessed through samples, akin to model-free reinforcement learning. It is based on a sum-of-squares representation of the Hamiltonian, and extends a previous method from polynomial optimization to the generic case of smooth problems. Such a representation is infinite-dimensional and relies on a particular space of functions \endash a reproducing kernel Hilbert space \endash chosen to fit the structure of the control problem. After subsampling, it leads to a practical method that amounts to solving a semi-definite program. We illustrate our approach numerically on a low-dimensional control problem.	https://dx.doi.org/10.1109/CDC51059.2022.9992396	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bertsekas2022	Newton's method for reinforcement learning and model predictive control		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129617849&doi=10.1016\%2fj.rico.2022.100121&partnerID=40&md5=16006cc28238828a32db21a506214d3f	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bhanumathi2017	A guide for the selection of routing protocols in WBAN for healthcare applications		https://doi.org/10.1186/s13673-017-0105-6	Excluded	conflict_resolution	E5: Other not a paper,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bhatt2020	Software-Level Accuracy Using Stochastic Computing With Charge-Trap-Flash Based Weight Matrix	The in-memory computing paradigm with emerging memory devices has been recently shown to be a promising way to accelerate deep learning. Resistive processing unit (RPU) has been proposed to enable the vector-vector outer product in a crossbar array using a stochastic train of identical pulses to enable one-shot weight update, promising intense speed-up in matrix multiplication operations, which form the bulk of training neural networks. However, the performance of the system suffers if the device does not satisfy the condition of linear conductance change over around 1,000 conductance levels. This is a challenge for nanoscale memories. Recently, Charge Trap Flash (CTF) memory was shown to have a large number of levels before saturation, but variable non-linearity. In this paper, we explore the trade-off between the range of conductance change and linearity. We show, through simulations, that at an optimum choice of the range, our system performs nearly as well as the models trained using exact floating point operations, with less than 1\% reduction in the performance. Our system reaches an accuracy of 97.9\% on MNIST dataset, 89.1\% and 70.5\% accuracy on CIFAR-10 and CIFAR-100 datasets (using pre-extracted features). We also show its use in reinforcement learning, where it is used for value function approximation in Q-Learning, and learns to complete an episode the mountain car control problem in around 146 steps. Benchmarked to state-of-the-art, the CTF based RPU shows best in class performance to enable software equivalent performance.	https://dx.doi.org/10.1109/IJCNN48605.2020.9206631	Excluded	new_screen	E1: Does not define or use a RL method,E1: Does not define or use a RL method	4
RL4SE	Bhattacharjee2022	Real-Time SIL Validation of a Novel PMSM Control Based on Deep Deterministic Policy Gradient Scheme for Electrified Vehicles	Vector control plays a critical role in a permanent magnet synchronous motor (PMSM) drive to deliver the desired torque in electrified vehicle applications. Motor speed and stator current control depend on various nonlinear motor parameters that influence the performance of PMSM. Moreover, tuning of speed and current controller parameters using conventional control techniques also depends on these PMSM parameters. To enhance the robustness of vector control and tracking methodology against PMSM parameter uncertainties and load disturbances, a novel deep reinforcement learning (DRL) based advanced speed and current control technique is proposed in this article. The proposed method mitigates the effects of disturbance due to parameter variations as well as the load torque. The novel architecture delivers closed-loop reinforcement learning agents trained with the deep deterministic policy gradient learning algorithm in the plant environment where the cost of exploration is expensive. First, an overview and need for the proposed DRL vector control architecture are provided. Subsequently, the design and training methods for the proposed DRL controller are elicited. Thereafter, the proposed control scheme is validated with real-time software-in-the-loop testing under various conditions and compared against adaptive proportional\endashintegral control of the same PMSM in OPAL-RT simulator.	https://dx.doi.org/10.1109/TPEL.2022.3153845	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bhattacharyya2019	QFlow: A Reinforcement Learning Approach to High QoE Video Streaming over Wireless Networks		https://doi.org/10.1145/3323679.3326523	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bhattacharyya2022	QFlow: A Learning Approach to High QoE Video Streaming at the Wireless Edge	The predominant use of wireless access networks is for media streaming applications. However, current access networks treat all packets identically, and lack the agility to determine which clients are most in need of service at a given time. Software reconfigurability of networking devices has seen wide adoption, and this in turn implies that agile control policies can be now instantiated on access networks. Exploiting such reconfigurability requires the design of a system that can enable a configuration, measure the impact on the application performance (Quality of Experience), and adaptively select a new configuration. Effectively, this feedback loop is a Markov Decision Process whose parameters are unknown. The goal of this work is to develop QFlow, a platform that instantiates this feedback loop, and instantiate a variety of control policies over it. We use the popular application of video streaming over YouTube as our use case. Our context is priority queueing, with the action space being that of determining which clients should be assigned to each queue at each decision period. We first develop policies based on model-based and model-free reinforcement learning. We then design an auction-based system under which clients place bids for priority service, as well as a more structured index-based policy. Through experiments, we show how these learning-based policies on QFlow are able to select the right clients for prioritization in a high-load scenario to outperform the best known solutions with over 25\% improvement in QoE, and a perfect QoE score of 5 over 85\% of the time.	https://dx.doi.org/10.1109/TNET.2021.3106675	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bhonker2017	Playing SNEs in the retro learning environment			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bhutto2022	Reinforced Transformer Learning for VSI-DDoS Detection in Edge Clouds	Edge-driven software applications often deployed as online services in the cloud-to-edge continuum lack significant protection for services and infrastructures against emerging cyberattacks. Very-Short Intermittent Distributed Denial of Service (VSI-DDoS) attack is one of the biggest factors for diminishing the Quality of Services (QoS) and Quality of Experiences (QoE) for users on edge. Unlike conventional DDoS attacks, these attacks live for a very short time (on the order of a few milliseconds) in the traffic to deceive users with a legitimate service experience. To provide protection, we propose a novel and efficient approach for detecting VSI-DDoS attacks using reinforced transformer learning that mitigates the tail latency and service availability problems in edge clouds. In the presence of attacks, the users' demand for availing ultra-low latency and high throughput services deployed on the edge, can never be met. Moreover, these attacks send very-short intermittent requests towards the target services that enforce longer delays in users' responses. The assimilation of transformer with deep reinforcement learning accelerates detection performance under adverse conditions by adapting the dynamic and the most discernible patterns of attacks (e.g., multiplicative temporal dependency, attack dynamism). The extensive experiments with testbed and benchmark datasets demonstrate that the proposed approach is suitable, effective, and efficient for detecting VSI-DDoS attacks in edge clouds. The results outperform state-of-the-art methods with $0.9\\%-3.2\\%$ higher accuracy in both datasets.	https://dx.doi.org/10.1109/ACCESS.2022.3204812	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Biagioni2022	PowerGridworld: a framework for multi-agent reinforcement learning in power systems		https://doi.org/10.1145/3538637.3539616	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Biloki2019	Neural program planner for structured predictions			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Binxiang2019	A Deep Reinforcement Learning Malware Detection Method Based on PE Feature Distribution	Existing anti-virus software and malware detection methods, including signature-based and the machine learning-based malware detection methods, are unable to update the virus database in real time, resulting in poor resistance to malware variants. To solve this problem, this paper proposes a novel malware detection method based on deep reinforcement learning, which combines the advantages of Q-learning and neural network. Q-learning action selection strategy is adopted while solving the problem of high dimensional state space. Theoretical analysis and experimental results show that the proposed method can not only detect malware variants efficiently, but also perform well in many well-known anti-virus software, which is a new direction in the field of malware detection.	https://dx.doi.org/10.1109/ICISCE48695.2019.00014	Included	conflict_resolution		4
RL4SE	Bischoff2014	Policy search for learning robot control using sparse data	In many complex robot applications, such as grasping and manipulation, it is difficult to program desired task solutions beforehand, as robots are within an uncertain and dynamic environment. In such cases, learning tasks from experience can be a useful alternative. To obtain a sound learning and generalization performance, machine learning, especially, reinforcement learning, usually requires sufficient data. However, in cases where only little data is available for learning, due to system constraints and practical issues, reinforcement learning can act suboptimally. In this paper, we investigate how model-based reinforcement learning, in particular the probabilistic inference for learning control method (Pilco), can be tailored to cope with the case of sparse data to speed up learning. The basic idea is to include further prior knowledge into the learning process. As Pilco is built on the probabilistic Gaussian processes framework, additional system knowledge can be incorporated by defining appropriate prior distributions, e.g. a linear mean Gaussian prior. The resulting Pilco formulation remains in closed form and analytically tractable. The proposed approach is evaluated in simulation as well as on a physical robot, the Festo Robotino XT. For the robot evaluation, we employ the approach for learning an object pick-up task. The results show that by including prior knowledge, policy learning can be sped up in presence of sparse data.	https://dx.doi.org/10.1109/ICRA.2014.6907422	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Biswas2017	Machine learning for run-time energy optimisation in many-core systems	In recent years, the focus of computing has moved away from performance-centric serial computation to energy-efficient parallel computation. This necessitates run-time optimisation techniques to address the dynamic resource requirements of different applications on many-core architectures. In this paper, we report on intelligent run-time algorithms which have been experimentally validated for managing energy and application performance in many-core embedded system. The algorithms are underpinned by a cross-layer system approach where the hardware, system software and application layers work together to optimise the energy-performance trade-off. Algorithm development is motivated by the biological process of how a human brain (acting as an agent) interacts with the external environment (system) changing their respective states over time. This leads to a pay-off for the action taken, and the agent eventually learns to take the optimal/best decisions in future. In particular, our online approach uses a model-free reinforcement learning algorithm that suitably selects the appropriate voltage-frequency scaling based on workload prediction to meet the applications' performance requirements and achieve energy savings of up to 16\% in comparison to state-of-the-art-techniques, when tested on four ARM A15 cores of an ODROID-XU3 platform.	https://dx.doi.org/10.23919/DATE.2017.7927243	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Blau2011	INCENTIVES AND PERFORMANCE IN LARGE-SCALE LEAN SOFTWARE DEVELOPMENT An Agent-based Simulation Approach	The application of lean principles and agile project management techniques in the domain of large-scale software product development has gained tremendous momentum over the last decade. However, a simple transfer of good practices from the automotive industry combined with experiences from agile development on a team level is not possible due to fundamental differences stemming from the particular domain specifics - i.e. different types of products and components (material versus immaterial goods), knowledge work versus production systems as well as established business models. Especially team empowerment and the absence of a a hierarchical control on all levels impacts goal orientation and business optimization. In such settings, the design of adequate incentive schemes in order to align local optimization and opportunistic behavior with the overall strategy of the company is a crucial activity of central importance. Following an agent-based simulation approach with reinforcement learning, we (i) address the question of how information regarding backlog item dependencies is shared within and in between development teams on the product level subject to different incentive schemes. We (ii) compare different incentive schemes ranging from individual to team-based compensation. Based on our results, we are (iii) able to provide recommendations on how to design such incentives, what their effect is, and how to chose an adequate development structure to foster overall software product development flow by means of more economic decisions and thus resulting in a shorter time to market. For calibrating our simulation, we rely on practical experience from a very large software company piloting and implementing lean and agile for about three years.		Included	conflict_resolution		4
RL4SE	Blau2013	Steering through Incentives in Large-Scale Lean Software Development	The application of lean principles and agile project management techniques in the domain of large-scale software product development has gained tremendous momentum over the last decade. This results in empowerment of individuals which leads to increased flexibility but at the same time sacrifices managerial control through traditional steering practices. Hence, the design of adequate incentive schemes in order to align local optimization and opportunistic behavior with the overall strategy of the company is a crucial activity from a business perspective. Following an agent-based simulation approach with reinforcement learning, we (i) address the question of how information regarding backlog item dependencies is shared within and in between development teams on the product level subject to different incentive schemes. We (ii) compare different incentive schemes ranging from individual to team-based compensation. Based on our results, we are (iii) able to provide recommendations on how to design suitable incentive schemes in order to enable a goal-oriented steering of individual behavior in order to support the overall company objectives.		Included	new_screen		4
RL4SE	Bloem2015	Ground Delay Program Analytics with Behavioral Cloning and Inverse Reinforcement Learning	Historical data are used to build two types of models that predict Ground Delay Program implementation decisions and produce insights into how and why those decisions are made. More specifically, behavioral cloning and inverse reinforcement learning models are built that predict hourly Ground Delay Program implementation at Newark Liberty International and San Francisco International airports. Data available to the models include actual and scheduled air traffic metrics and observed and forecasted weather conditions. The developed random forest models are substantially better at predicting hourly Ground Delay Program implementation for these airports than the developed inverse reinforcement learning models. However, all of the models struggle to predict the initialization and cancellation of Ground Delay Programs. The structure of the models are also investigated in order to gain insights into Ground Delay Program implementation decision making. Notably, characteristics of both types of model suggest that Ground Delay Program implementation decisions are more tactical than strategic: they are made primarily based on conditions now or conditions anticipated in only the next couple of hours.	https://dx.doi.org/10.2514/1.I010304	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bloembergen2015	Evolutionary Dynamics of Multi-Agent Learning: A Survey	The interaction of multiple autonomous agents gives rise to highly dynamic and non-deterministic environments, contributing to the complexity in applications such as automated financial markets, smart grids, or robotics. Due to the sheer number of situations that may arise, it is not possible to foresee and program the optimal behaviour for all agents beforehand. Consequently, it becomes essential for the success of the system that the agents can learn their optimal behaviour and adapt to new situations or circumstances. The past two decades have seen the emergence of reinforcement learning, both in single and multi-agent settings, as a strong, robust and adaptive learning paradigm. Progress has been substantial, and a wide range of algorithms are now available. An important challenge in the domain of multi-agent learning is to gain qualitative insights into the resulting system dynamics. In the past decade, tools and methods from evolutionary game theory have been successfully employed to study multi-agent learning dynamics formally in strategic interactions. This article surveys the dynamical models that have been derived for various multi-agent reinforcement learning algorithms, making it possible to study and compare them qualitatively. Furthermore, new learning algorithms that have been introduced using these evolutionary game theoretic tools are reviewed. The evolutionary models can be used to study complex strategic interactions. Examples of such analysis are given for the domains of automated trading in stock markets and collision avoidance in multi-robot systems. The paper provides a roadmap on the progress that has been achieved in analysing the evolutionary dynamics of multi-agent learning by highlighting the main results and accomplishments.	https://dx.doi.org/10.1613/jair.4818	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bogaerts2020	Connecting the CoppeliaSim robotics simulator to virtual reality	The CoppeliaSim VR Toolbox provides a set of tools to experience CoppeliaSim robot simulation software in Virtual Reality and to return user interactions. Its primary focus is to create a platform that enables the fast prototyping and verification of robotic systems. Moreover, the generality of the toolbox ensures that it can be valuable in other contexts like robotics education, human-robot interaction or reinforcement learning. The software is designed to have a low entry threshold for moderately complex use cases, but can be extended to perform very complex visualizations for more experienced users. (c) 2020 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).	https://dx.doi.org/10.1016/j.softx.2020.100426	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bolcato2022	On the Value of Using 3D Shape and Electrostatic Similarities in Deep Generative Methods	Multiparameter optimization, the heart of drug design, isstill an open challenge. Thus, improved methods for automatedcompound design with multiple controlled properties are desired.Here, we present a significant extension to our previously describedfragment-based reinforcement learning method (DeepFMPO) for thegeneration of novel molecules with optimal properties. As before, thegenerative process outputs optimized molecules similar to the inputstructures, now with the improved feature of replacing parts of thesemolecules with fragments of similar three-dimensional (3D) shape and electrostatics. We developed and benchmarked a new pythonpackage, ESP-Sim, for the comparison of the electrostatic potential and the molecular shape, allowing the calculation of high-qualitypartial charges (e.g., RESP with B3LYP/6-31G**) obtained using the quantum chemistry program Psi4. By performing comparisonsof 3D fragments, we can simulate 3D properties while overcoming the notoriously difficult step of accurately describing bioactiveconformations. The new improved generative (DeepFMPO v3D) method is demonstrated with a scaffold-hopping exerciseidentifying CDK2 bioisosteres. The code is open-source and freely available	https://www.ncbi.nlm.nih.gov/pubmed/35271260	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bollinger2016	Multi-agent reinforcement learning for optimizing technology deployment in distributed multi-energy systems			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bonarini1997	Anytime learning and adaptation of structured fussy behaviors	We present an approach to support effective learning and adaptation of behaviors for autonomous agents with reinforcement learning algorithms. These methods can identify control systems that optimize a reinforcement program, which is, usually, a straightforward representation of the designer's goals. Reinforcement learning algorithms usually are too slow to be applied in real time on embodied agents, although they provide a suitable way to represent the desired behavior. We have tackled three aspects of this problem: the speed of the algorithm, the learning procedure, and the control system architecture. The learning algorithm we have developed includes features to speed up learning, such as niche-based learning, and a representation of the control modules in terms of fuzzy rules that reduces the search space and improves robustness to noisy data. Our learning procedure exploits methodologies such as learning from easy missions and transfer of policy from simpler environments to the more complex. The architecture of our control system is layered and modular, so that each module has a low complexity and can be learned in a short time. The composition of the actions proposed by the modules is either learned or predefined. Finally, we adopt an anytime learning approach to improve the qualify of She control system on-line and to adapt it to dynamic environments. The experiments we present in this article concern learning to reach another moving agent in a real, dynamic environment that includes nontrivial situations such as that in which the moving target is faster than the agent and that in which the target is hidden by obstacles.	https://dx.doi.org/10.1177/105971239700500304	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bonarini1997a	Anytime learning and adaptation of structured fuzzy behaviors		https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031287709&doi=10.1177\%2f105971239700500304&partnerID=40&md5=dbd83b523d7788d95252264c0473053b	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bonarini2001	Learning fuzzy classifier systems for multi-agent coordination	We present ELF, a learning fuzzy classifier system (LFCS), and its application to the field of Learning Autonomous Agents. In particular, we will show how this kind of Reinforcement Learning systems can be successfully applied to learn both behaviors and their coordination for Autonomous Agents. We will discuss the importance of knowledge representation approach based on fuzzy sets to reduce the search space without losing the required precision. Moreover, we will show how we have applied ELF to learn the distributed coordination among agents which can exchange information with each other. The experimental validation has been done on software agents interacting in a real-time task. (C) 2001 Elsevier Science Inc. All rights reserved.	https://dx.doi.org/10.1016/S0020-0255(01)00149-9	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bonati2021	Intelligence and Learning in O-RAN for Data-Driven NextG Cellular Networks	Next generation (NextG) cellular networks will be natively cloud-based and built on programmable, virtualized, and disaggregated architectures. The separation of control functions from the hardware fabric and the introduction of standardized control interfaces will enable the definition of custom closed-control loops, which will ultimately enable embedded intelligence and real-time analytics, thus effectively realizing the vision of autonomous and self-optimizing networks. This article explores the disaggregated network architecture proposed by the O-RAN Alliance as a key enabler of NextG networks. Within this architectural context, we discuss the potential, the challenges, and the limitations of data-driven optimization approaches to network control over different timescales. We also present the first large-scale integration of O-RAN-compliant software components with an open source full-stack softwarized cellular network. Experiments conducted on Colosseum, the world's largest wireless network emulator, demonstrate closed-loop integration of real-time analytics and control through deep reinforcement learning agents. We also show the feasibility of radio access network (RAN) control through xApps running on the near-real-time RAN intelligent controller to optimize the scheduling policies of coexisting network slices, leveraging the O-RAN open interfaces to collect data at the edge of the network.	https://dx.doi.org/10.1109/MCOM.101.2001120	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bonissone1995	Fuzzy controllers and fuzzy expert systems: Industrial applications of fuzzy technology		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079240074&doi=10.1117\%2f12.211793&partnerID=40&md5=541678530f35522b27485160cbbc2584	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bonissone1995a	Industrial Applications of Fuzzy Logic at General Electric		https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029276283&doi=10.1109\%2f5.364490&partnerID=40&md5=3f6ff5697ed0ed5c5aaa844f4885d0f2	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Bonsignorio2008	A comparison between under actuated and active bipedal locomotion gait control			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bosello2019	From Programming Agents to Educating Agents endash A Jason-Based Framework for Integrating Learning in the Development of Cognitive Agents		https://doi.org/10.1007/978-3-030-51417-4_9	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bosello2020	From Programming Agents to Educating Agents - A Jason-Based Framework for Integrating Learning in the Development of Cognitive Agents	Recent advances and successes of machine learning techniques are paving the way to what is referred as Software 2.0 era and cognitive computing, in which traditional programming and software development is meant to be replaced by such techniques for many applications. If we consider agent-oriented programming, we believe that such developments trigger new interesting scenarios blending cognitive architecture such as the BDI one and techniques like Reinforcement Learning (RL) even more deeply compared to what has been proposed so far in the literature. In that perspective, we aim at exploring the integration of cognitive agent-oriented programming based on BDI with learning techniques so as to systematically exploit them in the agent development stage. The approach should support the design of BDI agents in which some plans can be explicitly programmed and others instead can be learned by the agent during the development/engineering stage. In that view, the development of an agent is metaphorically similar to an education process, in which first an agent is created with a set of basic programmed plans and then grow up in order to learn plans to achieve the goals for which the agent is meant to be designed. This paper presents and discusses this medium-term view, introducing a first model for a BDI agent programming framework integrating RL, a first implementation based on Jason programming language/platform and sketching a roadmap for this research line.	https://dx.doi.org/10.1007/978-3-030-51417-4_9	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bottinger2018	Deep Reinforcement Fuzzing	Fuzzing is the process of finding security vulnerabilities in input-processing code by repeatedly testing the code with modified inputs. In this paper, we formalize fuzzing as a reinforcement learning problem using the concept of Markov decision processes. This in turn allows us to apply state-of-the-art deep Q-learning algorithms that optimize rewards, which we define from runtime properties of the program under test. By observing the rewards caused by mutating with a specific set of actions performed on an initial program input, the fuzzing agent learns a policy that can next generate new higher-reward inputs. We have implemented this new approach, and preliminary empirical evidence shows that reinforcement fuzzing can outperform baseline random fuzzing.	https://dx.doi.org/10.1109/SPW.2018.00026	Included	new_screen		4
RL4SE	Boubin2022	MARbLE: Multi-Agent Reinforcement Learning at the Edge for Digital Agriculture	Digital agriculture, hailed as the fourth great agricultural revolution, employs software-driven autonomous agents for in-field crop management. Edge computing resources deployed near crop fields support autonomous agents with substantial computational needs for tasks such as AI inference. In large fields, using multiple autonomous agents, called swarms, can speed up crop management tasks if sufficient edge resources are provisioned. However, to use swarms today, farmers and software developers craft their own standalone solutions that are either simple and ineffective or complicated and hard-to-reproduce. We present MARbLE, a platform for developing and managing swarms. MARbLE provides an easy-to-use programming paradigm that helps users build swarm workloads using multi-agent reinforcement learning. Developers supply just two functions Map() and Eval(). The platform automatically compiles and deploys swarms and continuously updates the reinforcement learning models that govern their actions. Developers can experiment with multiple swarm and edge resource configurations both in simulation and with actual in-field runs. We studied real UAV swarms conducting digital agriculture missions. We observe that swarms demanded edge computing resources in bursts; the ratio of average to peak demand was 2.9X. MARbLE uses energy-saving load balancing policies to duty cycle machines during workload demand troughs, leveraging workload patterns to save edge energy. Using MARbLE, we found that four-agent swarms with load balancing techniques sped up missions by 2.1X and reduced edge energy usage by up to 2X compared to state of the art autonomous swarms.	https://dx.doi.org/10.1109/SEC54971.2022.00013	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Boubin2019	Autonomic Computing Challenges in Fully Autonomous Precision Agriculture	Precision agriculture examines crop fields, gathers data, analyzes crop health and informs field management. This data driven approach can reduce fertilizer runoff, prevent crop disease and increase yield. Frequent data collection improves outcomes, but also increases operating costs. Fully autonomous aerial systems (FAAS) can capture detailed images of crop fields without human intervention. They can reduce operating costs significantly. However, FAAS software must embed agricultural expertise to decide where to fly, which images to capture and when to land. This paper explores fully autonomous precision agriculture where FAAS map crop fields frequently. We have designed hardware and software architecture. We use unmanned aerial systems, edge computing components and software driven by reinforcement learning and ensemble models. In early results, we have collected data from an Ohio cornfield. We use this data to simulate a FAAS modeling crop yield. Our results (1) show that our approach predicts yield well and (2) can quantify computational demand. Computational costs can be prohibitive. We discuss how research on adaptive systems can reduce costs and enable fully autonomous precision agriculture. We also provide our simulation tools and dataset as part of our open source FAAS middleware, SoftwarePilot.	https://dx.doi.org/10.1109/ICAC.2019.00012	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bouzy2006	Monte-Carlo Go Reinforcement Learning Experiments	This paper describes experiments using reinforcement learning techniques to compute pattern urgencies used during simulations performed in a Monte-Carlo Go architecture. Currently, Monte-Carlo is a popular technique for computer Go. In a previous study, Monte-Carlo was associated with domain-dependent knowledge in the Go-playing program Indigo. In 2003, a 3times3 pattern database was built manually. This paper explores the possibility of using reinforcement learning to automatically tune the 3times3 pattern urgencies. On 9times9 boards, within the Monte-Carlo architecture of Indigo, the result obtained by our automatic learning experiments is better than the manual method by a 3-point margin on average, which is satisfactory. Although the current results are promising on 19times19 boards, obtaining strictly positive results with such a large size remains to be done	https://dx.doi.org/10.1109/CIG.2006.311699	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Brandao2020	Decision support framework for the stock market using deep reinforcement learning	In stock markets, investors adopt different strategies to identify a sequence of profitable investment decisions to maximize their profits. To support the decision of investors, machine learning (ML) software is being applied. In particular, deep learning (DL) approaches are attractive since the stock market parameter presents a highly non-linear behavior, and since DL techniques can track short time and long time variations. In contrast to supervised ML techniques, deep reinforcement learning (DRL) gathers DL's benefits and adds the real-time adaptation and improvement of the machine learning model. In this paper, we propose a decision support framework for the stock market based on DRL. By learning the trading rules, our framework recognizes patterns, maximizes the profit obtained, and provides recommendations to the investors. The proposed DRL framework outperforms the state-of-the-art framework with 0.86 \% of F1 score for buy operations and 0.88 \% of F1 score for sale operations in terms of evaluating the positioning strategy.	https://dx.doi.org/10.1109/WCNPS50723.2020.9263712	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Brandes2020	RF Waveform Synthesis Guided by Deep Reinforcement Learning	In this work, we demonstrate a system that enhances radio frequency (RF) fingerprints of individual transmitters via waveform modification to uniquely identify them amidst an ensemble of identical transmitters. This has the potential to enable secure identification, even in the presence of stolen and retransmitted unique device identifiers that are present in the transmitted waveforms, and ensures robust communications. This approach also lends itself to steganography as the waveform modifications can themselves encode information. Our system uses Bayesian program learning to learn specific characteristics of a set of emitters, and integrates the learned programs into a reinforcement learning architecture to build a policy for actions applied to the digital waveform before transmission. This allows the system to learn how to modify waveforms that leverage and emphasize inherent differences within RF front-ends to enhance their distinct characteristics while maintaining robust communications. In this ongoing research, we demonstrate our system in a small population, and provide a road map to expand it to larger populations that are expected in today's interconnected spaces.	https://dx.doi.org/10.1109/WIFS49906.2020.9360894	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Brandt1995	Teaching literature searching in the context of the World Wide Web	As part of the required curriculum for medical students, we devised a literature-searching practicum that has been used for two years. In both years, we stressed going beyond the skills needed for using a particular searching program, towards a more conceptual approach to information searching. In the first year, the practicum was taught in a traditional lecture/hands-on format. In the second year, the lecture was replaced by a World Wide Web-based tutorial (http:@www.welch.jhu.edu/Education/tutorials/pra cticum.html). To our knowledge, this is the first Web-based resource intended to teach students about appropriate use of search technology. Comparison of student evaluations showed no difference in attitude toward the two versions of the practicum, and observation of student performance suggested similar levels of proficiency. We conclude that placing these educational materials on the Web (1) makes us practice what we preach; (2) is as effective as traditional teaching methods; and (3) gives students a resource for reinforcement learning.		Excluded	new_screen	E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for	4
RL4SE	Brassai2021	Intelligent agent based low level control of complex robotic systems	The use of intelligent agents, trained with reinforcement learning methods for control of complex mechanical systems, like humanoid robots has the potential to revolutionize the way we think about control problems. This way of learning is very similar to how we humans learn most of the things in our early age, thus proving really promising if we want to make robots able to learn tasks that require some form of intelligence. Throughout the research presented in this paper, a deep neural network based intelligent agent, with Actor-Critic architecture was trained with the Deep Deterministic Policy Gradient algorithm for the purpose of controlling a custom designed humanoid robot. For the training of the agent a simulation model of the physical robot is developed and integrated into a customizable simulated environment. The idea of low, actuator level control of complex systems by neural networks formulates the problem into a more abstract form while keeping the full control of the system without having to deal with the actual level of complexity. This can be further enhanced by expanding the abstraction from the software level to include some part of the hardware as well.	https://dx.doi.org/10.1109/INES52918.2021.9512904	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Brewer2020	Can Mindfulness Mechanistically Target Worry to Improve Sleep Disturbances? Theory and Study Protocol for App-Based Anxiety Program	Objective: Anxiety is associated with sleep disturbance and insomnia. Mindfulness-based interventions, such as mindfulness-based stress reduction, have shown consistent anxiety reduction. Mindfulness training has been theorized to affect reinforcement learning, affecting habitual behaviors such as smoking and overeating, but a direct mechanistic link between the use of mindfulness training for anxiety reduction and improvement in sleep has not been studied. Moreover, the mechanisms by which mindfulness might affect worry and subsequent sleep disturbances have not been elucidated. This study protocol evaluates the impact an app-based mindfulness training program for anxiety might have on decreasing worry and improvement in sleep. Method: A randomized controlled study will be conducted in approximately 80 adults with worry that interferes with their sleep. Participants will be randomly allocated (1:1) to two groups: treatment-as-usual (TAU) or TAU + App-Based Mindfulness Training (Unwinding Anxiety app). The primary outcomes will be the non-reactivity subscale of the Five Facet Mindfulness Questionnaire and Patient-Reported Outcomes Measurement Information System sleep quality measures (Baer et al., 2008; Yu et al., 2011). Secondary outcomes will include the Penn State Worry Questionnaire. Generalized Anxiety Disorder-7, and Multidimensional Assessment of Interoceptive Awareness Scale (Mehling et al., 2012; Meyer. Miller. Metzger, & Borkovec. 1990; Spitzer. Kroenke, Williams. & Lowe, 2006). Discussion: This study will be the first to test the mechanism of app-based mindfulness training on worry and sleep disturbance. Testing the mechanistic effects of mindfulness training using the science of behavior change framework will help move the field forward both in further elucidation of potential mechanisms of mindfulness (e.g., targeting reinforcement learning) and determining whether such a platform might be a viable method for delivering high-fidelity treatment at scale and for a low cost.	https://www.ncbi.nlm.nih.gov/pubmed/32833479	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Brien2015	Shapley Value Estimation for Compensation of Participants in Demand Response Programs	Designing fair compensation mechanisms for demand response (DR) is challenging. This paper models the problem in a game theoretic setting and designs a payment distribution mechanism based on the Shapley value (SV). As exact computation of the SV is in general intractable, we propose estimating it using a reinforcement learning algorithm that approximates optimal stratified sampling. We apply this algorithm to a DR program that utilizes the SV for payments and quantify the accuracy of the resulting estimates.	https://dx.doi.org/10.1109/TSG.2015.2402194	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bruns2020	Early verification of ISA extension specifications using deep reinforcement learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091290343&doi=10.1145\%2f3386263.3406901&partnerID=40&md5=4d97bec747b4e08e91ac9a94d851d017	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Budyal2011	Unicast Quality of Service Routing in Mobile Ad Hoc Networks Based on Neuro-fuzzy Agents	This position paper presents Quality of Service (QoS) routing model in Mobile Ad hoc Networks (MANETs) by using software agents that employ fuzzy logic and neural networks for intelligent routing. It uses Dynamic Source Routing (DSR) in MANETs to find various paths and attributes. Fuzzy static agents decide whether each node on the path satisfies QoS requirement for multimedia application. The static neuro-fuzzy agents are used for training and learning to optimize the input and output fuzzy membership functions according to user requirement, and Q-learning (reinforcement learning) static agent is employed for fuzzy inference instead of experts experience. Mobile agents are used to maintain and repair the paths.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bujgoi2021	Intelligent Control of a Permanent Magnet DC Motor	This paper presents the use of artificial intelligence for the control of a permanent magnet DC motor. The system can be found especially in robotic systems that perform repetitive operations. The control law is generated by an intelligent Reinforcement Learning algorithm. From the numerous variants of this type of algorithm, the Policy Iteration type algorithm was chosen. The algorithm was experimentally implemented using a data acquisition system and Matlab/Simulink software. The control system was tested for several variants of the load (by changing its inertia moment). The simulation and experimental results show that the intelligent control method based on reinforcement learning has better trajectory tracking and vibrations suppression.	https://dx.doi.org/10.1109/ICCC51557.2021.9454643	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bulbul2022	Reinforcement Learning assisted Routing for Time Sensitive Networks	Recent developments in real-time critical systems pave the way for different application scenarios such as Industrial IoT with various quality-of-service (QoS) requirements. The most critical common feature of such applications is that they are sensitive to latency and jitter. Thus, it is desired to perform flow placements strategically considering application requirements due to limited resource availability. In this paper, path computation for time-sensitive networks is investigated while satisfying individual end-to-end delay requirements of critical traffic. The problem is formulated as a mixed-integer linear program (MILP) which is NP-hard with exponentially increasing computational complexity as the network size expands. To solve the MILP with high efficiency, we propose a reinforcement learning (RL) algorithm that learns the best routing policy by continuously interacting with the network environment. The proposed learning algorithm determines the variable action set at each decision-making state and captures different execution times of the actions. The reward function in the proposed algorithm is carefully designed for meeting individual flow deadlines. Simulation results indicate that the proposed reinforcement learning algorithm can produce near-optimal flow allocations (close by ~1.5 \%) and scales well even with large topology sizes.	https://dx.doi.org/10.1109/GLOBECOM48099.2022.10001630	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bunel2018	Leveraging grammar and reinforcement learning for neural program synthesis			Included	new_screen		4
RL4SE	Cai2021	APPM: Adaptive Parallel Processing Mechanism for Service Function Chains	By replacing traditional hardware-based middleboxes with software-based Virtual Network Functions (VNFs) running on general-purpose servers, network function virtualization represents a promising technique to reduce the cost of service creation and increase the agility of network operations. Typically, Service Function Chains (SFCs) are adopted to orchestrate dynamical network services and facilitate management of network applications. Recently, SFC parallelism that implements parallel processing of VNFs has been investigated to further improve SFC service quality. However, the unreasonable service graph of parallel processing in existing parallelized SFCs (PSFCs) might cause excessive resource consumption; incoordination between PSFC deployment and scheduling also increases the queuing delay of VNFs and degrades PSFC performance. In this article, an adaptive parallel processing optimization mechanism (APPM) is proposed to self-adaptively adjust the service graph of PSFCs and intelligently solve the joint problem of PSFC deployment and scheduling. Specifically, APPM uses a parallelism optimization algorithm (POA) based on the bin packing problem with soft bin capacity to optimize the structure of the PSFC service graph. Afterward, APPM employs a joint optimization algorithm based on reinforcement learning (JORL) to jointly deploy and schedule the PSFCs optimized by POA via the online perception of environment status. Simulation results showed that POA reduces the SFC parallelism degree and resource consumption by about 35\%; JORL lowers SFC delay by reducing the queuing delay and has better overall performance than the state of the art algorithms even with limited resources.	https://dx.doi.org/10.1109/TNSM.2021.3052223	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cai2022	Multipath Routing for Traffic Engineering with Hypergraph Attention Enhanced Multi-Agent Reinforcement Learning	Traffic Engineering (TE) is an important mechanism of network performance optimization. Some traditional TE approaches mainly focus on static mapping of traffic flow to some available paths without considering the current network utilization or traffic load. Another class of TE methods is heuristic algorithm based on exact mathematical models, which has low adaptability in the face of complex and changing network environments. In this paper, we design a model-free and experience-driven TE method based on Reinforcement Learning (RL). In particular, we present a Multi-Agent RL (MARL) modelling for multipath routing with the objective to minimize the average flow completion time (FCT). Each router in the network is treated as an independent agent and decides how to split the flow originated from it across multiple paths. An efficient learning algorithm for MARL is developed. It employs the hypergraph attention mechanism to extract some relational representations of the network state by considering the graph nature of the routing paths. Moreover, it relies on the Software Defined Network (SDN) for centralized training, while it allows distributed execution without the need for a central controller. To evaluate our approach, comprehensive tests are conducted on two popular network topologies. The results show that 1) our approach significantly reduces FCT and provides better throughput compared to four baseline approaches, and 2) our approach is robust under various traffic load environments and the advantage over other methods is more significant in high traffic load environments.	https://dx.doi.org/10.1109/WOCC55104.2022.9880574	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cai2022a	A Robust and Learning Approach for Multi-Phase Aerial Search with UAVs		https://doi.org/10.1145/3503047.3503067	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cai2021a	MRDRL-ROS: A Multi Robot Deep Reinforcement Learning Platform based on Robot Operating System		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122433708&doi=10.1088\%2f1742-6596\%2f2113\%2f1\%2f012086&partnerID=40&md5=cecb9bf741a767904d312d86c7e95858	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Camara2015	A multi-agent system with reinforcement learning agents for biomedical text mining		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963548204&doi=10.1145\%2f2808719.2812596&partnerID=40&md5=b0f53ffb31f9502e8c0f28fc918125fd	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Candelieri2019	Business Information Systems for the Cost/Energy Management of Water Distribution Networks: A Critical Appraisal of Alternative Optimization Strategies	The objective of this paper is to show how smart water networks enable new strategies for the energy cost management of the network, more precisely Pump Scheduling Optimization. This problem is traditionally solved using mathematical programming and, more recently, nature inspired meta-heuristics. The schedules obtained by these methods are typically not robust both respect to random variations in the water demand and the non-linear features of the model. The authors consider three alternative optimization strategies: (i) global optimization of black-box functions, based on a Gaussian model and the use of the hydraulic simulator (EPANET) to evaluate the objective function; (ii) Multi Stage Stochastic Programming, which models the stochastic evolution of the water demand through a scenario analysis to solve an equivalent large scale linear program; and finally (iii), Approximate Dynamic Programming, also known as Reinforcement Learning. With reference to real life experimentation, the last two strategies offer more modeling flexibility, are demand responsive and typically result in more robust solutions (i.e. pump schedules) than mathematical programming. More specifically, Approximate Dynamic Programming works on minimal modelling assumption and can effectively leverage on line data availability into robust on-line Pump Scheduling Optimization.	https://dx.doi.org/10.1007/978-3-030-04849-5_1	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Candrawati2016	Adaptive approach in handling human inactivity in computer power management			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Canino2018	Stochastic energy optimization for mobile GPS applications		https://doi.org/10.1145/3236024.3236076	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cao2001	An application of GPS systems on a mobile robot	The purpose of this paper is to describe the use of Global Positioning Systems (GPS) as geographic information and navigational system for a ground based mobile robot. Several low cost wireless systems are now available for a variety of innovative automobile applications including location, messaging and tracking and security. Experiments were conducted with a test bed mobile robot, Bearcat II, for point-to-point motion using a Motorola GPS in June 2001. The Motorola M12 Oncore GPS system is connected to the Bearcat U main control computer through a RS232 interface. A mapping program is used to define a desired route. Then GPS information may be displayed for verification. However, the GPS information is also used to update the control points of the mobile robot using a reinforcement learning method. Local position updates are also used when found in the environment. The significance of the method is in extending the use of GPS to local vehicle control that requires more resolution that is available from the raw data using the adaptive control method.	https://dx.doi.org/10.1117/12.444198	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cao2022	Ensemble Approaches for Test Case Prioritization in UI Testing		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137162453&doi=10.18293\%2fSEKE2022-148&partnerID=40&md5=3b9a5bf69ec1ae27c7d0f606fd7f08d9	Excluded	conflict_resolution	E1: Does not define or use a RL method,E1: Does not define or use a RL method	4
RL4SE	Cao2021	Automatic HMI Structure Exploration Via Curiosity-Based Reinforcement Learning	Discovering the underlying structure of HMI software efficiently and sufficiently for the purpose of testing without any prior knowledge on the software logic remains a difficult problem. The key challenge lies in the complexity of the HMI software and the high variance in the coverage of current methods. In this paper, we introduce the PathFinder, an effective and automatic HMI software exploration framework. PathFinder adopts a curiosity-based reinforcement learning framework to choose actions that lead to the discovery of more unknown states. Additionally, PathFinder progressively builds a navigation model during the exploration to further improve state coverage. We have conducted experiments on both simulations and real-world HMI software testing environment, which comprise a full tool chain of automobile dashboard instrument cluster. The exploration coverage outperforms manual and fuzzing methods which are the current industrial standards.	https://dx.doi.org/10.1109/ASE51524.2021.9678703	Included	conflict_resolution		4
RL4SE	Cardellini2022	irs-partition: An Intrusion Response System utilizing Deep Q-Networks and system partitions	Intrusion Response is a relatively new field of research. Recent approaches for the creation of Intrusion Response Systems (IRSs) use Reinforcement Learning (RL) as a primary technique for the optimal or near-optimal selection of the proper countermeasure to take in order to stop or mitigate an ongoing attack. However, most of them do not consider the fact that systems can change over time or, in other words, that systems exhibit non-stationary behaviors. Furthermore, stateful approaches, such as those based on RL, suffer from the curse of dimensionality, due to the state space growing exponentially with the size of the protected system. In this paper, we introduce and develop an IRS software prototype, named irs-partition. It leverages the partitioning of the protected system and Deep Q-Networks to address the curse of dimensionality by supporting a multi-agent formulation. Furthermore, it exploits transfer learning to follow the evolution of non-stationary systems. ?? 2022 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).	https://dx.doi.org/10.1016/j.softx.2022.101120	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cardoso2000	Using and Evaluating Adaptive Agents for Electronic Commerce Negotiation			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cardoso1999	A Multi-agent System for Electronic Commerce including Adaptive Strategic Behaviours			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cardozo2022	Next Generation Context-oriented Programming: Embracing Dynamic Generation of Adaptations	Context-oriented Programming (COP) first appeared in 2005 as a way to enable the dynamic adaptation of software systems to specific situations in their surrounding environment. Multiple COP languages have since been proposed, and used in numerous adaptive systems areas, enabling dynamic swapping and composition of adaptive behavior at run-time. However, until recently, all approaches relied on the offline pre-definition of adaptive behavior, limiting the adaptations to only those foreseen at design time. Auto-COP recently emerged as an approach to shift adaptation definition to run-time, if and when the need for adaptations to new contexts arises, by utilizing reinforcement learning techniques. In this paper, we use Auto-COP as a starting point to discuss the research path to achieve a completely dynamic adaptive system. We discuss the potential benefits of such an automated Al-based approach, present several application domain categories where dynamic adaptation definition would enable adaptivity breakthroughs, and discuss open challenges in developing such a fully automated approach.	https://dx.doi.org/10.5381/jot.2022.21.2.a5	Excluded	conflict_resolution	E3: Only conceptual results are reported,E3: Only conceptual results are reported	4
RL4SE	Cardozo2017	Peace COrP: learning to solve conflicts between contexts		https://doi.org/10.1145/3117802.3117803	Included	conflict_resolution		4
RL4SE	Carfora2020	Dialogue management in conversational agents through psychology of persuasion and machine learning		https://doi.org/10.1007/s11042-020-09178-w	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Carlucho2019	Double Q-PID algorithm for mobile robot control		https://doi.org/10.1016/j.eswa.2019.06.066	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Caro2022	AI-as-a-Service Toolkit for Human-Centered Intelligence in Autonomous Driving	This paper presents a proof-of-concept implementation of the AI-as-a-Service toolkit developed within the H2020 TEACHING project and designed to implement an autonomous driving personalization system according to the output of an automatic driver's stress recognition algorithm, both of them realizing a Cyber-Physical System of Systems. In addition, we implemented a data-gathering subsystem to collect data from different sensors, i.e., wearables and cameras, to automatize stress recognition. The system was attached for testing to a driving emulation software, CARLA, which allows testing the approach's feasibility with minimum cost and without putting at risk drivers and passengers. At the core of the relative subsystems, different learning algorithms were implemented using Deep Neural Networks, Recurrent Neural Networks, and Reinforcement Learning.	https://dx.doi.org/10.1109/PerComWorkshops53856.2022.9767501	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Carvalho2012	An adaptive multi-agent-based approach to smart grids control and optimization		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857620276&doi=10.1007\%2fs12667-012-0054-0&partnerID=40&md5=7c73763863269e6ab4c80f8f0e5fe4da	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cazenave2022	Mobile Networks for Computer Go		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097440578&doi=10.1109\%2fTG.2020.3041375&partnerID=40&md5=14f27c19f85ef7f9bba980a6f8e519ca	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Celik2019	Chance-Constrained Trajectory Optimization for Non-linear Systems with Unknown Stochastic Dynamics	Iterative trajectory optimization techniques for non-linear dynamical systems are among the most powerful and sample-efficient methods of model-based reinforcement learning and approximate optimal control. By leveraging time-variant local linear-quadratic approximations of system dynamics and reward, such methods can find both a target-optimal trajectory and time-variant optimal feedback controllers. However, the local linear-quadratic assumptions are a major source of optimization bias that leads to catastrophic greedy updates, raising the issue of proper regularization. Moreover, the approximate models' disregard for any physical state-action limits of the system causes further aggravation of the problem, as the optimization moves towards unreachable areas of the state-action space. In this paper, we address the issue of constrained systems in the scenario of online-fitted stochastic linear dynamics. We propose modeling state and action physical limits as probabilistic chance constraints linear in both state and action and introduce a new trajectory optimization technique that integrates these probabilistic constraints by optimizing a relaxed quadratic program. Our empirical evaluations show a significant improvement in learning robustness, which enables our approach to perform more effective updates and avoid premature convergence observed in state-of-the-art algorithms.	https://dx.doi.org/10.1109/IROS40897.2019.8967794	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cen2013	Reinforcement learning in information searching			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chakole2021	A Q-learning agent for automated trading in equity stock markets	Trading strategies play a vital role in Algorithmic trading, a computer program that takes and executes automated trading decisions in the stock market. The conventional wisdom is that the same trading strategy is not profitable for all stocks all the time. The selection of a trading strategy for the stock at a particular time instant is the major research problem in the stock market trading. An optimal dynamic trading strategy generated from the current pattern of the stock price trend can attempt to solve this problem. Reinforcement Learning can find this optimal dynamic trading strategy by interacting with the actual stock market as its environment. The representation of the state of the environment is crucial for performance. We have proposed two different ways to represent the discrete states of the environment. In this work, we trained the trading agent using the Q-learning algorithm of Reinforcement Learning to find optimal dynamic trading strategies. We experimented with the two proposed models on real stock market data from the Indian and American stock markets. The proposed models outperformed the Buy-and-Hold and Decision-Tree based trading strategy in terms of profitability. (C) 2020 Elsevier Ltd. All rights reserved.	https://dx.doi.org/10.1016/j.eswa.2020.113761	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chalita2016	Reinforcement learning in a bio-connectionist model based in the thalamo-cortical neural circuit	In a previous study, we presented a program to simulate a particular dynamic of the thalamocortical biological system. The method used was called bio-connectionism which linked the thalanno-cortical mechanism reproduced with animal perception. In this presentation, a reinforcement learning program is supported by this mechanism. In a game world designed to test the model developed, the agent is assigned to a character that must learn by trial and error from its own experience upon recognition of aversive and appetitive patterns. The results confirm, support and extend the notion of configuration, a term familiar with sparse coding principles. If, as it is documented, this mechanism observed in sensory areas can be thought as condition of perception, the brain areas taken together each in its interaction with a respective sub-thalamic nucleus are suspected to be considered as condition of cognition. We introduce some philosophical questions derived from the experimental results in the discussion section. (C) 2016 Elsevier B.V. All rights reserved.	https://dx.doi.org/10.1016/j.bica.2016.03.001	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chan2019	Autonomous Imaging and Mapping of Small Bodies Using Deep Reinforcement Learning	The mapping and navigation around small unknown bodies continues to be an extremely interesting and exciting problem in the area of space exploration. Traditionally, the spacecraft trajectory for mapping missions is designed by human experts using hundreds of hours of human time to supervise the navigation and orbit selection process. While the current methodology has performed adequately for previous missions (such as Rosetta, Hayabusa and Deep Space), as the demands for mapping missions expand, additional autonomy during the mapping and navigation process will become necessary for mapping spacecraft. In this work we provide the framework for optimizing the autonomous imaging and mapping problem as a Partially Observable Markov Decision Process (POMDP). In addition, we introduce a new simulation environment which simulates the orbital mapping of small bodies and demonstrate that policies trained with our POMDP formulation are able to maximize map quality while autonomously selecting orbits and supervising imaging tasks. We conclude with a discussion of integrating Deep Reinforcement Learning modules with classic flight software systems, and some of the challenges that could be encountered when using Deep RL in flight-ready systems.	https://dx.doi.org/10.1109/AERO.2019.8742147	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chandrasekar2022	Hybrid Deep Learning Approach for Improved Network Connectivity in Wireless Sensor Networks	Wireless sensor networks occupy a prominent role in industrial as well as scientific applications. Lifetime enhancement and coverage are the major factors considered while designing the network. Various research models are evolved by considering the scheduling and routing process to solve the network lifetime issues. However, coverage and connectivity is another important factor that affects the lifetime of the remaining nodes. When a large number of sensors are deployed randomly, scheduling is preferred to enhance the network lifetime, but it leads to coverage issues. Other than scheduling, node damage, battery exhaustion, software and hardware failures might lead to coverage issues. Preserving the network connectivity while maximizing the network coverage is a crucial task in wireless sensor networks. To preserve the network connectivity and improve the wireless sensor networks coverage this research work presents a hybrid deep learning approach using a deep neural network and reinforcement learning algorithm. The Proposed model is experimentally verified and compared with conventional deep neural network and reinforcement learning algorithms to demonstrate the better balancing characteristics between network coverage and lifetime.	https://dx.doi.org/10.1007/s11277-022-10052-1	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chang2021	Accuracy vs. Efficiency: Achieving both Through Hardware-Aware Quantization and Reconfigurable Architecture with Mixed Precision	We propose a hardware/software co-design framework, which leverages hardware-aware quantization and a reconfigurable processor to improve the computational efficiency of convolutional neural networks (CNNs) on tiny IoT devices based on reconfigurable platforms. Firstly, we proposed a multi-objective optimization value function that can weigh accuracy, the size of CNN models, and computational delay, to improve the efficiency of the mixed- precision quantization algorithm based on deep reinforcement learning. Secondly, we propose a reconfigurable CNN processor that can adapt to the computing characteristics of various quantized CNN models, as well as a reconfigurable computing array and an on-chip elastic buffer, to improve the performance and computing efficiency on edge equipment. Finally, we demonstrate the effectiveness of the proposed co-design method through an extensive evaluation of the Ultra96-V2 platform. With respect to the well-known CNNs\emdashVGG-16, ResNet-50, and MobileNet-V2, the experimental result shows that the throughput of 216.6 GOPS, 214.0 GOPS, and 53.6 GOPS, the computing efficiency of 0.63GOPS/DSP, 0.64GOPS/DSP, and 0.24 GOPS/DSP, respectively. In addition, achieving a better optimized trade-off between the computing efficiency and accuracy compared with the recently proposed CNN processor with fixed bit-width and mixed-precision.	https://dx.doi.org/10.1109/ISPA-BDCloud-SocialCom-SustainCom52081.2021.00033	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chao2015	November 2015: This Month in JoVE	Here's a look at what's coming up in the November 2015 issue of JoVE: The Journal of Visualized Experiments. In JoVE Neuroscience, we know that fruit flies (Drosophila melanogaster) are a lot like humans in many ways-especially because they like their personal space. And in fruit flies, this preferred social distance can be measured using the social space assay. McNiel et al. demonstrate this straightforward protocol, which requires only simple equipment and experimental setups. Flies are blown into a social chamber and forced to form a tight group. Then they're allowed to take their preferred distance from one another. These distances are measured and processed with free online software (ImageJ). This social space assay provides a simple yet powerful paradigm for analyzing the underlying neurogenetics and environmental factors of social behavior. In JoVE Behavior, humans have a natural ability to acquire new motor skills, and this ability is crucial for upper limb amputees as they learn the complex control schemes for advanced multifunctional prosthetics. This month, Roche et al. present a case study of a structured rehabilitation method, which aims to improve multifunctional prosthetic control. Their subject underwent a structured protocol of imitation, repetition, and reinforcement learning. The subject demonstrated improvement in a widely used hand function test. This study suggests that a structured rehabilitation method may facilitate proficiency for multifunctional prosthetic control, and provides basis for larger clinical studies. Stress is a major concept in JoVE Behavior, and comprises various physiological responses to challenges. Among other responses, stress increases body temperature, which provides a quantitative measure of this response. However, the very act of measuring body temperature can be stressful to subjects, especially if they're wild animals. So Jerem et al. present a protocol for noninvasively measuring temperature in wild birds using infrared thermography. Their set-up is equipped with bird food and an infrared camera. This takes a thermal video of the bird before and after the researcher remotely closes the box, which acts as a mild acute stressor. The skin around the bird's eye is the warmest area in the image, and this protocol provides a time series of eye-region temperature with fine temporal resolution. With further validation, this method may prove valuable for studying the dynamics of the stress response for a wide range of researchers from environmental science to medicine. You've just had a sneak peek of the November 2015 issue of JoVE. Visit the website to see the full-length articles, plus many more, in JoVE: The Journal of Visualized Experiments.	https://dx.doi.org/10.3791/5758	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E5: Other not a paper	4
RL4SE	Chellaswamy2022	6-phase DFIG for wind energy conversion system: A hybrid approach	This research presents a novel wind power system based on a six-phase doubly-fed induction generator (DFIG). Optimization approaches are required to improve the efficiency of the traditional controllers. This study introduces a blended method for DFIG-based wind power transformation systems that combines quantum process and deep reinforcement learning (QPDRL) to improve control efficiency. It will be driven by using online control algorithms to eliminate the optimizing step and upgrade online control strategies. The proposed QPDRL can prevent local optimum solutions, forecast the future essential phase, and update DFIG-based wind power plants' regulation methods online. For two distinct scenarios, the QPDRL was contrasted with the proportional integral derivative (PID) controller, fractional-order PID, and reinforcement learning (for changeable air velocity, there are two types of arbitrary and step amplitudes). Matlab software was used to experiment. As air velocity variations exist, the findings revealed a 62\% reduction in the DC link voltage ripples and a 99\% reduction in speed overshoot with wind velocities overrun. Finally, comparing PID controls revealed a 42.15 percent reduction in grid current THD and an 11.38 percent reduction in the generator current.	https://dx.doi.org/10.1016/j.seta.2022.102497	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2020	Reinforcement Learning on Computational Resource Allocation of Cloud-based Wireless Networks	Wireless networks used for Internet of Things (IoT) are expected to largely involve cloud-based computing and processing. Softwarised and centralised signal processing and network switching in the cloud enables flexible network control and management. In a cloud environment, dynamic computational resource allocation is essential to save energy while maintaining the performance of the processes. The stochastic features of the Central Processing Unit (CPU) load variation as well as the possible complex parallelisation situations of the cloud processes makes the dynamic resource allocation an interesting research challenge. This paper models this dynamic computational resource allocation problem into a Markov Decision Process (MDP) and designs a model-based reinforcement-learning agent to optimise the dynamic resource allocation of the CPU usage. Value iteration method is used for the reinforcement-learning agent to pick up the optimal policy during the MDP. To evaluate our performance we analyse two types of processes that can be used in the cloud-based IoT networks with different levels of parallelisation capabilities, i.e., Software-Defined Radio (SDR) and Software-Defined Networking (SDN). The results show that our agent rapidly converges to the optimal policy, stably performs in different parameter settings, outperforms or at least equally performs compared to a baseline algorithm in energy savings for different scenarios.	https://dx.doi.org/10.1109/WF-IoT48130.2020.9221234	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2021	Grey-box Fuzzing With Deep Reinforcement Learning And Process Trace Back	Grey-box fuzzing, as a software testing technique, can find possible program bugs such as memory leaks, assertion failures and invalid input by generate random data then input it into a program, and monitor program exceptions. With program running, grey-box fuzzing collected the branch information in order to guide choosing next seeds. In this paper, we try to use the concept of Markov decision processes to formalize grey-box fuzzing as a deep reinforcement learning problem, and use process trace back(Intel Process Trace) to collect branch information in order to improve its efficiency toward binary programs. The experiments show this approach can outperform baseline random fuzzing and gain performance improvement.	https://dx.doi.org/10.1109/AEMCSE51986.2021.00238	Included	new_screen		4
RL4SE	Chen2019	Reinforcement-Learning-Based Test Program Generation for Software-Based Self-Test	Software-based Self-test (SBST) has been recognized as a promising complement to scan-based structural Built-in Self-test (BIST), especially for in-field self-test applications. In response to the ever-increasing complexities of the modern CPU designs, machine learning algorithms have been proposed to extract processor behavior from simulation data and help constrain ATPG to generate functionally-compatible patterns. However, these simulation-based approaches in general suffer sample inefficiency, i.e., only a small portion of the simulation traces are relevant to fault detection. Inspired by the recent advances in reinforcement learning (RL), we propose an RL-based test program generation technique for transition delay fault (TDF) detection. During the training process, knowledge learned from the simulation data is employed to tune the simulation policy; this close-loop approach significantly improves data efficiency, compared to previous open-loop approaches. Furthermore, RL is capable of dealing with delayed responses, which is common when executing processor instructions. Using the trained RL model, instruction sequences that bring the processor to the fault-sensitizing states, i.e., TDF test patterns, can be generated. The proposed test program generation technique is applied to a MIPS32 processor. For TDF, the fault coverage is 94.94\%, which is just 2.57\% less than the full-scan based approach.	https://dx.doi.org/10.1109/ATS47505.2019.00013	Included	new_screen		4
RL4SE	Chen2019a	Reinforcement-Learning Based Test Program Generation for Software-Based Self-Test	Software-based Self-test (SBST) has been recognized as a promising complement to scan-based structural Built-in Self-test (BIST), especially for in-field self-test applications. In response to the ever-increasing complexities of the modern CPU designs, machine learning algorithms have been proposed to extract processor behavior from simulation data and help constrain ATPG to generate functionally-compatible patterns. However, these simulation-based approaches in general suffer sample inefficiency, i.e., only a small portion of the simulation traces are relevant to fault detection. Inspired by the recent advances in reinforcement learning (RL), we propose an RL-based test program generation technique for transition delay fault (TDF) detection. During the training process, knowledge learned from the simulation data is employed to tune the simulation policy; this close-loop approach significantly improves data efficiency, compared to previous open-loop approaches. Furthermore, RL is capable of dealing with delayed responses, which is common when executing processor instructions. Using the trained RL model, instruction sequences that bring the processor to the fault-sensitizing states, i.e., TDF test patterns, can be generated. The proposed test program generation technique is applied to a MIPS32 processor. For TDF, the fault coverage is 94.94\%, which is just 2.57\% less than the full-scan based approach.	https://dx.doi.org/10.1109/ATS47505.2019.00013	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2021a	Traffic flow of connected and automated vehicles in Smart Cities: Human-Centric	This paper proposes a novel Connected and Automated Vehicle (CAV) model as a scanner for heterogeneous traffic flows, which employs CAV on the road to detect traffic flow characteristics in multiple traffic scenes through various sensors. The model contains the hardware platform and software algorithm of CAV, and the analysis of traffic flow detection and simulation by Flow Project from Mobile Sensing Lab at UC Berkeley and Amazon AWS Machine Learning research grants based on SUMO, where the driving of the car is mainly controlled by Reinforcement Learning (RL). The simulation results showed that the traffic flow scanning, tracking and data recording by CAV are continuous and effective for the wide range and identification confirm function when CAV are in one lane. The effective detection area of CAV is in bow shape in the heterogeneous traffic flow, the occlusion rate is not associated with the lane position of CAV. Therefore, the calculated results should be filtered and optimized to enhance the confidence of heterogeneous traffic data collected. Currently, standards or most practitioners are not aware of this.	https://dx.doi.org/10.1109/SWC50871.2021.00049	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2022	The Scanner of Heterogeneous Traffic Flow in Smart Cities by an Updating Model of Connected and Automated Vehicles	The problems of traditional traffic flow detection and calculation methods include limited traffic scenes, high system costs, and lower efficiency over detecting and calculating. Therefore, in this paper, we presented the updating Connected and Automated Vehicles (CAVs) model as the scanner of heterogeneous traffic flow, which uses various sensors to detect the characteristics of traffic flow in several traffic scenes on the roads. The model contains the hardware platform, software algorithm of CAV, and the analysis of traffic flow detection and simulation by Flow Project, where the driving of vehicles is mainly controlled by Reinforcement Learning (RL). Finally, the effectiveness of the proposed model and the corresponding swarm intelligence strategy is evaluated through simulation experiments. The results showed that the traffic flow scanning, tracking, and data recording performed continuously by CAVs are effective. The increase in the penetration rate of CAVs in the overall traffic flow has a significant effect on vehicle detection and identification. In addition, the vehicle occlusion rate is independent of the CAV lane position in all cases. The complete street scanner is a new technology that realizes the perception of the human settlement environment with the help of the Internet of Vehicles based on 5G communications and sensors. Although there are some shortcomings in the experiment, it still provides an experimental reference for the development of smart vehicles.	https://dx.doi.org/10.1109/TITS.2022.3165155	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2018	The Research on Adaptive Reinforcement Learning Technique Based on Convex Polyhedra Abstraction Domain		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048118328&doi=10.11897\%2fSP.J.1016.2018.00112&partnerID=40&md5=9344f17a7113afbe67927bc811137d33	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2022a	LAMP: Load-Balanced Multipath Parallel Transmission in Point-to-Point NoCs	Network-on-chip (NoC) is an emerging paradigm that is able to connect a significant amount of processing elements (PEs). However, as a distributed subsystem, NoC resources have not been exploited to the fullest. Multipath parallel transmission, which splits one message into multiple parts and sends them simultaneously, shows its efficiency in utilizing NoC resources and further reducing the transmission latency. However, this method is not fully optimized in previous works, especially for emerging point-to-point NoCs due to the following reasons: 1) only limited shortest paths are chosen; 2) static message splitting strategy without considering NoC utilization state increases contentions; and 3) the optimization of hardware that supports multipath parallel transmission is missing, resulting in additional overheads. Thus, we propose LAMP, a software and hardware collaborated design to efficiently utilize resources and reduce latency in point-to-point NoCs through the load-balanced multipath parallel transmission. Specifically, we propose a reinforcement learning-based algorithm to decide when and how to split messages, and which path should be used according to traffic loads. Also, the temporal and spatial load-balancing algorithms are proposed so that the message size is adjusted properly to utilize NoC resources. Moreover, we revise the hardware design to support multipath parallel transmission efficiently. Extensive experiments show that our algorithm achieves a remarkable performance improvement (+18.0\% to +29.9\%) when compared with the state-of-the-art dual-path algorithm. Our hardware design decreases power and area consumption by 23.2\% and 10.3\% over the dual-path hardware.	https://dx.doi.org/10.1109/TCAD.2022.3151021	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2022b	Neurotrie: Deep Reinforcement Learning-based Fast Software IPv6 Lookup	IPv6 has shown notable growth in recent years, imposing the need for high-speed IPv6 lookup. As the forwarding rate of virtual switches continues increasing, software-based IPv6 lookup without using special hardware such as TCAM, GPU, and FPGA is of academic interest and industrial importance. Existing studies achieve fast software IPv4 lookup by reducing the operation number, as well as reducing the memory footprint so as to benefit from CPU cache. However, in the situation of 128-bit IPv6 addresses, it is challenging to keep both operation numbers and memory footprints small. To address the issue, we propose the Neurotrie data structure, which supports fast lookup and arbitrary strides. Thus, a good balance can be made between trie depth and memory footprint by computing the proper stride for each Neurotrie node. We model the optimal Neurotrie problem which minimizes the depth with limited memory footprint and develop a pseudo-polynomial time baseline algorithm to construct Neurotrie using dynamic programming. To improve the performance and reduce the computation complexity, we develop a deep reinforcement learning-based approach, which leverages a deep neural network to construct Neurotrie efficiently, based on characteristics captured from real IPv6 prefixes. We further refine the data structure and develop an efficient mechanism for routing updates. Experiments on real routing tables show that Neurotrie achieves a lookup rate 34\% higher than that of state-of-the-art approaches.	https://dx.doi.org/10.1109/ICDCS54860.2022.00093	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2021b	Parallel Multipath Transmission for Burst Traffic Optimization in Point-to-Point NoCs		https://doi.org/10.1145/3453688.3461521	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2018a	A Q-learning-based network content caching method	Cloud computing provides users with a distributed computing environment offering on-demand services. As its technologies become gradually mature and its application becomes more universal, cloud computing greatly reduces users' costs while increasing working efficiency of enterprises and individuals (Futur Gener Comput Syst 25:599-616, 2009). Software as a service (SaaS), as a kind of information servicing model based on cloud platforms, is rising with the developments of Internet technologies and the maturing of application software. The responsibility of a SaaS server is to timely and accurately satisfy users' needs for information. An intelligent and efficient content caching solution or method plays a vital role in that. This paper proposes a reinforcement learning (RL)-based content caching method named time-based Q Cacher (TQC) which effectively solves the problem of low hit ratio of server caching and ultimately achieves an intelligent, flexible, and highly adaptable content caching model.	https://dx.doi.org/10.1186/s13638-018-1268-1	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2020a	QMORA: A Q-Learning based Multi-objective Resource Allocation Scheme for NFV Orchestration	To satisfy the various quality-of-service (QoS) re-quirements with minimum network costs, network functions virtualization (NFV) is proposed as an emerging wireless architecture that migrates network functions from dedicated hardware appliances to software instances running in virtual computing platforms. One crucial issue in NFV is to solve the orchestration of virtualized network functions (VNFs) to reduce costs and to improve the management flexibility of telecommunications service providers (TSPs). Particular, multiple objectives are required to be considered for orchestrating VNFs in order to achieve overall system performance. This can be optimally solved in small scale using integer linear programming (ILP) algorithms with high accuracy but low time efficiency. On the other hand, heuristic algorithms can be applied for solving part of the objectives in NFV resource allocation with high time efficiency but low accuracy. To tackle the above challenges, QMORA, a ${Q}$-learning based multi-objective resource allocation approach, is proposed to solve multi-objective optimization in NFV orchestration (NFVO) efficiently and accurately. Particularly, the approach includes reinforcement learning module and VNFs placement module. Reinforcement learning module is responsible for generating the ``best'' candidate paths. VNFs placement module is responsible for selecting optimal nodes on the generated candidate paths to host VNFs required for flows. The simulation results in the real ISP topology show that the proposed QMORA can balance the multi-objective including maximizing number of flows admitted to the network, minimizing path stretch, balancing the load among VNF instances and minimizing link occupation rate compared with other heuristic approaches.	https://dx.doi.org/10.1109/VTC2020-Spring48590.2020.9128963	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2020b	Enhanced Compiler Bug Isolation via Memoized Search	Compiler bugs can be disastrous since they could affect all the software systems built on the buggy compilers. Meanwhile, diagnosing compiler bugs is extremely challenging since usually limited debugging information is available and a large number of compiler files can be suspicious. More specifically, when compiling a given bug-triggering test program, hundreds of compiler files are usually involved, and can all be treated as suspicious buggy files. To facilitate compiler debugging, in this paper we propose the first reinforcement compiler bug isolation approach via structural mutation, called RecBi. For a given bug-triggering test program, RecBi first augments traditional local mutation operators with structural ones to transform it into a set of passing test programs. Since not all the passing test programs can help isolate compiler bugs effectively, RecBi further leverages reinforcement learning to intelligently guide the process of passing test program generation. Then, RecBi ranks all the suspicious files by analyzing the compiler execution traces of the generated passing test programs and the given failing test program following the practice of compiler bug isolation. The experimental results on 120 real bugs from two most popular C open-source compilers, i.e., GCC and LLVM, show that RecBi is able to isolate about 23\%/58\%/78\% bugs within Top-l/Top-5/Top-10 compiler files, and significantly outperforms the state-of-the-art compiler bug isolation approach by improving 92.86\%/55.56\%/25.68\% isolation effectiveness in terms of Top-l/Top-5/Top-10 results.		Included	new_screen		4
RL4SE	Chen2019b	Relational Verification using Reinforcement Learning	Relational verification aims to prove properties that relate a pair of programs or two different runs of the same program. While relational properties (e.g., equivalence, non-interference) can be verified by reducing them to standard safety, there are typically many possible reduction strategies, only some of which result in successful automated verification. Motivated by this problem, we propose a new relational verification algorithm that learns useful reduction strategies using reinforcement learning. Specifically, we show how to formulate relational verification as a Markov decision process (MDP) and use reinforcement learning to synthesize an optimal policy for the underlying MDP. The learned policy is then used to guide the search for a successful verification strategy. We have implemented this approach in a tool called COEUS and evaluate it on two benchmark suites. Our evaluation shows that COEUS solves significantly more problems within a given time limit compared to multiple baselines, including two state-of-the-art relational verification tools.	https://dx.doi.org/10.1145/3360567	Included	conflict_resolution		4
RL4SE	Chen2022c	Gait Parameter Optimization of Quadruped Robot Under Energy Consumption Index Based on Reinforcement Learning	In this paper, the gait optimization of quadruped bionic robot is studied under the unified energy consumption index. Firstly, an energy consumption index of quadruped robot is established. Secondly, the reinforcement learning method is used to optimize the gait parameters of the quadruped robot, so that the quadruped robot can gradually find the gait parameter combination with the lowest energy consumption in the current state in the interaction with the environment. In order to verify the effectiveness of this method, this paper completes the optimization of gait parameters combined with MIT cheetah software and DDQN network.	https://dx.doi.org/10.1109/ICARM54641.2022.9959165	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2021c	Urban Parking Scheme in Hangzhou Based on Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101723706&doi=10.1088\%2f1755-1315\%2f638\%2f1\%2f012002&partnerID=40&md5=548cf506f8f47341509f4f3b8ee7fe9e	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2021d	Software-in-the-Loop Combined Reinforcement Learning Method for Dynamic Response Analysis of FOWTs	Floating offshore wind turbines (FOWTs) still face many challenges on how to better predict the dynamic responses. Artificial intelligence (AI) brings a new solution to overcome these challenges with intelligent strategies. A new AI technology-based method, named SADA, is proposed in this paper for the prediction of dynamic responses of FOWTs. Firstly, the methodology of SADA is introduced with the selection of Key Disciplinary Parameters (KDPs). The AI module in SADA was built in a coupled aero-hydro-servo-elastic in-house program DARwind and the policy decision is provided by the machine learning algorithms deep deterministic policy gradient (DDPG). Secondly, a set of basin experimental results of a Hywind Spar-type FOWT were employed to train the AI module. SADA weights KDPs by DDPG algorithms' actor network and changes their values according to the training feedback of 6DOF motions of Hywind platform through comparing the DARwind simulation results and that of experimental data. Many other dynamic responses that cannot be measured in basin experiment could be predicted in higher accuracy with this intelligent DARwind. Finally, the case study of SADA method was conducted and the results demonstrated that the mean values of the platform's motions can be predicted by AI-based DARwind with higher accuracy, for example the maximum error of surge motion is reduced by 21\%. This proposed SADA method takes advantage of numerical-experimental method and the machine learning method, which brings a new and promising solution for overcoming the handicap impeding direct use of traditional basin experimental technology in FOWTs design.	https://dx.doi.org/10.3389/fmars.2020.628225	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2021e	Software-in-the-Loop Combined Machine Learning for Dynamic Responses Analysis of Floating Offshore Wind Turbines	Artificial intelligence (AI) brings a new solution to overcome the challenges of Floating offshore wind turbines (FOWTs) to better predict the dynamic responses with intelligent strategies. A new AI-based software-in-the-loop method, named SADA is introduced in this paper for the prediction of dynamic responses of FOWTs, which is proposed based on an in-house programme DARwind. DARwind is a coupled aero-hydro-servo-elastic in-house program for FOWTs, and a reinforcement learning method with exhaust algorithm and deep deterministic policy gradient (DDPG) are embedded in DARwind as an AI module. Firstly, the methodology is introduced with the selection of Key Disciplinary Parameters (KDPs). Secondly, Brute-force Method and DDPG algorithms are adopted to changes the KDPs' values according to the feedback of 6DOF motions of Hywind Spar-type platform through comparing the DARwind simulation results and those of basin experimental data. Therefore, many other dynamic responses that cannot be measured in basin experiment can be predicted in good accuracy with SADA method. Finally, the case study of SADA method was conducted and the results demonstrated that the mean values of the platform's motions can be predicted with higher accuracy. This proposed SADA method takes advantage of numerical-experimental method, basin experimental data and the machine learning technology, which brings a new and promising solution for overcoming the handicap impeding direct use of conventional basin experimental way to analyze FOWT's dynamic responses during the design phase.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2019c	An Adaptive Control Method for Arterial Signal Coordination Based on Deep Reinforcement Learning	Traffic signal control is a complex problem and it is difficult to determine an optimal strategy to control multi-directional traffic at multiple intersections. Recent years have witnessed numerous successes of deep learning neural networks in the fields of artificial intelligence. Motivated by the dominant performance of neural networks, this study attempts to develop a novel adaptive signal control approach by fusing deep learning (DL) and reinforcement learning (RL), i.e., deep reinforcement learning (DRL), for arterial signal coordination. DRL can considerably improve the ability to deal with large amounts of data processing, systematic perception and expression, which is key to coordinated control of arterial intersections. The proposed algorithm is implemented by utilizing real-time traffic detection data and aims to optimize the hybrid global and local reward functions. The experimental results obtained by traffic simulation software SUMO demonstrate the advantage of the proposed approach, as well as its efficiency and effectiveness compared with fixed-time and actual signal control methods.	https://dx.doi.org/10.1109/ITSC.2019.8917051	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2019d	An Adaptive Control Method for Arterial Signal Coordination Based on Deep Reinforcement Learning<sup>*</sup>		https://doi.org/10.1109/ITSC.2019.8917051	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2011	On optimizing vehicular dynamic spectrum access networks: Automation and learning in mobile wireless environments	In this paper, we propose a novel architecture for optimizing the overall performance of vehicular dynamic spectrum access (VDSA) networks. Due to the high level of mobility for vehicles operating under highway conditions, coupled with spatially variant spectrum allocation across a large geographical region, we envision that future vehicular communications will employ a form of dynamic spectrum access (DSA) in order to facilitate wireless transmissions between vehicles and with roadside infrastructure. In particular, the VDSA concept will be enabled by a combination of software-defined radio (SDR) technology, spectral occupancy databases, and machine learning techniques for enabling network automation. A vehicular networking scenario is substantially different relative to a generic mobile scenario with respect to the high level of mobility involved, the predictable trajectories of the vehicular traffic, and the overall scale of the network range. Consequently, the proposed architecture is designed to enable VDSA in a more flexible wireless spectrum environment by leveraging the cognitive radio concept and existing wireless spectrum databases actively being developed while simultaneously being compatible with current spectrum regulations. Regarding practical issues for vehicular communications, vehicle mobility is taken into account in order to ensure primary user protection, databases and channel priority schemes are used in order to record temporal and spatial channel heterogeneity, and vehicle path prediction techniques are employed in order to enhance channel access in this operating environment. Specifically, we show the advantages of employing the proposed learning architecture via a case study where reinforcement learning is used in order to achieve intelligent channel selection within a realistic VDSA environment. Moreover, performance enhancements in terms of channel switching times, interference, and throughput are shown via computer simulations.	https://dx.doi.org/10.1109/VNC.2011.6117122	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2021f	User Preference-Based Demand Response for Smart Home Energy Management Using Multiobjective Reinforcement Learning	A well-designed demand response (DR) program is essential in smart home to optimize energy usage according to user preferences. In this study, we proposed a multiobjective reinforcement learning (MORL) algorithm to design a DR program. The proposed approach improved conventional algorithms by mitigating the effect of the change in user preferences and addressed the uncertainty induced by future price and renewable energy generation. Because two Q-tables were used, the proposed algorithm simultaneously considers electricity cost and user dissatisfaction; when user preference changes, the proposed MORL algorithm uses the previous experience to customize appliances' scheduling and swiftly achieve the best objective value. The generalizability of the proposed algorithm is high. Therefore, the algorithm can be implemented in a smart home equipped with an energy storage system, renewable energy source, and various types of appliances such as inflexible, time-flexible, and power-flexible ones. Numerical analysis using real-world data revealed that in case of price and renewable uncertainty, the proposed approach can deliver excellent performance after a change of user preference; it achieved 8.44\% cost reduction as compared with mixed-integer nonlinear programming based DR while increasing the dissatisfaction level only by 1.37\% on average.	https://dx.doi.org/10.1109/ACCESS.2021.3132962	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2021g	Optimal demand response strategy of commercial building-based virtual power plant using reinforcement learning	In this paper, the optimal demand response strategy of a commercial building-based virtual power plant with real-world implementation in heavily urbanised area is studied. Instead of modelling the decision-making process as an optimisation problem, a reinforcement learning method is used to seek the optimal strategy, which could update its performance with minimal manpower manipulation. Specifically, the data collection from several commercial buildings, including hotel, shopping mall and office, in Huangpu district, Shanghai city is analysed to deploy the demand response program. Compared with the conventional demand response strategy based on optimisation, the learnt strategy does not rely on the forecasting information as input and could adapt to the changing demand response incentive automatically. It may not produce the best result every time, but can guarantee the benefit in a non-deterministic way in long-term operation. The real-world deployment of the Huangpu virtual power plant involving hardware and software platform is also introduced, as well as its future development projection.	https://dx.doi.org/10.1049/gtd2.12179	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2013	Ketamine use among regular tobacco and alcohol users as revealed by respondent-driven sampling in Taipei: Prevalence, expectancy, and users' risky decision making	The popularity of ketamine for recreational use among young people began to increase, particularly in Asia, in 2000. To gain more knowledge about the use of ketamine among high-risk individuals, a respondent-driven sampling (RDS) was implemented among regular alcohol and tobacco users in the Taipei metropolitan area from 2007 to 2010. The sampling was initiated in three different settings (i.e., 2 in the community and 1 in a clinic) to recruit seed individuals. Each participant was asked to refer one to five friends known to be regular tobacco smokers and alcohol drinkers to participate in the present study. Incentives were offered differentially upon the completion of an interview and successful referral. Information pertaining to drug use experience was collected by an audio computer-assisted self-interview instrument. Software built for RDS analyses was used for data analyses. Of the 1,115 participants recruited, about 11.7\% of the RDS respondents reported ever having used ketamine. Positive expectancy of ketamine use was positively associated with ketamine use; by contrast, negative expectancy was inversely associated with ketamine use. Decision-making characteristics as measured on the Iowa Gambling Task (IGT) using reinforcement learning models revealed that ketamine users learned less from the most recent event than both tobacco- and drug-naive controls and regular tobacco and alcohol users. These findings about ketamine use among young people have implications for its prevention and intervention. Copyright (C) 2013, Food and Drug Administration, Taiwan. Published by Elsevier Taiwan LLC. All rights reserved.	https://www.ncbi.nlm.nih.gov/pubmed/25264412	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Chen2018b	Reinforcement learningendashbased QoS/QoE-aware service function chaining in software-driven 5G slices		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054060336&doi=10.1002\%2fett.3477&partnerID=40&md5=b832b11a85d46c908ca5658dbf093a00	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2018c	Reinforcement learningendashbased QoS/QoE?aware service function chaining in software?driven 5G slices		https://doi.org/10.1002/ett.3477	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2018d	Reinforcement learning-based QoS/QoE-aware service function chaining in software-driven 5G slices	With the ever-growing diversity of devices and applications that will be connected to 5G networks, flexible and agile service orchestration with acknowledged quality of experience (QoE) that satisfies the end user's functional and quality-of-service (QoS) requirements is necessary. Software-defined networking (SDN) and network function virtualization (NFV) are considered key enabling technologies for 5G core networks. In this regard, this paper proposes a reinforcement learning-based QoS/QoE-aware service function chaining (SFC) scheme in SDN/NFV-enabled 5G slices. First, it implements a lightweight QoS information collector based on the Link Layer Discovery Protocol, which works in a piggyback fashion on the southbound interface of the SDN controller, to enable QoS-awareness. Then, a deep Q-network-based orchestration agent is designed to support SFC in the context of NFV. The agent takes into account the QoE and QoS as key aspects to formulate the reward so that it is expected to maximize QoE while respecting QoS constraints. The experiment results show that the proposed framework exhibits good performance in QoE provisioning and QoS requirements maintenance for SFC in dynamic network environments.	https://dx.doi.org/10.1002/ett.3477	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2018e	Towards synthesizing complex programs from input-output examples			Included	new_screen		4
RL4SE	Chen2018f	Human-like longitudinal velocity control based on continuous reinforcement learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044201074&doi=10.1061\%2f9780784480915.100&partnerID=40&md5=d58324d107f48b0ab009ebb1e7a67b97	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2022d	Resource Allocation with Workload-Time Windows for Cloud-Based Software Services: A Deep Reinforcement Learning Approach	As the workloads and service requests in cloud computing environments change constantly, cloud-based software services need to adaptively allocate resources for ensuring the Quality-of-Service (QoS) while reducing resource costs. However, it is very challenging to achieve adaptive resource allocation for cloud-based software services with complex and variable system states. Most of the existing methods only consider the current condition of workloads, and thus cannot well adapt to real-world cloud environments subject to fluctuating workloads. To address this challenge, we propose a novel Deep Reinforcement learning based resource Allocation method with workload-time Windows (DRAW) for cloud-based software services that considers both the current and future workloads in the resource allocation process. Specifically, an original Deep Q-Network (DQN) based prediction model of management operations is trained based on workload-time windows, which can be used to predict appropriate management operations under different system states. Next, a new feedback-control mechanism is designed to construct the objective resource allocation plan under the current system state through iterative execution of management operations. Extensive simulation results demonstrate that the prediction accuracy of management operations generated by the proposed DRAW method can reach 90.69\%. Moreover, the DRAW can achieve the optimal/near-optimal performance and outperform other classic methods by 3~13\% under different scenarios.	https://dx.doi.org/10.1109/TCC.2022.3169157	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2022e	Study on numerical simulation of complex wave based on deep reinforcement learning method			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2022f	Resource Allocation for Cloud-Based Software Services Using Prediction-Enabled Feedback Control With Reinforcement Learning	With time-varying workloads and service requests, cloud-based software services necessitate adaptive resource allocation for guaranteeing Quality-of-Service (QoS) and reducing resource costs. However, due to the ever-changing system states, resource allocation for cloud-based software services faces huge challenges in dynamics and complexity. The traditional approaches mostly rely on expert knowledge or numerous iterations, which might lead to weak adaptiveness and extra costs. Moreover, existing RL-based methods target the environment with the fixed workload, and thus they are unable to effectively fit in the real-world scenarios with variable workloads. To address these important challenges, we propose a Prediction-enabled feedback Control with Reinforcement learning based resource Allocation (PCRA) method. First, a novel Q-value prediction model is designed to predict the values of management operations (by Q-values) at different system states. The model uses multiple prediction learners for making accurate Q-value prediction by integrating the Q-learning algorithm. Next, the objective resource allocation plans can be found by using a new feedback-control based decision-making algorithm. Using the RUBiS benchmark, simulation results demonstrate that the PCRA chooses the management operations of resource allocation with 93.7 percent correctness. Moreover, the PCRA achieves optimal/near-optimal performance, and it outperforms the classic ML-based and rule-based methods by 5$\sim$?7\% and 10$\sim$?13\%, respectively.	https://dx.doi.org/10.1109/TCC.2020.2992537	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2018g	DQN-Based Power Control for IoT Transmission against Jamming	Internet of Things (IoTs) have to address jammers, with goal to interrupt the communication of the energy- constrained IoT devices and sometimes even cause denial-of-service attacks. In this paper, we propose a deep reinforcement learning based power control scheme for IoT devices to improve the transmission efficiency and save energy. This scheme depends on the current IoT transmission status and the jamming strength and applies deep Q-network (DQN) to determine the transmit power without being aware of the IoT topology and the jamming model. This scheme is implemented on the universal software radio peripherals for the anti- jamming communication performance evaluation. Experimental results show that this scheme improves the signal-to-interference-plus-noise of the IoT signals compared with the benchmark Q-learning based power control scheme against jamming.	https://dx.doi.org/10.1109/VTCSpring.2018.8417695	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2007	Genetic network programming with sarsa learning and its application to creating stock trading rules		https://www.scopus.com/inward/record.uri?eid=2-s2.0-55749086694&doi=10.1109\%2fCEC.2007.4424475&partnerID=40&md5=4c30d9a4fb58ed219a3a53bb50f142ab	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2007a	Trading rules on stock markets using genetic network programming with sarsa learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548070518&doi=10.1145\%2f1276958.1277232&partnerID=40&md5=98b21b20f0b081312b620ba7ed484eaa	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2008	Real Time Updating Genetic Network Programming for adapting to the change of stock prices		https://www.scopus.com/inward/record.uri?eid=2-s2.0-55749084369&doi=10.1109\%2fCEC.2008.4630824&partnerID=40&md5=8488425567a922dcc588bdef450ffffd	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2020c	Program Synthesis Using Deduction-Guided Reinforcement Learning		https://doi.org/10.1007/978-3-030-53291-8_30	Included	new_screen		4
RL4SE	Chen2021h	Demand and Capacity Balancing Technology Based on Multi-agent Reinforcement Learning	To effectively solve Demand and Capacity Balancing (DCB) in large-scale and high-density scenarios through the Ground Delay Program (GDP) in the pre-tactical stage, a sequential decision-making framework based on a time window is proposed. On this basis, the problem is transformed into Markov Decision Process (MDP) based on local observation, and then Multi-Agent Reinforcement Learning (MARL) method is adopted. Each flight is regarded as an independent agent to decide whether to implement GDP according to its local state observation. By designing the reward function in multiple combinations, a Mixed Competition and Cooperation (MCC) mode considering fairness is formed among agents. To improve the efficiency of MARL, we use the double Q-Learning Network (DQN), experience replay technology, adaptive ?-greedy strategy and Decentralized Training with Decentralized Execution (DTDE) framework. The experimental results show that the training process of the MARL method is convergent, efficient and stable. Compared with the Computer-Assisted Slot Allocation (CASA) method used in the actual operation, the number of flight delays and the average delay time is reduced by 33.7\% and 36.7\% respectively.	https://dx.doi.org/10.1109/DASC52595.2021.9594343	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2022g	Smart scheduler: an adaptive NVM-aware thread scheduling approach on NUMA systems	NVM provides large memory capacity, long-term data durability, and high memory bandwidth for multi-thread applications on cloud servers. Nowadays, cloud servers often employ NUMA architecture, where the thread scheduling mechanism plays a vital role in overall system performance because of the NUMA property. However, with the increase in server resources' diversity, i.e., hybrid memory systems using DRAM and NVM on NUMA nodes, the exploration space for thread scheduling is expanding rapidly. Unfortunately, the existing thread schedulers, including rule-based algorithms and scheduling domain methods, cannot provide ideal scheduling solutions in such complicated cases. And, those thread schedulers neglect customized heterogeneous memory structures, thus degrading overall system performance. Fortunately, reinforcement learning can choose actions with maximum rewards values in a specific environment, leading the scheduler towards an optimal solution. In this paper, we propose a thread scheduling approach, i.e., Smart Scheduler, by leveraging a reinforcement learning method. Smart Scheduler takes OS event information as input, extends LinUCB to explore the scheduling space, and guides thread-level scheduling. We evaluate Smart Scheduler on the off-the-shelf server equipped with NVM. The experimental results show that the proposed Smart Scheduler can converge faster (usually within 20 actions) than rule-based algorithms and scheduling domain methods and reduce program execution time by up to 59.9\%. It also outperforms rule-based algorithms and scheduling domain methods by 4.1\% and 19.1\% in quality of service latency.	https://dx.doi.org/10.1007/s42514-022-00110-2	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2017	Lifelong Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141899245&doi=10.1007\%2f978-3-031-01575-5_6&partnerID=40&md5=90bb7f41a27f27278b6a4aeb9499a67c	Excluded	new_screen	E5: Other not a paper,E5: Other not a paper	4
RL4SE	Cheng2019	Traffic Distribution Algorithm Based on Multi-Agent Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084285103&doi=10.13190\%2fj.jbupt.2019-140&partnerID=40&md5=0420e3d67092ad333a7bfb974801ec98	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cherubini2010	Policy gradient learning for quadruped soccer robots		https://doi.org/10.1016/j.robot.2010.03.008	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chetty2020	Virtual Network Function Embedding under Nodal Outage using Reinforcement Learning	With the emergence of various types of applications such as delay-sensitive applications, future communication networks are expected to be increasingly complex and dynamic. Network Function Virtualization (NFV) provides the necessary support towards efficient management of such complex networks, by disintegrating the dependency on the hardware devices via virtualizing the network functions and placing them on shared data centres. However, one of the main challenges of the NFV paradigm is the resource allocation problem which is known as NFV-Resource Allocation (NFV-RA). NFV-RA is a method of deploying software-based network functions on the substrate nodes, subject to the constraints imposed by the underlying infrastructure and the agreed Service Level Agreement (SLA). This work investigates the potential of Reinforcement Learning (RL) as a fast yet accurate means (as compared to integer linear programming) for deploying the softwarized network functions onto substrate networks under several Quality of Service (QoS) constraints. In addition to the regular resource constraints and latency constraints, we introduced the concept of a complete outage of certain nodes in the network. This outage can be either due to a disaster or unavailability of network topology information due to proprietary and ownership issues. We have analyzed the network performance on different network topologies, different capacities of the nodes and the links, and different degrees of the nodal outage. The computational time escalated with the increase in the network density to achieve the optimal solutions; this is because Q-Learning is an iterative process which results in a slow exploration. Our results also show that for certain topologies and a certain combination of resources, we can achieve between 7090\% service acceptance rate even with a 40\% nodal outage.	https://dx.doi.org/10.1109/ANTS50601.2020.9342803	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chien2020	Q-learning based collaborative cache allocation in mobile edge computing		https://doi.org/10.1016/j.future.2019.08.032	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chinchali2018	Cellular Network Traffic Scheduling with Deep Reinforcement Learning	"Modern mobile networks are facing unprecedented growth in demand due to a new class of traffic from Internet of Things (IoT) devices such as smart wearables and autonomous cars. Future networks must schedule delay-tolerant software updates, data backup, and other transfers from IoT devices while maintaining strict service guarantees for conventional real-time applications such as voice-calling and video. This problem is extremely challenging because conventional traffic is highly dynamic across space and time, so its performance is significantly impacted if all IoT traffic is scheduled immediately when it originates. In this paper, we present a reinforcement learning (RL) based scheduler that can dynamically adapt to traffic variation, and to various reward functions set by network operators, to optimally schedule IoT traffic. Using 4 weeks of real network data from downtown Melbourne, Australia spanning diverse traffic patterns, we demonstrate that our RI. scheduler can enable mobile networks to carry 14.7\% more data with minimal impact on existing traffic, and outperforms heuristic schedulers by more than 2x Our work is a valuable step towards designing autonomous, ""self-driving"" networks that learn to manage themselves from past data."		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chishti2018	Self-Driving Cars Using CNN and Q-Learning	DrivingMatter is an experiment carried out to understand the deeper side of an autonomous car. In 1900s, idea was to drive car on Moon from Earth. This was initial motivation which grew from there and now expanding to complex system of roads in the real world. A book-sized Raspberry Pi based autonomous car is built to carry out the experiment on hardware. Software side was accomplished by developing a Python based library for controlling and communicating with car over a network or locally within the car. For environment learning two methodologies are practiced; Supervised learning: Drove the car on an environment/road and collected 3, 000+ data-points. Based on this a CNN model was trained which achieved 73 \% test 89 \% train accuracy. Reinforcement learning: Car is trained for three different road signs; Stop, No left, and Traffic light using DQN with existing CNN model. These road signs are detected in the environment using OpenCV cascade classifiers.	https://dx.doi.org/10.1109/INMIC.2018.8595684	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Choe2007	Autonomous learning of the semantics of internal sensory states based on motor exploration		https://www.scopus.com/inward/record.uri?eid=2-s2.0-34447330416&doi=10.1142\%2fS0219843607001102&partnerID=40&md5=aaecb5e87678e124e3e348119fcd0ce3	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Choi2020	Reinforcement Learning for Safety-Critical Control under Model Uncertainty, using Control Lyapunov Functions and Control Barrier Functions	In this paper, the issue of model uncertainty in safety-critical control is addressed with a data-driven approach. For this purpose, we utilize the structure of an input-ouput linearization controller based on a nominal model along with a Control Barrier Function and Control Lyapunov Function based Quadratic Program (CBF-CLF-QP). Specifically, we propose a novel reinforcement learning framework which learns the model uncertainty present in the CBF and CLF constraints, as well as other control-affine dynamic constraints in the quadratic program. The trained policy is combined with the nominal model based CBF-CLF-QP, resulting in the Reinforcement Learning-based CBF-CLF-QP (RL-CBF-CLF-QP), which addresses the problem of model uncertainty in the safety constraints. The performance of the proposed method is validated by testing it on an underactuated nonlinear bipedal robot walking on randomly spaced stepping stones with one step preview, obtaining stable and safe walking under model uncertainty.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chouaki2022	Implementing reinforcement learning for on-demand vehicle rebalancing in MATSim		https://doi.org/10.1016/j.procs.2022.03.020	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Choy2006	Neural Networks for Continuous Online Learning and Control	This paper proposes a new hybrid neural network (NN) model that employs a multistage online learning process to solve the distributed control problem with an infinite horizon. Various techniques such as reinforcement learning and evolutionary algorithm are used to design the multistage online learning process. For this paper, the infinite horizon distributed control problem is implemented in the form of real-time distributed traffic signal control for intersections in a large-scale traffic network. The hybrid neural network model is used to design each of the local traffic signal controllers at the respective intersections. As the state of the traffic network changes due to random fluctuation of traffic volumes, the NN-based local controllers will need to adapt to the changing dynamics in order to provide effective traffic signal control and to prevent the traffic network from becoming overcongested. Such a problem is especially challenging if the local controllers are used for an infinite horizon problem where online learning has to take place continuously once the controllers are implemented into the traffic network. A comprehensive simulation model of a section of the Central Business District (CBD) of Singapore has been developed using PARAMICS microscopic simulation program. As the complexity of the simulation increases, results show that the hybrid NN model provides significant improvement in traffic conditions when evaluated against an existing traffic signal control algorithm as well as a new, continuously updated simultaneous perturbation stochastic approximation-based neural network (SPSA-NN). Using the hybrid NN model, the total mean delay of each vehicle has been reduced by 78\% and the total mean stoppage time of each vehicle has been reduced by 84\% compared to the existing traffic signal control algorithm. This shows the efficacy of the hybrid NN model in solving large-scale traffic signal control problem in a distributed manner. Also, it indicates the possibility of using the hybrid NN model for other applications that are similar in nature as the infinite horizon distributed control problem	https://www.ncbi.nlm.nih.gov/pubmed/17131665	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Christensen2020	Demand Response through Price-setting Multi-agent Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097564993&doi=10.1145\%2f3427773.3427862&partnerID=40&md5=859c18585aa6799a3712f883f81f96cc	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chung2022	Fast DNN-based Mechatronics Prototyping Platform on Robotic Arm Control	In industrial applications, a robotic controller re-quires a low-latency computation process for real-time con-straints. In the meantime, more controllers are designed with DNN-based reinforcement learning, which needs increasing computation power. In this demo, we developed a fast prototyping infrastructure in AI -based mechatronics. Our software/hardware co-optimization incorporates a cyber-physical system (CPS), a host computer, and a DNN-based accelerator on an FPGA. The holistic accelerator is built upon the ESP SoC (System-on-Chip) platform with the high-level synthesis (HLS) technique and an improved interface. Our demonstration on an intelligent robotic arm showcases 101 times speedup over a CPU-based software implementation.	https://dx.doi.org/10.1109/AICAS54282.2022.9869932	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chuyen2021	Reinforcement learning-based method for autonomous navigation of mobile robots in unknown environments: An experimental demonstration		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120080704&doi=10.1504\%2fijamechs.2021.119117&partnerID=40&md5=3c9a5f7f4797ef24ec38c694eccdeed7	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Clark2020	Language Support for Multi Agent Reinforcement Learning		https://doi.org/10.1145/3385032.3385041	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Coelho2009	How much management is management enough? Providing monitoring processes with online adaptation and learning capability	Recent investigations of management traffic patterns in production networks suggest that just a small and static set of management data tends to be used, the flow of management data is relatively constant, and the operations in use for manager-agent communication are reduced to a few, sometimes obsolete set. This is an indication of lack of progress of monitoring processes, taking into account their strategic role and potential, for example, to anticipate and prevent faults, performance bottlenecks, and security problems. One of the main reasons for such limitation relies on the fact that operators, who still are a fundamental element of the monitoring control loop, can no longer handle the rapidly increasing size and heterogeneity of both hardware and software components that comprise modern networked computing systems. This form of human-in-the-loop management certainly hampers timely adaptation of monitoring processes. To tackle this issue, this paper presents a model, inspired by the reinforcement learning theory, for adaptive network, service and application monitoring. The model is instantiated through a prototypical implementation of an autonomic element, which, based on historical and even unexpected values retrieved for management objects, dynamically widens or restricts the set of management objects to be monitored.	https://dx.doi.org/10.1109/INM.2009.5188826	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cohen2022	A Reinforcement-Learning Style Algorithm for Black Box Automata	The analysis of hardware and software systems is often applied to a model of a system rather than to the system itself. Obtaining a faithful model for a system may sometimes be a complex task. For learning the regular (finite automata) structure of a black box system, Angluin's $L^{*}$ algorithm and its successors employ membership and equivalence queries. The regular positive-negative inference (RPNI) family of algorithms use a less powerful capability of collecting observations for learning, with no control on selecting the inputs. We suggest and study here an alternative approach for learning, which is based on calculating utility values, obtained as a discounted sum of rewards, in the style of reinforcement learning. The utility values are used to classify the observed input prefixes into different states, and then to construct the learned automaton structure. We show cases where this classification is not enough to separate the prefixes, and subsequently remedy the situation by exploring deeper than the current prefix: checking the consistency between descendants of the current prefix that are reached with the same sequence of inputs. We show the connection of this algorithm with the RPNI algorithm and compare between these two approaches experimentally.	https://dx.doi.org/10.1109/MEMOCODE57689.2022.9954382	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Collander2021	Learning the Next Best View for 3D Point Clouds via Topological Features	In this paper, we introduce a reinforcement learning approach utilizing a novel topology-based information gain metric for directing the next best view of a noisy 3D sensor. The metric combines the disjoint sections of an observed surface to focus on high-detail features such as holes and concave sections. Experimental results show that our approach can aid in establishing the placement of a robotic sensor to optimize the information provided by its streaming point cloud data. Furthermore, a labeled dataset of 3D objects, a CAD design for a custom robotic manipulator, and software for the transformation, union, and registration of point clouds has been publicly released to the research community.	https://dx.doi.org/10.1109/ICRA48506.2021.9561389	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Collins2014	Applying reinforcement learning to an insurgency Agent-based Simulation		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928108114&doi=10.1177\%2f1548512913501728&partnerID=40&md5=a288035ca31e62b5d252638e56535fbf	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Collins2008	Reinforcement learning for live musical agents			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Colucci2021	MLComp: A Methodology for Machine Learning-based Performance Estimation and Adaptive Selection of Pareto-Optimal Compiler Optimization Sequences	Embedded systems have proliferated in various consumer and industrial applications with the evolution of Cyber-Physical Systems and the Internet of Things. These systems are subjected to stringent constraints so that embedded software must be optimized for multiple objectives simultaneously, namely reduced energy consumption, execution time, and code size. Compilers offer optimization phases to improve these metrics. However, proper selection and ordering of them depends on multiple factors and typically requires expert knowledge. State-of-the-art optimizers facilitate different platforms and applications case by case, and they are limited by optimizing one metric at a time, as well as requiring a time-consuming adaptation for different targets through dynamic profiling. To address these problems, we propose the novel MLComp methodology, in which optimization phases are sequenced by a Reinforcement Learning-based policy. Training of the policy is supported by Machine Learning-based analytical models for quick performance estimation, thereby drastically reducing the time spent for dynamic profiling. In our framework, different Machine Learning models are automatically tested to choose the best-fitting one. The trained Performance Estimator model is leveraged to efficiently devise Reinforcement Learning-based multi-objective policies for creating quasi-optimal phase sequences. Compared to state-of-the-art estimation models, our Performance Estimator model achieves lower relative error (< 2\%) with up to 50 $\times$ faster training time over multiple platforms and application domains. Our Phase Selection Policy improves execution time and energy consumption of a given code by up to 12\% and 6\%, respectively. The Performance Estimator and the Phase Selection Policy can be trained efficiently for any target platform and application domain.	https://dx.doi.org/10.23919/DATE51398.2021.9474158	Included	conflict_resolution		4
RL4SE	Compton2022	Deep Reinforcement Learning for Active Structure Stabilization		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135086875&doi=10.1007\%2f978-3-031-04122-8_2&partnerID=40&md5=3e89929b1b9a631250fdeee0f54488b7	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cone2021	Distributed and adaptive multi-uas behaviors to support nuclear response			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Confido2022	Reinforcing Penetration Testing Using AI	In a space mission operations context, assets of very high value are operated by several data systems. The security and resilience of these data systems has become a primary concern, due to the increasing ubiquity of the internet and mutability of the threat landscape. Penetration testing is a well established method to identify system security weaknesses, however, is typically high-cost and effort-intensive. The European Space Agency has developed a prototype automated penetration testing framework, called PenBox. Given the volume of parameters and possibilities for the PenBox automated test execution sequence, applied Machine Learning theory presents an interesting opportunity for enhancement. This paper presents the approach and results obtained towards research on the integration of Artificial Intelligence (AI) techniques, in particular Reinforcement Learning (RL), to PenBox. The adopted approach to frame this case study involves the Q-learning paradigm that seeks to learn a policy that maximizes the total reward, according to the quality associated with the certain action to take; the reward itself is the pivotal element that shapes the intelligent agent's behaviour through the trial-and-error theory, without any human interaction. To build this representative space, an existing network simulator was used and further adopted to improve representativeness and alignment with possible PenBox actions. In this paper, we highlight how Deep Q-Learning ensures a certain level of randomization, thus providing the basis for future evolution of the developed AI model to suit the real-world autonomous penetration testing software, with the aim of optimizing the process in terms of performance and cost effectiveness, and enabling an innovative autonomous capability for optimal attack path definitions without human in the loop.	https://dx.doi.org/10.1109/AERO53065.2022.9843459	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Constantinescu2021	Efficiency and productivity for decision making on low-power heterogeneous CPU plus GPU SoCs	Markov decision processes provide a formal framework for a computer to make decisions autonomously and intelligently when the effects of its actions are not deterministic. This formalism has had tremendous success in many disciplines; however, its implementation on platforms with scarce computing capabilities and power, as it happens in robotics or autonomous driving, is still limited. To solve this computationally complex problem efficiently under these constraints, high-performance accelerator hardware and parallelized software come to the rescue. In particular, in this work, we evaluate off-line-tuned static and dynamic versus adaptive heterogeneous scheduling strategies for executing value iteration-a core procedure in many decision-making methods, such as reinforcement learning and task planning-on a low-power heterogeneous CPU+GPU SoC that only uses 10-15 W. Our experimental results show that by using CPU+GPU heterogeneous strategies, the computation time and energy required are considerably reduced. They can be up to 54\% (61\%) faster and 57\% (65\%) more energy-efficient with respect to multicore-TBB-(or GPU-only-OpenCL-) implementation. Additionally, we also explore the impact of increasing the abstraction level of the programming model to ease the programming effort. To that end, we compare the TBB+OpenCL vs. the TBB+oneAPI implementations of our heterogeneous schedulers, observing that oneAPI versions result in up to 5x less programming effort and only incur in 3-8\% of overhead if the scheduling strategy is selected carefully.	https://dx.doi.org/10.1007/s11227-020-03257-3	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Constantinescu2021a	Efficiency and productivity for decision making on low-power heterogeneous CPU+GPU SoCs		https://doi.org/10.1007/s11227-020-03257-3	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Corcoran2021	Reinforcement Learning for Automated Energy Efficient Mobile Network Performance Tuning	Modern mobile networks are increasingly complex from a resource management perspective, with diverse combinations of software, infrastructure elements and services that need to be configured and tuned for correct and efficient operation. It is well accepted in the communications community that appropriately dimensioned, efficient and reliable configurations of systems like 5G or indeed its predecessor 4G is a massive technical challenge. One promising avenue is the application of machine learning methods to apply a data-driven and continuous learning approach to automated system performance tuning. We demonstrate the effectiveness of policy-gradient reinforcement learning as a way to learn and apply complex interleaving patterns of radio resource block usage in 4G and 5G, in order to automate the reduction of cell edge interference. We show that our method can increase overall spectral efficiency up to 25\% and increase the overall system energy efficiency up to 50\% in very challenging scenarios by learning how to do more with less system resources. We also introduce a flexible phased and continuous learning approach that can be used to train a bootstrap model in a simulated environment after which the model is transferred to a live system for continuous contextual learning.	https://dx.doi.org/10.23919/CNSM52442.2021.9615550	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Correa-Jullian2020	Operation scheduling in a solar thermal system: A reinforcement learning-based framework	Reinforcement learning (RL) provides an alternative method for designing condition-based decision making in engineering systems. In this study, a simple and flexible RL tabular Q-learning framework is employed to identify the optimal operation schedules for a solar hot water system according to action-reward feedback. The system is simulated in TRNSYS software. Three energy sources must supply a building's hot-water demand: low-cost heat from solar thermal collectors and a heat-recovery chiller, coupled to a conventional heat pump. Key performance indicators are used as rewards for balancing the system's performance with regard to energy efficiency, heat-load delivery, and operational costs. A sensitivity analysis is performed for different reward functions and meteorological conditions. Optimal schedules are obtained for selected scenarios in January, April, July, and October, according to the dynamic conditions of the system. The results indicate that when solar radiation is widely available (October through April), the nominal operation schedule frequently yields the highest performance. However, the obtained schedule differs when the solar radiation is reduced, for instance, in July. On average, with prioritization of the efficient use of both low-cost energy sources, the performance in July can be on average 21\% higher than under nominal schedule-based operation.	https://dx.doi.org/10.1016/j.apenergy.2020.114943	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cota2022	Roadmap for development of skills in Artificial Intelligence by means of a Reinforcement Learning model using a DeepRacer autonomous vehicle	Using Deepracer, through experimentation and simulation, theoretical concepts can be applied to a practical application of reinforcement learning (RL) in a real-life problem. It was considered as a highly useful tool to develop many direct and transversal competences that students need to work within the field of artificial intelligence (AI). Nowadays the combination of Hardware, Computer Vision methods and Machine Learning (ML) algorithms for the development of controllers for vehicle driving automation have facilitated the development of solutions for this problem. The intention of this work is to show a Roadmap that was formulated to learn AI, ML and RL competencies required to prepare undergraduate students for the industry of this area, following a structured mostly practical learning plan using Deepracer AWS platform and local alternatives for training; and a physical vehicle as primary tools that have made an incredibly compact setup process and reduced complexity in the educational researching field related to learning autonomous vehicles (AV) software development process. The AWS DeepRacer framework includes all needed hardware based on a two front-camera vehicle for stereo vision and a LiDAR sensor, besides it provides a powerful computation computer for high performance in scale autonomous vehicles. The roadmap was executed testing and comparing different RL models over the modalities that Deepracer provides (by comparing both local and AWS console training) documentation of the execution of the generated Roadmap is shown to ensure that should be considered as a stable learning system that could be followed by college programs. Finally, models were tested on a physical track built, and limitations, considerations and improvements for this Roadmap are explained as a contribution for future work.	https://dx.doi.org/10.1109/EDUCON52537.2022.9766659	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cowger2020	Research Report: ICARUS: Understanding de Facto Formats by Way of Feathers and Wax		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099727186&doi=10.1109\%2fSPW50608.2020.00067&partnerID=40&md5=fa5faadbe571bed09065876c6f08b480	Excluded	new_screen	E5: Other not a paper,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cox2001	Neural adaptive control of LoFLYTE(R)	A major goal in flight control over the past decade has been the development of reconfigurable flight control systems which can adapt their gains in real-time to compensate for aircraft damage and in-flight system failures. The purpose of this paper is to describe the controller developed for the LoFLYTE(R) aircraft, which is a testbed for neural networks research. The LoFLYTE(R) control system is based on the Accurate Automation Corp. Neural Adaptive Controller (NAC) which is designed to achieve this goal. The LoFLYTE(R) program is an active flight test program at the Air Force Flight Test Center at Edwards Air Force Base, with the objective of demonstrating a neural network control system for a waverider vehicle. The AAC control system has two innovative components: an adaptive actuator/flight surface controller, and a learning/adaptive stability augmentation system designed with neural network and reinforcement learning techniques.	https://dx.doi.org/10.1109/ACC.2001.946345	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cox2001a	Neural adaptive control of LoFLYTEtextregistered			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Crandall2022	Rxn Rover: automation of chemical reactions with user-friendly, modular software	The automation of chemical reactions in research and development can be an enabling technology to reduce cost and waste generation in light of technology transformation towards renewable feedstocks and energy in chemical industry. Automation of reaction optimization, in particular, would remove the need for expert input by designing algorithms to statistically analyze the reaction and automatically generate suggested results. In addition, automation can save time and resources, and reduce random human error. However, automation software is commonly coupled to a specific laboratory or device setup or not freely available for use. Rxn Rover is an open-source, modular automation platform for reaction discovery and optimization. Primarily targetting smaller research groups, it is designed using interchangeable plugins to be flexible and easy to integrate into a variety of laboratory environments. Using the Rxn Rover plugin architecture, novel optimization algorithms, analysis instrumentation, and reactor components can be used with minimal or no programming experience. The capability of Rxn Rover is demonstrated in the optimization of a reduction reaction of imine to amine, relevant to energy conversion and manufacturing of fine and commodity chemicals. The reaction was optimized separately using optimizer plugins for SQSnobFit, a Python implementation of the SNOBFIT global optimization algorithm, and Deep Reaction Optimizer (DRO), a deep reinforcement learning algorithm designed for reaction optimization. Using plugins designed for pumps, temperature controllers, and an online liquid chromatography system, the flow reaction was able to be controlled by each algorithm to automate reaction optimization for up to three days, at which point the results were gathered. A successful optimization was performed with SQSnobFit, achieving 70\% yield and 95\% selectivity, while no successful optimizations were achieved with DRO. Regardless of algorithm performance, Rxn Rover was able to successfully automate both multi-day optimization searches.	https://dx.doi.org/10.1039/d1re00265a	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Csaji2006	Reinforcement learning in a distributed market-based production control system	The paper presents an adaptive iterative distributed scheduling algorithm that operates in a market-based production control system. The manufacturing system is agentified, thus, every machine and job is associated with its own software agent. Each agent learns how to select presumably good schedules, by this way the size of the search space can be reduced. In order to get adaptive behavior and search space reduction, a triple-level learning mechanism is proposed. The top level of learning incorporates a simulated annealing algorithm, the middle (and the most important) level contains a reinforcement learning system, while the bottom level is done by a numerical function approximator, such as an artificial neural network. The paper suggests a cooperation technique for the agents, as well. It also analyzes the time and space complexity of the solution and presents some experimental results. (C) 2006 Elsevier Ltd. All rights reserved.	https://dx.doi.org/10.1016/j.aei.2006.01.001	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cui2012	Learning-based ship design optimization approach		https://doi.org/10.1016/j.cad.2011.06.011	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cui2022	Andes_gym: A Versatile Environment for Deep Reinforcement Learning in Power Systems	This paper presents andes_gym, a versatile and high-performance reinforcement learning environment for power system studies. The environment leverages the modeling and simulation capability of ANDES and the reinforcement learning (RL) environment OpenAI Gym to enable the prototyping and demonstration of RL algorithms for power systems. The architecture of the proposed software tool is elaborated to provide the ``observation'' and ``action'' interfaces for RL algorithms. An example is shown to rapidly prototype a load-frequency control algorithm based on RL trained by available algorithms. The proposed environment is highly generalized by supporting all the power system dynamic models available in ANDES and numerous RL algorithms available for OpenAI Gym.	https://dx.doi.org/10.1109/PESGM48719.2022.9916967	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cui2022a	Hierarchical Learning Approach for Age-of-Information Minimization in Wireless Sensor Networks	In this paper, we focus on a multi-user wireless network coordinated by a multi-antenna access point (AP). Each user can generate the sensing information randomly and report it to the AP. The freshness of information is measured by the age of information (AoI). We formulate the AoI minimization problem by jointly optimizing the users' scheduling and transmission control strategies. Moreover, we employ the intelligent reflecting surface (IRS) to enhance the channel conditions and thus reduce the transmission delay by controlling the AP's beamforming vector and the IRS's phase shifting matrices. The resulting AoI minimization becomes a mixed-integer program and difficult to solve due to uncertain information of the sensing data arrivals at individual users. By exploiting the problem structure, we devised a hierarchical deep reinforcement learning (DRL) framework to search for optimal solution in two iterative steps. Specifically, the users' scheduling strategy is firstly determined by the outer-loop DRL approach, and then the inner-loop optimization adapts either the uplink information transmission or downlink energy transfer to all users. Our numerical results verify that the proposed algorithm can outperform typical baselines in terms of the average AoI performance.	https://dx.doi.org/10.1109/WoWMoM54355.2022.00024	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cunha2020	Towards a common environment for learning scheduling algorithms	We propose a way to model and integrate HPC scheduling simulators into a popular Reinforcement Learning toolkit. We show experimentally that such an approach not only aids researchers being able to iterate faster by means of software reuse, but also to achieve state-of-the-art performance with 10x less interactions with the environment. We validate the simulation model's correctness by using unit tests, assertions and experimental comparisons. We also share an open source implementation of the model that will benefit researchers in resource management tasks assisted by Machine Learning.	https://dx.doi.org/10.1109/MASCOTS50786.2020.9285940	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Czibula2018	An effective approach for determining the class integration test order using reinforcement learning		https://doi.org/10.1016/j.asoc.2018.01.042	Included	new_screen		4
RL4SE	Dabbaghjamanesh2021	Reinforcement Learning-Based Load Forecasting of Electric Vehicle Charging Station Using Q-Learning Technique	The electric vehicles' (EVs) rapid growth can potentially lead power grids to face new challenges due to load profile changes. To this end, a new method is presented to forecast the EV charging station loads with machine learning techniques. The plug-in hybrid EVs (PHEVs) charging can be categorized into three main techniques (smart, uncoordinated, and coordinated). To have a good prediction of the future PHEV loads in this article, the Q-learning technique, which is a kind of the reinforcement learning, is used for different charging scenarios. The proposed Q-learning technique improves the forecasting of the conventional artificial intelligence techniques such as the recurrent neural network and the artificial neural network. Results prove that PHEV loads can accurately be forecasted by using the Q-learning technique under three different scenarios (smart, uncoordinated, and coordinated). The simulations of three different scenarios are obtained in the Keras open source software to validate the effectiveness and advantages of the proposed Q-learning technique.	https://dx.doi.org/10.1109/TII.2020.2990397	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Daher2018	Softwarized and distributed learning for SON management systems	Self-Organizing Networks (SON) functions have already proven to be useful for network operations. However, a higher automation level is required to make a network enabled with SON capabilities respond as a whole to the operator's objectives. For this purpose, a Policy Based SON Management (PBSM) layer has been proposed to manage the deployed SON functions. In this paper, we propose to empower the PBSM with cognition capability in order to manage efficiently SON enabled networks. We focus particularly on the implementation of such a Cognitive PBSM (C-PBSM) on a large scale network and propose a scalable approach based on distributed Reinforcement Learning (RL): RL agents are deployed on different clusters of the network. These clusters should be defined in such a way that the RL agents can learn independently. As the interaction between these clusters may evolve in time due for instance to traffic dynamics, we propose a flexible implementation of this C-PBSM framework with dynamic clustering to adapt to network's evolutions. We show how this flexible implementation is rendered possible under Software Defined Networks (SDN) framework. We also assess the performance of the proposed distributed learning approach on an LTE-A simulator.	https://dx.doi.org/10.1109/NOMS.2018.8406173	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Dai2020	CoinDICE: Off-policy confidence interval estimation			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Dai2022	Survey of Data-Driven Application Self-Adaptive Technology		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141530424&doi=10.7544\%2fissn1000-1239.20210221&partnerID=40&md5=35aaf67fcd4f78ddedf070f57389918e	Excluded	new_screen	E5: Other not a paper,E1: Does not define or use a RL method	4
RL4SE	Dai2019	Learning transferable graph exploration			Included	new_screen		4
RL4SE	Dai2021	Lyapunov-stable neural-network control	Deep learning has had a far reaching impact in robotics. Specifically, deep reinforcement learning algorithms have been highly effective in synthesizing neural-network controllers for a wide range of tasks. However, despite this empirical success, these controllers still lack theoretical guarantees on their performance, such as Lyapunov stability (i.e., all trajectories of the closed-loop system are guaranteed to converge to a goal state under the control policy). This is in stark contrast to traditional model-based controller design, where principled approaches (like LQR) can synthesize stable controllers with provable guarantees. To address this gap, we propose a generic method to synthesize a Lyapunov-stable neural-network controller, together with a neural-network Lyapunov function to simultaneously certify its stability. Our approach formulates the Lyapunov condition verification as a mixed-integer linear program (MIP). Our MIP verifier either certifies the Lyapunov condition, or generates counter examples that can help improve the candidate controller and the Lyapunov function. We also present an optimization program to compute an inner approximation of the region of attraction for the closed-loop system. We apply our approach to robots including an inverted pendulum, a 2D and a 3D quadrotor, and showcase that our neural-network controller outperforms a baseline LQR controller. The code is open sourced at https://github.com/StanfordASL/neural-network-lyapunov.		Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Dalgkitsis2022	SafeSCHEMA: Multi-domain Orchestration of Slices based on SafeRL for B5G Networks	The success of Software-Defined Networking and Network Function Virtualization enabled providers to partially automate the decision-making of various network operating workflows. Network slicing is an innate feature of the Fifth Generation (5G) and Beyond-5G (B5G) networks, bringing many advantages but also increasing the complexity of managing such networks. In this context, automation tools based on Artificial Intelligence (AI) have become a preeminent instrument of automation used by the industry, as it would be impossible to manually provision and manage such a vast and dynamic infrastructure. Safe interaction of the AI agents with the network is one of the predominant challenges, especially when Reinforcement Learning (RL) is used in critical environments. This is particularly important when the RL agent actions have a high or irreversible impact on the network or service. Slice management is one of the major features that operators want to automate, to offer future services at large scales with manageable complexity to the operator. However, during the exploration phase, RL agents can cause significant performance degradation during operation and possibly introduce irreversible damage to the service being offered. To address this major challenge, we propose a multi-agent, modular, SafeRL architecture for distributed slice orchestration. We study the problem of zero-touch slice management and orchestration, in the context of Ultra-Reliable Low Latency Communication services for B5G networks. Our results demonstrate improved performance over competing solutions, while ensuring the safety of the performed actions during real-time slice orchestration.	https://dx.doi.org/10.1109/GLOBECOM48099.2022.10001219	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Dalgkitsis2022a	SCHE2MA: Scalable, Energy-Aware, Multidomain Orchestration for Beyond-5G URLLC Services	The evolution of Software-Defined Networking (SDN) and Network Function Virtualization (NFV) in the telecommunications industry have intensified the issues of network management at large scales. Dynamic service orchestration and adaptive resource allocation became a necessity for network operators to manage the rapid growth of users and data-intensive applications. The impact of network automation on energy consumption and overall operating costs is often overlooked. Guaranteeing strict performance constraints of Ultra-Reliable Low Latency Communication (URLLC) services while enhancing energy efficiency is one of the major critical problems of future communication networks, given the urgency to reduce carbon emissions and energy consumption. In this work, we study the problem of zero-touch Service Function Chain (SFC) orchestration for multi-domain networks, targeting the latency reduction of URLLC services while improving energy efficiency for beyond-5G networks. Specifically, we propose SCHE2MA, a Service CHain Energy-Efficient Management framework based on distributed Reinforcement Learning (RL), that can intelligently deploy SFCs with shared VNFs per se into a multi-domain network. Finally, we evaluate SCHE2MA through model validation and simulation while demonstrating its ability to jointly reduce average service latency by 103.4\% and energy consumption by 17.1\% compared to a centralized RL solution.	https://dx.doi.org/10.1109/TITS.2022.3202312	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Dalgkitsis2020	Dynamic Resource Aware VNF Placement with Deep Reinforcement Learning for 5G Networks	The increasing demand for fast, reliable, and robust network services has driven the telecommunications industry to design novel network architectures that employ Network Functions Virtualization and Software Defined Networking. Despite the advancements in cellular networks, there is a need for an automatic, self-adapting orchestrating mechanism that can manage the placement of resources. Deep Reinforcement Learning can perform such tasks dynamically, without any prior knowledge. In this work, we leverage a Deep Deterministic Policy Gradient Reinforcement Learning algorithm, to fully automate the Virtual Network Functions deployment process between edge and cloud network nodes. We evaluate the performance of our implementation and compare it with alternative solutions to prove its superiority while demonstrating results that pave the way for Experiential Network Intelligence and fully automated, Zero touch network Service Management.	https://dx.doi.org/10.1109/GLOBECOM42002.2020.9322512	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Dana2015	Programs via Reinforcement	This chapter contains sections titled: 5.1 Evaluating a Program, 5.2 Reinforcement Learning Algorithms, 5.3 Learning in the Basal Ganglia, 5.4 Learning to Set Cortical Synapses, 5.5 Learning to Play Backgammon, 5.6 Backgammon as an Abstract Model, 5.7 Summary		Excluded	new_screen	E5: Other not a paper,E5: Other not a paper	4
RL4SE	Dang2018	SvgAI emdash Training artificial intelligent agent to use SVG editor	Deep reinforcement learning has been successfully used to train artificial intelligent (AI) agents to outperform humans in many tasks as well as to enhance the capability in robotic automation. In this paper, we propose a framework to train an AI agent to use scalable vector graphic (SVG) editor to draw SVG images. Hence, the objective of this AI agent is to draw SVG images that are similar as much as possible to their target raster images. We find that it is crucial to distinguish the action space into two sets and apply a different exploration policy on each set during the training process. Evaluations show that our proposed dual-exploration policy greatly stabilizes the training process and increases the accuracy of the AI agent. SVG images produced by the proposed AI agent also have superior quality compared to popular raster-to-SVG conversion software.	https://dx.doi.org/10.23919/ICACT.2018.8323671	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Dang2018a	SvgAI - Training Artificial Intelligent Agent to use SVG Editor	Deep reinforcement learning has been successfully used to train artificial intelligent (AI) agents to outperform humans in many tasks as well as to enhance the capability in robotic automation. In this paper, we propose a framework to train an AI agent to use scalable vector graphic (SVG) editor to draw SVG images. Hence, the objective of this AI agent is to draw SVG images that are similar as much as possible to their target raster images. We find that it is crucial to distinguish the action space into two sets and apply a different exploration policy on each set during the training process. Evaluations show that our proposed dual-exploration policy greatly stabilizes the training process and increases the accuracy of the AI agent. SVG images produced by the proposed AI agent also have superior quality compared to popular raster-to-SVG conversion software.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Daradkeh2022	Lurkers versus Contributors: An Empirical Investigation of Knowledge Contribution Behavior in Open Innovation Communities		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144679055&doi=10.3390\%2fjoitmc8040198&partnerID=40&md5=02411437fa49940d6b5724659513650d	Excluded	new_screen	E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for	4
RL4SE	Das2016	Workload Change Point Detection for Runtime Thermal Management of Embedded Systems	Applications executed on multicore embedded systems interact with system software [such as the operating system (OS)] and hardware, leading to widely varying thermal profiles which accelerate some aging mechanisms, reducing the lifetime reliability. Effectively managing the temperature therefore requires: 1) autonomous detection of changes in application workload and 2) appropriate selection of control levers to manage thermal profiles of these workloads. In this paper, we propose a technique for workload change detection using density ratio-based statistical divergence between overlapping sliding windows of CPU performance statistics. This is integrated in a runtime approach for thermal management, which uses reinforcement learning to select workload-specific thermal control levers by sampling on-board thermal sensors. Identified control levers override the OSs native thread allocation decision and scale hardware voltage-frequency to improve average temperature, peak temperature, and thermal cycling. The proposed approach is validated through its implementation as a hierarchical runtime manager for Linux, with heuristic-based thread affinity selected from the upper hierarchy to reduce thermal cycling and learningbased voltage-frequency selected from the lower hierarchy to reduce average and peak temperatures. Experiments conducted with mobile, embedded, and high performance applications on ARM-based embedded systems demonstrate that the proposed approach increases workload change detection accuracy by an average 3.4$\times$, reducing the average temperature by 4 \degreeC-25 \degreeC, peak temperature by 6 \degreeC-24 \degreeC, and thermal cycling by 7\%-35\% over state-of-the-art approaches.	https://dx.doi.org/10.1109/TCAD.2015.2504875	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Das2015	Hardware-software interaction for run-time power optimization: A case study of embedded Linux on multicore smartphones	Applications running on smartphones interact with the hardware and the system software differently, resulting in widely varying power consumption and hence thermal profiles. Typically, these smartphone platforms expose some hardware power control features to users, controlled through software governors such as cpufreq for dynamic voltage-frequency scaling (DVFS) and cpuquiet for dynamic core selection (DCS). Operating systems on these platforms manage these governors conservatively, independent of application's performance requirement. To address this, we propose an alternative approach, which uses reinforcement learning to explore the trade-off between power saving opportunities using DVFS and DCS and application's performance at run-time. The objective is to reduce power consumption, taking into consideration dynamic power, leakage power, and the inter-dependency between temperature and power. The reinforcement learning-based control is validated as a case-study on ARM A15-based nvidia's tegra smartphone through its implementation as a run-time manager (RTM). This RTM interfaces with different hardware performance counters and the embedded Linux Operating System through (1) the cpuquiet API to select cores at run-time; and (2) the cpufreq API to scale the frequency of active cores. Experiments with mobile and high performance applications demonstrate that the proposed approach achieves an average 22\% (7-40\%) power reduction compared to existing techniques.	https://dx.doi.org/10.1109/ISLPED.2015.7273508	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Dass2021	Reinforcement Learning for Generating Secure Configurations	Many security problems in software systems are because of vulnerabilities caused by improper configurations. A poorly configured software system leads to a multitude of vulnerabilities that can be exploited by adversaries. The problem becomes even more serious when the architecture of the underlying system is static and the misconfiguration remains for a longer period of time, enabling adversaries to thoroughly inspect the software system under attack during the reconnaissance stage. Employing diversification techniques such as Moving Target Defense (MTD) can minimize the risk of exposing vulnerabilities. MTD is an evolving defense technique through which the attack surface of the underlying system is continuously changing. However, the effectiveness of such dynamically changing platform depends not only on the goodness of the next configuration setting with respect to minimization of attack surfaces but also the diversity of set of configurations generated. To address the problem of generating a diverse and large set of secure software and system configurations, this paper introduces an approach based on Reinforcement Learning (RL) through which an agent is trained to generate the desirable set of configurations. The paper reports the performance of the RL-based secure and diverse configurations through some case studies.	https://dx.doi.org/10.3390/electronics10192392	Included	new_screen		4
RL4SE	David2022	Devs Model Construction As A Reinforcement Learning Problem	Simulators are crucial components in many software-intensive systems, such as cyber-physical systems and digital twins. The inherent complexity of such systems renders the manual construction of simulators an error-prone and costly endeavor, and automation techniques are much sought after. However, current automation techniques are typically tailored to a particular system and cannot be easily transposed to other settings. In this paper, we propose an approach for the automated construction of simulators that can overcome this limitation, based on the inference of Discrete Event System Specifications (DEVS) models by reinforcement learning. Reinforcement learning allows inferring knowledge on the construction process of the simulator, instead of inferring the simulator itself. This, in turn, fosters reuse across different systems. DEVS further improves the reusability of this knowledge, as the vast majority of simulation formalisms can be efficiently translated to DEVS. We demonstrate the performance and generalizability of our approach on an illustrative example implemented in Python and Tensorforce.	https://dx.doi.org/10.23919/ANNSIM55834.2022.9859369	Included	new_screen		4
RL4SE	DePersis2021	Low-complexity learning of Linear Quadratic Regulators from noisy data		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102857100&doi=10.1016\%2fj.automatica.2021.109548&partnerID=40&md5=6724257a7cb362226c496306a9f0a045	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	DeRaedt2008	Logical and Relational Learning	I use the term logical and relational learning (LRL) to refer to the subfield of machine learning and data mining that is concerned with learning in expressive logical or relational representations. It is the union of inductive logic programming, (statistical) relational learning and multi-relational data mining and constitutes a general class of techniques and methodology for learning from structured data (such as graphs, networks, relational databases) and background knowledge. During the course of its existence, logical and relational learning has changed dramatically. Whereas early work was mainly concerned with logical issues (and even program synthesis from examples), in the 90s its focus was on the discovery, of new and interpretable knowledge from structured data, often in the form of rules or patterns. Since then Hie range of tasks to which logical and relational learning has been applied has significantly broadened and now covers almost all machine learning problems and settings. Today, there exist logical and relational learning methods for reinforcement learning, statistical learning. distance- and kernel-based learning in addition to traditional symbolic machine learning approaches. At the same time, logical and relational learning problems are appearing everywhere. Advances in intelligent systems are enabling the generation of high-level symbolic and structured data in a wide variety of domains, including the semantic web, robotics, vision, social networks, and the life sciences, which ill turn raises new challenges and opportunities for logical and relational learning. These developments have led to a new view on logical and relational learning and its role in machine learning and artificial intelligence. In this talk, I shall reflect oil this view by identifying some of the lessons learned ill logical and relational learning and formulating some challenges, for future developments.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	DeWinter2023	Single assembly sequence to flexible assembly plan by Autonomous Constraint Generation	The factory of the future is steering away from conventional assembly line production with sequential conveyor technology, towards flexible assembly lines, where products dynamically move between work-cells. Flexible assembly lines are significantly more complex to plan compared to sequential lines. Therefore there is an increased need for autonomously generating flexible robot-centered assembly plans. The novel Autonomous Constraint Generation (ACG) method presented here will generate a dynamic assembly plan starting from an initial assembly sequence, which is easier to program. Using a physics simulator, variations of the work-cell configurations from the initial sequence are evaluated and assembly constraints are autonomously deduced. Based on that the method can generate a complete assembly graph that is specific to the robot and work-cell in which it was initially programmed, taking into account both part and robot collisions. A major advantage is that it scales only linearly with the number of parts in the assembly. The method is compared to previous research by applying it to the Cranfield Benchmark problem. Results show a 93\% reduction in planning time compared to using Reinforcement Learning Search. Furthermore, it is more accurate compared to generating the assembly graph from human interaction. Finally, applying the method to a real life industrial use case proves that a valid assembly graph is generated within reasonable time for industry.	https://dx.doi.org/10.1016/j.rcim.2022.102417	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	DeWinter2019	Accelerating Interactive Reinforcement Learning by Human Advice for an Assembly Task by a Cobot	The assembly industry is shifting more towards customizable products, or requiring assembly of small batches. This requires a lot of reprogramming, which is expensive because a specialized engineer is required. It would be an improvement if untrained workers could help a cobot to learn an assembly sequence by giving advice. Learning an assembly sequence is a hard task for a cobot, because the solution space increases drastically when the complexity of the task increases. This work introduces a novel method where human knowledge is used to reduce this solution space, and as a result increases the learning speed. The method proposed is the IRL-PBRS method, which uses Interactive Reinforcement Learning (IRL) to learn from human advice in an interactive way, and uses Potential Based Reward Shaping (PBRS), in a simulated environment, to focus learning on a smaller part of the solution space. The method was compared in simulation to two other feedback strategies. The results show that IRL-PBRS converges more quickly to a valid assembly sequence policy and does this with the fewest human interactions. Finally, a use case is presented where participants were asked to program an assembly task. Here, the results show that IRL-PBRS learns quickly enough to keep up with advice given by a user, and is able to adapt online to a changing knowledge base.	https://dx.doi.org/10.3390/robotics8040104	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Dean2019	Emergent Scheduling of Distributed Execution Frameworks	Distributed execution Frameworks (DEFs) provide a platform for handling the increasing volume of data available to distributed computational processes, forming the creation and usage of a large number of DEFs for performing distributed computations. For example, sorting and analyzing large data sets through map and reduce operations, performing a set of operations across points in a data stream to provide near real-time analysis, and the training and testing of machine learning models for varying methods of learning, such as, supervised, unsupervised and reinforcement learning, exploiting the vast amounts of data available. Leading to varying DEFs becoming optimal for either fine or coarse grained computations, for example Apache Spark provides a framework for coarse grained data parallel processes providing data locality adding latency to scheduling decisions which would hinder performance of fine-grained computation. Whereas Ray and Apache Flink provide solutions to avoid the latency incurred by the scheduling method used by apache Spark while potentially incurring longer job completion times as data locality is no longer a priority. Therefore, this PhD will focus on overcoming the issue of trading performance for differing workloads by exploiting the capabilities presented by emergent software systems which learn how to assemble and re-assemble themselves in response to their current deployment conditions and input pattern. This allows the creation of a component based DEF capable of altering both the local behaviour of a DEF (i.e. Local Schedulers and placement polices within a centralised scheduler) to potentially improve the performance of single DEF as well as global behaviour of a DEF, for example the adaptation of a centralised to two-level scheduler.	https://dx.doi.org/10.1109/FAS-W.2019.00063	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E5: Other not a paper	4
RL4SE	Degrave2019	A Differentiable Physics Engine for Deep Learning in Robotics	An important field in robotics is the optimization of controllers. Currently, robots are often treated as a black box in this optimization process, which is the reason why derivative-free optimization methods such as evolutionary algorithms or reinforcement learning are omnipresent. When gradient-based methods are used, models are kept small or rely on finite difference approximations for the Jacobian. This method quickly grows expensive with increasing numbers of parameters, such as found in deep learning. We propose the implementation of a modern physics engine, which can differentiate control parameters. This engine is implemented for both CPU and GPU. Firstly, this paper shows how such an engine speeds up the optimization process, even for small problems. Furthermore, it explains why this is an alternative approach to deep Q-learning, for using deep learning in robotics. Finally, we argue that this is a big step for deep learning in robotics, as it opens up new possibilities to optimize robots, both in hardware and software.	https://www.ncbi.nlm.nih.gov/pubmed/30899218	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Dehghani2022	Facilitating the migration to the microservice architecture via model-driven reverse engineering and reinforcement learning		https://doi.org/10.1007/s10270-022-00977-3	Included	new_screen		4
RL4SE	DelDuchetto2020	Are You Still With Me? Continuous Engagement Assessment From a Robot's Point of View	Continuously measuring the engagement of users with a robot in a Human-Robot Interaction (HRI) setting paves the way toward in-situ reinforcement learning, improve metrics of interaction quality, and can guide interaction design and behavior optimization. However, engagement is often considered very multi-faceted and difficult to capture in a workable and generic computational model that can serve as an overall measure of engagement. Building upon the intuitive ways humans successfully can assess situation for a degree of engagement when they see it, we propose a novel regression model (utilizing CNN and LSTM networks) enabling robots to compute a single scalar engagement during interactions with humans from standard video streams, obtained from the point of view of an interacting robot. The model is based on a long-term dataset from an autonomous tour guide robot deployed in a public museum, with continuous annotation of a numeric engagement assessment by three independent coders. We show that this model not only can predict engagement very well in our own application domain but show its successful transfer to an entirely different dataset (with different tasks, environment, camera, robot and people). The trained model and the software is available to the HRI community, at https://github.com/LCAS/engagement_detector, as a tool to measure engagement in a variety of settings.	https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091878799&doi=10.3389\%2ffrobt.2020.00116&partnerID=40&md5=243624c34a613d2eb6f7fe6d75a73700	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Delcourt2021	Automatic and Intelligent Composition of Pervasive Applications	Opportunistic service composition is a novel and disruptive approach for building software in dynamic and open pervasive environments. It aims to tackle the growing complexity of software design in such environments by dynamically providing relevant applications in the absence of explicit user needs: an intelligent engine composes software components that are present in the pervasive environment in order to build user-tailored context-adapted applications, relying on reinforcement learning. This demonstration presents the current status of our opportunistic composition prototype: the intelligent engine builds pervasive applications in bottom-up mode from actual software components that are discovered via the UPnP (Universal Plug and Play) protocol, taking into account learned user preferences.	https://dx.doi.org/10.1109/PerComWorkshops51409.2021.9430950	Excluded	conflict_resolution	E5: Other not a paper,E5: Other not a paper	4
RL4SE	Deltetto2021	Exploring the Potentialities of Deep Reinforcement Learning for Incentive-Based Demand Response in a Cluster of Small Commercial Buildings	Demand Response (DR) programs represent an effective way to optimally manage building energy demand while increasing Renewable Energy Sources (RES) integration and grid reliability, helping the decarbonization of the electricity sector. To fully exploit such opportunities, buildings are required to become sources of energy flexibility, adapting their energy demand to meet specific grid requirements. However, in most cases, the energy flexibility of a single building is typically too small to be exploited in the flexibility market, highlighting the necessity to perform analysis at a multiple-building scale. This study explores the economic benefits associated with the implementation of a Reinforcement Learning (RL) control strategy for the participation in an incentive-based demand response program of a cluster of commercial buildings. To this purpose, optimized Rule-Based Control (RBC) strategies are compared with a RL controller. Moreover, a hybrid control strategy exploiting both RBC and RL is proposed. Results show that the RL algorithm outperforms the RBC in reducing the total energy cost, but it is less effective in fulfilling DR requirements. The hybrid controller achieves a reduction in energy consumption and energy costs by respectively 7\% and 4\% compared to a manually optimized RBC, while fulfilling DR constraints during incentive-based events.	https://dx.doi.org/10.3390/en14102933	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Deng2022	Resource Provisioning for Mitigating Edge DDoS Attacks in MEC-Enabled SDVN	Vehicular ad hoc network (VANET) has become an accessible technology for improving road safety and driving experience, the problems of heterogeneity and lack of resources it faces have also attracted widespread attention. With the development of software-defined networking (SDN) and multiaccess edge computing (MEC), a variety of resource allocation strategies in MEC-enabled software-defined networking-based VANET (SDVN) have been proposed to solve these problems. However, we note that few of these work involves the situation where SDVN is under Distributed Denial of Service (DDoS) attacks. Actually, Internet of Things (IoT) devices are extremely easy to be compromised by malicious users, and compromised IoT devices may be used to launch edge DDoS attacks against the MEC servers in MEC-enabled SDVN at any time. In this article, we propose a graph neural network (GNN)-based collaborative deep reinforcement learning (GCDRL) model to generate the resource provisioning and mitigating strategy. The model evaluates the trust value of the vehicles, formulates mitigation of edge DDoS attacks and resource provisioning strategies to ensure that the MEC servers can work normally under edge DDoS attacks. In addition, GNN is adopted in the DRL model to extract the structure feature of the graph composed of MEC servers, and help transfer computing tasks between MEC servers to alleviate the problem of resources imbalance between them. Experimental results show that the method of estimating the vehicular trust value is effective, and our method can make the average throughput of edge nodes more stable and lower down the average delay and the average energy consumption under the edge DDoS attack. Also, a real-world case study is conducted to verify our conclusion.	https://dx.doi.org/10.1109/JIOT.2022.3189975	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Denoyelle2022	Rapid Execution Time Estimation for Heterogeneous Memory Systems Through Differential Tracing	As the complexity of compute nodes in high-performance computing (HPC) keeps increasing, systems equipped with heterogeneous memory devices are becoming paramount. Efficiently utilizing heterogeneous memory-based systems, however, poses significant challenges to application developers. System-software-level transparent solutions utilizing artificial intelligence and machine learning approaches, in particular nonsupervised learning-based methods such as reinforcement learning, may come to the rescue. However, such methods require rapid estimation of execution runtime as a function of the data layout across memory devices for exploring different data placement strategies, rendering architecture-level simulators impractical for this purpose. In this paper we propose a differential tracing-based approach using memory access traces obtained by high-frequency sampling-based methods (e.g., Intel's PEBS) on real hardware using of different memory devices. We develop a runtime estimator based on such traces that provides an execution time estimate orders of magnitude faster than full-system simulators. On a number of HPC miniapplications we show that the estimator predicts runtime with an average error of 4.4\% compared to measurements on real hardware.	https://dx.doi.org/10.1007/978-3-031-07312-0_13	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Denoyelle2022a	Rapid Execution Time Estimation for Heterogeneous Memory Systems Through Differential Tracing		https://doi.org/10.1007/978-3-031-07312-0_13	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Desai2021	Advances in Code Summarization	Several studies have suggested that comments describing source code can help mitigate the burden of program understanding. However, software systems usually lack adequate comments and even when present, the comments may be obsolete or unhelpful. Researchers have addressed this issue by automatically generating comments from source code, a task referred to as Code Summarization. In this technical presentation, we take a deeper look at some of the significant, recent works in the area of code summarization and how each of them attempts to take a new perspective of this task including methods leveraging RNNs, Transformers, Graph neural networks and Reinforcement learning.	https://dx.doi.org/10.1109/ICSE-Companion52605.2021.00141	Excluded	new_screen	E5: Other not a paper,E1: Does not define or use a RL method	4
RL4SE	Deshpande2022	Energy Management Simulation with Multi-Agent Reinforcement Learning: An Approach to Achieve Reliability and Resilience	The share of energy produced by small-scale renewable energy sources, including photovoltaic panels and wind turbines, will significantly increase in the near future. These systems will be integrated in microgrids to strengthen the independence of energy consumers. This work deals with energy management in microgrids, taking into account the volatile nature of renewable energy sources. In the developed approach, Multi-Agent Reinforcement Learning is applied, where agents represent microgrid components. The individual agents are trained to make good decisions with respect to adapting to the energy load in the grid. Training of agents leverages the historic energy profile data for energy consumption and renewable energy production. The implemented energy management simulation shows good performance and balances the energy flows. The quantitative performance evaluation includes comparisons with the exact solutions from a linear program. The computational results demonstrate good generalisation capabilities of the trained agents and the impact of these capabilities on the reliability and resilience of energy management in microgrids.	https://dx.doi.org/10.3390/en15197381	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Desnos2022	Ultra-Fast Machine Learning Inference through C Code Generation for Tangled Program Graphs	Tangled Program Graph (TPG) is a Reinforcement Learning (RL) technique based on genetic programming concepts. On state-of-the-art learning environments, TPGs have been shown to offer comparable competence with Deep Neural Networks (DNNs), for a fraction of their computational and storage cost. The contribution of this paper focuses on accelerating the inference of pre-trained TPGs, through the generation of standalone C code. While the training process of TPGs, based on genetic evolution principles, requires the use of flexible data structures supporting random mutations, this flexibility is no longer needed when focusing on the inference process. Evaluation of the proposed approach on four computing platforms, including embedded CPUs, produces an acceleration of the TPG inference by a factor 50 compared to state-of-the-art implementations. The inference performance obtained within a complex RL environment range between hundreds of nano-seconds to micro-seconds, making this approach highly competitive for edge Artificial Intelligence (AI).	https://dx.doi.org/10.1109/SiPS55645.2022.9919237	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Desnos2021	Gegelati: Lightweight Artificial Intelligence through Generic and Evolvable Tangled Program Graphs		https://doi.org/10.1145/3441110.3441575	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Dezhabad2018	Learning-based dynamic scalable load-balanced firewall as a service in network function-virtualized cloud computing environments		https://doi.org/10.1007/s11227-018-2387-5	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Dheenadayalan2017	Policy Gradient Reinforcement Learning for I/O Reordering on Storage Servers	Deep customization of storage architectures to the applications they support is often undesirable - nature of application data is dynamic, applications are replaced far more often than storage systems are and usage patterns change dynamically with time. A continuously learning software intervention that dynamically adapts to the changing workload pattern would be the easiest way to bridge this 'gap'. As borne out by our experiments, the overhead induced by such software interventions turns out to be negligible for large-scale storage systems. Reinforcement Learning offers a way to dynamically learn from a continuous data stream and take appropriate actions towards optimizing a future goal. We adapt policy gradient reinforcement learning to learn a policy that minimizes I/O wait time that in turn maximizes I/O throughput. A set of discrete actions consisting of switches between scheduling schemes is considered to dynamically re-order client-specific I/O operations. Results reveal that I/O reordering policy learned using reinforcement learning results in significant improvement in the overall I/O throughput.	https://dx.doi.org/10.1007/978-3-319-70087-8_87	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Di2021	A survey on autonomous vehicle control in the era of mixed-autonomy: From physics-based to AI-guided driving policy learning	This paper serves as an introduction and overview of the potentially useful models and methodologies from artificial intelligence (AI) into the field of transportation engineering for autonomous vehicle (AV) control in the era of mixed autonomy when AVs drive alongside human-driven vehicles (HV). It is the first-of-its-kind survey paper to comprehensively review literature in both transportation engineering and AI for mixed traffic modeling. We will discuss state-of-the-art applications of AI-guided methods, identify opportunities and obstacles, and raise open questions. We divide the stage of AV deployment into four phases: the pure HVs, the HV-dominated, the AVdominated, and the pure AVs. This paper is primarily focused on the latter three phases. Models used for each phase are summarized, encompassing game theory, deep (reinforcement) learning, and imitation learning. While reviewing the methodologies, we primarily focus on the following research questions: (1) What scalable driving policies are to control a large number of AVs in mixed traffic comprised of human drivers and uncontrollable AVs? (2) How do we estimate human driver behaviors? (3) How should the driving behavior of uncontrollable AVs be modeled in the environment? (4) How are the interactions between human drivers and autonomous vehicles characterized? We also provide a list of public datasets and simulation software related to AVs. Hopefully this paper will not only inspire our transportation community to rethink the conventional models that are developed in the data-shortage era, but also start conversations with other disciplines, in particular robotics and machine learning, to join forces towards creating a safe and efficient mixed traffic ecosystem.	https://dx.doi.org/10.1016/j.trc.2021.103008	Excluded	new_screen	E5: Other not a paper,E2: Software engineering is not the problem RL is used for	4
RL4SE	Dias2019	Cloud-Empowered, Self-Managing Wireless Sensor Networks: Interconnecting Management Operations at the Application Layer	This article discusses the design and implementation of a scalable system architecture that integrates wireless sensor networks (WSNs) into the Internet of Things (IoT) and exploits cloud services to autonomously configure wireless sensor nodes to measure and transmit sensed data only at periods when the environment changes more often. The implementation relies on software-defined networking (SDN) features to simplify WSN management and exploits the power of existing cloud computing platforms to execute a reinforcement learning algorithm that makes decisions based on the environment's evolution.	https://dx.doi.org/10.1109/MCE.2018.2868110	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Diegues2015	Self-tuning Intel Restricted Transactional Memory	The Transactional Memory (TM) paradigm aims at simplifying the development of concurrent applications by means of the familiar abstraction of atomic transaction. After a decade of intense research, hardware implementations of TM have recently entered the domain of mainstream computing thanks to Intel's decision to integrate TM support, codenamed RTM (Reduced Transactional Memory), in their last generation of processors. In this work we shed light on a relevant issue with great impact on the performance of Intel's RTM: the correct tuning of the logic that regulates how to cope with failed hardware transactions. We show that the optimal tuning of this policy is strongly workload dependent, and that the relative difference in performance among the various possible configurations can be remarkable (up to 10 x slow-downs). We address this issue by introducing a simple and effective approach that aims to identify the optimal RTM configuration at run-time via lightweight reinforcement learning techniques. The proposed technique requires no off-line sampling of the application, and can be applied to optimize both the cases in which a single global lock or a software TM implementation is used as fall-back synchronization mechanism. We propose and evaluate different designs for the proposed self-tuning mechanisms, which we integrated with GCC in order to achieve full transparency for the programmers. Our experimental study, based on standard TM benchmarks, demonstrates average gains of 60\% over any static approach while remaining within 5\% from the performance of manually identified optimal configurations. (C) 2015 Elsevier B.V. All rights reserved.	https://dx.doi.org/10.1016/j.parco.2015.10.001	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Digney1996	Skill transfer and training in emergent hierarchical control systems	For robots expected to perform tasks of complexity approaching those performed by humans (or other animals) it is becoming clear that robots must be capable of both learning from instruction as well as discovering and learning autonomously. Presented in this paper are methods through which novice robots can be prepared for their future endeavours, not through explicit specification and memorization, but through shaping methods that do not compromise the robot's autonomous learning capabilities. This shaping provides the robot with initial guidance that it is free to use, neglect or alter as it sees fit. Two methods of shaping are discussed: scaffolding actions and staged learning. In general, the robot must be able to transport what it has previously learned (either by chance or in a regimented training program) and apply it to new (possibly related) situations. This paper builds upon recent work in nested Q-learning that allows for the generation of hierarchical control structures and reactive responses in reinforcement learning domains.	https://dx.doi.org/10.1109/ISIC.1996.556180	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Dillien2019	Reinforcement learning in child molesters	BACKGROUND: Child molesters form a heterogeneous group, but one generally shared characteristic is maladaptive, rigid behaviour. Impairments in reinforcement learning may explain these maladaptive tendencies, but this has not been systematically investigated. Further, it is not known if such impairments vary with subtype of child molesters. AIMS: To investigate the presence of impairments in reinforcement learning among child molesters and to test for differences in patterns of impairment with subtype. METHODS: A group of 59 child molesters was recruited from several prisons in a two-stage screening process, the first using records and the second interview; a comparison group of 33 offenders who had never committed a sex offence and who denied paedophile ideation was similarly recruited; 36 nonoffender comparison men were recruited by social media and word of mouth. Each was asked to perform a probabilistic reversal learning task, in which stimulus-outcome contingencies had to be learned. RESULTS: Child molesters, as a group, made significantly more errors on the probabilistic reversal learning task than the nonoffenders; the comparison offenders and the nonoffenders gained similar scores, although findings may have been confounded by older age in the child molester group. Nonpaedophilic child molesters had significantly worse scores than paedophilic child molesters. CONCLUSIONS: Child molesters, especially those not diagnosed with paedophilia, have deficits during both the acquisition and reversal of contingencies, suggesting reinforcement learning deficits that may undermine their capacity to benefit maximally from therapy without preliminary work to repair those deficits, possibly in conjunction with extending the offender programmes. Testing before programme entry would enable accurate targeting of scarce resources in this respect.	https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057783069&doi=10.1002\%2fcbm.2097&partnerID=40&md5=507d29bf68ac7a5c39ed14b423df21ca	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Dillmann2022	Biomorphic robot controls: event driven model free deep SNNs for complex visuomotor tasks		https://doi.org/10.1007/s10015-022-00769-4	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Ding2021	Multi-Agent Reinforcement Learning for Urban Crowd Sensing with For-Hire Vehicles	Recently, vehicular crowd sensing (VCS) that leverages sensor-equipped urban vehicles to collect city-scale sensory data has emerged as a promising paradigm for urban sensing. Nowadays, a wide spectrum of VCS tasks are carried out by for-hire vehicles (FHVs) due to various hardware and software constraints that are difficult for private vehicles to satisfy. However, such FHV-enabled VCS systems face a fundamental yet unsolved problem of striking a balance between the order-serving and sensing outcomes. To address this problem, we propose a novel graph convolutional cooperative multi-agent reinforcement learning (GCC-MARL) framework, which helps FHVs make distributed routing decisions that cooperatively optimize the system-wide global objective. Specifically, GCC-MARL meticulously assigns credits to agents in the training process to effectively stimulate cooperation, represents agents' actions by a carefully chosen statistics to cope with the variable agent scales, and integrates graph convolution to capture useful spatial features from complex large-scale urban road networks. We conduct extensive experiments with a real-world dataset collected in Shenzhen, China, containing around 1 million trajectories and 50 thousand orders of 553 taxis per-day from June 1st to 30th, 2017. Our experiment results show that GCC-MARL outperforms state-of-the-art baseline methods in order-serving revenue, as well as sensing coverage and quality.	https://dx.doi.org/10.1109/INFOCOM42981.2021.9488713	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ding2022	Generation Method of Class Integration Test Order Based on Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129922746&doi=10.13328\%2fj.cnki.jos.006555&partnerID=40&md5=8d1b6a5986109ab1e8c41072ea218996	Excluded	new_screen	E5: Other not a paper,E2: Software engineering is not the problem RL is used for	4
RL4SE	Dinh2022	Supervised-learning-based hour-ahead demand response for a behavior-based home energy management system approximating MILP optimization	The demand response (DR) program of a traditional home energy management system (HEMS) usually controls or schedules appliances to monitor energy usage, minimize energy cost, and maximize user comfort. In this study, instead of interfering with appliances and changing residents' behavior, the proposed hour-ahead DR strategy first learns the appliance usage behavior of residents; subsequently, based on this knowledge, it silently controls the energy storage system (ESS) and renewable energy system (RES) to minimize the daily energy cost. To accomplish the goal, the proposed deep neural networks (DNNs) of this DR approximate the MILP optimization using supervised learning. The training datasets are created from the optimal outputs of an MILP solver using historical data. After training, in each time slot, these DNNs are used to control the ESS and RES using the real-time data of the surrounding environment. For comparison, we develop two different strategies, namely, the mull-agent reinforcement learning-based strategy, which is an hour-ahead strategy, and the forecast-based MILP strategy, which is a day-ahead strategy. For evaluation and verification, the proposed approaches are applied to three different real-world homes with real-world real-time global horizontal irradiation and prices. Numerical results verify the effectiveness and superiority of the proposed MILP-based supervised learning strategy, in terms of the daily energy cost.	https://dx.doi.org/10.1016/j.apenergy.2022.119382	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Dinu2022	Reinforcement Learning Made Affordable for Hardware Verification Engineers	Constrained random stimulus generation is no longer sufficient to fully simulate the functionality of a digital design. The increasing complexity of today's hardware devices must be supported by powerful development and simulation environments, powerful computational mechanisms, and appropriate software to exploit them. Reinforcement learning, a powerful technique belonging to the field of artificial intelligence, provides the means to efficiently exploit computational resources to find even the least obvious correlations between configuration parameters, stimuli applied to digital design inputs, and their functional states. This paper, in which a novel software system is used to simplify the analysis of simulation outputs and the generation of input stimuli through reinforcement learning methods, provides important details about the setup of the proposed method to automate the verification process. By understanding how to properly configure a reinforcement algorithm to fit the specifics of a digital design, verification engineers can more quickly adopt this automated and efficient stimulus generation method (compared with classical verification) to bring the digital design to a desired functional state. The results obtained are most promising, with even 52 times fewer steps needed to reach a target state using reinforcement learning than when constrained random stimulus generation was used.	https://www.ncbi.nlm.nih.gov/pubmed/36363907	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Divenyi2012	Adaptive Multiagent Model Based on Reinforcement Learning for Distributed Generation Systems	Distributed generation have been widely spread in the last decades raising a lot of questions regarding the safe and high-quality operation of the power systems. The investigation of these questions requires a proper model considering the different technical, economical and legal aspects. The goal of our research was to develop a multiagent system where rational agents control each distributed generation unit. Based on intelligent agent-program the agents are able to optimize their operations taking several viewpoints into account, like fulfilling the contractual obligations, considering the technical constraints and maximizing the realized profit in a continuously varying market environment. This paper describes a simple reinforcement learning method resulting in an adaptive agent-program. The agents are informed about their realized profits and they apply this information to evaluate their former decisions and to adjust the parameters of their agent-program. The verification of the model proved that the developed agent-program provides acceptable results compared to the real productions.	https://dx.doi.org/10.1109/DEXA.2012.31	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Divya2019	ReTra: Reinforcement based Traffic Load Balancer in Fog based Network	As the trend moves along the use of IoT devices and greater dependence on the internet which creates a huge amount of data, the traditional architecture consisting of cloud and the networking devices fail to provide real-time decisions. Thus to satisfy the need of the real-world applications, a paradigm of computing and an intelligent routing environment was introduced. The era of fog computing and Software-defined networking(SDN) increased the performance of the traditional cloud-based architecture. With the increasing data bombardment from various sources, without proper load balancing, the refined architecture also cannot render maximum performance to the application. In view of this our paper deals with the real-time fog computing environment created with the support of SDN. On top of this platform, a novel approach for load balancing based on reinforcement learning has been proposed. The algorithm understands the behavior of the network and balances the load to provide the maximum possible availability of the resources. The distributed nature of this architecture also makes the network resilient. This paper gives an overview of the built architecture and the proposed novel architecture.	https://dx.doi.org/10.1109/ICCCNT45670.2019.8944487	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Dixit2021	Deep Learning Algorithms for Cybersecurity Applications: A Technological and Status Review	Cybersecurity mainly prevents the hardware, software, and data present in the system that has an active internet connection from external attacks. Organizations mainly deploy cybersecurity for their databases and systems to prevent it from unauthorized access. Different forms of attacks like phishing, spear-phishing, a drive-by attack, a password attack, denial of service, etc. are responsible for these security problems In this survey, we analyzed and reviewed the usage of deep learning algorithms for Cybersecurity applications. Deep learning which is also known as Deep Neural Networks includes machine learning techniques that enable the network to learn from unsupervised data and solve complex problems. Here, 80 papers from 2014 to 2019 have been used and successfully analyzed. Deep learning approaches such as Convolutional Neural Network (CNN), Auto Encoder (AE), Deep Belief Network (DBN), Recurrent Neural Network (RNN), Generative Adversal Network (GAN) and Deep Reinforcement Learning (DIL) are used to categorize the papers referred. Each specific technique is effectively discussed with its algorithms, platforms, dataset, and potential benefits. The paper related to deep learning with cybersecurity is mainly published in the year 2018 in a large number and 18\% of published articles originate from the UK. In addition, the papers are selected from a variety of journals, and 30\% of papers used are from the Elsevier journal. From the experimental analysis, it is clear that the deep learning model improved the accuracy, scalability, reliability, and performance of the cybersecurity applications when applied in realtime. (C) 2020 Elsevier Inc. All rights reserved.	https://dx.doi.org/10.1016/j.cosrev.2020.100317	Excluded	new_screen	E5: Other not a paper,E1: Does not define or use a RL method	4
RL4SE	Dobre2022	Immersive machine learning for social attitude detection in virtual reality narrative games		https://doi.org/10.1007/s10055-022-00644-4	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Dobrescu2020	Hardware-in-Loop Assessment of Control Architectures	This paper proposes a method of testing by simulation of automatic control solutions of manufacturing processes that are performed on a mechatronic laboratory line. The method is based on the use of Digital Twin technology and allows the evaluation by simulation of both the hardware infrastructure of the controllers (Hardware in the Loop simulation performed as a system with discrete events) and the efficiency of control procedures (Software in the Loop simulation performed with a Reinforcement Learning algorithm).	https://dx.doi.org/10.1109/ICSTCC50638.2020.9259636	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Dobrynin2021	Reinforcement learning in the intelligent robot control task			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Dodaro2020	Managing caching strategies for stream reasoning with reinforcement learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091964374&doi=10.1017\%2fS147106842000037X&partnerID=40&md5=485964501772107f70ae12feb4785d0c	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Dohi2016	Dynamic software availability model with rejuvenation		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992653743&doi=10.15807\%2fjorsj.59.270&partnerID=40&md5=c81730172176a128503312d399ce8533	Included	new_screen		4
RL4SE	Dong2021	Generative Adversarial Network-Based Transfer Reinforcement Learning for Routing With Prior Knowledge	With the incremental deployment of software defined networking, the routing algorithms have gained more power on observability and controllability. Deep reinforcement learning, as an experience-driven approach, shows considerable potential in routing problem with the help of the centralized controller. It is an adaptive, lightweight, and model-free approach to coping with dynamic runtime status, large-scale traffic, and heterogeneous objective of SDN routing. However, it is still not suitable for the variable and complex emerging networks, because the huge training cost prevents fast convergence in a varying or discrepant environment. In this paper, we propose a transfer reinforcement learning algorithm to improve the training efficiency, and handle the variation in network status and topology. Specifically, we leverage the generative adversarial network to learn domain-invariant features that is suitable for deep reinforcement learning-based routing in different network environments. This mechanism utilizes the previous model and accelerates the training process. We implement our routing algorithm in the production level software switches and controller, while evaluating it comprehensively with many topologies and network status distributions. The experimental results show that our work not only outperforms the state-of-the-art deep reinforcement learning-based routing frameworks, but also has more training efficiency than the naive transfer learning algorithm both on different topologies and network status distributions.	https://dx.doi.org/10.1109/TNSM.2021.3077249	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Dongsun2009	Reinforcement learning-based dynamic adaptation planning method for architecture-based self-managed software	Recently, software systems face dynamically changing environments, and the users of the systems provide changing requirements at run-time. Self-management is emerging to deal with these problems. One of the key issues to achieve self-management is planning for selecting appropriate structure or behavior of self-managed software systems. There are two types of planning in self-management: off-line and on-line planning. Recent discussion has focused on off-line planning which provides static relationships between environmental changes and software configurations. In on-line planning, a software system can autonomously derive mappings between environmental changes and software configurations by learning its dynamic environment and using its prior experience. In this paper, we propose a reinforcement learning-based approach to on-line planning in architecture-based self-management. This approach enables a software system to improve its behavior by learning the results of its behavior and by dynamically changing its plans based on the learning in the presence of environmental changes. The paper presents a case study to illustrate the approach and its result shows that reinforcement learning-based on-line planning is effective for architecture-based self-management.	https://dx.doi.org/10.1109/SEAMS.2009.5069076	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Donyanavard2019	SOSA: Self-Optimizing Learning with Self-Adaptive Control for Hierarchical System-on-Chip Management		https://doi.org/10.1145/3352460.3358312	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Dowling2020	New framework for adaptive and agile honeypots	This paper proposes a new framework for the development and deployment of honeypots for evolving malware threats. As new technological concepts appear and evolve, attack surfaces are exploited. Internet of things significantly increases the attack surface available to malware developers. Previously independent devices are becoming accessible through new hardware and software attack vectors, and the existing taxonomies governing the development and deployment of honeypots are inadequate for evolving malicious programs and their variants. Malware-propagation and compromise methods are highly automated and repetitious. These automated and repetitive characteristics can be exploited by using embedded reinforcement learning within a honeypot. A honeypot for automated and repetitive malware (HARM) can be adaptive so that the best responses may be learnt during its interaction with attack sequences. HARM deployments can be agile through periodic policy evaluation to optimize redeployment. The necessary enhancements for adaptive, agile honeypots require a new development and deployment framework.	https://dx.doi.org/10.4218/etrij.2019-0155	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Doya2007	Designing the reward system: Computational and biological principles		https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548814972&doi=10.1109\%2fFOCI.2007.371540&partnerID=40&md5=d25882aea94eb61e44048682d3d2e0eb	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E5: Other not a paper	4
RL4SE	Doya2007a	Reinforcement learning: Computational theory and biological mechanisms	"Reinforcement learning is a computational framework for an active agent to learn behaviors on the basis of a scalar reward signal. The agent can be an animal, a human, or an artificial system such as a robot or a computer program. The reward can be food, water, money, or whatever measure of the performance of the agent. The theory of reinforcement learning, which was developed in an artificial intelligence community with intuitions from animal learning theory, is now giving a coherent account on the function of the basal ganglia. It now serves as the ""common language"" in which biologists, engineers, and social scientists can exchange their problems and findings. This article reviews the basic theoretical framework of reinforcement learning and discusses its recent and future contributions toward the understanding of animal behaviors and human decision making."	https://www.ncbi.nlm.nih.gov/pubmed/19404458	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E5: Other not a paper	4
RL4SE	Du2021	A 4$times$4 Sudoku Solving Model Based on Multi-layer Perceptron		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119400646&doi=10.1109\%2fEIECS53707.2021.9587969&partnerID=40&md5=8c37f3d449dc14d08d5876bb53c04992	Excluded	new_screen	E5: Other not a paper,E2: Software engineering is not the problem RL is used for	4
RL4SE	Dudarenko2019	Reinforcement Learning Approach for Navigation of Ground Robotic Platform in Statically and Dynamically Generated Environments	This paper considers robotic platform navigation in terms of logistics, movement and track routing within indoor environments. Smart navigation and platform routing using a neural network are investigated. The paper discusses environment modeling with Unity ML software suite in static (prefabricated) and dynamically generated environments. Along with reinforcement learning, a procedural generation approach and its possible industrial applications are considered. The proposed algorithm for environment generation is characterized by higher performance comparing to analogues and allows to avoid model overfitting. (C) 2019, IFAC (International Federation of Automatic Control) Hosting by Elsevier Ltd. All rights reserved.	https://dx.doi.org/10.1016/j.ifacol.2019.12.579	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Duggins2022	Reinforcement Learning, Social Value Orientation, and Decision Making: Computational Models and Empirical Validation			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Duque2022	Community energy storage operation via reinforcement learning with eligibility traces	The operation of a community energy storage system (CESS) is challenging due to the volatility of photovoltaic distributed generation, electricity consumption, and energy prices. Selecting the optimal CESS setpoints during the day is a sequential decision problem under uncertainty, which can be solved using dynamic learning methods. This paper proposes a reinforcement learning (RL) technique based on temporal difference learning with eligibility traces (ET). It aims to minimize the day-ahead energy costs while maintaining the technical limits at the grid coupling point. The performance of the RL is compared against an oracle based on a deterministic mixed-integer second-order constraint program (MISOCP). The use of ET boosts the RL agent learning rate for the CESS operation problem. The ET effectively assigns credit to the action sequences that bring the CESS to a high state of charge before the peak prices, reducing the training time. The case study shows that the proposed method learns to operate the CESS effectively and ten times faster than common RL algorithms applied to energy systems such as Tabular Q-learning and Fitted-Q. Also, the RL agent operates the CESS 94\% near the optimal, reducing the energy costs for the end-user up to 12\%.	https://dx.doi.org/10.1016/j.epsr.2022.108515	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Durmaz2022	Intelligent software debugging: A reinforcement learning approach for detecting the shortest crashing scenarios	The Quality Assurance (QA) team verifies software for months before its release decisions. Nevertheless, some crucial bugs remain undetected in manual testing. These bugs would make the system unusable on field, thus merchant loses money then manufacturer loses its customers. Thus, automatic software testing methods have become inevitable to catch more bugs. To locate and repair bugs with an emphasis on the crash scenarios, we present in this work a reinforcement learning (RL) approach for finding and simplifying the input sequence(s) leading to a system crash or blocking, which represents the goal state of the RL problem. We aim at obtaining the shortest input sequence for the same bug so that developers would analyze agent's actions causing crashes or freeze. We first simplify the given crash scenario using Recursive Delta Debugging (RDD), then we apply RL algorithms to explore a possibly shorter crashing sequence. We approach the exploration of crash scenarios as a RL problem where the agent first attains the goal state of crash/blocking by executing inputs, then shortens the input sequence with the help of the rewarding mechanism. We apply both model-free on-policy and model-based planning-capable RL agents to our problem. Furthermore, we present a novel RL approach, involving Detected Goal Catalyst (DGC), which reduces the time complexity by avoiding grappling with convergence via stopping learning at a small variance and attaining the shortest crash sequence with an algorithm that recursively removes the unrelated actions. Experiments show DGC significantly improves the learning performance of both SARSA and Prioritized Sweeping algorithms on obtaining the shortest path.	https://dx.doi.org/10.1016/j.eswa.2022.116722	Included	new_screen		4
RL4SE	Dusparic2007	Research Issues in Multiple Policy Optimization Using Collaborative Reinforcement Learning	Self-organizing techniques have successfully been used to optimize software systems, such as optimization of route stability in ad hoc network routing and optimization of the use of storage space or processing power using load balancing. Existing self-organizing techniques typically focus on a single, usually implicitly specified, system goal and tune systems parameters towards optimally meeting that goal. In this paper, we consider optimization of large-scale multi-agent ubiquitous computing environments, such as urban traffic control. Applications in this class are typically required to optimize towards multiple goals simultaneously. Additionally, these multiple goals can potentially be conflicting, change over time, and apply to various parts of the system such as a single agent, a group of agents, or the system as a whole. In contrast to existing self-organizing systems in which agents are homogeneous to the extent that they are working towards a common goal, agents in these systems are heterogeneous in that they may have differing goals. Thus, existing self-organizing optimization techniques must be extended to deal with multiple goal optimization and the resulting heterogeneity of agents. In this paper we present a research agenda for extending collaborative reinforcement learning (CRL), an existing self-organizing optimization technique, to support multiple policy optimization.	https://dx.doi.org/10.1109/SEAMS.2007.17	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E3: Only conceptual results are reported	4
RL4SE	Dusparic2017	Residential demand response: Experimental evaluation and comparison of self-organizing techniques		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023608184&doi=10.1016\%2fj.rser.2017.07.033&partnerID=40&md5=178a135960b4fe64cd6d5503145c5627	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Dutreilh2011	Using Reinforcement Learning for Autonomic Resource Allocation in Clouds: Towards a Fully Automated Workflow	Dynamic and appropriate resource dimensioning is a crucial issue in cloud computing. As applications go more and more 24/7, online policies must be sought to balance performance with the cost of allocated virtual machines. Most industrial approaches to date use ad hoc manual policies, such as threshold based ones. Providing good thresholds proved to be tricky and hard to automatize to fit every application requirement. Research is being done to apply automatic decision-making approaches, such as reinforcement learning. Yet, they face a lot of problems to go to the field: having good policies in the early phases of learning, time for the learning to converge to an optimal policy and coping with changes in the application performance behavior over time. In this paper, we propose to deal with these problems using appropriate initialization for the early stages as well as convergence speedups applied throughout the learning phases and we present our first experimental results for these. We also introduce a performance model change detection on which we are currently working to complete the learning process management. Even though some of these proposals were known in the reinforcement learning field, the key contribution of this paper is to integrate them in a real cloud controller and to program them as an automated workflow.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ebin2019	Knowledge assimilation of machines using various approaches			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E5: Other not a paper	4
RL4SE	Edwards2021	Automatic Tuning for Data-driven Model Predictive Control	Model predictive control (MPC) is a powerful feedback technique that is often used in data-driven robotics. The performance of data-driven MPC depends on the accuracy of the model, which often requires careful tuning. Furthermore, specifying the task with an objective function and synthesizing a feedback policy are not straightforward and typically lead to suboptimal solutions driven by trial and error. To address these challenges, we present a method to jointly optimize the data-driven system identification, task specification, and control synthesis of unknown dynamical systems. We use our method to develop AutoMPC3, a software package designed to automate and optimize data-driven MPC. Empirical evaluation on the pendulum swing-up, cart-pole swing-up, and half-cheetah running demonstrates that our method finds data-driven control policies that outperform offline reinforcement learning, without any hand-tuning.	https://dx.doi.org/10.1109/ICRA48506.2021.9562025	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Eitan2011	Adaptive Behavioral Programming	We introduce a way to program adaptive reactive systems, using behavioral, scenario-based programming. Extending the semantics of live sequence charts with reinforcements allows the programmer not only to specify what the system should do or must not do, but also what it should try to do, in an intuitive and incremental way. By integrating scenario-based programs with reinforcement learning methods, the program can adapt to the environment, and try to achieve the desired goals. Visualization methods and modular learning decompositions, based on the unique structure of the program, are suggested, and result in an efficient development process and a fast learning rate.	https://dx.doi.org/10.1109/ICTAI.2011.109	Included	new_screen		4
RL4SE	ElZaatari2021	iTP-LfD: Improved task parametrised learning from demonstration for adaptive path generation of cobot	An approach of Task-Parameterised Learning from Demonstration (TP-LfD) aims at automatically adapting the movements of collaborative robots (cobots) to new settings using knowledge learnt from demonstrated paths. The approach is suitable for encoding complex relations between a cobot and its surrounding, i.e., task-relevant objects. However, further efforts are still required to enhance the intelligence and adaptability of TP-LfD for dynamic tasks. With this aim, this paper presents an improved TP-LfD (iTP-LfD) approach to program cobots adaptively for a variety of industrial tasks. iTP-LfD comprises of three main improvements over other developed TP-LfD approaches: 1) detecting generic visual features for frames of reference (frames) in demonstrations for path reproduction in new settings without using complex computer vision algorithms, 2) minimising redundant frames that belong to the same object in demonstrations using a statistical algorithm, and 3) designing a reinforcement learning algorithm to eliminate irrelevant frames. The distinguishing characteristic of the iTP-LfD approach is that optimal frames are identified from demonstrations by simplifying computational complexity, overcoming occlusions in new settings, and boosting the overall performance. Case studies for a variety of industrial tasks involving different objects and scenarios highlight the adaptability and robustness of the iTP-LfD approach.	https://dx.doi.org/10.1016/j.rcim.2020.102109	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Elgendy2021	Joint computation offloading and task caching for multi-user and multi-task MEC systems: reinforcement learning-based algorithms		https://doi.org/10.1007/s11276-021-02554-w	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Eliassi-Rad2003	A system for building intelligent agents that learn to retrieve and extract information	We present a system for rapidly and easily building instructable and self-adaptive software agents that retrieve and extract information. Our Wisconsin Adaptive Web Assistant ( WAWA) constructs intelligent agents by accepting user preferences in the form of instructions. These user-provided instructions are compiled into neural networks that are responsible for the adaptive capabilities of an intelligent agent. The agent's neural networks are modified via user-provided and system-constructed training examples. Users can create training examples by rating Web pages (or documents), but more importantly WAWA's agents uses techniques from reinforcement learning to internally create their own examples. Users can also provide additional instruction throughout the life of an agent. Our experimental evaluations on a 'home-page finder' agent and a 'seminar-announcement extractor' agent illustrate the value of using instructable and adaptive agents for retrieving and extracting information.	https://dx.doi.org/10.1023/A:1024009718142	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	El-Tantawy2012	Multi-Agent Reinforcement Learning for Integrated Network of Adaptive Traffic Signal Controllers (MARLIN-ATSC)	Traffic congestion in Greater Toronto Area costs Canada $ 6 billion /year and is expected to grow up to $ 15 billion /year in the next few decades. Adaptive Traffic Signal Control(ATSC) is a promising technique to alleviate traffic congestion. For medium-large transportation networks, coordinated ATSC is becoming a challenging problem because the number of system states and actions grows exponentially as the number of networked intersections grows. Efficient and robust controllers can be designed using a multi-agent reinforcement learning (MARL) approach in which each controller (agent) is responsible for the control of traffic lights around a single traffic junction. This paper presents a novel, decentralized and coordinated adaptive real-time traffic signal control system using Multi-Agent Reinforcement Learning for Integrated Network of Adaptive Traffic Signal Controllers (MARLINATSC) that aims to minimize the total vehicle delay in the traffic network. The system is tested using microscopic traffic simulation software (PARAMICS) on a network of 5 signalized intersections in Downtown Toronto. The performance of MARLIN-ATSC is compared against two approaches: the conventional pretimed signal control (B1) and independent RL-based control agents (B2), i.e. with no coordination. The results show that network-wide average delay savings range from 32\% to 63\% relative to B1 and from 7\% to 12\% relative to B2 under different demand levels and arrival profiles.	https://dx.doi.org/10.1109/ITSC.2012.6338707	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Emam2018	Inferring Extended Probabilistic Finite-State Automaton Models from Software Executions	Behavioral models are useful tools in understanding how programs work. Although several inference approaches have been introduced to generate extended finite-state automatons from software execution traces, they suffer from accuracy, flexibility, and decidability issues. In this article, we apply a hybrid technique to use both reinforcement learning and stochastic modeling to generate an extended probabilistic finite state automaton from software traces. Our approach-ReHMM (Reinforcement learning-based Hidden Markov Modelling)-is able to address the problems of inflexibility and un-decidability reported in other state-of-the-art approaches. Experimental results indicate that ReHMM outperforms other inference algorithms.	https://dx.doi.org/10.1145/3196883	Included	new_screen		4
RL4SE	Eniser2022	Metamorphic relations via relaxations: an approach to obtain oracles for action-policy testing		https://doi.org/10.1145/3533767.3534392	Included	conflict_resolution		4
RL4SE	Ertel2009	The Teaching-Box: A universal robot learning framework	There exist many powerful machine learning software libraries [15], which help the engineer to build robots that learn autonomously. However, engineering of an autonomous robot still is a challenging and time consuming task even with these learning libraries. With the open source Teaching-Box presented here, the ``training'' of a robot becomes easier due to the following features. The Java library of the Teaching-Box provides algorithms for reinforcement learning as well as for learning by demonstration (utilizing supervised learning algorithms) and data structures for exchanging policies between the different ways of learning. As an initial policy one can even take a manually coded behaviour and then improve it for example with reinforcement learning. A human trainer feedback (e.g. via the speech interface) can be used to increase the learning speed. The Eclipse based GUI facilitates the design of the robot learning projects and visualizes the learning process. For connecting the various modules of a project, open interface standards such as RL-Glue are used and an easy integration of the Teaching-Box into standard robot middleware is possible.		Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Esnaashari2021	Automation of software test data generation using genetic algorithm and reinforcement learning		https://doi.org/10.1016/j.eswa.2021.115446	Included	new_screen		4
RL4SE	Esparcia-Alcazar2017	Evolving Rules for Action Selection in Automated Testing via Genetic Programming - A First Approach	Tools that perform automated software testing via the user interface rely on an action selection mechanism that at each step of the testing process decides what to do next. This mechanism is often based on random choice, a practice commonly referred to as monkey testing. In this work we evaluate a first approach to genetic programming (GP) for action selection that involves evolving IF-THEN-ELSE rules; we carry out experiments and compare the results with those obtained by random selection and also by Q-learning, a reinforcement learning technique. Three applications are used as Software Under Test (SUT) in the experiments, two of which are proprietary desktop applications and the other one an open source web-based application. Statistical analysis is used to compare the three action selection techniques on the three SUTs; for this, a number of metrics are used that are valid even under the assumption that access to the source code is not available and testing is only possible via the GUI. Even at this preliminary stage, the analysis shows the potential of GP to evolve action selection mechanisms.	https://dx.doi.org/10.1007/978-3-319-55792-2_6	Excluded	conflict_resolution	E1: Does not define or use a RL method,E1: Does not define or use a RL method	4
RL4SE	Esparcia-Alcazar2018	Using genetic programming to evolve action selection rules in traversal-based automated software testing: results obtained with the TESTAR tool	Traversal-based automated software testing involves testing an application via its graphical user interface (GUI) and thereby taking the user's point of view and executing actions in a human-like manner. These actions are decided on the fly, as the software under test (SUT) is being run, as opposed to being set up in the form of a sequence prior to the testing, a sequence that is then used to exercise the SUT. In practice, random choice is commonly used to decide which action to execute at each state (a procedure commonly referred to as monkey testing), but a number of alternative mechanisms have also been proposed in the literature. Here we propose using genetic programming (GP) to evolve such an action selection strategy, defined as a list of IF-THEN rules. Genetic programming has proved to be suited for evolving all sorts of programs, and rules in particular, provided adequate primitives (functions and terminals) are defined. These primitives must aim to extract the most relevant information from the SUT and the dynamics of the testing process. We introduce a number of such primitives suited to the problem at hand and evaluate their usefulness based on various metrics. We carry out experiments and compare the results with those obtained by random selection and also by Q-learning, a reinforcement learning technique. Three applications are used as Software Under Test (SUT) in the experiments. The analysis shows the potential of GP to evolve action selection strategies.	https://dx.doi.org/10.1007/s12293-018-0263-8	Included	new_screen		4
RL4SE	Esteso2022	Reinforcement learning applied to production planning and control	The objective of this paper is to examine the use and applications of reinforcement learning (RL) techniques in the production planning and control (PPC) field addressing the following PPC areas: facility resource planning, capacity planning, purchase and supply management, production scheduling and inventory management. The main RL characteristics, such as method, context, states, actions, reward and highlights, were analysed. The considered number of agents, applications and RL software tools, specifically, programming language, platforms, application programming interfaces and RL frameworks, among others, were identified, and 181 articles were sreviewed. The results showed that RL was applied mainly to production scheduling problems, followed by purchase and supply management. The most revised RL algorithms were model-free and single-agent and were applied to simplified PPC environments. Nevertheless, their results seem to be promising compared to traditional mathematical programming and heuristics/metaheuristics solution methods, and even more so when they incorporate uncertainty or non-linear properties. Finally, RL value-based approaches are the most widely used, specifically Q-learning and its variants and for deep RL, deep Q-networks. In recent years however, the most widely used approach has been the actor-critic method, such as the advantage actor critic, proximal policy optimisation, deep deterministic policy gradient and trust region policy optimisation.	https://dx.doi.org/10.1080/00207543.2022.2104180	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Eto2008	Simulation-Based Optimization Approach for Software Cost Model with Rejuvenation		https://doi.org/10.1007/978-3-540-69295-9_18	Included	new_screen		4
RL4SE	Everett2021	Neural Network Verification in Control	Learning-based methods could provide solutions to many of the long-standing challenges in control. However, the neural networks (NNs) commonly used in modern learning approaches present substantial challenges for analyzing the resulting control systems' safety properties. Fortunately, a new body of literature could provide tractable methods for analysis and verification of these high dimensional, highly nonlinear representations. This tutorial first introduces and unifies recent techniques (many of which originated in the computer vision and machine learning communities) for verifying robustness properties of NNs. The techniques are then extended to provide formal guarantees of neural feedback loops (e.g., closed-loop system with NN control policy). The provided tools are shown to enable closed-loop reachability analysis and robust deep reinforcement learning.Software\endash https://github.com/mit-acl/nn_robustness_analysis	https://dx.doi.org/10.1109/CDC45484.2021.9683154	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Fabarisov2022	FIDGET: Deep Learning-Based Fault Injection Framework for Safety Analysis and Intelligent Generation of Labeled Training Data	Since the introduction of the term Cyber-Physical Systems (CPS) in 2006, they came to a long way. CPS are now autonomous and networked systems of systems with state-space exceeding the capabilities of conventional risk analysis methods. Model-based fault injection methods allow assessment of a system's fault tolerance not only during its design phase but also in the course of operation. This allows the evaluation of updates and new modules before deploying such changes to a real system. Such operational model-based fault injection on a system's digital twin can ensure continuous safety throughout all system life cycles.Modern risk analysis tools and Machine Learning-based safety methods require vast amounts of representative input and training data. Such methods not only will require mountains of erroneous time-series data from a myriad of operational cycles, but also corresponding fault parameter labels. As the state space of the system component explodes in complexity, it becomes problematic to cover all possible component fault combinations. As such, only those faults that could lead to potential failures or increased risk scenarios are of interest for automated safety assessment methodologies. It is clear that an intelligent and effective model-based fault injection method is required for the operational safety assessment of industrial CPS.Recently we introduced a new model-based fault injection method implemented as a highly customizable Simulink block called FIBlock. It supports the model-based injection of typical faults of CPS components such as sensors, software, computing, and network hardware. In this paper, we proposed a Deep Learning-based approach for model-based fault injection called FIDGET. It extends the FIBlock with Deep Reinforcement Learning capabilities. We employed a Deep Deterministic Policy Gradient algorithm with Long Short-Term Memory (LSTM) architecture to train the Reinforcement Learning agent to per-form the automated search of fault parameters that yield the biggest system response. It allows automatic generation of labeled training data for further use in risk analysis tools or to train fault classifiers. The generated training data consists of errors that lead to the biggest response (i.e., disturbance) of the system.	https://dx.doi.org/10.1109/ETFA52439.2022.9921507	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Fagg1992	A Model of Primate Visual-Motor Conditional Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002869092&doi=10.1177\%2f105971239200100102&partnerID=40&md5=22e5344fb857748e82e82bf55b791f2a	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Fagg1994a	A reinforcement-learning approach to reactive control policy design for autonomous robots	"Within the field of robotics, much recent attention has been given to control techniques that have been termed reactive or behavior-based. The design of such control systems for even a remotely interesting task is typically a laborious effort, requiring many hours of experimental ""tweaking"" as the actual behavior of the system is observed by the system designer. In this paper, the authors present a neural-based reinforcement learning approach to the design of reactive control policies in which the designer specifies the the desired behavior of the system, rather than the control program that produces the desired behavior.<>"	https://dx.doi.org/10.1109/ROBOT.1994.351013	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Fan2022	Optimizing quantum circuit placement via machine learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137452748&doi=10.1145\%2f3489517.3530403&partnerID=40&md5=d29c4cffa5f621c9385b8a477ebbc44d	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Fan2022a	DRL-D: Revenue-Aware Online Service Function Chain Deployment via Deep Reinforcement Learning	Network function virtualization (NFV) is a promising paradigm where network functions are migrated from dedicated hardware appliances onto software middleboxes to promote service agility and reduce management costs. Benefiting from the NFV, the service function chain (SFC) has emerged as a popular network service form. It allows network traffic to pass through a series of virtual network functions in a specific order required by the business logic to arrange a complex service. However, SFC deployment is facing new challenges in seeking a trade-off between pursuing the objective of high long-term average revenue and making decisions in an online manner. In this paper, we propose DRL-D, a deep reinforcement learning-based approach for the online SFC deployment problem to satisfy different demands of SFC requests within resource constraints of the underlying infrastructure. DRL-D aims to maximize the long-term average revenue by combining the strengths of the graph convolutional network in learning a comprehensive representation of network state and the temporal-difference learning in generating deployment solutions for the SFC requests on the fly. Then a heuristic algorithm and a new prioritized experience replay technique are integrated to optimize the DRL framework and reduce the time complexity. Experimental results demonstrate the superiority of our DRL-D approach when compared with other benchmarks in terms of the long-term average revenue, acceptance ratio, and revenue-to-cost ratio. Performance evaluation shows that DRL-D possesses good robustness under different scales of physical networks and achieves excellent deployment performance within acceptable runtime.	https://dx.doi.org/10.1109/TNSM.2022.3181517	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bastani2022	Interpretable, Verifiable, and Robust Reinforcement Learning via Program Synthesis		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128972873&doi=10.1007\%2f978-3-031-04083-2_11&partnerID=40&md5=a0e1d14bc23db27169312d44a8c416cd	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Baumgart2022	Optimal Control of Traffic Flow Based on Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140486269&doi=10.1007\%2f978-3-031-17098-0_16&partnerID=40&md5=e25f17844dfcf791eb4ea28fe9eca140	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Belinsky2021	Optimal Control of Energy Pipeline Systems Based on Deep Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097936601&doi=10.1007\%2f978-3-030-59126-7_148&partnerID=40&md5=8ee84e99e740475e40d4206d9c89e085	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Berndt2016	Anticipatory behavior of software agents in self-organizing negotiations		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942337515&doi=10.1007\%2f978-3-319-22599-9_15&partnerID=40&md5=bf1e9738b0d8a1912d84be128c2abb3c	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bosello2019_1	From Programming Agents to Educating Agents -- A Jason-Based Framework For Integrating Learning in the Development Of Cognitive Agents	Recent advances and successes of machine learning techniques are paving the way to what is referred as Software 2.0 era and cognitive computing, in which traditional programming and software development is meant to be replaced by such techniques for many applications. If we consider agent-oriented programming, we believe that such developments trigger new interesting scenarios blending cognitive architecture such as the BDI one and techniques like Reinforcement Learning (RL) even more deeply compared to what has been proposed so far in the literature. In that perspective, we aim at exploring the integration of cognitive agent-oriented programming based on BDI with learning techniques so as to systematically exploit them in the agent development stage. The approach should support the design of BDI agents in which some plans can be explicitly programmed and others instead can be learned by the agent during the development/engineering stage. In that view, the development of an agent is metaphorically similar to an education process, in which first an agent is created with a set of basic programmed plans and then grow up in order to learn plans to achieve the goals for which the agent is meant to be designed. This paper presents and discusses this medium-term view, introducing a first model for a BDI agent programming framework integrating RL, a first implementation based on Jason programming language/platform and sketching a roadmap for this research line.	https://dx.doi.org/10.1007/978-3-030-51417-4_9	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bottinger2019	Hunting bugs with nature-inspired fuzzing	Motivated by the urgent need for secure software, we construct new testing methods inspired by biology to improve current development life cycles. We connect probability theory with current testing technologies by formulating feedback-driven fuzzing in the language of stochastic processes. This mathematical model allows us to translate deep results from probability theory into algorithms for software testing. Exploring the full capabilities of our model leads us to the application of reinforcement learning methods, which turns out to be a fruitful new direction in software testing.	https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118017701&doi=10.1049\%2fPBSE010E_ch13&partnerID=40&md5=4dd5eff135e9135f2497c21713f56821	Included	new_screen		4
RL4SE	Campos2003	Abalearn: A risk-sensitive approach to self-play learning in abalone	This paper presents Abalearn, a self-teaching Abalone program capable of automatically reaching an intermediate level of play without needing expert-labeled training examples, deep searches or exposure to competent play. Our approach is based on a reinforcement learning algorithm that is risk-seeking, since defensive players in Abalone tend to never end a game. We show that it is the risk-sensitivity that allows a successful self-play training. We also propose a set of features that seem relevant for achieving a good level of play. We evaluate our approach using a fixed heuristic opponent a's a benchmark, pitting our agents against human players online and comparing samples of our agents at different times of training.	https://doi.org/10.1007/978-3-540-39857-8_6	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cao2022a	A Holistic Automated Software Structure Exploration Framework for Testing	Exploring the underlying structure of a Human-Machine Interface (HMI) product effectively while adhering to the pre-defined test conditions and methodology is critical for validating the quality of the software. We propose an reinforcement-learning powered Automated Software Structure Exploration Framework for Testing (ASSET), which is capable of interacting with and analyzing the HMI software under testing (SUT). The main challenge is to incorporate the human instructions into the ASSET phase by using the visual feedback such as the downloaded image sequence from the HMI, which could be difficult to analyze. Our framework combines both computer vision and natural language processing techniques to understand the semantic meanings of the visual feedback. Building on the semantic understanding, we develop a rules-guided software exploration algorithm via reinforcement learning and deterministic finite automaton (DFA). We conducted experiments on HMI software in actual production phase and demonstrate that the exploration coverage and efficiency of our framework outperforms current start-of-art methods.	https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139791555&doi=10.3233\%2fFAIA220259&partnerID=40&md5=fd3ca208c7ae75abf03f186e5bb075bf	Included	new_screen		4
RL4SE	Cardoso2001	A platform for electronic commerce with adaptive agents			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cave2019	Can Machines Learn Whether Machines Are Learning to Collude?		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076570267&doi=10.1007\%2f978-3-030-34770-3_11&partnerID=40&md5=38ac58411686214e8329af79f73de432	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cazenave2019	Spatial Average Pooling for Computer Go	Computer Go has improved up to a superhuman level thanks to Monte Carlo Tree Search (MCTS) combined with Deep Learning. The best computer Go programs use reinforcement learning to train a policy and a value network. These networks are used in a MCTS algorithm to provide strong computer Go players. In this paper we propose to improve the architecture of a value network using Spatial Average Pooling.	https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069212784&doi=10.1007\%2f978-3-030-24337-1_6&partnerID=40&md5=5ba206b88987db7a4ee31bc582f07cca	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2010	Neuroeconomics: A viewpoint from agent-based computational economics	Recently, the relation between neuroeconomics and agent-based computational economics (ACE) has become an issue concerning the agent-based economics community. Neuroeconomics can interest agent-based economists when they are inquiring for the foundation or the principle of the software-agent design, normally known as agent engineering. It has been shown in many studies that the design of software agents is non-trivial and can determine what will emerge from the bottom. Therefore, it has been quested for rather a period regarding whether we can sensibly design these software agents, including both the choice of software agent models, such as reinforcement learning, and the parameter setting associated with the chosen model, such as risk attitude. In this chapter, we shall start a formal inquiry by focusing on examining the models and parameters used to build software agents.	https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900218594&doi=10.4018\%2f978-1-60566-898-7.ch003&partnerID=40&md5=274e5706b97fc19dc64c9ca56d5d16e2	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2018c_1	Reinforcement learningendashbased QoS/QoE?aware service function chaining in software?driven 5G slices		https://doi.org/10.1002/ett.3477	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chhatbar2020	MACER: A modular framework for accelerated compilation error repair		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089615679&doi=10.1007\%2f978-3-030-52237-7_9&partnerID=40&md5=ad1d7015c5cf051b8b757569533b6e34	Excluded	new_screen	E1: Does not define or use a RL method,E1: Does not define or use a RL method	4
RL4SE	Choe2004	Autonomous acquisition of the meaning of sensory states through sensory-invariance driven action	How can artificial or natural agents autonomously gain understanding of its own internal (sensory) state? This is an important question not just for physically embodied agents but also for software agents in the information technology environment. In this paper, we investigate this issue in the context of a simple biologically motivated sensorimotor agent. We observe and acknowledge, as many other researchers do, that action plays a key role in providing meaning to the sensory state. However, our approach differs from the others: We propose a new learning criterion, that of on-going maintenance of sensory invariance. We show that action sequence resulting from reinforcement learning of this criterion accurately portrays the property of the input that triggered a certain sensory state. This way, the meaning of a sensory state can be firmly grounded on the choreographed action which maintains invariance in the internal state.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	DePaula2009	Data-driven generation of multi-modal control programs for continuous-discrete processes		https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649124458&doi=10.1016\%2fS1570-7946\%2809\%2970044-6&partnerID=40&md5=dcac95672e0bed894c933380090d87f9	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Deshpande2022a	Post-hoc Explainable Reinforcement Learning Using Probabilistic Graphical Models		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125253577&doi=10.1007\%2f978-3-030-95502-1_28&partnerID=40&md5=70743eb19f8c8cbbb2a72463b7c24ad0	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	ElBouchefry2020	Learning in Big Data: Introduction to Machine Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104976699&doi=10.1016\%2fB978-0-12-819154-5.00023-0&partnerID=40&md5=e8197a3afc15c1283c7ba1bdbc955570	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Eremin2021	A Reinforcement Learning Approach for Task Assignment in IoT Distributed Platform		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104319334&doi=10.1007\%2f978-3-030-67892-0_31&partnerID=40&md5=d3a4239c4ccc2166f60758af34a3ffd5	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Etheve2020	Reinforcement Learning for Variable Selection in a Branch and Bound Algorithm		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092172979&doi=10.1007\%2f978-3-030-58942-4_12&partnerID=40&md5=3fc1f10687d9832d677b65627de1815b	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Fagg1994	Reinforcement learning for robotic reaching and grasping		https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956789090&doi=10.1016\%2fS0166-4115\%2808\%2961283-2&partnerID=40&md5=1e29e2ac963e6feb32edc343f26b9fbd	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Fagg2021	Rapid reinforcement learning for reactive control policy design in autonomous robots	This paper describes work in progress on a neural-based reinforcement learning architecture for the design of reactive control policies for an autonomous robot. Reinforcement learning techniques allow a programmer to specify the control program at the level of the desired behavior of the robot, rather than at the level of the program that generates the behavior. In this paper, we explicitly begin to address the issue of state representation which can greatly affect the system's ability to learn quickly and to apply what has already been learned to novel situations. Finally, we demonstrate the architecture as applied towards a real robot that is learning to move safely about its environment.	https://www.taylorfrancis.com/chapters/edit/10.4324/9781315784076-23/rapid-reinforcement-learning-reactive-control-policy-design-autonomous-robots-andrew-fagg-david-lotspeich-joel-hoff-george-bekey	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2018c_2	Reinforcement learningendashbased QoS/QoE?aware service function chaining in software?driven 5G slices		https://doi.org/10.1002/ett.3477	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2018c_3	Reinforcement learningendashbased QoS/QoE?aware service function chaining in software?driven 5G slices		https://doi.org/10.1002/ett.3477	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2018c_4	Reinforcement learningendashbased QoS/QoE?aware service function chaining in software?driven 5G slices		https://doi.org/10.1002/ett.3477	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2018c_5	Reinforcement learningendashbased QoS/QoE?aware service function chaining in software?driven 5G slices		https://doi.org/10.1002/ett.3477	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2018c_6	Reinforcement learningendashbased QoS/QoE?aware service function chaining in software?driven 5G slices		https://doi.org/10.1002/ett.3477	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2018c_7	Reinforcement learningendashbased QoS/QoE?aware service function chaining in software?driven 5G slices		https://doi.org/10.1002/ett.3477	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Fan2019	Position Control and Production of Various Strategies for Deep Learning Go Programs	Computer Go programs have exceeded top-level human players by using deep learning and reinforcement learning techniques. On the other hand, Entertainment Go AI or Coaching Go AI are also interesting directions which have not been well investigated. Several researches have been done for entertaining beginners or intermediate players. Position control or producing various strategies are important tasks, and some methods have been proposed and evaluated using a traditional Monte-Carlo tree search program. In this paper, we try to adapt the method to LeelaZero, a program based on AlphaGo Zero. There are some critical differences between the previous program and the new program. For example the new program does not use random simulations to the ends of games, then the previous method for producing various strategies cannot be used. In this paper we summarized the differences and some expected problems, and proposed several approaches to solve the problems. It was shown that the modified LeelaZero could play gently against weaker players (48\% won against a program Ray). Through experiments using human subjects, it was shown that the average number of unnatural moves per game was 1.22, where that by a simple method without considering naturalness was 2.29. Also we evaluated the proposed method for training center-oriented and edge/corner-oriented players, and it was confirmed that human players could identify the produced strategy (center or edge/corner) with a probability of 71.88\%.	https://dx.doi.org/10.1109/TAAI48200.2019.8959895	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Fandango2022	ARORA & NavSim: a simulator system for training autonomous agents with geospecific data		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145225864&doi=10.1117\%2f12.2647733&partnerID=40&md5=e83efb0db5828f1f6a11ddf563d23636	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Fang2021	Simultaneous Localization of Multiple Defects in Software Testing Based on Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113377243&doi=10.1007\%2f978-3-030-82562-1_16&partnerID=40&md5=85b9d4932782ea92399d5786667791c4	Included	new_screen		4
RL4SE	Fang2010	Research and application of multi-agent model for aircraft PHM	With the increasing scale of aircraft, the PHM (Prognostics and Health Management) structure of real-time sensor-based embedded aircraft software systems become more complicated, thus data-collection becomes inefficient and the system may be invalid due to varieties of failures. In this paper, the information collection and storage and subsystem fault diagnosis and forecasting methods in these real-time embedded software systems are investigated, and a new model base on multi-agent SMDP (semi-Markov decision processes) reinforcement learning is developed. The experimental results show the model reduces the rate of data loss efficiently, and increases the average utilization of CPU, and thus improves the information collection speed and detection accuracy.	https://dx.doi.org/10.1109/ICIME.2010.5478124	Excluded	conflict_resolution	E5: Other not a paper,E5: Other not a paper	4
RL4SE	Fang2019	Evading Anti-Malware Engines With Deep Reinforcement Learning	To reduce the risks of malicious software, malware detection methods using machine learning have received tremendous attention in recent years. Most of the conventional methods are based on supervised learning, which relies on static features with definite labels. However, recent studies have shown the models based on supervised learning are vulnerable to deliberate attacks. This work tends to expose and demonstrate the weakness in these models. A DQEAF framework using reinforcement learning to evade anti-malware engines is presented. DQEAF trains an AI agent through a neural network by constantly interacting with malware samples. Actions are a set of reasonable modifications, which do not damage samples' structure and functions. The agent selects the optimal sequence of actions to modify the malware samples, thus they can bypass the detection engines. The training process depends on the characteristics of the raw binary stream features of samples. The experiments show that the proposed method has a success rate of 75\%. The efficacy of the proposed DQEAF has also been evaluated by other families of malicious software, which shows good robustness.	https://dx.doi.org/10.1109/ACCESS.2019.2908033	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Fanti2022	Safety and Comfort in Autonomous Braking System with Deep Reinforcement Learning	Safety issues related to autonomous vehicles are of great concern both in the academy and industry, identifying the braking system performance as a crucial research field. In this work, an autonomous braking system based on deep reinforcement learning is proposed, employing an intelligent agent trained in city scenarios to manage both pedestrians' safety and passengers' comfort. The agent is modelled via the deep deterministic policy gradient algorithm in a software environment and its performance is tested showing good results in maximizing both pedestrians' safety and passengers' comfort.	https://dx.doi.org/10.1109/SMC53654.2022.9945383	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Farhan2020	Reinforcement Learning in Anylogic Simulation Models: A Guiding Example Using Pathmind	Reinforcement Learning has recently gained a lot of exposure in the simulation industry. In this paper, we demonstrate the use of reinforcement learning in AnyLogic software models using Pathmind. A coffee shop simulation is built to train a barista to make correct operational decisions and improve efficiency that directly affects customer service time. The trained policy outperforms rule-based functions in terms of customer service time and throughput.	https://dx.doi.org/10.1109/WSC48552.2020.9383916	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Farjadnasab2022	Model-free LQR design by Q-function learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121563871&doi=10.1016\%2fj.automatica.2021.110060&partnerID=40&md5=5cc9945f6b69f859d137931898c79d81	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Fasel2009	A task specification language for bootstrap learning			Excluded	new_screen	E5: Other not a paper,E2: Software engineering is not the problem RL is used for	4
RL4SE	Fathabadi2018	A model-based framework for software portability and verification in embedded power management systems	Run-Time Management (RTM) systems are used in embedded systems to dynamically adapt hardware performance to minimise energy consumption. A significant challenge is that RTM software can require laborious manual adjustment across different hardware platforms due to the diversity of architecture characteristics. Model-driven development offers the potential to simplify the management of platform diversity by shifting the focus away from hand-written platform-specific code to platform-independent models from which platform specific implementations are automatically generated. Furthermore, the use of formal verification provides the means to ensure that implementations are correct-by-construction. In this paper, we present a framework for automatic generation of RTM implementations from platform-independent formal models. The methodology in designing the RTM systems uses a high-level mathematical language, Event-B, which can describe systems at different abstraction levels. A code generation tool is used to translate platform-independent Event-B RTM models to platform-specific implementations in C. Formal verification is used to ensure correctness of the Event-B models. The portability offered by our methodology is validated by modelling a Reinforcement Learning (RL) based RTM for two embedded applications and generating implementations for three different platforms (ARM Cortex-A8, A7 and A15) that all achieve energy savings on the respective platforms.	https://dx.doi.org/10.1016/j.sysarc.2017.12.001	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Faust2018	Resilient Computing with Reinforcement Learning on a Dynamical System: Case Study in Sorting	This paper formulates general computation as a feedback-control problem, which allows the agent to autonomously overcome some limitations of standard procedural language programming: resilience to errors and early program termination. Our formulation considers computation to be trajectory generation in the program's variable space. The computing then becomes a sequential decision making problem, solved with reinforcement learning (RL), and analyzed with Lyapunov stability theory to assess the agent's resilience and progression to the goal. We do this through a case study on a quintessential computer science problem, array sorting. Evaluations show that our RL sorting agent makes steady progress to an asymptotically stable goal, is resilient to faulty components, and performs less array manipulations than traditional Quicksort and Bubble sort.	https://dx.doi.org/10.1109/CDC.2018.8619634	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Fazlyab2019	Efficient and Accurate Estimation of Lipschitz Constants for Deep Neural Networks	Tight estimation of the Lipschitz constant for deep neural networks (DNNs) is useful in many applications ranging from robustness certification of classifiers to stability analysis of closed-loop systems with reinforcement learning controllers. Existing methods in the literature for estimating the Lipschitz constant suffer from either lack of accuracy or poor scalability. In this paper, we present a convex optimization framework to compute guaranteed upper bounds on the Lipschitz constant of DNNs both accurately and efficiently. Our main idea is to interpret activation functions as gradients of convex potential functions. Hence, they satisfy certain properties that can be described by quadratic constraints. This particular description allows us to pose the Lipschitz constant estimation problem as a semidefinite program (SDP). The resulting SDP can be adapted to increase either the estimation accuracy (by capturing the interaction between activation functions of different layers) or scalability (by decomposition and parallel implementation). We illustrate the utility of our approach with a variety of experiments on randomly generated networks and on classifiers trained on the MNST and Iris datasets. In particular, we experimentally demonstrate that our Lipschitz bounds are the most accurate compared to those in the literature. We also study the impact of adversarial training methods on the Lipschitz bounds of the resulting classifiers and show that our bounds can be used to efficiently provide robustness guarantees.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Fee2011	A hypothesis for basal ganglia-dependent reinforcement learning in the songbird	"Most of our motor skills are not innately programmed, but are learned by a combination of motor exploration and performance evaluation, suggesting that they proceed through a reinforcement learning (RL) mechanism. Songbirds have emerged as a model system to study how a complex behavioral sequence can be learned through an RL-like strategy. Interestingly, like motor sequence learning in mammals, song learning in birds requires a basal ganglia (BG)-thalamocortical loop, suggesting common neural mechanisms. Here, we outline a specific working hypothesis for how BG-forebrain circuits could utilize an internally computed reinforcement signal to direct song learning. Our model includes a number of general concepts borrowed from the mammalian BG literature, including a dopaminergic reward prediction error and dopamine-mediated plasticity at corticostriatal synapses. We also invoke a number of conceptual advances arising from recent observations in the songbird. Specifically, there is evidence for a specialized cortical circuit that adds trial-to-trial variability to stereotyped cortical motor programs, and a role for the BG in ""biasing"" this variability to improve behavioral performance. This BG-dependent ""premotor bias"" may in turn guide plasticity in downstream cortical synapses to consolidate recently learned song changes. Given the similarity between mammalian and songbird BG-thalamocortical circuits, our model for the role of the BG in this process may have broader relevance to mammalian BG function."	https://www.scopus.com/inward/record.uri?eid=2-s2.0-81255157549&doi=10.1016\%2fj.neuroscience.2011.09.069&partnerID=40&md5=972990a9c9b53d3c02a218fb36141832	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Feher2020	Fast prototype framework for deep reinforcement learning-based trajectory planner		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092310763&doi=10.3311\%2fPPTR.15837&partnerID=40&md5=1074437f3d8cf84211fa058fa8e1039d	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Feldt2015	Broadening the Search in Search-Based Software Testing: It Need Not Be Evolutionary	Search-based software testing (SBST) can potentially help software practitioners create better test suites using less time and resources by employing powerful methods for search and optimization. However, research on SBST has typically focused on only a few search approaches and basic techniques. A majority of publications in recent years use some form of evolutionary search, typically a genetic algorithm, or, alternatively, some other optimization algorithm inspired from nature. This paper argues that SBST researchers and practitioners should not restrict themselves to a limited choice of search algorithms or approaches to optimization. To support our argument we empirically investigate three alternatives and compare them to the de facto SBST standards in regards to performance, resource efficiency and robustness on different test data generation problems: classic algorithms from the optimization literature, bayesian optimization with gaussian processes from machine learning, and nested monte carlo search from game playing / reinforcement learning. In all cases we show comparable and sometimes better performance than the current state-of-the-SBST-art. We conclude that SBST researchers should consider a more general set of solution approaches, more consider combinations and hybrid solutions and look to other areas for how to develop the field.	https://dx.doi.org/10.1109/SBST.2015.8	Excluded	conflict_resolution	E1: Does not define or use a RL method,E1: Does not define or use a RL method	4
RL4SE	Felsberg2005	A COSPAL subsystem: Solving a shape-sorter puzzle			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Fendji2022	Automatic Speech Recognition Using Limited Vocabulary: A Survey		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134881097&doi=10.1080\%2f08839514.2022.2095039&partnerID=40&md5=6df7dfa5222484f0464629ed9a8457a0	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Feng2022	GANDSE: Generative Adversarial Network based Design Space Exploration for Neural Network Accelerator Design		https://doi.org/10.1145/3570926	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Feng2021	A Collision Avoidance Method Based on Deep Reinforcement Learning	This paper set out to investigate the usefulness of solving collision avoidance problems with the help of deep reinforcement learning in an unknown environment, especially in compact spaces, such as a narrow corridor. This research aims to determine whether a deep reinforcement learning-based collision avoidance method is superior to the traditional methods, such as potential field-based methods and dynamic window approach. Besides, the proposed obstacle avoidance method was developed as one of the capabilities to enable each robot in a novel robotic system, namely the Self-reconfigurable and Transformable Omni-Directional Robotic Modules (STORM), to navigate intelligently and safely in an unknown environment. A well-conceived hardware and software architecture with features that enable further expansion and parallel development designed for the ongoing STORM projects is also presented in this work. A virtual STORM module with skid-steer kinematics was simulated in Gazebo to reduce the gap between the simulations and the real-world implementations. Moreover, comparisons among multiple training runs of the neural networks with different parameters related to balance the exploitation and exploration during the training process, as well as tests and experiments conducted in both simulation and real-world, are presented in detail. Directions for future research are also provided in the paper.	https://dx.doi.org/10.3390/robotics10020073	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Feng2020	Environmental adaptive Urban traffic signal control based on reinforcement learning algorithm		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096425591&doi=10.1088\%2f1742-6596\%2f1650\%2f3\%2f032097&partnerID=40&md5=19603c193c7f4074e4177b140658fd8b	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Feng2021a	Survey of testing techniques of autonomous driving software		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100248268&doi=10.11834\%2fjig.200493&partnerID=40&md5=1516fa91d3a7097fcba0a600feb00187	Excluded	conflict_resolution	E5: Other not a paper,E5: Other not a paper	4
RL4SE	Feng2019	Method of artificial intelligence algorithm to improve the automation level of Rietveld refinement	In this paper, artificial Intelligence (AI) algorithm is used in the Rietveld refinement process instead of the human decision. The program, PowderBot, is developed based on Fullprof engine, which proves the effectiveness of AI in Rietveld refinement. In this program, the decision making in refinement process is modelled as a Markov decision process (MDP), and solved by a reinforcement learning algorithm. PowderBot is designed to be a self-learning system capable of conducing structure refinement without human intervention. The program has already been successfully applied to Rietveld refinements. We hope this paper will encourage more Rietveld programs become more intelligent by the help of AI algorithm.	https://dx.doi.org/10.1016/j.commatsci.2018.10.006	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ferchichi2021	A Reinforcement Learning Approach to Feature Model Maintainability Improvement	Software Product Lines (SPLs) evolve when there are changes in their core assets (e.g., feature models and reference architecture). Various approaches have addressed assets evolution by applying evolution operations (e.g., adding a feature to a feature model and removing a constraint). Improving quality attributes (e.g., maintainability and flexibility) of core assets is a promising field in SPLs evolution. Providing a proposal based on a decision maker to support this field is a challenge that grows over time. A decision maker helps the human (e.g., domain expert) to choose the convenient evolution scenarios (change operations) to improve quality attributes of a core asset. To tackle this challenge, we propose a reinforcement learning approach to improve the maintainability of a PL feature model. By learning various evolution operations and based on its decision maker, this approach is able to provide the best evolution scenarios to improve the maintainability of a FM. In this paper, we present the reinforcement learning approach we propose illustrated by a running example associated to the feature model of a Graph Product Line (GPL).	https://dx.doi.org/10.5220/0010480203890396	Included	new_screen		4
RL4SE	Ferdowsi2021	Neural Combinatorial Deep Reinforcement Learning for Age-Optimal Joint Trajectory and Scheduling Design in UAV-Assisted Networks	In this article, an unmanned aerial vehicle (UAV)-assisted wireless network is considered in which a battery-constrained UAV is assumed to move towards energy-constrained ground nodes to receive status updates about their observed processes. The UAV's flight trajectory and scheduling of status updates are jointly optimized with the objective of minimizing the normalized weighted sum of Age of Information (NWAoI) values for different physical processes at the UAV. The problem is first formulated as a mixed-integer program. Then, for a given scheduling policy, a convex optimization-based solution is proposed to derive the UAV's optimal flight trajectory and time instants on updates. However, finding the optimal scheduling policy is challenging due to the combinatorial nature of the formulated problem. Therefore, to complement the proposed convex optimization-based solution, a finite-horizon Markov decision process (MDP) is used to find the optimal scheduling policy. Since the state space of the MDP is extremely large, a novel neural combinatorial-based deep reinforcement learning (NCRL) algorithm using deep Q-network (DQN) is proposed to obtain the optimal policy. However, for large-scale scenarios with numerous nodes, the DQN architecture cannot efficiently learn the optimal scheduling policy anymore. Motivated by this, a long short-term memory (LSTM)-based autoencoder is proposed to map the state space to a fixed-size vector representation in such large-scale scenarios while capturing the spatio-temporal interdependence between the update locations and time instants. A lower bound on the minimum NWAoI is analytically derived which provides system design guidelines on the appropriate choice of importance weights for different nodes. Furthermore, an upper bound on the UAV's minimum speed is obtained to achieve this lower bound value. The numerical results also demonstrate that the proposed NCRL approach can significantly improve the achievable NWAoI per process compared to the baseline policies, such as weight-based and discretized state DQN policies.	https://dx.doi.org/10.1109/JSAC.2021.3065049	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ferigo2020	Gym-Ignition: Reproducible Robotic Simulations for Reinforcement Learning	This paper presents Gym-Ignition, a new framework to create reproducible robotic environments for reinforcement learning research. It interfaces with the new generation of Gazebo, part of the Ignition Robotics suite, which provides three main improvements for reinforcement learning applications compared to the alternatives: 1) the modular architecture enables using the simulator as a C++ library, simplifying the interconnection with external software; 2) multiple physics and rendering engines are supported as plugins, simplifying their selection during the execution; 3) the new distributed simulation capability allows simulating complex scenarios while sharing the load on multiple workers and machines. The core of Gym-Ignition is a component that contains the Ignition Gazebo simulator and exposes a simple interface for its configuration and execution. We provide a Python package that allows developers to create robotic environments simulated in Ignition Gazebo. Environments expose the common OpenAI Gym interface, making them compatible out-of-the-box with third-party frameworks containing reinforcement learning algorithms. Simulations can be executed in both headless and GUI mode, the physics engine can run in accelerated mode, and instances can be parallelized. Furthermore, the Gym-Ignition software architecture provides abstraction of the Robot and the Task, making environments agnostic on the specific runtime. This abstraction allows their execution also in a real-time setting on actual robotic platforms, even if driven by different middlewares.	https://dx.doi.org/10.1109/SII46433.2020.9025951	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ferreira2018	A method for the online construction of the set of states of a Markov decision process using answer set programming		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049012571&doi=10.1007\%2f978-3-319-92058-0_1&partnerID=40&md5=c67f15fc0e460b48cd460803c02ee319	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ferreira2017	Multi-objective reinforcement learning-based deep neural networks for cognitive space communications	Future communication subsystems of space exploration missions can potentially benefit from software-defined radios (SDRs) controlled by machine learning algorithms. In this paper, we propose a novel hybrid radio resource allocation management control algorithm that integrates multi-objective reinforcement learning and deep artificial neural networks. The objective is to efficiently manage communications system resources by monitoring performance functions with common dependent variables that result in conflicting goals. The uncertainty in the performance of thousands of different possible combinations of radio parameters makes the trade-off between exploration and exploitation in reinforcement learning (RL) much more challenging for future critical space-based missions. Thus, the system should spend as little time as possible on exploring actions, and whenever it explores an action, it should perform at acceptable levels most of the time. The proposed approach enables on-line learning by interactions with the environment and restricts poor resource allocation performance through `virtual environment exploration'. Improvements in the multi-objective performance can be achieved via transmitter parameter adaptation on a packet-basis, with poorly predicted performance promptly resulting in rejected decisions. Simulations presented in this work considered the DVB-S2 standard adaptive transmitter parameters and additional ones expected to be present in future adaptive radio systems. Performance results are provided by analysis of the proposed hybrid algorithm when operating across a satellite communication channel from Earth to GEO orbit during clear sky conditions. The proposed approach constitutes part of the core cognitive engine proof-of-concept to be delivered to the NASA Glenn Research Center SCaN Testbed located on-board the International Space Station.	https://dx.doi.org/10.1109/CCAAW.2017.8001880	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ferreira2018a	Multiobjective Reinforcement Learning for Cognitive Satellite Communications Using Deep Neural Network Ensembles	Future spacecraft communication subsystems will potentially benefit from software-defined radios controlled by artificial intelligence algorithms. In this paper, we propose a novel radio resource allocation algorithm leveraging multiobjective reinforcement learning and artificial neural network ensembles able to manage available resources and conflicting mission-based goals. The uncertainty in the performance of thousands of possible radio parameter combinations and the dynamic behavior of the radio channel over time producing a continuous multidimensional state\endashaction space requires a fixed-size memory continuous state\endashaction mapping instead of the traditional discrete mapping. In addition, actions need to be decoupled from states in order to allow for online learning, performance monitoring, and resource allocation prediction. The proposed approach leverages the authors' previous research on constraining decisions predicted to have poor performance through ''virtual environment exploration.'' The simulation results show the performance for different communication mission profiles, and accuracy benchmarks are provided for the future research reference. The proposed approach constitutes part of the core cognitive engine proof-of-concept delivered to the NASA John H. Glenn Research Center's SCaN Testbed radios on-board the International Space Station.	https://dx.doi.org/10.1109/JSAC.2018.2832820	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Fields2006	A Symmetric Multiprocessor Architecture for Multi-Agent Temporal Difference Learning	Temporal difference learning methods have been successfully applied to a wide range of stochastic learning and control problems. In addition to correctness, one metric of a technique's performance is its learning rate - the number of iterations required to converge to an optimal solution. The learning rate can be increased by using multiple agents that can share experience. In a software environment, the potential speedup from additional agents is limited, since adding agents significantly increases the burden of computation and/or hinders real-time processing. To address this problem, this paper presents a parameterized hardware model of a multi-agent system based on a shared-memory Symmetric Multiprocessor (SMP). To the author's knowledge, this is the first application of an SMP architecture to a multi-agent reinforcement learning system. The control model employed is a multi-agent variation of the Sarsa(?) algorithm. Several hardware optimizations schemes are investigated with respect to feasibility and expected performance. The system is modeled using a cycle-accurate simulation in SystemC. The results indicate that real-time learning rates can be significantly improved by employing the proposed parallel hardware implementation.	https://dx.doi.org/10.1109/MWSCAS.2006.382109	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Floreano1998	Hardware solutions for evolutionary robotics		https://www.scopus.com/inward/record.uri?eid=2-s2.0-38049084257&doi=10.1007\%2f3-540-64957-3_69&partnerID=40&md5=2f3c6e906146e0b521bef4805d670b19	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Forero2021	Active Queue-Management Policies for Undersea Networking via Deep Reinforcement Learning	Future undersea networks require effective network resource management strategies able to support the increasing demand for undersea data. Due to the limited bandwidth and high delays characteristic of underwater acoustic communications, queueing management policies for in-transit data are expected to support the quality-of-service requirements that software applications need while helping to manage network congestion. This work develops a queue management policy for in-transit data inspired by weighted-fair queueing. Our policy dynamically allocates portions of the available transmission bandwidth to different traffic types flowing through the queues. The policy uses user-defined traffic prioritization, queueing-delay requirements, and the history of queue-occupancy levels to define the allocation of bandwidth. We use soft actor-critic, a type of deep reinforcement learning algorithm, to train a learning agent. Additionally, we include an active-queue management policy tuned to identify congestion for individual traffic types via the instantaneous queue sojourn-time experienced by packets. The active-queue management policy interacts with the learning agent and with the congestion-control protocol used by the source to mitigate congestion. The performance of our proposed algorithm is illustrated via numerical simulations using an environment developed in OpenAI Gym.	https://dx.doi.org/10.23919/OCEANS44145.2021.9706025	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Forootani2022	An Advanced Satisfaction-Based Home Energy Management System Using Deep Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129609664&doi=10.1109\%2fACCESS.2022.3172327&partnerID=40&md5=5819f762c707e6083e9a0b95b6710c72	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Fraga2007	The challenge of training new architects: An ontological and reinforcement-learning methodology		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866533608&doi=10.4304\%2fjsw.5.1.24-28&partnerID=40&md5=a78446a204a71273b3cb093639d5514d	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Fraga2007a	Training Initiative for New Software/Enterprise Architects: An Ontological Approach	In this paper, we describe the importance of new software/enterprise architects in the discipline of software architecture and enterprise architecture. Both are often idealized as super heroes with a lot of qualities that are very infrequent in contemporary people. The enterprise/software architect role could be assumed by a group of people able to manage the qualities for the role. In any case, even a group or a single person must be educated in the discipline by training courses, new methodologies of learning, or traditional university studies. In order to improve the process of becoming a new architect we propose a methodology based on ontological structures and reinforcement learning.	https://dx.doi.org/10.1109/WICSA.2007.48	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E5: Other not a paper	4
RL4SE	Fraija2022	A Discount-Based Time-of-Use Electricity Pricing Strategy for Demand Response With Minimum Information Using Reinforcement Learning	Demand Response (DR) programs show great promise for energy saving and load profile flattening. They bring about an opportunity for indirect control of end-users' demand based on different price policies. However, the difficulty in characterizing the price-responsive behavior of customers is a significant challenge towards an optimal selection of these policies. This paper proposes a Demand Response Aggregator (DRA) for transactive policy generation by combining a Reinforcement Learning (RL) technique on the aggregator side with a convex optimization problem on the customer side. The proposed DRA can maintain users' privacy by exploiting the DR as the only source of information. In addition, it can avoid mistakenly penalizing users by offering price discounts as an incentive to realize a satisfying multi-agent environment. With an ensured convergence, the resultant DRA is capable of learning adaptive Time-of-Use (ToU) tariffs and generating near-to-optimal price policies. Moreover, this study suggests an off-line training procedure that can deal with issues related to the convergence time of RL algorithms. The suggested process can notably expedite the DRA convergence and, in turn, enable online applications. The developed method is applied to a set of residential agents in order to benefit them by regulating their thermal loads according to generated price policies. The efficiency of the proposed approach is thoroughly evaluated from the standpoint of the aggregator and customers in terms of load shifting and comfort maintenance, respectively. Besides, the superior performance of the selected RL method is represented through a comparative study. An additional assessment is also conducted by use of a coordination algorithm to validate the competitiveness of the recommended DR program. The multifaceted evaluation demonstrates that the designed scheme can significantly improve the quality of the aggregated load profile with a low reduction in the aggregator's income.	https://dx.doi.org/10.1109/ACCESS.2022.3175839	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Framling2008	Light-weight reinforcement learning with function approximation for real-life control tasks	Despite the impressive achievements of reinforcement learning (RL) in playing Backgammon already in the beginning of the 90's, relatively few successful real-world applications of RL have been reported since then. This could be due to the tendency of RL research to focus on discrete Markov Decision Processes that make it difficult to handle tasks with continuous-valued features. Another reason could be a tendency to develop continuously more complex mathematical RL models that are difficult to implement and operate. Both of these issues are addressed in this paper by using the gradient-descent Sarsa(lambda) method together with a Normalised Radial Basis Function neural net. The experimental results on three typical benchmark control tasks show that these methods outperform most previously reported results on these tasks, while remaining computationally feasible to implement even as embedded software. Therefore the presented results can serve as a reference both regarding learning performance and computational applicability of RL for real-life applications.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Francisco2022	A Recommendation Module based on Reinforcement Learning to an Intelligent Tutoring System for Software Maintenance	The demand for qualified professionals to work with Software Maintenance (SM) brings challenges to computer education. These challenges are related to SM's inherent complexity and the teacher's significant work in providing adequate support in practical SM activities. In this context, Artificial Intelligence (AI) based techniques, such as recommendations, can play a central role in developing Intelligent Tutoring Systems (ITS) to focus the teaching-learning process. The literature points out a lack of ITS to SM and that most of them do not use AI-based techniques to recommend content to the students. In this work, we present an Expert Knowledge Module (EKM) for an ITS specially designed for SM. To model the EKM content, we did a deep analysis of the ACM curricula regarding SM topics and the syllabus related to SM from all Brazilian public universities. The content recommendation engine uses the Q-Learning algorithm, a well-known Reinforcement Learning (RL) AI-based technique. Using simulation-based experiments, we could verify the efficiency of the Q-Learning-based recommendation mechanism to propose contents using the ITS's EKM properly. This work highlights how AI-based techniques can enhance and improve SM's teaching-learning process using ITS and advance this research area.	https://dx.doi.org/10.5220/0011083900003182	Included	conflict_resolution		4
RL4SE	Frank2012	REFLEXIVE COLLISION RESPONSE WITH VIRTUAL SKIN Roadmap Planning Meets Reinforcement Learning	Prevalent approaches to motion synthesis for complex robots offer either the ability to build up knowledge of feasible actions through exploration, or the ability to react to a changing environment, but not both. This work proposes a simple integration of roadmap planning with reflexive collision response, which allows the roadmap representation to be transformed into a Markov Decision Process. Consequently, roadmap planning is extended to changing environments, and the adaptation of the map can be phrased as a reinforcement learning problem. An implementation of the reflexive collision response is provided, such that the reinforcement learning problem can be studied in an applied setting. The feasibility of the software is analyzed in terms of runtime performance, and its functionality is demonstrated on the iCub humanoid robot.	https://dx.doi.org/10.5220/000388.5205420651	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Frisbie2022	AI-Enabled Jammer Deception Using Decoy Packets	In this work, we present a learning algorithm for a wireless communications network to transmit decoy packets to counter an adversarial sensing-reactive jammer. As the jammer is required to search across channels for data transmissions, decoy packets can have the effect of stalling the jammer on a particular channel, preventing it from continuing its search and leaving legitimate packets unimpeded. A reinforcement learning algorithm trains a deep neural network with an exploration-exploitation algorithm and experience replay. The state- and action-space and reward function are presented as components of the reinforcement learning framework. Our algorithm is tested with software simulations, modeling ZigBee communications nodes using time-division multiple access for medium access control. A reactive jammer is modeled in the simulation, with the goal of disrupting any detected ZigBee transmissions. A means to measure and distribute the reward function and system state to enable edge-learning in this context is presented as part of the implementation. The results demonstrate the effectiveness of our algorithm in mitigating the jamming attack, outperforming a random decoy strategy by a factor of two.	https://dx.doi.org/10.1109/GLOBECOM48099.2022.10001651	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Fu2022	Reinforcement Learning for guiding optimization processes in optical design		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141870798&doi=10.1117\%2f12.2632425&partnerID=40&md5=7da52b5e481e3b2a873c04132cf9fe59	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Fu2013	Parametric approximation policy iteration algorithm based on Gaussian process		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891278709&doi=10.3724\%2fSP.J.1001.2013.04466&partnerID=40&md5=84c9cab1d611eb56a8a1c01d7f62ac27	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Fu2020	Dads: Dynamic slicing continuously-running distributed programs with budget constraints		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097150414&doi=10.1145\%2f3368089.3417920&partnerID=40&md5=01f388b145085233f58f4a90b73d8ba0	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Fu2021	SEADS: Scalable and Cost-effective Dynamic Dependence Analysis of Distributed Systems via Reinforcement Learning		https://doi.org/10.1145/3379345	Included	new_screen		4
RL4SE	Fu2021a	Sleads: Scalable and Cost-effective Dynamic Dependence Analysis of Distributed Systems via Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099876671&doi=10.1145\%2f3379345&partnerID=40&md5=e4cb4b086ae2d3e6b1e2510ec2436f6d	Included	new_screen		4
RL4SE	Fu2021b	Containerized framework for building control performance comparisons: model predictive control vs deep reinforcement learning control		https://doi.org/10.1145/3486611.3492412	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Fu2021c	Policy Network Assisted Monte Carlo Tree Search for Intelligent Service Function Chain Deployment	Network function virtualization (NFV) simplies the coniguration and management of security services by migrating the network security functions from dedicated hardware devices to software middle-boxes that run on commodity servers. Under the paradigm of NFV, the service function chain (SFC) consisting of a series of ordered virtual network security functions is becoming a mainstream form to carry network security services. Allocating the underlying physical network resources to the demands of SFCs under given constraints over time is known as the SFC deployment problem. It is a crucial issue for infrastructure providers. However, SFC deployment is facing new challenges in trading off between pursuing the objective of a high revenue-to-cost ratio and making decisions in an online manner. In this paper, we investigate the use of reinforcement learning to guide online deployment decisions for SFC requests and propose a Policy network Assisted Monte Carlo Tree search approach named PACT to address the above challenge, aiming to maximize the average revenue-to-cost ratio. PACT combines the strengths of the policy network, which evaluates the placement potential of physical servers, and the Monte Carlo Tree Search, which is able to tackle problems with large state spaces. Extensive experimental results demonstrate that our PACT achieves the best performance and is superior to other algorithms by up to 30\% and 23.8\% on average revenue-to-cost ratio and acceptance rate, respectively.	https://dx.doi.org/10.1109/TrustCom53373.2021.00157	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Fuad2022	Adaptive Deep Q-Network Algorithm with Exponential Reward Mechanism for Traffic Control in Urban Intersection Networks	The demand for transportation has increased significantly in recent decades in line with the increasing demand for passenger and freight mobility, especially in urban areas. One of the most negative impacts is the increasing level of traffic congestion. A possible short-term solution to solve this problem is to utilize a traffic control system. However, most traffic control systems still use classical control algorithms with the green phase sequence determined, based on a specific strategy. Studies have proven that this approach does not provide the expected congestion solution. In this paper, an adaptive traffic controller was developed that uses a reinforcement learning algorithm called deep Q-network (DQN). Since the DQN performance is determined by reward selection, an exponential reward function, based on the macroscopic fundamental diagram (MFD) of the distribution of vehicle density at intersections was considered. The action taken by the DQN is determining traffic phases, based on various rewards, ranging from pressure to adaptive loading of pressure and queue length. The reinforcement learning algorithm was then applied to the SUMO traffic simulation software to assess the effectiveness of the proposed strategy. The DQN-based control algorithm with the adaptive reward mechanism achieved the best performance with a vehicle throughput of 56,384 vehicles, followed by the classical and conventional control methods, such as Webster (50,366 vehicles), max-pressure (50,541 vehicles) and uniform (46,241 vehicles) traffic control. The significant increase in vehicle throughput achieved by the adaptive DQN-based control algorithm with an exponential reward mechanism means that the proposed traffic control could increase the area productivity, implying that the intersections could accommodate more vehicles so that the possibility of congestion was reduced. The algorithm performed remarkably in preventing congestion in a traffic network model of Central Jakarta as one of the world's most congested cities. This result indicates that traffic control design using MFD as a performance measure can be a successful future direction in the development of reinforcement learning for traffic control systems.	https://dx.doi.org/10.3390/su142114590	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Funika2020	Automatic Management of Cloud Applications with Use of Proximal Policy Optimization		https://doi.org/10.1007/978-3-030-50371-0_6	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Furnkranz2000	Learning to use operational advice	We address the problem of advice-taking in a given domain, in particular for building a game-playing program. Our approach to solving it strives for the application of machine learning techniques throughout, i.e., for avoiding knowledge elicitation by any other means as much as possible. In particular, we build upon existing work on the operationalization of advice by machine and assume that advice is already available in operational form. The relative importance of this advice is, however, not yet known and can therefore not be utilized well by a program. This paper presents an approach to determine the relative importance for a given situation through reinforcement learning. We implemented this approach for the game of Hearts and gathered some empirical evidence on its usefulness through experiments. The results show that the programs built according to our approach learned to make good use of the given operational advice.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ganapathy2009	Neural Q-Learning controller for mobile robot	In recent years, increasing trend in application of autonomous mobile robot worldwide has highlighted the importance of path planning controller in robotics-related fields, especially where dynamic and unknown environment is involved. Writing a good robot controller program can be a very time consuming process. It is inevitably wasting of resources and efforts if we have to rewrite the controller over and over again whenever there is emergence of changes in the environment. Reinforcement Learning (RL) algorithms and Artificial Neural Network (ANN) are used to assist autonomous mobile robot to learn in an unrecognized environment. This research study is focused on exploring integration of multi-layer neural network and Q-Learning as an online learning controller. Learning process is divided into two stages. In the initial stage the agent will map the environment through collecting state-action information according to the Q-Learning procedure. Second training process involves neural network training which will utilize the state-action information gathered in earlier phase as training samples. During final application of the controller, Q-Learning would be used as the primary navigating tool whereas the trained neural network will be employed when approximation is needed. MATLAB simulation was developed to verify the validity of the algorithm before it is real-time implemented on the real world using Team AmigoBot\texttrademark robot. The results obtained from both simulation and actual application confirmed on-spot learning ability of the controller accompanied with certain degree of flexibility and robustness.	https://dx.doi.org/10.1109/AIM.2009.5229901	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Gangopadhyay2022	Safe and Stable RL (S2RL) Driving Policies Using Control Barrier and Control Lyapunov Functions		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126691776&doi=10.1109\%2fTIV.2022.3160202&partnerID=40&md5=31e11fda09114b3f400293c350f624a4	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Gangopadhyay2022a	Hierarchical Program-Triggered Reinforcement Learning Agents for Automated Driving	Recent advances in Reinforcement Learning (RL) combined with Deep Learning (DL) have demonstrated impressive performance in complex tasks, including autonomous driving. The use of RL agents in autonomous driving leads to a smooth human-like driving experience, but the limited interpretability of Deep Reinforcement Learning (DRL) creates a verification and certification bottleneck. Instead of relying on RL agents to learn complex tasks, we propose HPRL - Hierarchical Program-triggered Reinforcement Learning, which uses a hierarchy consisting of a structured program along with multiple RL agents, each trained to perform a relatively simple task. The focus of verification shifts to the master program under simple guarantees from the RL agents, leading to a significantly more interpretable and verifiable implementation as compared to a complex RL agent. The evaluation of the framework is demonstrated on different driving tasks, and National Highway Traffic Safety Administration (NHTSA) pre-crash scenarios using CARLA, an open-source dynamic urban simulation environment.	https://dx.doi.org/10.1109/TITS.2021.3096998	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ganguly2017	Decentralization of Control Loop for Self-Adaptive Software through Reinforcement Learning	In a decentralized self-adaptive software, multiple control loops provide self-adaptation capabilities to the components these manage. For this, these control loops need to coordinate to continuously satisfy some local QoS goals of each managed component and global QoS goals concerning the whole system in a changing environment. This is accomplished by choosing variants of the managed system (component) that satisfy these goals. As goal conformance requires coordination, a control loop requires choosing the variant that leads to maximum goal conformance considering the variant selection strategies by other control loops. An overall goal conformance calculation mechanism is also needed that captures the local and global goal violations. This paper proposes a decentralized reinforcement learning-based self-adaptation technique considering these issues. A reinforcement learning technique, Q-learning helps to learn the maximum achievable goal conformance choosing a specific variant. The other control loop strategies are estimated by observing their variant selection and incorporated with Q-learning for better variant selection. An overall goal conformance calculation technique is also proposed that dynamically adjusts weights on the local and global goals to emphasize violated goals. The proposed approach was evaluated using a service-based Tele Assistance System. It was compared with two approaches - random variant selection and variant selection ignoring other control loop strategies. The proposed technique outperformed both with maximum overall goal conformance. The proposed dynamic weight update mechanism was compared with a static weight-based one. The dynamic technique outperformed the static one continuously satisfying all the goals.	https://dx.doi.org/10.1109/APSECW.2017.26	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ganin2018	Synthesizing Programs for Images using Reinforced Adversarial Learning	Advances in deep generative networks have led to impressive results in recent years. Nevertheless, such models can often waste their capacity on the minutiae of datasets, presumably due to weak inductive biases in their decoders. This is where graphics engines may come in handy since they abstract away low-level details and represent images as high-level programs. Current methods that combine deep learning and renderers are limited by hand-crafted likelihood or distance functions, a need for large amounts of supervision, or difficulties in scaling their inference algorithms to richer datasets. To mitigate these issues, we present SPIRAL, an adversarially trained agent that generates a program which is executed by a graphics engine to interpret and sample images. The goal of this agent is to fool a discriminator network that distinguishes between real and rendered data, trained with a distributed reinforcement learning setup without any supervision. A surprising finding is that using the discriminator's output as a reward signal is the key to allow the agent to make meaningful progress at matching the desired output rendering. To the best of our knowledge, this is the first demonstration of an end-to-end, unsupervised and adversarial inverse graphics agent on challenging real world ( MNIST, OMNIGLOT, CELEBA) and synthetic 3D datasets. A video of the agent can be found at https://youtu.be/iSyvwAwa7vk.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Gankidi2017	FPGA architecture for deep learning and its application to planetary robotics	Autonomous control systems onboard planetary rovers and spacecraft benefit from having cognitive capabilities like learning so that they can adapt to unexpected situations in-situ. Q-learning is a form of reinforcement learning and it has been efficient in solving certain class of learning problems. However, embedded systems onboard planetary rovers and spacecraft rarely implement learning algorithms due to the constraints faced in the field, like processing power, chip size, convergence rate and costs due to the need for radiation hardening. These challenges present a compelling need for a portable, low-power, area efficient hardware accelerator to make learning algorithms practical onboard space hardware. This paper presents a FPGA implementation of Q-learning with Artificial Neural Networks (ANN). This method matches the massive parallelism inherent in neural network software with the fine-grain parallelism of an FPGA hardware thereby dramatically reducing processing time. Mars Science Laboratory currently uses Xilinx-Space-grade Virtex FPGA devices for image processing, pyrotechnic operation control and obstacle avoidance. We simulate and program our architecture on a Xilinx Virtex 7 FPGA. The architectural implementation for a single neuron Q-learning and a more complex Multilayer Perception (MLP) Q-learning accelerator has been demonstrated. The results show up to a 43-fold speed up by Virtex 7 FPGAs compared to a conventional Intel i5 2.3 GHz CPU. Finally, we simulate the proposed architecture using the Symphony simulator and compiler from Xilinx, and evaluate the performance and power consumption.	https://dx.doi.org/10.1109/AERO.2017.7943929	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Gao2021	Bansor: Improving Tensor Program Auto-Scheduling with Bandit Based Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123944774&doi=10.1109\%2fICTAI52525.2021.00045&partnerID=40&md5=d96a2633d2f297aa7da0d3349bacf06e	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Gao2018	Adversarial policy gradient for alternating markov games			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Gao2022	Power-Aware Traffic Engineering for Data Center Networks via Deep Reinforcement Learning	The issue of high energy consumption and low energy utilization in data center networks (DCNs) has always been the focus of attention of both academia and industry. One general solution is to select a subset of network devices that can meet the traffic transmission requirements, thereby turning off the remaining redundant devices. However, modeling the problem as integer linear programming introduces significant time overhead, while heuristic approaches often suffer from poor generalizability. In this paper, we propose GreenDCN.ai, a closed-loop control system, which utilizes In-band Network Telemetry to collect the network-wide device-internal state, and leverages a Deep Reinforcement Learning-based energy-saving algorithm to make rapid decisions to turn on or off network device ports in response to the real-time network state. The trained GreenDCN.ai can adaptively adjust its energy-saving strategy without human intervention when the DCN topology changes. Besides, based on the regularity of the DCN topology, we design two training complexity reduction methods to address the non-convergence issue under large-scale DCN topologies. Specifically, we split the large-scale DCN topology into sub-topologies for parallel training on each sub-topology without breaking the DCN topology connectivity. Evaluation on software P4 switches suggests that GreenDCN.ai can achieve stable convergence within 590 episodes, generate effective action decisions within $\boldsymbol{79}\upmu\mathrm{s}$, and save about 34\% to 39\% of the network energy consumption.	https://dx.doi.org/10.1109/GLOBECOM48099.2022.10001013	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Gao2022a	Improving DFIG performance under fault scenarios through evolutionary reinforcement learning based control	The doubly fed induction generator (DFIG) usually experiences high rotor current and DC capacitor link voltage spikes during system fault events. In this paper, a novel data-driven approach is proposed to enhance DFIG performance under fault scenarios. An advanced reinforcement learning algorithm called guided surrogate-gradient-based evolution strategy (GSES) is used to control the DFIG power and capacitor DC-link voltage by adjusting the optimal reference signals. This controller is able to prevent the DFIG rotor from over-current risk and maintain grid-connected operation. The proposed GSES-based control algorithm was evaluated through simulations on a 3.6-MW DFIG in the PSCAD/EMTDC software. Results have validated the effectiveness of the proposed GSES-based control algorithm in improving DFIG performance under various fault scenarios.	https://dx.doi.org/10.1049/gtd2.12563	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Gao2022b	MetisRL: A Reinforcement Learning Approach for Dynamic Routing in Data Center Networks		https://doi.org/10.1007/978-3-031-00126-0_44	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Gao2021a	A Multiphase Dynamic Deployment Mechanism of Virtualized Honeypots Based on Intelligent Attack Path Prediction		https://doi.org/10.1155/2021/6378218	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Gao2009	Q-learning based on particle swarm optimization for positioning system of underwater vehicles	The paper presents an intelligent underwater positioning system for remotely operated vehicle (ROV). We used multi-agents reinforcement learning algorithms based on particle swarm optimization fusing signals from ultra-short baseline (USBL) position sonar and pose sensors, so that the USBL can be accelerated and be in-phase with pose sensors. We proposed the frame work of the hardware of the intelligent navigation system, and the multithreading and modularizing software system. Navigation experiment taken in ship model tank indicated the feasibility of the proposed intelligent navigation system.	https://dx.doi.org/10.1109/ICICISYS.2009.5358098	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Garaffa2021	Reinforcement Learning for Mobile Robotics Exploration: A Survey	Efficient exploration of unknown environments is a fundamental precondition for modern autonomous mobile robot applications. Aiming to design robust and effective robotic exploration strategies, suitable to complex real-world scenarios, the academic community has increasingly investigated the integration of robotics with reinforcement learning (RL) techniques. This survey provides a comprehensive review of recent research works that use RL to design unknown environment exploration strategies for single and multirobots. The primary purpose of this study is to facilitate future research by compiling and analyzing the current state of works that link these two knowledge domains. This survey summarizes: what are the employed RL algorithms and how they compose the so far proposed mobile robot exploration strategies; how robotic exploration solutions are addressing typical RL problems like the exploration-exploitation dilemma, the curse of dimensionality, reward shaping, and slow learning convergence; and what are the performed experiments and software tools used for learning and testing. Achieved progress is described, and a discussion about remaining limitations and future perspectives is presented.	https://www.ncbi.nlm.nih.gov/pubmed/34767514	Excluded	new_screen	E5: Other not a paper,E5: Other not a paper	4
RL4SE	Garcia2006	Guided exploration in reinforcement learning. A priori knowledge and constraints relaxation		https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548119925&doi=10.3166\%2fria.20.235-275&partnerID=40&md5=29fe4b3cabf971b8fae9933941bf2fc4	Excluded	new_screen	E5: Other not a paper,E2: Software engineering is not the problem RL is used for	4
RL4SE	Garcia-Martinez2000	An integrated approach of learning, planning, and execution	"Agents (hardware or software) that act autonomously in an environment have to be able to integrate three basic behaviors: planning, execution, and learning. This integration is mandatory when the agent has no knowledge about how its actions can affect the environment, how the environment reacts to its actions, or, when the agent does not receive as an explicit input, the goals it must achieve. Without an ""a priori"" theory, autonomous agents should be able to self-propose goals, set-up plans for achieving the goals according to previously learned models of the agent and the environment, and learn those models from past experiences of successful and failed executions of plans. Planning involves selecting a goal to reach and computing a set of actions that will allow the autonomous agent to achieve the goal. Execution deals with the interaction with the environment by application of planned actions, observation of resulting perceptions, and control of successful achievement of the goals. Learning is needed to predict the reactions of the environment to the agent actions, thus guiding the agent to achieve its goals more efficiently. In this context, most of the learning systems applied to problem solving have been used to learn control knowledge for guiding the search for a plan, but few systems have focused on the acquisition of planning operator descriptions. As an example, currently, one of the most used techniques for the integration of (a way of) planning, execution, and learning is reinforcement learning. However, they usually do not consider the representation of action descriptions, so they cannot reason in terms of goals and ways of achieving those goals. In this paper, we present an integrated architecture, lope, that learns operator definitions, plans using those operators, and executes the plans for modifying the acquired operators. The resulting system is domain-independent, and we have performed experiments in a robotic framework. The results clearly show that the integrated planning, learning, and executing system outperforms the basic planner in that domain."	https://dx.doi.org/10.1023/A:1008134010576	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Garcia-Sillas2016	Learning from demonstration with Gaussian processes	There is huge potential in the field of robotics for the application of machine learning methodologies, particularly in the case of learning by demonstration, which considerably reduces the time required to program robotic actions, and in addition, makes robotic movements more natural. Within machine learning domain supervised, unsupervised and reinforcement learning classifications can be found. Among these, the most widely used is supervised learning. This allows two learning tasks: classification and regression. The Gaussian process model is one of the methodologies used for regression. Through regression, a learning process can be performed, allowing to learn by demonstration from a given data set. In this article, the development of a learning method is presented, it is based on Gaussian process regression and intended to be applied in robotic platforms which require to learn quickly and incrementally, since the robots today maintain more contact with the environment and therefore with the human. That is why the Gaussian processes have the characteristics required to develop this type of control for robots. In this paper, a non-parametric regression model such Gaussian process is investigated, as well as how this can be applied to learning from demonstration framework.	https://dx.doi.org/10.1109/MAIS.2016.7761899	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Garofalo2017	Cortical and striatal reward processing in Parkinson's disease psychosis	Psychotic symptoms frequently occur in Parkinson's disease (PD), but their pathophysiology is poorly understood. According to the National Institute of Health RDoc programme, the pathophysiological basis of neuropsychiatric symptoms may be better understood in terms of dysfunction of underlying domains of neurocognition in a trans-diagnostic fashion. Abnormal cortico-striatal reward processing has been proposed as a key domain contributing to the pathogenesis of psychotic symptoms in schizophrenia. This theory has received empirical support in the study of schizophrenia spectrum disorders and preclinical models of psychosis, but has not been tested in the psychosis associated with PD. We, therefore, investigated brain responses associated with reward expectation and prediction error signaling during reinforcement learning in PD-associated psychosis. An instrumental learning task with monetary gains and losses was conducted during an fMRI study in PD patients with (n = 12), or without (n = 17), a history of psychotic symptoms, along with a sample of healthy controls (n = 24). We conducted region of interest analyses in the ventral striatum (VS), ventromedial prefrontal and posterior cingulate cortices, and whole-brain analyses. There was reduced activation in PD patients with a history of psychosis, compared to those without, in the posterior cingulate cortex and the VS during reward anticipation (p < 0.05 small volume corrected). The results suggest that cortical and striatal abnormalities in reward processing, a putative pathophysiological mechanism of psychosis in schizophrenia, may also contribute to the pathogenesis of psychotic symptoms in PD. The finding of posterior cingulate dysfunction is in keeping with prior results highlighting cortical dysfunction in the pathogenesis of PD psychosis.	https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018728385&doi=10.3389\%2ffneur.2017.00156&partnerID=40&md5=c9240f977f9c417919eec4aa642775e1	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Gatti2011	A brief tutorial on reinforcement learning: The game of Chung Toi			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Gaul1999	Fuzzy-neuro-controlled verified instruction scheduler	We present a fuzzy-neuro approach for instruction scheduling in compilers for modern high performance processors. Instruction scheduling is an optimization problem in NP and is usually addressed with processor dependent heuristics or processor simplifying cost models. The costs for executing a given instruction sequence on the processor can not be determined exactly in practice, because the exact execution model is too complex or simply not available from the manufacturer. Our approach enables the compiler to adapt the cost measure dynamically by learning the processor behavior and typical optimization situations on the basis of reinforcement learning. Additionally, we are able to include fuzzy a priori scheduling knowledge and derive verified implementations by the technique of program checking.	https://dx.doi.org/10.1109/NAFIPS.1999.781818	Excluded	conflict_resolution	E5: Other not a paper,E5: Other not a paper	4
RL4SE	Gausseran2022	Reconfiguring Network Slices at the Best Time With Deep Reinforcement Learning	The emerging 5G induces a great diversity of use cases, a multiplication of the number of connections, an increase in throughput as well as stronger constraints in terms of quality of service such as low latency and isolation of requests. To support these new constraints, Network Function Virtualization (NFV) and Software Defined Network (SDN) technologies have been coupled to introduce the network slicing paradigm. Due to the high dynamicity of the demands, it is crucial to regularly reconfigure the network slices in order to maintain an efficient provisioning of the network. A major concern is to find the best frequency to carry out these reconfigurations, as there is a trade-off between a reduced network congestion and the additional costs induced by the reconfiguration. In this paper, we tackle the problem of deciding the best moment to reconfigure by taking into account this trade-off. By coupling Deep Reinforcement Learning for decision and a Column Generation algorithm to compute the reconfiguration, we propose Deep-REC and show that choosing the best time during the day to reconfigure allows to maximize the profit of the network operator while minimizing the use of network resources and the congestion of the network. Moreover, by selecting the best moment to reconfigure, our approach allows to decrease the number of needed reconfigurations compared to an algorithm doing periodic reconfigurations during the day.	https://dx.doi.org/10.1109/CloudNet55617.2022.9978878	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Gelenbe2020	IoT Network Attack Detection and Mitigation	Cyberattacks on the Internet of Things (IoT) can cause major economic and physical damage, and disrupt production lines, manufacturing processes, supply chains, impact the physical safety of vehicles, and damage the health of human beings. Thus we describe and evaluate a distributed and robust attack detection and mitigation system for network environments where communicating decision agents use Graph Neural Networks to provide attack alerts. We also present an attack mitigation system that uses a Reinforcement Learning driven Software Defined Network to process the alerts generated by the attack detection sysem, together with Quality of Service measurements, so as to re-route sensitive traffic away from compromised network paths using. Experimental results illustrate both the detection and re-routing scheme.	https://dx.doi.org/10.1109/MECO49872.2020.9134241	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Geramifard2015	RLPy: a value-function-based reinforcement learning framework for education and research			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Gerpott2022	Integration of the A2C Algorithm for Production Scheduling in a Two-Stage Hybrid Flow Shop Environment	The paper introduces an approach to apply reinforcement learning (RL) for production scheduling in a two-stage hybrid flow shop (THFS) production system. The Advantage-Actor Critic (A2C) method is used to train multiple agents to minimize the total tardiness and makespan of a production program. The two-stage hybrid flow shop scheduling problem is a NP-hard combinatorial optimization problem that describes a production system with two stages, each consisting of a set of parallel machines. Our concept combines a Discrete-Event Simulation with a pre-implemented RL, algorithm using Stable Baselines3. Since similar research often lacks concrete implementation information, the configuration of the OpenAI Gym interface and the agent-environment interaction is presented. (C) 2022 The Authors. Published by Elsevier B.V.	https://dx.doi.org/10.1016/j.procs.2022.01.256	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ghannadrezaii2021	Channel Quality Prediction for Adaptive Underwater Acoustic Communication	In this paper, the communication quality of an underwater acoustic link between two nodes is quantified by the predicted channel gain and delay spread using a stochastic and reinforcement learning model. The stochastic model generates an ensemble of time-varying channel characteristics by capturing the effect of known environmental changes including changes in sound speed profile, tides and bathymetry. Along with the stochastic model to capture the impact of unknown environmental parameters on channel quality a hidden Markov model is utilized to complement sparse channel measurements and predict the channel characteristics over a long time period spanning multiple days. In this work, the nodes are bottom mounted in a shallow turbulent water environment, with known tide cycles, physical oceanography conditions and channel geometry. As such, the channel characteristics can be estimated using a simulation software model at the remote nodes. While the simulation model is used to estimate the initial channel condition between the nodes in short-term deployment, as will be shown, the hidden Markov model provides an accurate channel characteristics prediction for long term deployment, which can be utilized by software-defined acoustic nodes such that they can adapt to the time varying acoustic channel.	https://dx.doi.org/10.1109/UComms50339.2021.9598150	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ghasemkhani2018	Reinforcement learning based pricing for demand response		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050302095&doi=10.1109\%2fICCW.2018.8403783&partnerID=40&md5=047c9eb4c9195bf6fd3b937ed6c84719	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ghimis2020	RIVER 2.0: an open-source testing framework using AI techniques		https://doi.org/10.1145/3416504.3424335	Included	new_screen		4
RL4SE	Ghosal2019	A Reinforcement Learning Based Network Scheduler for Deadline-Driven Data Transfers	We consider a science network that runs applications requiring data transfers to be completed within a given deadline. The underlying network is a software defined network (SDN) that supports fine grain real-time network telemetry. Deadline-aware data transfer requests are made to a centralized network controller that schedules the flows by setting pacing rates of the deadline flows and metering the background traffic at the ingress routers. The goal of the scheduling algorithm is to maximize the number of flows that meet the deadline while maximizing the network utilization. In this paper, we develop a Reinforcement Learning (RL) agent based network controller and compare its performance with well-known heuristics. For a network consisting of a single bottleneck link, we show that the RL-agent based network controller performs as well as Earliest Deadline First (EDF), which is known to be optimal. We also show that the RL-agent performs significantly better than an idealized TCP protocol in which the bottleneck link capacity is equally shared among the competing flows. We also study the sensitivity of the RL-agent controller for different parameter settings and reward functions.	https://dx.doi.org/10.1109/GLOBECOM38437.2019.9013255	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ghosh2018	Visual Search as a Probabilistic Sequential Decision Process in Software Autonomous System	Software systems now can replace a vast amount of human resources with their quasi-autonomous features. Adaptive nature helps the system to adjust their behavior in response to their environmental changes. Most of these changes are unknown which put the system under uncertainty while making decisions. Decision-making in the real-time environment requires through observation for searching and analyzing different features of the environment in which the systems are running to understand how humans select relevant information. In this way, a human and a software system can jointly evaluate the situations and make the decision. Earlier visual search (VS) models believe that the eyes move to locations with maximum saliency and every eye movement is planned. It is hard to choose any optimal searching process when the operating characteristics of any visual system vary over time. Furthermore, this method does not account how the brain coordinates its different regions to process information over time. To address these limitations, we propose visual process as an instance of deep reinforcement learning which represents a step towards building autonomous systems to understand the visual world more accurately. In this paper, we begin with different visual search algorithms, including reinforcement learning, followed by probabilistic sequential decision making. In parallel, we focus on various existing areas of related research and their advantages and drawbacks within the domain. Finally, we propose a model which can address the shortcomings of the existing algorithms.	https://dx.doi.org/10.1109/SECON.2018.8479127	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ghosh2021	Mapping Language to Programs using Multiple Reward Components with Inverse Reinforcement Learning			Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Giachino2021	Reinforcement learning for content's customization: a first step of experimentation in Skyscanner	Purpose The aim of the paper is to test and demonstrate the potential benefits in applying reinforcement learning instead of traditional methods to optimize the content of a company's mobile application to best help travellers finding their ideal flights. To this end, two approaches were considered and compared via simulation: standard randomized experiments or A/B testing and multi-armed bandits. Design/methodology/approach The simulation of the two approaches to optimize the content of its mobile application and, consequently, increase flights conversions is illustrated as applied by Skyscanner, using R software. Findings The first results are about the comparison between the two approaches - A/B testing and multi-armed bandits - to identify the best one to achieve better results for the company. The second one is to gain experiences and suggestion in the application of the two approaches useful for other industries/companies. Research limitations/implications The case study demonstrated, via simulation, the potential benefits to apply the reinforcement learning in a company. Finally, the multi-armed bandit was implemented in the company, but the period of the available data was limited, and due to its strategic relevance, the company cannot show all the findings. Practical implications The right algorithm can change according to the situation and industry but would bring great benefits to the company's ability to surface content that is more relevant to users and help improving the experience for travellers. The study shows how to manage complexity and data to achieve good results. Originality/value The paper describes the approach used by an European leading company operating in the travel sector in understanding how to adapt reinforcement learning to its strategic goals. It presents a real case study and the simulation of the application of A/B testing and multi-armed bandit in Skyscanner; moreover, it highlights practical suggestion useful to other companies.	https://dx.doi.org/10.1108/IMDS-12-2019-0722	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Giardino2022	Low-Overhead Reinforcement Learning-Based Power Management Using 2QoSM	With the computational systems of even embedded devices becoming ever more powerful, there is a need for more effective and pro-active methods of dynamic power management. The work presented in this paper demonstrates the effectiveness of a reinforcement-learning based dynamic power manager placed in a software framework. This combination of Q-learning for determining policy and the software abstractions provide many of the benefits of co-design, namely, good performance, responsiveness and application guidance, with the flexibility of easily changing policies or platforms. The Q-learning based Quality of Service Manager (2QoSM) is implemented on an autonomous robot built on a complex, powerful embedded single-board computer (SBC) and a high-resolution path-planning algorithm. We find that the 2QoSM reduces power consumption up to 42\% compared to the Linux on-demand governor and 10.2\% over a state-of-the-art situation aware governor. Moreover, the performance as measured by path error is improved by up to 6.1\%, all while saving power.	https://dx.doi.org/10.3390/jlpea12020029	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Giernacki2020	Bebop 2 Quadrotor as a Platform for Research and Education in Robotics and Control Engineering	"In conducting research and teaching in fields related to unmanned aerial vehicles (UAVs), it is particularly important to select a universal, safe, open research platform and tools for rapid prototyping. Ready-to-use, low-cost micro-class UAVs such as Bebop 2 are successfully used in that regard. This article presents how to use the potential of this flying robot with Robot Operating System (ROS). The most important software solutions for the developed experimental testbed FlyBebop are characterized here. Their capabilities in research and education are exemplified using three distinct cases: 1) research results on the method of optimal, in-flight, iterative self-tuning of UAV position controller parameters (based only on current measurements), 2) the use of the reinforcement learning method in the autonomous landing of a single drone on a moving vehicle, 3) planning the movement of UAVs for autonomous video recording along the planned path in the arrangement: ""cameraman drone"" and ""lighting technician drones""."	https://dx.doi.org/10.1109/ICUAS48674.2020.9213872	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Goertzel2007	Probabilistic logic based reinforcement learning of simple embodied behaviors in a 3D simulation world	"We describe here the use of an integrative AI architecture to perform reinforcement learning of simple behaviors in the context of controlling a humanoid agent in a 3D simulation world. The AI architecture, the Novamente AI Engine, is extremely flexible, incorporating a variety of carefully intercoordinated learning processes; but the work described here relies primarily on the integration of probabilistic inference with statistical pattern mining based perception and functional program execution based agent control. The bulk of the paper describes how integrative intelligence is used to enable the system to learn to play the game of ""fetch"" in an embodied, simulation world context."		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Goertzel2007a	The Novamente Artificial Intelligence Engine		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880320061&doi=10.1007\%2f978-3-540-68677-4_3&partnerID=40&md5=a4145134bab06a5065f1d91019f6e264	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Goertzel2010	A General Intelligence Oriented Architecture for Embodied Natural Language Processing	A software architecture is described which enables a virtual agent in an online virtual world to carry out simple English language interactions grounded in its perceptions and actions. The use of perceptions to guide anaphor resolution is discussed, along with the use of natural language generation to answer simple questions about the observed world. This architecture has been implemented within the larger PetBrain system, which is built on the OpenCog open-source AI software framework and architected based on the OpenCogPrime design for integrative ACT, and has previously been used for nonlinguistic intelligent behaviors such as imitation and reinforcement learning.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Golkhou2004	Application of actor-critic reinforcement learning method for control of a sagittal arm during oscillatory movement		https://www.scopus.com/inward/record.uri?eid=2-s2.0-14644412922&doi=10.4015\%2fs1016237204000426&partnerID=40&md5=deaa76aa5bbed74218bf0e499d2356f6	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Gong2021	Improving HW/SW Adaptability for Accelerating CNNs on FPGAs Through A Dynamic/Static Co-Reconfiguration Approach	With the continuous evolution of Convolutional Neural Networks (CNNs) and the improvement of the computing capability of FPGAs, the deployment of CNN accelerator based on FPGA has become more and more popular in various computing scenarios. The key element of implementing these accelerators is to take full advantage of underlying hardware characteristics to adapt to the computational features of the software-level CNN model. To achieve this goal, however, previous designs mainly focus on the static hardware reconfiguration pattern, which is not flexible enough and can hardly make the accelerator architecture and the CNN features fully fit, resulting in inefficient computations and data communications. By leveraging the dynamic partial reconfiguration technology equipped in the modern FPGA devices, in this article, we propose a new accelerator architecture for implementing CNNs on FPGAs in which static and dynamic reconfigurabilities of the hardware are cooperatively utilized to maximize the acceleration efficiency. Based on this architecture, we further present a systematic design and optimization methodology for implementing the specific CNN model in the particular computing scenario, in which a static design space exploration method and a reinforcement learning-based decision method are proposed to obtain the optimal static hardware configuration and run-time reconfiguration strategy respectively. We evaluate our proposal by implementing three widely used CNN models, AlexNet, VGG16C, and ResNet34, on the Xilinx ZCU102 FPGA platform. Experimental results show that our implementations on average can achieve 683 GOPS under 16-bit fixed data type and 1.37 TOPS under 8-bit fixed data type for three targeted CNN models, and improve the computational density from 1.1$\times$ to 1.91$\times$ compared with previous implementations on the same type of FPGA platform.	https://dx.doi.org/10.1109/TPDS.2020.3046762	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Gong2022	N3H-Core: Neuron-designed Neural Network Accelerator via FPGA-based Heterogeneous Computing Cores		https://doi.org/10.1145/3490422.3502367	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Gonsalves2021	Integrated deep learning for self-driving robotic cars		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126402950&doi=10.1016\%2fB978-0-323-85498-6.00010-1&partnerID=40&md5=1b27cf6be8ddbea509c490d1e7e283fa	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Gonul2019	An expandable approach for design and personalization of digital, just-in-time adaptive interventions	Objective: We aim to deliver a framework with 2 main objectives: 1) facilitating the design of theory-driven, adaptive, digital interventions addressing chronic illnesses or health problems and 2) producing personalized intervention delivery strategies to support self-management by optimizing various intervention components tailored to people's individual needs, momentary contexts, and psychosocial variables. Materials and Methods: We propose a template-based digital intervention design mechanism enabling the configuration of evidence-based, just-in-time, adaptive intervention components. The design mechanism incorporates a rule definition language enabling experts to specify triggering conditions for interventions based on momentary and historical contextual/personal data. The framework continuously monitors and processes personal data space and evaluates intervention-triggering conditions. We benefit from reinforcement learning methods to develop personalized intervention delivery strategies with respect to timing, frequency, and type (content) of interventions. To validate the personalization algorithm, we lay out a simulation testbed with 2 personas, differing in their various simulated real-life conditions. Results: We evaluate the design mechanism by presenting example intervention definitions based on behavior change taxonomies and clinical guidelines. Furthermore, we provide intervention definitions for a real-world care program targeting diabetes patients. Finally, we validate the personalized delivery mechanism through a set of hypotheses, asserting certain ways of adaptation in the delivery strategy, according to the differences in simulation related to personal preferences, traits, and lifestyle patterns. Conclusion: While the design mechanism is sufficiently expandable to meet the theoretical and clinical intervention design requirements, the personalization algorithm is capable of adapting intervention delivery strategies for simulated real-life conditions.	https://www.ncbi.nlm.nih.gov/pubmed/30590757	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Gonzalez2017	Dynamic Decision Making: Learning Processes and New Research Directions	Objective: The aim of this manuscript is to provide a review of contemporary research and applications on dynamic decision making (DDM). Background: Since early DDM studies, there has been little systematic progress in understanding decision making in complex, dynamic systems. Our review contributes to better understanding of decision making processes in dynamic tasks. Method: We discuss new research directions in DDM to highlight the value of simplification in the study of complex decision processes, divided into experimental and theoretical/computational approaches, and focus on problems involving control tasks and search-and-choice tasks. In computational modeling, we discuss recent developments in instance-based learning and reinforcement learning that advance modeling the processes of dynamic decisions. Results: Results from DDM research reflect a trend to scale down the complexity of DDM tasks to facilitate the study of the process of decision making. Recent research focuses on the dynamic complexity emerging from the interactions of actions and outcomes over time even in simple dynamic tasks. Conclusion: The study of DDM in theory and practice continues to be a priority area of research. New research directions can help the human factors community to understand the effects of experience, knowledge, and adaption processes in DDM tasks, but research challenges remain to be addressed, and the recent perspectives discussed can help advance a systematic DDM research program. Application: Classical domains, such as automated pilot systems, fighting fires, and medical emergencies, continue to be central applications of basic DDM research, but new domains, such as cybersecurity, climate change, and forensic science, are emerging as other important applications.	https://www.ncbi.nlm.nih.gov/pubmed/28548893	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Gonzalez-Rodriguez2021	Uncertainty-Aware Autonomous Mobile Robot Navigation with Deep Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116848757&doi=10.1007\%2f978-3-030-77939-9_7&partnerID=40&md5=bd9a2fbcc172edcf91e7195df766f1ff	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Goodspeed2007	Work in progress - enhancing reinforcement learning class curriculum using a Matlab interface library for use with the Sony AIBO robot	In an effort to mitigate the inherent complexities of embedded robotics programming, a novel Matlab-based interface library with matching firmware is presented. The platform was developed at the University of Tennessee for use in a course on reinforcement learning. Using this software interface, students were able to easily implement machine learning schemes and apply them to the Sony AIBO robot, a flexible platform with diverse capabilities. The behavior development process for the AIBO is traditionally limited to programming in a C++ environment. To avoid the overhead of such tedious programming, the Matlab interface was designed to allow the user to both implement functions and behaviors already developed for the AIBO, as well as to construct new behaviors using existing components as building blocks. Moreover, since the interface relies on a bidirectional network socket connection, the user is offered full control of the robot from any (remote) networked computer.	https://dx.doi.org/10.1109/FIE.2007.4418006	Excluded	new_screen	E5: Other not a paper,E5: Other not a paper	4
RL4SE	Gorobtsov2022	Optimal Greedy Control in Reinforcement Learning	We consider the problem of dimensionality reduction of state space in the variational approach to the optimal control problem, in particular, in the reinforcement learning method. The control problem is described by differential algebraic equations consisting of nonlinear differential equations and algebraic constraint equations interconnected with Lagrange multipliers. The proposed method is based on changing the Lagrange multipliers of one subset based on the Lagrange multipliers of another subset. We present examples of the application of the proposed method in robotics and vibration isolation in transport vehicles. The method is implemented in FRUND-a multibody system dynamics software package.	https://www.ncbi.nlm.nih.gov/pubmed/36433518	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Gouel2022	Zeph & Iris map the internet		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125851123&doi=10.1145\%2f3523230.3523232&partnerID=40&md5=ed1c91317e4215e20a97571d31d9c46c	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Gouel2022a	Zeph & Iris map the internet: A resilient reinforcement learning approach to distributed IP route tracing		https://doi.org/10.1145/3523230.3523232	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Grace2008	Overview of cognitive radio and cognitive networks research at the University of York	Cognitive radio and cognitive networking will provide the next big innovations in wireless communications, they will: drastically improve the utilisation of the radio spectrum; increase the freedom for operators and users, boosting revenues. Heterogeneous systems should be allowed to freely use pooled spectrum: research should focus on techniques that maintain QoS; power efficient modulation techniques can be exploited. Incorporating intelligence is crucial to success: emphasis should be placed on distributed techniques; reinforcement learning looks very promising. Cognitive radio does not necessarily require significant complexity: a fully reconfigurable software radio is not required; applying intelligence can reduce complexity.	https://dx.doi.org/10.1049/ic:20080389	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Graepel2016	AlphaGo - Mastering the game of go with deep neural networks and tree search			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Graf2015	Adaptive Playouts in Monte-Carlo Tree Search with Policy-Gradient Reinforcement Learning	Monte-Carlo Tree Search evaluates positions with the help of a playout policy. If the playout policy evaluates a position wrong then there are cases where the tree-search has difficulties to find the correct move due to the large search-space. This paper explores adaptive playout-policies which improve the playout-policy during a tree-search. With the help of policy-gradient reinforcement learning techniques we optimize the playout-policy to give better evaluations. We tested the algorithm in Computer Go and measured an increase in playing strength of more than 100 ELO. The resulting program was able to deal with difficult test-cases which are known to pose a problem for Monte-Carlo-Tree-Search.	https://dx.doi.org/10.1007/978-3-319-27992-3_1	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Graf2016	Adaptive playouts for online learning of policies during Monte Carlo Tree Search		https://doi.org/10.1016/j.tcs.2016.06.029	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Graham2010	Multi-Agent Reinforcement Learning - An Exploration Using Q-Learning	It is possible to exploit automated learning from sensed data for practical applications - in essence facilitating reasoning about particular problem domains based on a combination of environmental awareness and insights elicited from past decisions. We explore some enhanced Reinforcement Learning (RL) methods used for achieving such machine learning using software agents in order to address two questions. Can RL implementations/methods be accelerated by using a Multi-Agent approach? Can an agent learn composite skills in single-pass?	https://dx.doi.org/10.1007/978-1-84882-983-1_21	Excluded	new_screen	E3: Only conceptual results are reported,E2: Software engineering is not the problem RL is used for	4
RL4SE	Graule2022	SoMoGym: A Toolkit for Developing and Evaluating Controllers and Reinforcement Learning Algorithms for Soft Robots	Soft robotsoffer a host of benefits over traditional rigid robots, including inherent compliance that lets them passively adapt to variable environments and operate safely around humans and fragile objects. However, that same compliance makes it hard to use model-based methods in planning tasks requiring high precision or complex actuation sequences. Reinforcement learning (RL) can potentially find effective control policies, but training RL using physical soft robots is often infeasible, and training using simulations has had a high barrier to adoption. To accelerate research in control and RL for soft robotic systems, we introduce SoMoGym (Soft Motion Gym), a software toolkit that facilitates training and evaluating controllers for continuum robots. SoMoGym provides a set of benchmark tasks in which soft robots interact with various objects and environments. It allows evaluation of performance on these tasks for controllers of interest, and enables the use of RL to generate new controllers. Custom environments and robots can likewise be added easily. We provide and evaluate baseline RL policies for each of the benchmark tasks. These results show that SoMoGym enables the use of RL for continuum robots, a class of robots not covered by existing benchmarks, giving them the capability to autonomously solve tasks that were previously unattainable.	https://dx.doi.org/10.1109/LRA.2022.3149580	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Greasley2020	Architectures for Combining Discrete-event Simulation and Machine Learning	A significant barrier to the combined use of simulation and machine learning (ML) is that practitioners in each area have differing backgrounds and use different tools. From a review of the literature this study presents five options for software architectures that combine simulation and machine learning. These architectures employ configurations of both simulation software and machine learning software and thus require skillsets in both areas. In order to further facilitate the combined use of these approaches this article presents a sixth option for a software architecture that uses a commercial off-the-shelf (COTS) DES software to implement both the simulation and machine learning algorithms. A study is presented of this approach that incorporates the use of a type of ML termed reinforcement learning (RL) which in this example determines an approximate best route for a robot in a factory moving from one physical location to another whilst avoiding fixed barriers. The study shows that the use of an object approach to modelling of the COTS DES Simio enables an ML capability to be embedded within the DES without the use of a programming language or specialist ML software.	https://dx.doi.org/10.5220/0009767600470058	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Greasley2020a	Implementing reinforcement learning in simio discrete-event simulation software			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Greasley2020b	A simulation of autonomous robot movement directed by reinforcement learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097717661&doi=10.46354\%2fi3m.2020.emss.002&partnerID=40&md5=44e6bd6b22d74087e4415eb36ee3bc5e	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Gregory2015	Punishment and psychopathy: A case-control functional MRI investigation of reinforcement learning in violent antisocial personality disordered men	BACKGROUND: Men with antisocial personality disorder show lifelong abnormalities in adaptive decision making guided by the weighing up of reward and punishment information. Among men with antisocial personality disorder, modification of the behaviour of those with additional diagnoses of psychopathy seems particularly resistant to punishment. METHODS: We did a case-control functional MRI (fMRI) study in 50 men, of whom 12 were violent offenders with antisocial personality disorder and psychopathy, 20 were violent offenders with antisocial personality disorder but not psychopathy, and 18 were healthy non-offenders. We used fMRI to measure brain activation associated with the representation of punishment or reward information during an event-related probabilistic response-reversal task, assessed with standard general linear-model-based analysis. FINDINGS: Offenders with antisocial personality disorder and psychopathy displayed discrete regions of increased activation in the posterior cingulate cortex and anterior insula in response to punished errors during the task reversal phase, and decreased activation to all correct rewarded responses in the superior temporal cortex. This finding was in contrast to results for offenders without psychopathy and healthy non-offenders. INTERPRETATION: Punishment prediction error signalling in offenders with antisocial personality disorder and psychopathy was highly atypical. This finding challenges the widely held view that such men are simply characterised by diminished neural sensitivity to punishment. Instead, this finding indicates altered organisation of the information-processing system responsible for reinforcement learning and appropriate decision making. This difference between violent offenders with antisocial personality disorder with and without psychopathy has implications for the causes of these disorders and for treatment approaches. FUNDING: National Forensic Mental Health Research and Development Programme, UK Ministry of Justice, Psychiatry Research Trust, NIHR Biomedical Research Centre.	https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922783478&doi=10.1016\%2fS2215-0366\%2814\%2900071-6&partnerID=40&md5=905597e181b7efd23a1f4af0ec32aa30	Excluded	new_screen	E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for	4
RL4SE	Groce2012	Lightweight Automated Testing with Adaptation-Based Programming	"This paper considers the problem of testing a container class or other modestly-complex API-based software system. Past experimental evaluations have shown that for many such modules, random testing and shape abstraction based model checking are effective. These approaches have proven attractive due to a combination of minimal requirements for tool/language support, extremely high usability, and low overhead. These ""lightweight"" methods are therefore available for almost any programming language or environment, in contrast to model checkers and concolic testers. Unfortunately, for the cases where random testing and shape abstraction perform poorly, there have been few alternatives available with such wide applicability. This paper presents a generalizable approach based on reinforcement learning (RL), using adaptation-based programming (ABP) as an interface to make RL-based testing (almost) as easy to apply and adaptable to new languages and environments as random testing. We show how learned tests differ from random ones, and propose a model for why RL works in this unusual (by RL standards) setting, in the context of a detailed large-scale experimental evaluation of lightweight automated testing methods."	https://dx.doi.org/10.1109/ISSRE.2012.1	Included	new_screen		4
RL4SE	Grossberg2019	The Embodied Brain of SOVEREIGN2: From Space-Variant Conscious Percepts During Visual Search and Navigation to Learning Invariant Object Categories and Cognitive-Emotional Plans for Acquiring Valued G	This article develops a model of how reactive and planned behaviors interact in real time. Controllers for both animals and animats need reactive mechanisms for exploration, and learned plans to efficiently reach goal objects once an environment becomes familiar. The SOVEREIGN model embodied these capabilities, and was tested in a 3D virtual reality environment. Neural models have characterized important adaptive and intelligent processes that were not included in SOVEREIGN. A major research program is summarized herein by which to consistently incorporate them into an enhanced model called SOVEREIGN2. Key new perceptual, cognitive, cognitive-emotional, and navigational processes require feedback networks which regulate resonant brain states that support conscious experiences of seeing, feeling, and knowing. Also included are computationally complementary processes of the mammalian neocortical What and Where processing streams, and homologous mechanisms for spatial navigation and arm movement control. These include: Unpredictably moving targets are tracked using coordinated smooth pursuit and saccadic movements. Estimates of target and present position are computed in the Where stream, and can activate approach movements. Motion cues can elicit orienting movements to bring new targets into view. Cumulative movement estimates are derived from visual and vestibular cues. Arbitrary navigational routes are incrementally learned as a labeled graph of angles turned and distances traveled between turns. Noisy and incomplete visual sensor data are transformed into representations of visual form and motion. Invariant recognition categories are learned in the What stream. Sequences of invariant object categories are stored in a cognitive working memory, whereas sequences of movement positions and directions are stored in a spatial working memory. Stored sequences trigger learning of cognitive and spatial/motor sequence categories or plans, also called list chunks, which control planned decisions and movements toward valued goal objects. Predictively successful list chunk combinations are selectively enhanced or suppressed via reinforcement learning and incentive motivational learning. Expected vs. unexpected event disconfirmations regulate these enhancement and suppressive processes. Adaptively timed learning enables attention and action to match task constraints. Social cognitive joint attention enables imitation learning of skills by learners who observe teachers from different spatial vantage points.	https://www.ncbi.nlm.nih.gov/pubmed/31333437	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Grudic2003	Using policy gradient reinforcement learning on autonomous robot controllers	Robot programmers can often quickly program a robot to approximately execute a task under specific environment conditions. However, achieving robust performance under more general conditions is significantly more difficult. We propose a framework that starts with an existing control system and uses reinforcement feedback from the environment to autonomously improve the controller's performance. We use the policy gradient reinforcement learning (PGRL) framework, which estimates a gradient (in controller space) of improved reward, allowing the controller parameters to be incrementally updated to autonomously achieve locally optimal performance. Our approach is experimentally verified on a Cye robot executing a room entry and observation task, showing significant reduction in task execution time and robustness with respect to un-modelled changes in the environment.	https://dx.doi.org/10.1109/IROS.2003.1250662	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Gu2020	Towards Learning-automation IoT Attack Detection through Reinforcement Learning	As a massive number of the Internet of Things (IoT) devices are deployed, the security and privacy issues in IoT arouse more and more attention. The IoT attacks are causing tremendous loss to the IoT networks and even threatening human safety. Compared to traditional networks, IoT networks have unique characteristics, which make the attack detection more challenging. First, the heterogeneity of platforms, protocols, software, and hardware exposes various vulnerabilities. Second, in addition to the traditional high-rate attacks, the low-rate attacks are also extensively used by IoT attackers to obfuscate the legitimate and malicious traffic. These low-rate attacks are challenging to detect and can persist in the networks. Last, the attackers are evolving to be more intelligent and can dynamically change their attack strategies based on the environment feedback to avoid being detected, making it more challenging for the defender to discover a consistent pattern to identify the attack. In order to adapt to the new characteristics in IoT attacks, we propose a reinforcement learning-based attack detection model that can automatically learn and recognize the transformation of the attack pattern. Therefore, we can continuously detect IoT attacks with less human intervention. In this paper, we explore the crucial features of IoT traffics and utilize the entropy-based metrics to detect both the high-rate and low-rate IoT attacks. Afterward, we leverage the reinforcement learning technique to continuously adjust the attack detection threshold based on the detection feedback, which optimizes the detection and the false alarm rate. We conduct extensive experiments over a real IoT attack data set and demonstrate the effectiveness of our IoT attack detection framework.	https://dx.doi.org/10.1109/WoWMoM49955.2020.00029	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Gu2012	IDES: Self-adaptive Software with Online Policy Evolution Extended from Rainbow	One common approach or framework of self-adaptive software is to incorporate a control loop that monitoring, analyzing, deciding and executing over a target system using predefined rules and policies. Unfortunately, policies or utilities in such approaches and frameworks are statically and manually defined. The empirical adaptation policies and utility profiles cannot change with environment thus cannot make robust and assurance decisions. Various efficiency improvements have been introduced to online evolution of self-adaptive software itselfhowever, there is no framework with policy evolution in policy-based self-adaptive software such as Rainbow. Our approach, embodied in a system called IDES(Intelligent Decision System) uses reinforcement learning to provides an architecture based self-adaptive framework. We associate each policy with a preference value.During the running time the system automatically assesses system utilities and use reinforcement learning to update policy preference. We evaluate our approach and framework by an example system for bank dispatching. The experiment results reveal the intelligence and reactiveness of our approach and framework.		Included	new_screen		4
RL4SE	Guan2022	HierRL: Hierarchical Reinforcement Learning for Task Scheduling in Distributed Systems	The distributed system Ray has attracted much attention for many decision-making applications. It provides a flexible and powerful distributed running mechanism for the training of the learning algorithms, which could map the computation tasks to the resources automatically. Task scheduling is a critical component in Ray, adopting a two-layer structure. It uses a simple general scheduling principle, which leaves much space to optimize. In this paper, we will study the two-layer scheduling problem in Ray, setting it as an optimization problem. We firstly present a comprehensive formulation for the problem and point out that it is a NP-hard problem. Then we design a hierarchical reinforcement learning method, named HierRL, which consists of a high-level agent and a low-level agent. Sophisticated state space, action space, and reward function are designed for this method. In the high level, we devise a value-based reinforcement learning method, which allocates a task to an appropriate node of the low level. With tasks allocated from the high level and generated from applications, a low-level reinforcement learning method is constructed to select tasks from the queue to be executed. A hierarchical policy learning method is introduced for the training of the two-layer agents. Finally, we simulate the two-layer scheduling procedure in a public platform, Cloudsim, with tasks from a real Dataset generated by the Alibaba Cluster Trace Program. The results show that the proposed method performs much better than the original scheduling method of Ray.	https://dx.doi.org/10.1109/IJCNN55064.2022.9892507	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Guan2022a	Attentive Reinforcement Learning for Scheduling Problem with Node Auto-scaling	The distributed system Ray has attracted much attention in decision-making applications, which could greatly accelerate the training efficiency for intelligent algorithms. Task scheduling is one of the critical technologies in Ray, in which the number of resource nodes could be auto-scaling, i.e., automatically increasing or decreasing according to the workload. The adopted scheduling strategy is simple, which leaves much space to be optimized. In this paper, we consider designing a reinforcement learning method to optimize the scheduling problem in Ray. We propose an attentive reinforcement learning method, designing an attention-based state encoder that could efficiently extract the system state in the situation of the varying number of resource nodes. At the same time, an action mask mechanism filters invalid actions. Further, to improve the learning efficiency in the environment with the varied number of nodes, we design a curriculum learning method, which trains the method by gradually increasing the number of nodes in the scheduling process. Finally, we use the real data generated by the Alibaba Cluster Trace Program to test in the simulation platform CloudSim. The experimental results show that the proposed method effectively scales down the completion time of tasks compared to the original algorithm in Ray.	https://dx.doi.org/10.1109/SMC53654.2022.9945288	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Guerin2017	Locally optimal control under unknown dynamics with learnt cost function: application to industrial robot positioning	Recent methods of Reinforcement Learning have enabled to solve difficult, high dimensional, robotic tasks under unknown dynamics using iterative Linear Quadratic Gaussian control theory. These algorithms are based on building a local time-varying linear model of the dynamics from data gathered through interaction with the environment. In such tasks, the cost function is often expressed directly in terms of the state and control variables so that it can be locally quadratized to run the algorithm. If the cost is expressed in terms of other variables, a model is required to compute the cost function from the variables manipulated. We propose a method to learn the cost function directly from the data, in the same way as for the dynamics. This way, the cost function can be defined in terms of any measurable quantity and thus can be chosen more appropriately for the task to be carried out. With our method, any sensor information can be used to design the cost function. We demonstrate the efficiency of this method through simulating, with the V-REP software, the learning of a Cartesian positioning task on several industrial robots with different characteristics. The robots are controlled in joint space and no model is provided a priori. Our results are compared with another model free technique, consisting in writing the cost function as a state variable.	https://dx.doi.org/10.1088/1742-6596/783/1/012036	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Gul2020	Blockchain based healthcare system with Artificial Intelligence	Blockchain is not only for the financial domain anymore. It evolves to accommodate a broad range of domains and applications where trust and privacy are required for smooth transitions. As blockchain evolving, researchers are automating the blockchain tasks for better security and performance. The blockchain management system monitors specific tasks like transaction management, consensus, block security, and blockchain network security. Blockchain management system comprises of specialized engineers and blockchain software platform. This study explores the opportunity for Machine learning concepts to work with blockchain system management to automate tasks in the healthcare scenario. Reinforcement learning is used in this study to automate blockchain tasks with multiagents. Our study found that agents can be trained and perform tasks listed under the healthcare system's blockchain management system. Our study also suggests that storing and accessing data is efficient with machine learning concepts.	https://dx.doi.org/10.1109/CSCI51800.2020.00138	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Gulrez2022	High Performance on Atari Games Using Perceptual Control Architecture Without Training		https://doi.org/10.1007/s10846-022-01747-5	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Guo2021	Intelligent Vehicle Path Planning Considering Side Slip of Surrounding Vehicles in Icy and Snowy Environment	Under the icy and snowy environment, the road conditions are complex and changeable, especially the reduction of road adhesion coefficient, which brings great challenges to the driving safety of intelligent vehicles. Aiming at the problem that the peripheral vehicle may be unstable at any time in the ice and snow environment, a path planning method considering the side slip of the peripheral vehicle in the icy and snowy environment is proposed in this paper. Firstly, reinforcement learning and artificial potential field are combined to transform the force received by the vehicle in the artificial potential field into the objective function in reinforcement learning for reinforcement learning training. Then, because this paper studies the path planning in the icy and snowy environment, considering that the sideslip of the obstacle vehicle will bring a safety threat to the vehicle, the dynamic rectangular virtual repulsion field of the obstacle vehicle is adjusted accordingly during the sideslip. So that the vehicle will be more repulsed by the obstacle vehicle at this time, and guide the vehicle to avoid the dangerous vehicle in time. Finally, the path planning results are simulated and verified in Matlab / CarSim software. The simulation results show that the path planning method in this paper can effectively make the vehicle avoid obstacles safely and stably, and the planned path meets the dynamic requirements of the vehicle and driving stability.	https://dx.doi.org/10.1109/CVCI54083.2021.9661245	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Guo2020	Trusted Cloud-Edge Network Resource Management: DRL-Driven Service Function Chain Orchestration for IoT	Private and public networks sharing resources for Internet of Things (IoT) network through network function virtualization (NFV) and software-defined networking (SDN) forms a heterogeneous cloud-edge environment. However, the heterogeneous cloud-edge network faces trust and adaptation issues in resource allocation. To address these two problems, we introduce consortium blockchain and deep reinforcement learning (DRL) to construct the trusted and auto-adjust service function chain (SFC) orchestration architecture. In the architecture, this article integrates the consortium blockchain into the distributed SFC orchestration model to realize trusted resource sharing. In addition, for realizing auto-adjusted service provision, this article designs a dynamic hierarchical SFC orchestration algorithm (DHSOA) based on DRL to minimize the orchestration cost and improve the quality of service. Moreover, considering the dynamics of network entities, this article proposes a time-slotted model to support dynamic service migration which adapts to the high-mobility IoT network. The simulation results show that DHSOA has better performance than the link-state routing algorithm and deep Q -network placement algorithm not only in cost saving of 15.8\% and 10.1\% but also in time saving of 22.0\% and 10.0\%.	https://dx.doi.org/10.1109/JIOT.2019.2951593	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Guo2023	Feature transfer learning by reinforcement learning for detecting software defect	Software defects, produced inevitably in software projects, seriously affect the efficiency of software testing and maintenance. An appealing solution is the software defect prediction (SDP) that has achieved good performance in many software projects. However, the difference between features and the difference of the same feature between training data and test data may degrade defect prediction performance if such differences violate the model's assumption. To address this issue, we propose a SDP method based on feature transfer learning (FTL), which performs a transformation sequence for each feature in order to map the original features to another feature space. Specifically, FTL first uses the reinforcement learning scheme that automatically learns a strategy for transferring the potential feature knowledge from the training data. Then, we use the learned feature knowledge to inspire the transformation of the test data. The classifier is trained by the transformed training data and predicts defects for transformed test data. We evaluate the validity of FTL on 43 projects from PROMISE and NASA MDP using three classifiers, logistic regression, random forest, and Naive Bayes (NB). Experimental results indicate that FTL is better than the original classifiers and has the best performance on the NB classifier. For PROMISE, after using FTL, the average results of F1-score, AUC, MCC are 0.601, 0.757, and 0.350 respectively, which are 24.9\%, 2.6\%, and 16.7\% higher than the original NB classifier results. The number of projects with improved performance accounts for 83.87\%, 83.87\%, and 64.52\%. Similarly, FTL performs well on NASA MDP. Besides, compared with four feature engineering (FE) methods, FTL achieves an excellent improvement on most projects and the average performance is also better than or close to the FE methods.	https://dx.doi.org/10.1002/spe.3152	Included	new_screen		4
RL4SE	Guo2022	Convex Programs and Lyapunov Functions for Reinforcement Learning: A Unified Perspective on the Analysis of Value-Based Methods		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138490665&doi=10.23919\%2fACC53348.2022.9867291&partnerID=40&md5=a7620915745928c3e98a0f9244e513f3	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Guo2022a	Reinforcement Learning for Optimal Control of a District Cooling Energy Plant	District cooling energy plants (DCEPs) consisting of chillers, cooling towers, and thermal energy storage (TES) systems consume a considerable amount of electricity. Optimizing the scheduling of the TES and chillers to take advantage of time-varying electricity price is a challenging optimal control problem. The classical method, model predictive control (MPC), requires solving a high dimensional mixed-integer nonlinear program (MINLP) because of the on/off actuation of the chillers and charge/discharge of TES, which are computationally challenging. RL is an attractive alternative: the real time control computation is a low-dimensional optimization problem that can be easily solved. However, the performance of an RL controller depends on many design choices.In this paper, we propose a Q-learning based reinforcement learning (RL) controller for this problem. Numerical simulation results show that the proposed RL controller is able to reduce energy cost over a rule-based baseline controller by approximately 8\%, comparable to savings reported in the literature with MPC for similar DCEPs. We describe the design choices in the RL controller, including basis functions, reward function shaping, and learning algorithm parameters. Compared to existing work on RL for DCEPs, the proposed controller is designed for continuous state and actions spaces.	https://dx.doi.org/10.23919/ACC53348.2022.9867239	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Gupta2018	Learning Autonomous Marine Behaviors in MOOS-IvP	Manually authoring and testing behaviors for autonomous marine vehicles can become tedious and impractical when faced with complex or rapidly changing adversarial situations. We address this problem by learning autonomous behaviors using deep reinforcement learning. We apply deep reinforcement learning, an approach that learns behaviors without relying on explicit vehicle models, to a game of capture the flag with multiple competing vehicles. We integrated deep reinforcement learning with MOOS-IvP, a software suite for marine robotics communication, control, and simulation, that allows the development and execution of behaviors for both underwater and surface vehicles. To our knowledge, this is the first application of reinforcement learning to this platform. We extended MOOS-IvP to create and train a neural net to learn autonomous behaviors for reaching the opponent's flag while avoiding an adversary exhibiting a defense behavior in simulation.	https://dx.doi.org/10.1109/OCEANS.2018.8604740	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Gupta2019	Deep reinforcement learning for syntactic error repair in student programs		https://doi.org/10.1609/aaai.v33i01.3301930	Included	new_screen		4
RL4SE	Gurenko2021	Intelligent System of Mooring Planning, Based on Deep Q-Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112703721&doi=10.1007\%2f978-3-030-79463-7_31&partnerID=40&md5=11de2844c3fce9b74a1c848df776d6d9	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Haag2005	Characterization of shape memory alloy behavior and position control using reinforcement learning			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Habiba2020	An Inexpensive Upgradation of Legacy Cameras Using Software and Hardware Architecture for Monitoring and Tracking of Live Threats	Surveillance through digital cameras is increasing exponentially. A majority of these cameras are not smart cameras; therefore, they send their video stream to a central server where it is processed and analyzed for any threats. Typically, human operators or machine learning algorithms at the cloud analyzed and processed the post-event videos to track and locate the perpetrator or victim. The centralized approach leads to two primary shortcomings: 1) the high cost of cloud infrastructure; 2) lack of instant tracking and detection of the threat. One solution is to replace these legacy cameras with the smart cameras so they can process information locally. Although the solution is costly, it could solve the real-time threat detection issues. However, the need for a central server remains there, to construct the path of threat, when threat moves from one camera view to another. The existing distributed architectures for threat tracking shifts the load of threat capturing and processing from a central server to the edge nodes, which in turn reduces the computational power but does not remove the role of the central server completely. These architectures don't equip each camera of processing and communicating with each other. Further, in the existing distributed architectures, the local cameras are not able to store the path of the threat individually and just transmit the captured trajectory to the central body. This research proposed a second alternative that makes use of legacy cameras through additional hardware and software components such that they can process information and collaborate locally. The research addresses the challenge by introducing a low cost distributed threat tracking framework that allows the single camera to identify the threat and communicate its information to other cameras without involving the central server. The framework stores the information in a lightweight architecture that is inspired by the blockchain storage algorithm. The system also allows querying the path traveled by the threat at any stage. To evaluate the system, we performed two simulated experiments: one with a central server and another with the proposed distributed system. The results of the experiments showed that the time to track the threat through the proposed system was lower than the existing centralized system. Moreover, the proposed system predicted the paths of threats with an accuracy of 85.49\%. In the future, the technique may be improved with reinforcement learning and other machine learning techniques.	https://dx.doi.org/10.1109/ACCESS.2020.2964778	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hacid2021	Quality-Based Reinforcement Learning in Intelligent Opportunistic Software Composition	Internet of Things and cyber-physical systems are characterised by openness and an increasing number of devices and their associated services. In a previous work, we have proposed to exploit opportunistically these services in order to automatically make emerge customised applications that suit user preferences. For that, we have developed a generic solution for bottom-up opportunistic service composition, based on reinforcement learning. In this work, it is extended to handle more efficiently the appearance of new components using service annotation and quality attributes in order to generalise and share knowledge with new discovered services. A didactic use case is used for illustration and demonstration purposes.	https://dx.doi.org/10.1109/WETICE53228.2021.00013	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hacker2022	A Motivating Case Study on Code Variant Selection by Reinforcement Learning	In this paper, we investigate the applicability of reinforcement learning as a possible approach to select code variants. Our approach is based on the observation that code variants are usually convertible between one another by code transformations. Actor-critic proximal policy optimization is identified as a suitable reinforcement learning algorithm. To study its applicability, a software framework is implemented and used to perform experiments on three different hardware platforms using a class of explicit solution methods for systems of ordinary differential equations as an example application.	https://dx.doi.org/10.1007/978-3-031-07312-0_15	Included	new_screen		4
RL4SE	Hacker2022a	A Motivating Case Study on Code Variant Selection by Reinforcement Learning		https://doi.org/10.1007/978-3-031-07312-0_15	Included	new_screen		4
RL4SE	Hackett2017	Implementation of a space communications cognitive engine	Although communications-based cognitive engines have been proposed, very few have been implemented in a full system, especially in a space communications system. In this paper, we detail the implementation of a multi-objective reinforcement-learning algorithm and deep artificial neural networks for the use as a radio-resource-allocation controller. The modular software architecture presented encourages re-use and easy modification for trying different algorithms. Various trade studies involved with the system implementation and integration are discussed. These include the choice of software libraries that provide platform flexibility and promote reusability, choices regarding the deployment of this cognitive engine within a system architecture using the DVB-S2 standard and commercial hardware, and constraints placed on the cognitive engine caused by real-world radio constraints. The implemented radio-resource-allocation-management controller was then integrated with the larger space-ground system developed by NASA Glenn Research Center (GRC).	https://dx.doi.org/10.1109/CCAAW.2017.8001607	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hackett2018	Implementation and On-Orbit Testing Results of a Space Communications Cognitive Engine	Cognitive algorithms for communications systems have been presented in literature, but very few have been integrated into a fielded system, especially space communications systems. In this paper, we describe the implementation of a multi-objective reinforcement-learning algorithm using deep artificial neural networks acting as a radio-resource-allocation controller. The developed software core is generic in nature and can be ported readily to another application. The cognitive engine algorithm implementation was characterized through a series of tests using both a ground-based system and a space-based system. The ground system comprised of engineering-model software-defined radios, commercial modems, and RF equipment emulating the targeted space-to-ground channel. The on-orbit communication system, including a space-based, remotely controlled transmitter, resides on the International Space Station and operates with a ground-based receiver at NASA Glenn Research Center. Through a series of on-orbit tests, the cognitive engine was tested in a highly dynamic channel and its performance is discussed and analyzed.	https://dx.doi.org/10.1109/TCCN.2018.2878202	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Haj-Ali2020	NeuroVectorizer: end-to-end vectorization with deep reinforcement learning		https://doi.org/10.1145/3368826.3377928	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hajimiri2014	An intelligent negotiator agent design for bilateral contracts of electrical energy	In this paper, an intelligent agent (using the Fuzzy SARSA learning approach) is proposed to negotiate for bilateral contracts (BC) of electrical energy in Block Forward Markets (BFM or similar market environments). In the BFM energy markets, the buyers (or loads) and the sellers (or generators) submit their bids and offers on a daily basis. The loads and generators could employ intelligent software agents to trade energy in BC markets on their behalves. Since each agent attempts to choose the best bid/offer in the market, conflict of interests might happen. In this work, the trading of energy in BC markets is modeled and solved using Game Theory and Reinforcement Learning (RL) approaches. The Stackelberg equation concept is used for the match making among load and generator agents. Then to overcome the negotiation limited time problems (it is assumed that a limited time is given to each generator-load pairs to negotiate and make an agreement), a Fuzzy SARSA Learning (FSL) method is used. The fuzzy feature of FSL helps the agent cope with continuous characteristics of the environment and also prevents it from the curse of dimensionality. The performance of the FSL (compared to other well-known traditional negotiation techniques, such as time-dependent and imitative techniques) is illustrated through simulation studies. The case study simulation results show that the FSL based agent could achieve more profits compared to the agents using other reviewed techniques in the BC energy market. (C) 2013 Elsevier Ltd. All rights reserved.	https://dx.doi.org/10.1016/j.eswa.2013.12.034	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hamza2021	HEVC rate-distortion optimization with source modeling		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111593564&doi=10.2352\%2fISSN.2470-1173.2021.10.IPAS-259&partnerID=40&md5=e180738673e1304a26cfa777854c18c2	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Han2022	Joint Federated Learning and Reinforcement Learning for Maritime Ad Hoc Networks: An Integration of Personalized Collaborative Route Planning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142889634&doi=10.1007\%2f978-3-031-19214-2_23&partnerID=40&md5=901ffb95fb952c49b4fa1ab3068eddf4	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Han2022a	Deep Reinforcement Learning for Intersection Signal Control Considering Pedestrian Behavior	Using deep reinforcement learning to solve traffic signal control problems is a research hotspot in the intelligent transportation field. Researchers have recently proposed various solutions based on deep reinforcement learning methods for intelligent transportation problems. However, most signal control optimization takes the maximization of traffic capacity as the optimization goal, ignoring the concerns of pedestrians at intersections. To address this issue, we propose a pedestrian-considered deep reinforcement learning traffic signal control method. The method combines a reinforcement learning network and traffic signal control strategy with traffic efficiency and safety aspects. At the same time, the waiting time of pedestrians and vehicles passing through the intersection is considered, and the Discrete Traffic State Encoding (DTSE) method is applied and improved to define the more comprehensive states and rewards. In the training of the neural network, the multi-process operation method is adopted, and multiple environments are run for training simultaneously to improve the model's training efficiency. Finally, extensive simulation experiments are conducted on actual intersection scenarios using the simulation software Simulation of Urban Mobility (SUMO). The results show that compared to Dueling DQN, the waiting time due to our method decreased by 58.76\% and the number of people waiting decreased by 51.54\%. The proposed method can reduce both the number of people waiting and the waiting time at intersections.	https://dx.doi.org/10.3390/electronics11213519	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Han2022b	Construction and Evolution of Fault Diagnosis Knowledge Graph in Industrial Process		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136861826&doi=10.1109\%2fTIM.2022.3200429&partnerID=40&md5=aa445efce30854078eb6c932fe92d604	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Han2018	A Solution Method of Poisson Points of Wine Problem Based on Q Learning	This paper applied reinforcement learning to solve the problem of Poisson point of wine. Unlike most current solutions that use greedy algorithms and depth-first search to solve the Poisson problem, the starting point of this paper is to make the program autonomously find the solution. Therefore, by improving the state space in the original Q-learning, we propose a Poisson'wine algorithm for real-time updating of the state space of the Q learning algorithm. The simulation results show that our algorithm can effectively solve the Poisson problem, and the algorithm design process is simple. The program relies on autonomous interaction with the environment to derive a wine distribution plan.	https://dx.doi.org/10.1109/CAC.2018.8623328	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Han2019	Artificial Intelligence-Based Handoff Management for Dense WLANs: A Deep Reinforcement Learning Approach	So far, the handoff management involved in the wireless local area network (WLAN) has mainly fallen into the handoff mechanism and the decision algorithm. The traditional handoff mechanism generates noticeable delays during the handoff process, resulting in discontinuity of service, which is more evident in dense WLANs. Inspired by software-defined networking (SDN), prior works put forward many seamless handoff mechanisms to ensure service continuity. With respect to the handoff decision algorithm, when to trigger handoff and which access point to reconnect to, however, are still tricky problems. In this paper, we first design a self-learning architecture applicable to the SDN-based WLAN frameworks. Along with it, we propose DCRQN, a novel handoff management scheme based on deep reinforcement learning, specifically deep Q-network. The proposed scheme enables the network to learn from actual users' behaviors and network status from scratch, adapting its learning in time-varying dense WLANs. Due to the temporal correlation property, the handoff decision is modeled as the Markov decision process (MDP). In the modeled MDP, the proposed scheme depends on the real-time network statistics at the time of decisions. Moreover, the convolutional neural network and the recurrent neural network are leveraged to extract fine-grained discriminative features. The numerical results through simulation demonstrate that DCRQN can effectively improve the data rate during the handoff process, outperforming the traditional handoff scheme.	https://dx.doi.org/10.1109/ACCESS.2019.2900445	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hao2022	A Deep Deterministic Policy Gradient Approach for Vehicle Speed Tracking Control With a Robotic Driver	In performance tests, replacing humans with robotic drivers has many advantages, such as high efficiency and high security. To realize the vehicle speed tracking control with a robotic driver, this article proposes a novel deep reinforcement learning (DRL) approach based on deep deterministic policy gradient (DDPG). Specifically, the design of the approach includes state space, action space, reward function, and control algorithm. Then, to shorten the training time, the proposed approach utilizes the basic fundamental relationship between vehicle speed and pedal opening to intervene in network exploration. Furthermore, to solve speed fluctuations in low-speed sections, the replay buffer is optimized by adding weighted training samples. Experiments are conducted on fifteen cars, and results show that the proposed algorithm can effectively control the vehicle speed. Generally, it only needs three or four episodes of training to meet the requirements. Compared with the Segment-PID method, the proposed method has a smoother speed and fewer overbound times. Note to Practitioners\emdashThis article was motivated by the algorithm of deep reinforcement learning (DRL), which can be applied to automatic control including vehicle speed tracking control with a robotic driver. With the complexity, the delay of the vehicle model, and the dead zone of the pedal, it is difficult for existing classic control theories to achieve satisfactory vehicle speed tracking. This article analyzes the mathematical model based on the DRL theory and the vehicle dynamic model, then builds the speed tracking controller based on the deep deterministic policy gradient (DDPG) algorithm, and makes corresponding optimizations for the problems in the experiment. Through the control software integrated with the DDPG algorithm, the operator can easily realize the training and testing of the neural network, which greatly reduces the time and cost of vehicle testing and is of great significance to the emission certification of new vehicles.	https://dx.doi.org/10.1109/TASE.2021.3088004	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hao2020	Human-Like Hybrid Caching in Software-Defined Edge Cloud	With the development of Internet of Things (IoT) and communication technology, the number of next-generation IoT devices has increased explosively, and the delay requirement for content requests is becoming progressively higher. Fortunately, the edge-caching scheme can satisfy users' demands for low latency of content. However, the existing caching schemes are not smart enough. To address these challenges, we propose a human-like hybrid caching architecture based on the software-defined edge cloud, which simultaneously considers the content popularity and the fine-grained user characteristics. Then, an optimization problem with a caching hit ratio as an optimization objective is formulated. To solve this problem, using reinforcement learning, we design a human-like hybrid caching algorithm. The extensive experiments show that compared with popular caching schemes, human-like hybrid caching schemes can improve the cache hit ratio by 20\%.	https://dx.doi.org/10.1109/JIOT.2019.2950688	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hara2022	Deep Reinforcement Learning with Graph Neural Networks for Capacitated Shortest Path Tour based Service Chaining	Network functions virtualization (NFV) realizes diverse and flexible network services by executing network functions on generic hardware as virtual network functions (VNFs). A certain network service is regarded as a sequence of VNFs, called service chain. The service chaining (SC) problem aims at finding an appropriate service path from an origin node to a destination node while executing the VNFs at the intermediate nodes in the required order under resource constraints on nodes and links. The SC problem belongs to the complexity class NP-hard. In our previous work, we modeled the SC problem as an integer linear program (ILP) based on the capacitated shortest path tour problem (CSPTP) where the CSPTP is an extended version of the SPTP with the node and link capacity constraints. We also developed the Lagrangian heuristics to achieve the balance between optimality and computational complexity. In this paper, we further propose a deep reinforcement learning (DRL) framework with the graph neural network (GNN) to realize the CSPTP-based SC adaptive to changes in service demand and/or network topology. Numerical results show that (1) the proposed framework achieves almost the same optimality as the ILP for the CSPTP-based SC and (2) it also works well without retraining even when the service demand changes or the network is partly damaged.	https://dx.doi.org/10.23919/CNSM55787.2022.9965166	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hasanbeig2022	LCRL: Certified Policy Synthesis via Logically-Constrained Reinforcement Learning	LCRL is a software tool that implements model-free Reinforcement Learning (RL) algorithms over unknown Markov Decision Processes (MDPs), synthesising policies that satisfy a given linear temporal specification with maximal probability. LCRL leverages partially deterministic finite-state machines known as Limit Deterministic B uchi Automata (LDBA) to express a given linear temporal specification. A reward function for the RL algorithm is shaped on-the-fly, based on the structure of the LDBA. Theoretical guarantees under proper assumptions ensure the convergence of the RL algorithm to an optimal policy that maximises the satisfaction probability. We present case studies to demonstrate the applicability, ease of use, scalability and performance of LCRL. Owing to the LDBA-guided exploration and LCRL model-free architecture, we observe robust performance, which also scales well when compared to standard RL approaches (whenever applicable to LTL specifications). Full instructions on how to execute all the case studies in this paper are provided on a GitHub page that accompanies the LCRL distribution www.github.com/grockious/lcrl.	https://dx.doi.org/10.1007/978-3-031-16336-4_11	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hasanbeig2022a	LCRL: Certified Policy Synthesis via Logically-Constrained Reinforcement Learning		https://doi.org/10.1007/978-3-031-16336-4_11	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hassan2020	Data-Driven Learning and Load Ensemble Control		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092373088&doi=10.1016\%2fj.epsr.2020.106780&partnerID=40&md5=9b3ac72de3d388bf2493f6de1798f2f1	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hassan2021	TCP Congestion Avoidance in Data Centres using Reinforcement Learning	TCP, a transport layer protocol which ensures the reliable delivery of information on the network, is the basis of Internet connectivity, with 85\% of the worlds Internet traffic being TCP based. TCP however, is slow to adapt to changes in the network, drastically reducing the throughput at the first sign of possible congestion, thereby preventing rapid restoration of the throughput. Mitigating this problem has been a very active area of research, as, until recently, the idea of using Artificial Intelligence (AI) in this space was relatively limited. Recently, Reinforcement Learning (RL), a form of AI, has been explored in the networking space, and in enhancing the performance of TCP,this paper aims to expand the use of RL for TCP (TCP-RL) in a software-defined data centre for the purpose of network congestion avoidance based on host-based TCP metrics. We demonstrate that our proposed approach is able to significantly reduce the impact of congestion on the end-to-end network throughput within the data centre.	https://dx.doi.org/10.23919/ICACT51234.2021.9370861	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hasson2015	A reinforcement learning approach to gait training improves retention	Many gait training programs are based on supervised learning principles: an individual is guided towards a desired gait pattern with directional error feedback. While this results in rapid adaptation, improvements quickly disappear. This study tested the hypothesis that a reinforcement learning approach improves retention and transfer of a new gait pattern. The results of a pilot study and larger experiment are presented. Healthy subjects were randomly assigned to either a supervised group, who received explicit instructions and directional error feedback while they learned a new gait pattern on a treadmill, or a reinforcement group, who was only shown whether they were close to or far from the desired gait. Subjects practiced for 10 min, followed by immediate and overnight retention and over-ground transfer tests. The pilot study showed that subjects could learn a new gait pattern under a reinforcement learning paradigm. The larger experiment, which had twice as many subjects (16 in each group) showed that the reinforcement group had better overnight retention than the supervised group (a 32\% vs. 120\% error increase, respectively), but there were no differences for over-ground transfer. These results suggest that encouraging participants to find rewarding actions through self-guided exploration is beneficial for retention.	https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940994537&doi=10.3389\%2ffnhum.2015.00459&partnerID=40&md5=c8813494119fc0cf280b45376dd92ea2	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Hayhoe2005	Eye movements in natural behavior	The classic experiments of Yarbus over 50 years ago revealed that saccadic eye movements reflect cognitive processes. But it is only recently that three separate advances have greatly expanded our understanding of the intricate role of eye movements in cognitive function. The first is the demonstration of the pervasive role of the task in guiding where and when to fixate. The second has been the recognition of the role of internal reward in guiding eye and body movements, revealed especially in neurophysiological studies. The third important advance has been the theoretical developments in the fields of reinforcement learning and graphic simulation. All of these advances are proving crucial for understanding how behavioral programs control the selection of visual information.	https://www.scopus.com/inward/record.uri?eid=2-s2.0-17444385937&doi=10.1016\%2fj.tics.2005.02.009&partnerID=40&md5=84e0c3d573ccb124d2e1bf7f7fb76fea	Excluded	new_screen	E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hazan2018	BindsNET: A Machine Learning-Oriented Spiking Neural Networks Library in Python	The development of spiking neural network simulation software is a critical component enabling the modeling of neural systems and the development of biologically inspired algorithms. Existing software frameworks support a wide range of neural functionality, software abstraction levels, and hardware devices, yet are typically not suitable for rapid prototyping or application to problems in the domain of machine learning. In this paper, we describe a new Python package for the simulation of spiking neural networks, specifically geared toward machine learning and reinforcement learning. Our software, called BindsNET(1), enables rapid building and simulation of spiking networks and features user-friendly, concise syntax. BindsNET is built on the PyTorch deep neural networks library, facilitating the implementation of spiking neural networks on fast CPU and GPU computational platforms. Moreover, the BindsNET framework can be adjusted to utilize other existing computing and hardware backends; e.g., TensorFlow and SpiNNaker. We provide an interface with the OpenAl gym library, allowing for training and evaluation of spiking networks on reinforcement learning environments. We argue that this package facilitates the use of spiking networks for large-scale machine learning problems and show some simple examples by using BindsNET in practice.	https://www.ncbi.nlm.nih.gov/pubmed/30631269	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	He2021	Research on Axle-Hole Assembly Method Based on Improved DDPG Algorithm	In view of the traditional teaching mode on industrial for assembly tasks involved in the attitude control, poor adaptability of assembly process problem such as open-loop control, designing an Improved Deep Deterministic Policy Gradient (DDPG) reinforcement learning algorithm to realize the axle-hole assembly task. This method obtains multi-dimensional information, including the end-effector's posture parameters and the robot joints, adopts the hierarchical reward mechanism function, to train robot searching, positioning and attitude-controlling skills, then finish the assembly position adjustment task; Finally, an improved DDPG algorithm based on improved hierarchical reward and multi-dimensional information is proposed to realize dynamic posture adjustment during robot assembly. We set up an experimental environment based on the Baxter robot in Gazebo simulation software. The experimental results show that, compared with DQN and DDPG algorithm, the improved DDPG algorithm performs well in the final training steps, rewards and success rate. Results compared with the previous two algorithms, the success rate increased by 21.7\% and 15.4\%. By comparing the changes of state dimension, the experiment further proves that the improved DDPG algorithm has the advantage of dynamically adjusting the pose in the process of axle-hole assembly task.	https://dx.doi.org/10.1109/ICRAS52289.2021.9476632	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	He2022	Imitative Reinforcement Learning Fusing Mask R-CNN Perception Algorithms	Autonomous urban driving navigation is still an open problem and has ample room for improvement in unknown complex environments. This paper proposes an end-to-end autonomous driving approach that combines Conditional Imitation Learning (CIL), Mask R-CNN with DDPG. In the first stage, data acquisition is first performed by using CARLA, a high-fidelity simulation software. Data collected by CARLA is used to train the Mask R-CNN network, which is used for object detection and segmentation. The segmented images are transformed into the backbone of CIL to perform supervised Imitation Learning (IL). DDPG means using Reinforcement Learning for further training in the second stage, which shares the learned weights from the pre-trained CIL model. The combination of the two methods is an innovative way of considering. The benefit is that it is possible to speed up training considerably and obtain super-high levels of performance beyond humans. We conduct experiments on the CARLA driving benchmark of urban driving. In the final experiments, our algorithm outperforms the original MP by 30\%, CIL by 33\%, and CIRL by 10\% in the most difficult tasks, dynamic navigation tasks, and in new environments and new weather, demonstrating that the two-stage framework proposed in this paper shows remarkable generalization capability in unknown environments on navigation tasks.	https://dx.doi.org/10.3390/app122211821	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	He2019	Reward of Reinforcement Learning of Test Optimization for Continuous Integration		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071576780&doi=10.13328\%2fj.cnki.jos.005714&partnerID=40&md5=459b2a838c59a22201ae4e6bc18c8518	Included	new_screen		4
RL4SE	He2015	Integrating Evolutionary Testing with Reinforcement Learning for Automated Test Generation of Object-Oriented Software	Recent advances in evolutionary test generation greatly facilitate the testing of Object-oriented (OO) software. Existing test generation approaches are still limited when the Software under test (SUT) includes Inherited class hierarchies (ICH) and Non-public methods (NPM). This paper presents an approach to generate test cases for OO software via integrating evolutionary testing with reinforcement learning. For OO software with ICH and NPM, two kinds of particular isomorphous substitution actions are presented and a Q-value matrix is maintained to assist the evolutionary test generation. A prototype called EvoQ is developed based on this approach and is applied to generate test cases for actual Java programs. Empirical results show that EvoQ can efficiently generate test cases for SUT with ICH and NPM and achieves higher branch coverage than two state-of-the-art test generation approaches within the same time budget.	https://dx.doi.org/10.1049/cje.2015.01.007	Included	new_screen		4
RL4SE	Heida2008	The basal ganglia		https://www.scopus.com/inward/record.uri?eid=2-s2.0-47849102761&doi=10.1007\%2f978-3-540-79462-2_2&partnerID=40&md5=4f704a8d81aba7f540df454ddf272092	Excluded	new_screen	E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hein2017	A benchmark environment motivated by industrial control problems	In the research area of reinforcement learning (RL), frequently novel and promising methods are developed and introduced to the RL community. However, although many researchers are keen to apply their methods on real-world problems, implementing such methods in real industry environments often is a frustrating and tedious process. Generally, academic research groups have only limited access to real industrial data and applications. For this reason, new methods are usually developed, evaluated and compared by using artificial software benchmarks. On one hand, these benchmarks are designed to provide interpretable RL training scenarios and detailed insight into the learning process of the method on hand. On the other hand, they usually do not share much similarity with industrial real-world applications. For this reason we used our industry experience to design a benchmark which bridges the gap between freely available, documented, and motivated artificial benchmarks and properties of real industrial problems. The resulting industrial benchmark (IB) has been made publicly available to the RL community by publishing its Java and Python code, including an OpenAI Gym wrapper, on Github. In this paper we motivate and describe in detail the IB's dynamics and identify prototypic experimental settings that capture common situations in real-world industry control problems.	https://dx.doi.org/10.1109/SSCI.2017.8280935	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Helali2022	Intelligent and compliant dynamic software license consolidation in cloud environment	Based on the virtualization technology and pushed by the softwarization paradigm and the actual demand for services and resources, commercial cloud data centers know an unprecedented expansion. The systematic presence of software and the services generated enabled the development of the already dense application expenses. This causes, not only a cost explosion, especially when proprietary solutions protected by licenses are in hand, but also, represents a critical need in terms of software asset and resource management at the SaaS level. In addition to these costs, inefficient resource utilization, and the resulting energy represent an important part of the operational expenditure of data centers and are still a hot topic despite the consolidation initiatives put in place. The main objective of the consolidation service is to maximize resource exploitation while minimizing energy consumption and costs, among others. Even so, we have noticed that the reported literature doesn't treat license management in the cloud environment as a whole, especially, from the resource management perspective and the overwhelming majority of the consolidation work focuses on resource optimization at the IaaS level. Therefore, we propose a reinforcement learning-based scheme that allows efficient use of resources and optimizes costs, energy consumption, and resource wastage, while remaining compliant. The experimental results show that our intelligent consolidator outperforms the baseline approaches according to the evaluation metrics used regardless of the resource heterogeneity and the data center dimensionality.	https://dx.doi.org/10.1007/s00607-022-01106-0	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Helle2017	Reinforcement learning for video encoder control in HEVC	In todays video compression systems, the encoder typically follows an optimization procedure to find a compressed representation of the video signal. While primary optimization criteria are bit rate and image distortion, low complexity of this procedure may also be of importance in some applications, making complexity a third objective. We approach this problem by treating the encoding procedure as a decision process in time and make it amenable to reinforcement learning. Our learning algorithm computes a strategy in a compact functional representation, which is then employed in the video encoder to control its search. By including measured execution time into the reinforcement signal with a lagrangian weight, we realize a trade-off between RD-performance and computational complexity controlled by a single parameter. Using the reference software test model (HM) of the HEVC video coding standard, we show that over half the encoding time can be saved at the same RD-performance.	https://dx.doi.org/10.1109/IWSSIP.2017.7965586	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Heo2018	Effective Program Debloating via Reinforcement Learning		https://doi.org/10.1145/3243734.3243838	Included	new_screen		4
RL4SE	Heseding2022	ReCEIF: Reinforcement Learning-Controlled Effective Ingress Filtering	Volumetric Distributed Denial of Service attacks forcefully disrupt the availability of online services by congesting network links with arbitrary high-volume traffic. This brute force approach has collateral impact on the upstream network infrastructure, making early attack traffic removal a key objective. To reduce infrastructure load and maintain service availability, we introduce ReCEIF, a topology-independent mitigation strategy for early, rule-based ingress filtering leveraging deep reinforcement learning. ReCEIF utilizes hierarchical heavy hitters to monitor traffic distribution and detect subnets that are sending high-volume traffic. Deep reinforcement learning subsequently serves to refine hierarchical heavy hitters into effective filter rules that can be propagated upstream to discard traffic originating from attacking systems. Evaluating all filter rules requires only a single clock cycle when utilizing fast ternary content-addressable memory, which is commonly available in software defined networks. To outline the effectiveness of our approach, we conduct a comparative evaluation to reinforcement learning-based router throttling.	https://dx.doi.org/10.1109/LCN53696.2022.9843478	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hey2007	Reduced complexity algorithms for cognitive packet networks			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hey2008	Reduced complexity algorithms for cognitive packet network routers		https://doi.org/10.1016/j.comcom.2008.04.026	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ho2015	Model-based reinforcement learning approach for planning in self-adaptive software system		https://doi.org/10.1145/2701126.2701191	Included	new_screen		4
RL4SE	Hoa2020	Reinforcement Learning based Method for Autonomous Navigation of Mobile Robots in Unknown Environments	The Reinforcement Learning is a subset of machine learning that deals with learning decisions from rewards given by the environment. The model classic reinforcement learning (RL) algorithms are usually applied to small sets of states and an action. However, in real applications, the state spaces are of a large scale and this will bring the problems in the generalization and the curse of dimensionality. In this research, authors integrate neural networks into reinforcement learning methods to generalize the value of all the states. The simulation results on the Gazebo software framework show the feasibility of the model proposed method algorithm. The robot can safely navigate an unprotected work environment and becomes a truly intelligent system with the ability to learn and adapt itself to the model.	https://dx.doi.org/10.1109/ICAMechS49982.2020.9310129	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Holmes2000	The learning classifier system: an evolutionary computation approach to knowledge discovery in epidemiologic surveillance	The learning classifier system (LCS) integrates a rule-based system with reinforcement learning and genetic algorithm-based rule discovery. This investigation reports on the design, implementation, and evaluation of EpiCS, a LCS adapted for knowledge discovery in epidemiologic surveillance. Using data from a large, national child automobile passenger protection program, EpiCS was compared with C4.5 and logistic regression to evaluate its ability to induce rules from data that could be used to classify cases and to derive estimates of outcome risk, respectively. The rules induced by EpiCS were less parsimonious than those induced by C4.5, but were potentially more useful to investigators in hypothesis generation. Classification performance of C4.5 was superior to that of EpiCS (P < 0.05). However, risk estimates derived by EpiCS were significantly more accurate than those derived by logistic regression (P < 0.05). (C) 2000 Elsevier Science B.V. All rights reserved.	https://www.ncbi.nlm.nih.gov/pubmed/10767616	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	HolmesParker2011	Agent-based resource allocation in dynamically formed CubeSat constellations			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hongvanthong2020	Novel Four-Layered Software Defined 5G Architecture for AI-based Load Balancing and QoS Provisioning	Software defined 5G network (SD-5G) is an evolving networking technology. The integration of SDN and 5G brings scalability, and efficiency. However, Quality of Service (QoS) provision is still challenging in SD-5G due to improper load balancing, traffic unawareness and so on. To overwhelm these issues this paper designs a novel load balancing scheme using Artificial Intelligence (AI) techniques. Firstly, novel four-layered SD-5G network is designed with user plane, smart data plane, load balancing plane, and distributed control plane. In the context to 5G, the data transmission rate must satisfy the QoS constraints based on the traffic type such as text, audio, video etc. Thus, the data from the user plane is classified by Smart Traffic Analyzer in the data plane. For traffic analysis, Enriched Neuro-Fuzzy (ENF) classifier is proposed. In the load balancing plane, Primary Load balancer and Secondary Load Balancer are deployed. This plane is responsible for balancing the load among controllers. For controller load balancing, switch migration is presented. Overloaded controller is predicted by Entropy function. Then decision for migration is made by Fitness-based Reinforcement Learning (F-RL) algorithm. Finally, the four-layered SD-5G network is modeled in the NS-3.26. The observations shows that the proposed work improves the SD-5G network in terms of Loss Rate, Packet Delivery Rate, Delay, and round trip time.	https://dx.doi.org/10.1109/ICCCS49078.2020.9118463	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hossain2022	Reconfigurable Intelligent Surfaces enabling Positioning, Navigation, and Timing Services	In this paper we exploit the advances provided by the Reconfigurable Intelligent Surfaces (RIS) technology, which allows for the software-defined control of the electromagnetic properties of the wireless medium, in order to introduce a low-cost and easily deployable ground-based alternative positioning, navigation, and timing (PNT) service. According to the proposed solution, the positioning of the target is performed by one original signal transmitted by one base station (BS), and three additional signals reflected by three RISs (selected from a set of available RISs), thus reducing significantly the required implementation and infrastructure cost. Different reinforcement learning algorithms, based on the gradient ascent and the log-linear learning models, are introduced and investigated to enable the target, to autonomously and dynamically select the optimal set of three RISs to be used for the optimization of its positioning accuracy. Subsequently, an iterative least square algorithm is realized to determine the position of the target. Detailed numerical results are presented that highlight the tradeoffs of the introduced reinforcement learning algorithms in terms of convergence and impact on achieved positioning precision, and demonstrate the superiority of the proposed methodology against existing ground-based PNT solutions.	https://dx.doi.org/10.1109/ICC45855.2022.9838473	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hossain2020	Deep Reinforcement Learning-based ROS-Controlled RC Car for Autonomous Path Exploration in the Unknown Environment	Nowadays, Deep reinforcement learning has become the front runner to solve problems in the field of robot navigation and avoidance. This paper presents a LiDAR-equipped RC car trained in the GAZEBO environment using the deep reinforcement learning method. This paper uses reshaped LiDAR data as the data input of the neural architecture of the training network. This paper also presents a unique way to convert the LiDAR data into a 2D grid map for the input of training neural architecture. It also presents the test result from the training network in different GAZEBO environment. It also shows the development of hardware and software systems of embedded RC car. The hardware system includes-Jetson AGX Xavier, teensyduino and Hokuyo LiDAR; the software system includes-ROS and Arduino C. Finally, this paper presents the test result in the real world using the model generated from training simulation.	https://dx.doi.org/10.23919/ICCAS50221.2020.9268370	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hovell2021	Acceleration-based quadrotor guidance under time delays using deep reinforcement learning			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hu2017	A Low-Cost Autonomous Robot and Educational Platform for Intelligent Automation Experiments	In this paper, we present a low-cost (less than $10) real-time mobile-robot platform: Aicar, for educational experiments in Intelligent Automation and related disciplines such as Artificial Intelligence and Robot Engineering. The Aicar platform uses Arduino open-source hardware and supports various programming interfaces including C/C++, Python and Matlab. With its attached android mobile phone, Aicar can support image capturing and video streaming for computer vision applications. With Aicar, various Intelligent Automation and Artificial Intelligence algorithms can be practiced, including rule based system, reinforcement learning and deep learning algorithms. Aicar platform is open sourced and the hardware design and software startup code for Aicar project is available for public access through the free online service.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hu2020	Application of Deep Reinforcement Learning in the Board Game	Deep reinforcement learning is one of the core technologies, which leads to artificial intelligence. It has been used widely not only in robot, autonomous driving, power station control and human-computer interaction in mobile e-commerce platforms, but also in gambling games. The game software AlphaGo of artificial intelligence defeated Ke Jie in 2017. Since then, the study of artificial intelligence and deep reinforcement is widely concerned. We build a Neural Network model using TensorFlow and Monte Carlo tree search algorithm, so that agents can reach the level of playing chess with human beings in board games. The gobang interface is realized by Python, and the PhoenixGo recurrence is completed. The computation is reduced and the training effect of the agent is improved by optimizing the establishment of gobang strategic value network. Black hand first can achieve a 73\% winning rate, and white hand first is 64\%, when playing against human players.	https://dx.doi.org/10.1109/ICIBA50161.2020.9277188	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hu2021	Counterexample generation in CPS model checking based on ARSG algorithm		https://doi.org/10.1504/ijcse.2021.115658	Excluded	new_screen	E5: Other not a paper,E1: Does not define or use a RL method	4
RL4SE	Hu2019	Research on 3D animation character design based on multimedia interaction		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064614521&doi=10.1007\%2fs11042-019-7538-z&partnerID=40&md5=3dac92733b6a235608bc3629887e65aa	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hu2022	Towards a Very Large Scale Traffic Simulator for Multi-Agent Reinforcement Learning Testbeds	Smart traffic control and management become an emerging application for Deep Reinforcement Learning (DRL) to solve traffic congestion problems in urban networks. Different traffic control and management policies can be tested on the traffic simulation. Current DRL-based studies are mainly supported by the microscopic simulation software (e.g., SUMO11https://www.eclipse.org/sumo/), while it is not suitable for city-wide control due to the computational burden and gridlock effect. To the best of our knowledge, there is a lack of studies on the large-scale traffic simulator for D RL testbeds. In view of this, we propose a meso-macro traffic simulator for very large-scale DRL scenarios. The proposed simulator integrates meso scopic and macroscopic traffic simulation models to improve efficiency and eliminate gridlocks. The meso scopic link model simulates flow dynamics on roads, and the macroscopic Bathtub model depicts vehicle movement in regions. Moreover, both types of models can be hybridized to accommodate various DRL tasks. The result shows that the developed simulator only takes 46 seconds to finish a 24-hour simulation in a very large city with 2.2 million vehicles, which is much faster than SUMO. In the future, the developed meso-macro traffic simulator could serve as a new environment for very large-scale DRL problems.	https://dx.doi.org/10.1109/ITSC55140.2022.9921887	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hua2020	Less is more: Data-efficient complex question answering over knowledge bases		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092912571&doi=10.1016\%2fj.websem.2020.100612&partnerID=40&md5=9c65c767c9d71d00d294ff9451c6fc24	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hua2020a	Few-Shot Complex Knowledge Base Question Answering via Meta Reinforcement Learning	Complex question-answering (CQA) involves answering complex natural-language questions on a knowledge base (KB). However, the conventional neural program induction (NPI) approach exhibits uneven performance when the questions have different types, harboring inherently different characteristics, e.g., difficulty level. This paper proposes a meta-reinforcement learning approach to program induction in CQA to tackle the potential distributional bias in questions. Our method quickly and effectively adapts the meta-learned programmer to new questions based on the most similar questions retrieved from the training data. The meta-learned policy is then used to learn a good programming policy, utilizing the trial trajectories and their rewards for similar questions in the support set. Our method achieves state-of-the-art performance on the CQA dataset (Saha et al., 2018) while using only five trial trajectories for the top-5 retrieved questions in each support set, and meta-training on tasks constructed from only 1\% of the training set.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Huang2021	Solving the shortest path interdiction problem via reinforcement learning	This paper addresses the shortest path interdiction problem, in which the leader aims to maximise the length of the shortest path that the follower can traverse subject to a limited interdiction budget. To solve this problem, we propose a reinforcement learning framework and use the pointer network to handle the situation of variable output sizes. To evaluate the performance of our proposed reinforcement learning model, we conduct extensive computational experiments on a set of instances that are generated from two different network topologies, i.e. the grid networks and the random graphs. To train the pointer network, we consider three different baselines, i.e. the exponential, critical, and rollout baselines, among which the rollout baseline policy achieves the best computational results, and thus is used as the default baseline during our computational experiments. Moreover, when the size of instances increases, we find that solving the equivalent single-level mixed integer program of the problem could be quite time-consuming, while our proposed reinforcement learning approach can still obtain solutions with good performance effectively for both the grid networks and the random graphs.	https://dx.doi.org/10.1080/00207543.2021.2002962	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Huang2021a	Ayudante: A Deep Reinforcement Learning Approach to Assist Persistent Memory Programming	Nonvolatile random-access memories (NVRAMs) are envisioned as a new tier of memory in future server systems. They enable a promising persistent memory (PM) technique, with comparable performance of DRAM and the persistence property of storage. However, programming PM imposes nontrivial labor effort on writing code to adopt new PM-aware libraries and APIs. In addition, non-expert PM code can be error-prone. In order to ease the burden of PM programmers, we propose Ayudante1, a deep reinforcement learning (RL)-based PM programming assistant framework consisting of two key components: a deep RL-based PM code generator and a code refining pipeline. Given a piece of C, C++, or Java source code developed for conventional volatile memory systems, our code generator automatically generates the corresponding PM code and checks its data persistence. The code refining pipeline parses the generated code to provide a report for further program testing and performance optimization. Our evaluation on an Intel server equipped with Optane DC PM demonstrates that both microbenchmark programs and a key-value store application generated by Ayudante pass PMDK checkers. Performance evaluation on the microbenchmarks shows that the generated code achieves comparable speedup and memory access performance as PMDK code examples.		Included	new_screen		4
RL4SE	Huang2022	Improving traffic signal control operations using proximal policy optimization	Existing traffic signal control systems present many limitations including fixed signal timing schemes, insufficient efficiency and flexibility, and difficulty in adapting to the changing traffic flows. In recent years, the development of deep reinforcement learning (RL) has shown great research potential and application prospects. This paper proposes an intersection signal control method based on the proximal policy optimization (PPO) method. Specifically, this paper uses the value vector representation method of traffic characteristics to encode the traffic state and then feeds the state encoding into the long short-term memory (LSTM) network to obtain the signal phase output. Finally, to obtain the optimal signal control strategy, the PPO algorithm is utilized to train the neural network and adjust the signal phase. The proposed algorithm is benchmarked against other classic RL and adaptive signal control schemes in an experimental environment based on the traffic simulation software simulation of urban mobility (SUMO). Experimental results showed that the proposed algorithm greatly improves traffic efficiency. Specifically, the mean velocity of the vehicles increases by 45.09\%, and the mean occupancy rate of each lane, the length of the longest jams during each step, and the mean halting duration dropped by 21.38\%, 25.86\%, and 12.94\%, respectively.	https://dx.doi.org/10.1049/itr2.12286	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Huang2019	Autophase: Compiler phase-ordering for hls with deep reinforcement learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068310151&doi=10.1109\%2fFCCM.2019.00049&partnerID=40&md5=bd1fed13e2b7649ccc84508410a92a89	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Huang2022a	A Collaborative Optimization Algorithm for Ship Damage Stability Design		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126118854&doi=10.1088\%2f1742-6596\%2f2203\%2f1\%2f012071&partnerID=40&md5=824adeb3b0312642d1ccd0b2cc66f219	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Huang2020	Towards automatically generating block comments for code snippets	Code commenting is a common programming practice of practical importance to help developers review and comprehend source code. There are two main types of code comments for a method: header comments that summarize the method functionality located before a method, and block comments that describe the functionality of the code snippets within a method. Inspired by the effectiveness of deep learning techniques in the NLP field, many studies focus on using the machine translation model to automatically generate comment for the source code. Because the data set of block comments is difficult to collect, current studies focus more on the automatic generation of header comments than that of block comments. However, block comments are important for program comprehension due to their explanation role for the code snippets in a method. To fill the gap, we have proposed an approach that combines heuristic rules and learning-based method to collect a large number of comment-code pairs from 1,032 open source projects in our previous study. In this paper, we propose a reinforcement learning-based method, RL-BlockCom, to automatically generate block comments for code snippets based on the collected comment-code pairs. Specifically, we utilize the abstract syntax tree (i.e., AST) of a code snippet to generate a token sequence with a statement-based traversal way. Then we propose a composite learning model, which combines the actor-critic algorithm of reinforcement learning with the encoder-decoder algorithm, to generate block comments. On the data set of the comment-code pairs, the BLEU-4 score of our method is 24.28, which outperforms the baselines and state-of-the-art in comment generation.	https://dx.doi.org/10.1016/j.infsof.2020.106373	Included	new_screen		4
RL4SE	Huo2018	Dyna-Q Algorithm for Path Planning of Quadrotor UAVs	In this paper, the problem of path planning of quadrotor unmanned aerial vehicles (UAVs) is investigated in the framework of reinforcement learning methodology. With the abstraction of the environment in the form of grid world in 2D, the design procedure is presented by utilizing the Dyna-Q algorithm, which is one of the reinforcement method combining both model-based and non-model framework. In this process, an optimal or suboptimal safe flight trajectory will be obtained by learning constantly and planning by simulated experience, thus calculative reward can be maximized efficiently. Matlab software is used for maze establishing and computation, and the effectiveness of the proposed method is illustrated by two typical examples.	https://dx.doi.org/10.1007/978-981-13-2853-4_27	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hurtado-Gomez2021	Traffic Signal Control System Based on Intelligent Transportation System and Reinforcement Learning	Traffic congestion has several causes, including insufficient road capacity, unrestricted demand and improper scheduling of traffic signal phases. A great variety of efforts have been made to properly program such phases. Some of them are based on traditional transportation assumptions, and others are adaptive, allowing the system to learn the control law (signal program) from data obtained from different sources. Reinforcement Learning (RL) is a technique commonly used in previous research. However, properly determining the states and the reward is key to obtain good results and to have a real chance to implement it. This paper proposes and implements a traffic signal control system (TSCS), detailing its development stages: (a) Intelligent Transportation System (ITS) architecture design for the TSCS; (b) design and development of a system prototype, including an RL algorithm to minimize the vehicle queue at intersections, and detection and calculation of such queues by adapting a computer vision algorithm; and (c) design and development of system tests to validate operation of the algorithms and the system prototype. Results include the development of the tests for each module (vehicle queue measurement and RL algorithm) and real-time integration tests. Finally, the article presents a system simulation in the context of a medium-sized city in a developing country, showing that the proposed system allowed reduction of vehicle queues by 29\%, of waiting time by 50\%, and of lost time by 50\%, when compared to fixed phase times in traffic signals.	https://dx.doi.org/10.3390/electronics10192363	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Huser2006	Multimodal learning of demonstrated grasping skills for flexibly handling grasped objects			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hutchison2008	Combining learning and evolution to develop high DOF robot control	This paper describes a method for developing control of high degree-of-freedom (DOF) mobile robots using the seventh generation (7G) system, a software system that incorporates learning, genetic algorithms, and scripting. The control agent is based on a neural network implementing a reinforcement learning process. The network accepts sensor data as input and learns to output control actions. A novel feature of the learning system allows the developer to insert a script as an alternative action output from the neural network learning system. The script significantly reduces the search space for the learning system even if it is sometimes wrong, thereby enabling the learning network to bootstrap toward more effective solutions. An integrated genetic algorithm system modifies parameters of the control agent to evolve the best control agent based on fitness. Fitness is measured by the success of a control agent in learning to control behavior of a simulated model of the robot in selected simulated terrains. An iterative process is described in which the control software is integrated with a simulation model of the robot running in a 3D physics-based simulation system. The method was used to develop control of the OmniTread OT-4, a high DOF serpentine robot.	https://dx.doi.org/10.1109/TEPRA.2008.4686690	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Huurman2020	Generating API Test Data Using Deep Reinforcement Learning		https://doi.org/10.1145/3387940.3392214	Included	new_screen		4
RL4SE	Hwangbo2020	Design of control framework based on deep reinforcement learning and Monte-Carlo sampling in downstream separation	This paper proposes a systematic framework to develop deep reinforcement learning (RL)-based algorithms for control system of downstream separation in biopharmaceutical process as follows. First, a simulation model as a digital twin is built and Monte-Carlo sampling generates substantial amounts of samples considering disturbances. Second, the deep RL-based control system is designed and the optimization subject to sample datasets is conducted. The methodology is implemented in a prototype software and relevant codes are shared by Mendeley Data. The proposed model is successfully applied to control the liquid-liquid extraction column for the recovery of fusidic acid as part of downstream processing. The resulting deep RL algorithm provides an operation performance with a better API recovery yield (32 \% higher than open loop operation) and lower deviations (23 \% lower than open loop operation) against disturbances. (C) 2020 Elsevier Ltd. All rights reserved.	https://dx.doi.org/10.1016/j.compchemeng.2020.106910	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Iannucci2020	A hybrid model-free approach for the near-optimal intrusion response control of non-stationary systems	Given the always increasing size of computer systems, manually protecting them in case of attacks is unfeasible and error-prone. For this reason, until now, several model-based Intrusion Response Systems (IRSs) have been proposed with the purpose of limiting the amount of work of the system administrators. However, since the most advanced IRSs adopt a stateful approach, they are subject to what Richard Bellman defined as the curse of dimensionality. Furthermore, modern computer systems are non-stationary, that is, they are subject to frequent changes in their configuration and in their software base, which in turn could make a model-based approach ineffective due to deviations in system behavior with respect to the model. In this paper we propose, to the best of our knowledge, the first approach based on deep reinforcement learning for the implementation of a hybrid model-free IRS. Experimental results show that the proposed IRS is able to deal with non-stationary systems, while reducing the time needed for the computation of the defense policies by orders of magnitude with respect to model-based approaches, and being still able to provide near-optimal rewards. (C) 2020 Elsevier B.V. All rights reserved.	https://dx.doi.org/10.1016/j.future.2020.03.018	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Icarte2022	Reward Machines: Exploiting Reward Function Structure in Reinforcement Learning		https://doi.org/10.1613/jair.1.12440	Excluded	new_screen	E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for	4
RL4SE	Iizuka2022	Deep Reinforcement Learning for Multi-agent Simulation using a partial floor field cutout	The purpose of this study is to easily create agent behavior for disaster evacuation simulation. Multi-agent simulation is commonly used in disaster evacuation simulations, but it is difficult to program agents. Floor field models and reinforcement learning have been proposed as a way of solving this problem. However, there are issues with unnatural stagnation or learning times. In this paper, we report the reinforcement learning method for an evacuation simulation agent, using the local environment. As a result of the experiment, it was confirmed that efficient movement of the agent can be achieved in a short learning time.	https://dx.doi.org/10.1109/IIAIAAI55812.2022.00137	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Inagaki2007	Learning evaluation functions of Shogi positions from different sets of games	This paper addresses learning of a reasonably accurate evaluation function of Shogi (Japanese Chess) positions through learning from records of games. Accurate evaluation of a Shogi position is indispensable for a computer Shogi program. A Shogi position is projected into several semantic features characterizing the position. Using such features as input, we employ reinforcement learning with a multi-layer perceptron as a nonlinear function approximator. We prepare two completely different sets of games: games played by computer Shogi programs and games played by professional Shogi players. Then we built two evaluation functions by separate learning based on two different sets of games, and compared the results to find several interesting tendencies.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Innes2008	Negative versus positive reinforcement: An evaluation of training strategies for rehabilitated horses		https://www.scopus.com/inward/record.uri?eid=2-s2.0-44949212920&doi=10.1016\%2fj.applanim.2007.08.011&partnerID=40&md5=2d7a68b8108a08ac5374d950efbc1bfa	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Iovino2020	Model Repair with Quality-Based Reinforcement Learning	Domain modeling is a core activity in Model-Driven Engineering, and these models must be correct. A large number of artifacts may be constructed on top of these domain models, such as instance models, transformations, and editors. Similar to any other software artifact, domain models are subject to the introduction of errors during the modeling process. There are a number of existing tools that reduce the burden of manually dealing with correctness issues in models. Although various approaches have been proposed to support the quality assessment of modeling artifacts in the past decade, the quality of the automatically repaired models has not been the focus of repairing processes. In this paper, we propose the integration of an automatic evaluation of domain models based on a quality model with a framework for personalized and automatic model repair. The framework uses reinforcement learning to find the best sequence of actions for repairing a broken model.	https://dx.doi.org/10.5381/jot.2020.19.2.a17	Included	new_screen		4
RL4SE	Isbell2006	Cobot in LambdaMOO: An Adaptive Social Statistics Agent		https://doi.org/10.1007/s10458-006-0005-z	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Islam2022	Software-Defined Routing Strategy Based on Reinforcement Learning in Smart Power Grid	To enable a set of advanced functionalities, the smart power grid relies on pervasive sensors and actuators for continuous monitoring and control. These field devices handle both regular and event-driven (emergency) data packets, which require different levels of quality-of-service (QoS) support (latency requirements). Unfortunately, the existing routing strategies in smart power grids do not guarantee differentiated QoS support that adapts to the network dynamics. To address this limitation, we propose a QoS-aware routing strategy in smart grids based on a software-defined network (SDN). The proposed framework establishes separate queues for event-driven and fixed-scheduling packets. Then, the routing strategy is designed based on reinforcement learning to enable adaptive QoS support. In this regard, updating the routing tables at the OpenFlow switches for each incoming packet is not practical and incurs additional delays that would violate the target QoS. Instead, the proposed routing strategy relies on two Q-learning agents that fix an optimal route for each source-destination pair and update the queue service rate for each switch along the route depending on the network condition (packet arrival rate). Our simulation results demonstrate that the proposed adaptive strategy offers effective QoS support compared with an SDN benchmark routing strategy based on Bellman-Ford. For instance, the average percentage of packets that violate their latency requirement is only 3\% compared with at least 30\% for the benchmark routing strategy.	https://dx.doi.org/10.1109/SoutheastCon48659.2022.9764077	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ivanovs2021	Perturbation-based methods for explaining deep neural networks: A survey		https://doi.org/10.1016/j.patrec.2021.06.030	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Iyer2018	Transparency and Explanation in Deep Reinforcement Learning Neural Networks	"Autonomous AI systems will be entering human society in the near future to provide services and work alongside humans. For those systems to be accepted and trusted, the users should be able to understand the reasoning process of the system, i.e. the system should be transparent. System transparency enables humans to form coherent explanations of the system's decisions and actions. Transparency is important not only for user trust, but also for software debugging and certification. In recent years, Deep Neural Networks have made great advances in multiple application areas. However, deep neural networks are opaque. In this paper, we report on work in transparency in Deep Reinforcement Learning Networks (DRLN). Such networks have been extremely successful in accurately learning action control in image input domains, such as Atari games. In this paper, we propose a novel and general method that (a) incorporates explicit object recognition processing into deep reinforcement learning models, (b) forms the basis for the development of ""object saliency maps"	https://dx.doi.org/10.1145/3278721.3278776	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jaafari2022	Traffic-aware Routing with Softwaredefined Networks Using Reinforcement Learning and Fuzzy Logic		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139987637&doi=10.47839\%2fijc.21.3.2687&partnerID=40&md5=c5ba4709a9b60229addde049b0e3d720	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jacek2016	Assessing the limits of program-specific garbage collection performance		https://doi.org/10.1145/2908080.2908120	Included	new_screen		4
RL4SE	Jacob2022	Autonomous Navigation of Drones Using Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122389463&doi=10.1007\%2f978-981-16-7220-0_10&partnerID=40&md5=3802f1341ca2d40a824eb5bb4c56cfd3	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jaensch2019	Reinforcement Learning of a Robot Cell Control Logic using a Software-in-the-Loop Simulation as Environment	This paper introduces a method for automatic robot programming of industrial robots using reinforcement learning on a Software-in-the-loop simulation. The focus of the the paper is on the higher levels of a hierarchical robot programming problem. While the lower levels the skills are stored as domain specific program code, the combination of the skills into a robot control program to solve a specific task is automated. The reinforcement learning learning approach allows the shopfloor workers and technicians just to define the end result of the manufacturing process through a reward function. The programming and process optimization is done within the learning procedure. The Software-in-the-loop simulation with the robot control software makes it possible to to interpret the real program code and generate the exact motion. The exact motion of the robot is needed in order to find not just an optimal but also a collision-free policy.	https://dx.doi.org/10.1109/AI4I46381.2019.00027	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jaensch2021	Curriculum Multi-Stage Reinforcement Learning for Automated Interlinked Production Systems on Virtual Commissioning Simulations	In order to automate the software engineering process of interlinked production systems, reinforcement learning applications can be used to learn the control flow logic on the basis of virtual production systems. Since the simulation- based prototypes are available for virtual commissioning (VC) anyway, they can be used simultaneously as reinforcement learning environments. In this work, the event-discrete flow logic for the transport and assembly of a target workpiece is learned automatically by reinforcement learning on the real use case of the VC simulation of a PLC-based production system. According to the idea of curriculum learning, the system is trained separately in subsystems to support its modularity and to reduce the complexity of the overall learning process. With regard to the learning processes, subsystems, sequence errors, termination criteria and necessary action and state adjustments typical for the PLC-based plant are identified and implemented in the VC simulation. The reward functions are derived with respect to the individual subsystems. The learned controls of the subsystems are then merged back together for a complete flow of the entire system.	https://dx.doi.org/10.1109/TransAI51903.2021.00031	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jagannath2019	Artificial Intelligence-based Cognitive Cross-layer Decision Engine for Next-Generation Space Mission	In this position paper, the authors argue the need for a novel framework that provides flexibility, autonomy and optimizes the use of scarce resources to ensure reliable communication during next-generation space missions. To this end, the authors present the shortcomings of existing space architectures and the challenges in realizing adaptive autonomous space-networking. In this regard, the authors aim to jointly exploit the immense capabilities of deep reinforcement learning (DRL) and cross-layer optimization by proposing an artificial intelligence-based cognitive cross-layer decision engine to bolster next-generation space missions. The presented software-defined cognitive cross-layer decision engine is designed for the resource-constrained Internet-of-Space-Things. The framework is designed to be flexible to accommodate varying (with time and location) requirements of multiple space missions such as reliability, throughput, delay, energy-efficiency among others. In this work, the authors present the formulation of the cross-layer optimization for multiple mission objectives that forms the basis of the presented framework. The cross-layer optimization problem is then modeled as a Markov Decision Process to be solved using deep reinforcement learning (DRL). Subsequently, the authors elucidate the DRL model and concisely explain the deep neural network architecture to perform the DRL. This position paper concludes by providing the different phases of the evaluation plan for the proposed cognitive framework.	https://dx.doi.org/10.1109/CCAAW.2019.8904895	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jagannath2022	MR-iNet Gym: Framework for Edge Deployment of Deep Reinforcement Learning on Embedded Software Defined Radio		https://doi.org/10.1145/3522783.3529530	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jagannath2022a	Digital Twin Virtualization with Machine Learning for IoT and Beyond 5G Networks: Research Directions for Security and Optimal Control		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134014423&doi=10.1145\%2f3522783.3529519&partnerID=40&md5=b7f4373332f528932c4d25aa60cea9d9	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jain2022	Reinforcement Learning assisted Loop Distribution for Locality and Vectorization	Loop distribution (loop fission) is a well known compiler optimization that splits the loop into multiple loops. Loop distribution can be seen as an enabler of various other optimizations with different goals, like, the parallelizability of the loop, the vectorizability of the loop, its locality characteristics. In this work, we present a Reinforcement Learning (RL) based approach to efficiently perform loop-distribution with the twin goals of optimizing for both vectorization as well as locality. Broadly, we generate the SCC Dependence Graph (SDG) for each loop of the program. Our RL model learns to predict the distribution order of the loop by performing topological walk of the graph. The reward to our model is computed using the instruction cost and number of cache misses of the loop. As the RL models need data for training purposes, we also propose a novel strategy to extend the training set by generating new loops. We show our results on x86 architecture on various benchmarks: from TSVC, LLVM-Test-Suite, PolyBench and PolyBench-NN. Our framework achieves an average improvement of 3.63\% on TSVC, 4.61\% on LLVM-Test-Suite Microbenchmarks, 1.78\% on PolyBench and 1.95\% on PolyBench-NN benchmark suites for execution time. The baseline is O3 compiler option of LLVM. We also show the improvements of our method on other performance metrics like Instruction Per Cycle (IPC), Number of loops distributed & vectorized, and L1 cache performance.	https://dx.doi.org/10.1109/LLVM-HPC56686.2022.00006	Included	conflict_resolution		4
RL4SE	Jang2021	Offline-online reinforcement learning for generalizing demand response price-setting to energy systems		https://doi.org/10.1145/3486611.3492391	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jang2021a	Offline-online reinforcement learning for energy pricing in office demand response: lowering energy and data costs		https://doi.org/10.1145/3486611.3486668	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jang2021b	Using Deep Q- Network in Bandwidth Allocation of Smart Homes	Smart homes provide users with a more convenient, comfortable, and safe living environment through various automation equipment, high-tech home appliances, and network services. With the development of the Internet of Things and widespread smart home, there are many Internet-enabled services in smart homes. Each kind of service has distinct service quality requirements. Remote disaster warning and remote monitoring and detection service emphasize realtime and reliability. UHD video (4K/8K) and VR /AR services require high transmission bandwidth. As the number of IoT-enabled equipment of smart homes increases, it becomes imperative to effectively allocate limited bandwidth resources and improve the overall network performance to ensure the effectiveness of various services. From the perspective of Internet service providers (ISP), this research studied deep reinforcement learning techniques cooperating with software-defined networks (SDN) to improve traditional smart homes' bandwidth management architecture and bandwidth allocation method. The SDN architecture separates the control plane and the data plane. It centralizes the control mechanism to simplify and support flexible network management. Deep reinforcement learning does not rely on labeled data instead of exploring the unknown environment and the environment's feedback on current as the basis for the subsequent action. This study used SDN to simulate the network environment from smart homes to an ISP. Due to the lack of a sufficient amount of labeled data, and it is not easy to establish a standard for data labeling, we used the Deep Q-network (DQN) of deep reinforcement learning in the bandwidth allocation for smart homes. The simulation results show that Deep Q-network can achieve high performance in both the jitter reduction ratio and the probability of successfully fulfilling bandwidth allocation termination conditions.	https://dx.doi.org/10.1109/IEMCON53756.2021.9623084	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jarosik2019	Automatic Ultrasound Guidance Based on Deep Reinforcement Learning	Ultrasound is becoming the modality of choice for everyday medical diagnosis, due to its mobility and decreasing price. As the availability of ultrasound diagnostic devices for untrained users grows, appropriate guidance becomes desirable. This kind of support could be provided by a software agent, who easily adapts to new conditions, and whose role is to instruct the user on how to obtain optimal settings of the imaging system during an examination. In this work, we verified the feasibility of implementing and training such an agent for ultrasound, taking the deep reinforcement learning approach. The tasks it was given were to find the optimal position of the transducer's focal point (FP task) and to find an appropriate scanning plane (PP task). The ultrasound environment consisted of a linear-array transducer acquiring information from a tissue phantom with cysts forming an object-of-interest (OOI). The environment was simulated in the Field-II software. The agent could perform the following actions: move the position of the probe to the left/right, move focal depth upwards/downwards, rotate the probe clockwise/counter-clockwise, or do not move. Additional noise was applied to the current probe setting. The only observations the agent received were B-mode frames. The agent acted according to stochastic policy modeled by a deep convolutional neural network, and was trained using the vanilla policy gradient update algorithm. After the training, the agent's ability to accurately locate the position of the focal depth and scanning plane improved. Our preliminary results confirmed that deep reinforcement learning can be applied to the ultrasound environment.	https://dx.doi.org/10.1109/ULTSYM.2019.8926041	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jarosik2021	Pixel-Wise Deep Reinforcement Learning Approach for Ultrasound Image Denoising	Ultrasound (US) imaging is widely used for the tissue characterization. However, US images commonly suffer from speckle noise, which degrades perceived image quality. Various deep learning approaches have been proposed for US image denoising, but most of them lack the interpretability of how the network is processing the US image (black box problem). In this work, we utilize a deep reinforcement learning (RL) approach, the pixelRL, to US image denoising. The technique utilizes a set of easily interpretable and commonly used filtering operations applied in a pixel-wise manner. In RL, software agents act in an unknown environment and receive appropriate numerical rewards. In our case, each pixel of the input US image has an agent and state of the environment is the current US image. Agents iteratively denoise the US image by executing the following pixel-wise pre-defined actions: Gaussian, bilateral, median and box filtering, pixel value increment/decrement and no action. The proposed approach can be used to generate action maps depicting operations applied to process different parts of the US image. Agents were pre-trained on natural gray-scale images and evaluated on the breast mass US images. To enable the evaluation, we artificially corrupted the US images with noise. Compared with the reference (noise free US images), filtration of the images with the proposed method increased the average peak signal-to-noise ratio (PSNR) score from 14 dB to 26 dB and increased the structure similarity index score from 0.22 to 0.54. Our work confirms that it is feasible to use pixel-wise RL techniques for the US image denoising.	https://dx.doi.org/10.1109/IUS52206.2021.9593591	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ja?kowski2018	Mastering 2048 With Delayed Temporal Coherence Learning, Multistage Weight Promotion, Redundant Encoding, and Carousel Shaping	2048 is an engaging single-player nondeterministic video puzzle game, which, thanks to the simple rules and hard-tomaster gameplay, has gained massive popularity in recent years. As 2048 can be conveniently embedded into the discrete-state Markov decision processes framework, we treat it as a testbed for evaluating existing and new methods in reinforcement learning. With the aim to develop a strong 2048 playing program, we employ temporal difference learning with systematic n-tuple networks. We show that this basic method can be significantly improved with temporal coherence learning, multistage function approximator with weight promotion, carousel shaping, and redundant encoding. In addition, we demonstrate how to take advantage of the characteristics of the n-tuple network, to improve the algorithmic effectiveness of the learning process by delaying the (decayed) update and applying lock-free optimistic parallelism to effortlessly make advantage of multiple CPU cores. This way, we were able to develop the best known 2048 playing program to date, which confirms the effectiveness of the introduced methods for discrete-state Markov decision problems.	https://dx.doi.org/10.1109/TCIAIG.2017.2651887	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Javadi2012	Improving student's modeling framework in a tutorial-like system based on Pursuit learning automata and reinforcement learning	Intelligent Tutorial Systems are educational software packages that occupy Artificial Intelligence (AI) techniques and methods to represent the knowledge, as well as to conduct the learning interaction. Tutorial-like systems simulates a Socratic model of learning for teaching uncertain course material by simulating the learning process for both Teacher and a School of Students. The Student is the center of attention in any Tutorial system. The proposed method in this paper improves the student's behavior model in a tutorial-Like system. In the proposed method, student model is determined by high level learning automata called Level Determinant Agent (LDA-LAQ), which attempts to characterize and improve the learning model of the students. LDA-LAQ actually use learning automata as a learning mechanism to show how the student is slow, normal or fast in the term of learning. This paper shows the new student how learning model increases speed accuracy using Pursuit learning automata and Reinforcement Learning.	https://dx.doi.org/10.1109/ICEELI.2012.6360564	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jayaweera2021	Multi-agent Learning based Anti-jamming Communications Against Cognitive Jammers	This paper proposes a spectrum-agile cognitive radio (CR) which may withstand, or outperform, a sophisticated jammer that adapts to the radio's channel selections. The spectrum co-existence of the CR and the jammer is modeled as a non-cooperative stochastic game and game-theoretic learning is allowed for both to learn effective policies in response to each other's actions. The CR is designed to use Win-or-Learn-Fast Policy Hill Climbing (WoLF-PHC) Reinforcement Learning to make better future channel selection decisions. A cognitive jammer (CJ) that time-interweaves jamming and sensing and uses No-regret Learning (NRL) to zero-in on channels containing signals is also developed. Both protocols were implemented on USRP software-defined radios. Over-the-air (OTA) tests showed that the developed CR performed 68\% better than a legacy radio against the cognitive jammer. Similarly, the CJ performed 39\% better than a non-learning jammer against the CR.	https://dx.doi.org/10.1109/WOCC53213.2021.9603201	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jeewantha2021	English Language Trainer for Non-Native Speakers using Audio Signal Processing, Reinforcement Learning, and Deep Learning	"Lack of basic proficiency and confidence in writing and speaking in English is one of the major social problems faced by most non-native English speakers. Although the general adult literacy rate in Sri Lanka is above average by world standards, the English literacy rate is just 22\% among the Sri Lankan adult population. Many individuals face setbacks in achieving their career and academic goals due to these language barriers. In a world where English has become a compulsory requirement to pursue higher education, career development, and performing day-to-day activities, ""English Buddy"" is a software solution developed to enhance the English learning experience of individuals in a more personalized and innovative way. The system provides an all-in-one solution while filling major research and market gaps in existing solutions in the e-learning domain. The system consists of a personalized learning environment, automated pronunciation error detection system, automated essay evaluation process, automated descriptive answer evaluation component based on semantic similarity, and real-time course content rating system. English Buddy is implemented using state-of-the-art technologies such as Audio Signal Processing, Reinforcement Learning, Deep Learning, and NLP. The LSTM, Sentiment Analysis, and Siamese network models have gained accuracy scores of 0.93, 0.92, and 0.81 respectively. Further, the UAT results proved that the personalized recommendations and pronunciation error detection processes are accurate and reliable. This research has been able to overcome the limitations of most existing solutions that follow traditional approaches and provide better results compared to the recent studies in the e-learning research domain."	https://dx.doi.org/10.1109/ICter53630.2021.9774785	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jehiel2001	Learning to play games in extensive form by valuation			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jeon2022	Dr.PathFinder: hybrid fuzzing with deep reinforcement concolic execution toward deeper path-first search		https://doi.org/10.1007/s00521-022-07008-8	Included	conflict_resolution		4
RL4SE	Jeon2022a	Design and Implementation of Simulation-Based Scheduling System with Reinforcement Learning for Re-Entrant Production Lines		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144845510&doi=10.3390\%2fmachines10121169&partnerID=40&md5=4032eaf9fc7a3a2be91233c57298e7ca	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jesus2019	Deep Deterministic Policy Gradient for Navigation of Mobile Robots in Simulated Environments	This paper presents a study of a deep reinforcement learning technique that uses a Deep Deterministic Policy Gradient network for application in navigation of mobile robots. In order for the robot to arrive to a target on a map, the network has 10 laser range findings, the previous linear and angular velocity, and relative position and angle of the mobile robot to the target as inputs. As outputs, the network has the linear and angular velocity. From the results analysis, it is possible to conclude that the deep reinforcement learning's algorithms, with continuous actions, are effective for decision-make of a robotic vehicle. However, it is necessary to create a good reward system for the intelligent agent to accomplish your objectives. This research uses different virtual simulation environments provided by ROBOTIS in the robot simulation software Gazebo in order to test the performance of the algorithm. A supplementary video can be accessed at the following link: https://youtu.be/NhGxEC3g7sU. That shows the performance of the proposed system.	https://dx.doi.org/10.1109/ICAR46387.2019.8981638	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jha2021	Reinforcement learning based weighted multipath routing for datacenter networks		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111924005&doi=10.1016\%2fj.matpr.2021.01.252&partnerID=40&md5=1098d4b8b3167c1e1da96b3245c861a7	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jia2022	Optimal Incentive Strategy in Cloud-Edge Integrated Demand Response Framework for Residential Air Conditioning Loads	In the residential demand response area, currently the incentive-based method (e.g., direct load control, DLC) may impair users' comfort and autonomy, while the price-based method can hardly guarantee users' engagements. This paper proposes an edge-cloud integrated demand response framework to achieve an effect-predictable residential demand response without harming users' benefits. First, we combine the cloud-computing resource (cloud) and the home-installed smart thermostats (edges) to formulate an efficient, cost-effective, and data-secured infrastructure to implement the demand response program. Then, we model the demand response problem between the load aggregator and its served residential users as a bi-level optimization problem, and the key is for the load aggregator to find the optimal incentive strategy. To solve this problem, we introduce an RL algorithm, i.e., Continuous Action Reinforcement Learning Automata, to quickly obtain the optimal incentive strategy under an incomplete information scenario. Simulation results based on 136 real-world residential users in Austin area demonstrate that the proposed CEI-DR framework can increase the social welfare by about $8.6/h compared to the traditional DLC method during a normal DR event.	https://dx.doi.org/10.1109/TCC.2021.3118597	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jiang2021	Exploring Dynamic Selection of Branch Expansion Orders for Code Generation	Due to the great potential in facilitating software development, code generation has attracted increasing attention recently. Generally, dominant models are Seq2Tree models, which convert the input natural language description into a sequence of tree-construction actions corresponding to the pre-order traversal of an Abstract Syntax Tree (AST). However, such a traversal order may not be suitable for handling all multi-branch nodes. In this paper, we propose to equip the Seq2Tree model with a context-based Branch Selector, which is able to dynamically determine optimal expansion orders of branches for multibranch nodes. Particularly, since the selection of expansion orders is a non-differentiable multi-step operation, we optimize the selector through reinforcement learning, and formulate the reward function as the difference of model losses obtained through different expansion orders. Experimental results and in-depth analysis on several commonly-used datasets demonstrate the effectiveness and generality of our approach.		Included	new_screen		4
RL4SE	Jiang2021a	Modeling the Self-navigation Behavior of Patients with Alzheimer's Disease in Virtual Reality		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102649405&doi=10.1007\%2f978-981-33-6549-0_11&partnerID=40&md5=c87454d49a6602758b12c6ede055d326	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jiang2006	A survey of multi-agent coordination			Excluded	new_screen	E5: Other not a paper,E5: Other not a paper	4
RL4SE	Jiang2020	Hardware/Software Co-Exploration of Neural Architectures	We propose a novel hardware and software co-exploration framework for efficient neural architecture search (NAS). Different from existing hardware-aware NAS which assumes a fixed hardware design and explores the NAS space only, our framework simultaneously explores both the architecture search space and the hardware design space to identify the best neural architecture and hardware pairs that maximize both test accuracy and hardware efficiency. Such a practice greatly opens up the design freedom and pushes forward the Pareto frontier between hardware efficiency and test accuracy for better design tradeoffs. The framework iteratively performs a two-level (fast and slow) exploration. Without lengthy training, the fast exploration can effectively fine-tune hyperparameters and prune inferior architectures in terms of hardware specifications, which significantly accelerates the NAS process. Then, the slow exploration trains candidates on a validation set and updates a controller using the reinforcement learning to maximize the expected accuracy together with the hardware efficiency. In this article, we demonstrate that the co-exploration framework can effectively expand the search space to incorporate models with high accuracy, and we theoretically show that the proposed two-level optimization can efficiently prune inferior solutions to better explore the search space. The experimental results on ImageNet show that the co-exploration NAS can find solutions with the same accuracy, 35.24\% higher throughput, 54.05\% higher energy efficiency, compared with the hardware-aware NAS.	https://dx.doi.org/10.1109/TCAD.2020.2986127	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jiang2022	Energy-Efficient Driving for Adaptive Traffic Signal Control Environment via Explainable Reinforcement Learning	Energy-efficient driving systems can effectively reduce energy consumption during vehicle operation. Most of the existing studies focus on the driving strategies in a fixed signal timing environment, whereas the standardized Signal Phase and Timing (SPaT) data can help the vehicle make the optimal decisions. However, with the development of artificial intelligence and communication techniques, the conventional fixed timing methods are gradually replaced by adaptive traffic signal control (ATSC) approaches. The previous studies utilized SPaT information that cannot be applied directly in the environment with ATSC. Thus, a framework is proposed to implement energy-efficient driving in the ATSC environment, while the ATSC is realized by the value-based reinforcement learning algorithm. After giving the optimal control model, the framework draws upon the Markov Decision Process (MDP) to make an approximation to the optimal control problem. The state sharing mechanism allows the vehicle to obtain the state information of the traffic signal agents. The reward function in MDP considers energy consumption, traffic mobility, and driving comfort. With the support of traffic simulation software SUMO, the vehicle agent is trained by Proximal Policy Optimization (PPO) algorithm, which enables the vehicle to select actions from continuous action space. The simulation results show that the energy consumption of the controlled vehicle can be reduced by 31.73\%similar to 45.90\% with a different extent of mobility sacrifice compared with the manual driving model. Besides, we developed a module based on SHapley Additive exPlanations (SHAP) to explain the decision process in each timestep of the vehicle. That can make the strategy more reliable and credible.	https://dx.doi.org/10.3390/app12115380	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jiang2020a	Revealing Much While Saying Less: Predictive Wireless for Status Update	Wireless communications for status update are becoming increasingly important, especially for machine-type control applications. Existing work has been mainly focused on Age of Information (AoI) optimizations. In this paper, a status-aware predictive wireless interface design, networking and implementation are presented which aim to minimize the status recovery error of a wireless networked system by leveraging online status model predictions. Two critical issues of predictive status update are addressed: practicality and usefulness. Link-level experiments on a Software-Defined-Radio (SDR) testbed are conducted and test results show that the proposed design can significantly reduce the number of wireless transmissions while maintaining a low status recovery error. A Status-aware Multi-Agent Reinforcement learning neTworking solution (SMART) is proposed to dynamically and autonomously control the transmit decisions of devices in an ad hoc network based on their individual statuses. System-level simulations of a multi dense platooning scenario are carried out on a road traffic simulator. Results show that the proposed schemes can greatly improve the platooning control performance in terms of the minimum safe distance between successive vehicles, in comparison with the AoI-optimized status-unaware and communication latency-optimized schemes-this demonstrates the usefulness of our proposed status update schemes in a real-world application.	https://dx.doi.org/10.1109/INFOCOM41043.2020.9155225	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jiao2021	Real-world ride-hailing vehicle repositioning using deep reinforcement learning	We present a new practical framework based on deep reinforcement learning and decision-time planning for real-world vehicle repositioning on ride-hailing (a type of mobility-on-demand, MoD) platforms. Our approach learns the spatiotemporal state-value function using a batch training algorithm with deep value networks. The optimal repositioning action is generated ondemand through value-based policy search, which combines planning and bootstrapping with the value networks. For the large-fleet problems, we develop several algorithmic features that we incorporate into our framework and that we demonstrate to induce coordination among the algorithmically-guided vehicles. We benchmark our algorithm with baselines in a ride-hailing simulation environment to demonstrate its superiority in improving income efficiency measured by income-per-hour. We have also designed and run a real-world experiment program with regular drivers on a major ride-hailing platform. We have observed significantly positive results on key metrics comparing our method with experienced drivers who performed idle-time repositioning based on their own expertise.	https://dx.doi.org/10.1016/j.trc.2021.103289	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jin2022	Secure H $infty$ control against time-delay attacks in cyber-physical systems		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121397624&doi=10.1080\%2f23307706.2021.2011792&partnerID=40&md5=d5421306d4e504449e2bf34c5743fc8e	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jin2022a	Secure H-infinity control against time-delay attacks in cyber-physical systems	This paper is concerned with the secure H-infinity control problem for cyber-physical systems (CPSs) under time-delay attacks, which cause the systems' information to be received with delays. Through transforming the H-infinity control problem to a two-player zero-sum game problem, the H-infinity controller can be obtained by solving the saddle point for the game. Due to the unknown system parameters, a model-free reinforcement learning (RL) method is applied, and the measurement feedback controller is designed based on the historical inputs and the tracking errors. To obviate the requirement of the initial admissible policy, the value iteration (VI) of the adaptive dynamic program (ADP) algorithm is proposed to solve the approximate saddle point. Moreover, the convergence of the proposed algorithm is proved. Finally, a networked circuit system is employed to show the effectiveness of the proposed methods.	https://dx.doi.org/10.1080/23307706.2021.2011792	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jin2015	Adaptive Group-Based Signal Control Using Reinforcement Learning with Eligibility Traces	Group-based signal controllers are widely deployed on urban networks in the Nordic countries. However, group-based signal controls are usually implemented with rather simple timing logics, e.g. vehicle actuated timing. In addition, group-based signal control systems with pre-defined signal parameter settings show relatively poor performances in a dynamically changed traffic environment. This study, therefore, presents an adaptive group-based signal control system capable of changing control strategies with respect to non-stationary traffic demands. In this study, signal groups are formulated as individual agents. The signal group agent learns from traffic environments and makes intelligent timing decisions according to the perceived system states. Reinforcement learning with multiple-step backups is applied as the learning algorithm. Agents on-line update their knowledge based on a sequence of states during the learning process rather than purely on the basis of single previous state. The proposed signal control system is integrated into a software-in-the-loop simulation (SILS) framework for evaluation purpose. In the testbed experiments, the proposed adaptive group-based control system is compared to a benchmark signal control system, the well-established group-based fixed-time control system. The simulation results demonstrate that learning-based and adaptive group-based signal control system owns its advantage in dealing with dynamic traffic environments in terms of improving traffic mobility efficiency.	https://dx.doi.org/10.1109/ITSC.2015.389	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jin2021	Optimal Policy Characterization Enhanced Actor-Critic Approach for Electric Vehicle Charging Scheduling in a Power Distribution Network	We study the scheduling of large-scale electric vehicle (EV) charging in a power distribution network under random renewable generation and electricity prices. The problem is formulated as a stochastic dynamic program with unknown state transition probability. To mitigate the curse of dimensionality, we establish the nodal multi-target (NMT) characterization of the optimal scheduling policy: all EVs with the same deadline at the same bus should be charged to approach a single target of remaining energy demand. We prove that the NMT characterization is optimal under arbitrarily random system dynamics. To adaptively learn the dynamics of system uncertainty, we propose a model-free soft-actor-critic (SAC) based method to determine the target levels for the characterized NMT policy. The proposed SAC + NMT approach significantly outperforms existing deep reinforcement learning methods (in our numerical experiments on the IEEE 37-node test feeder), as the established NMT characterization sharply reduces the dimensionality of neural network outputs without loss of optimality.	https://dx.doi.org/10.1109/TSG.2020.3028470	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jin2016	A Learning-based Adaptive Signal Control System with Function Approximation	Traffic signal control plays a crucial role in traffic management and operation practice. In the past decade, adaptive signal control systems have shown the abilities to improve the effectiveness of the transportation system in many aspects. This paper proposes an adaptive signal control system in the context of group-based phasing techniques. The adaptive signal control system is modeled as a multi agent System capable of acquiring knowledge on-line based on the perceived traffic states and the feedback from the external environment,. Reinforcement learning is applied as the learning algorithm resulting in intelligent timing decisions. Feature based function approximation method is incorporated into the reinforcement learning framework for the purpose of improving learning efficiency as well as the quality of signal timing decisions. The assessment of such a learning-based signal control system is carried out by using an opensource microscopic traffic simulation software, SUMO. A benchmarking system, the optimized group-based vehicle actuated signal control system, compared with the learning-based signal control systems regarding mobility efficiency. The simulation results show that the proposed adaptive group based signal control system has the potential to improve the mobility efficiency regardless of the settings of traffic demands. (C) 2016, IFAC (International Federation of Automatic Control) Hosting by Elsevier Ltd. All rights reserved.	https://dx.doi.org/10.1016/j.ifacol.2016.07.002	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jin2019	ScaRL: Service Function Chain Allocation Based on Reinforcement Learning in Mobile Edge Computing	Network Function Virtualization (NFV) and Mobile Edge Computing (MEC) are the two core technologies in 5G, therefore the service function chain allocation (SFC-A) in MEC plays an important role in enhancing the quality of experience (QoE) of the end users. Different from the SFC-A in cloud computing, SFC-A in MEC is more challenging due to the limited computing resources at edge servers. In this paper, we investigated the SFC-A problem in MEC with limited resources, and formulate the problem as an integer linear program problem with the objective of minimizing both the transmission latency and processing latency. Furthermore, to overcome the inherent shortcomings of heuristic solutions, we propose an algorithm, ScaRL, which leverages reinforcement learning to make the optimal decision by taking advantage of its trial-and-error mechanism, reward mechanism and exploration-exploitation ability. A series of simulations verified that ScaRL is effective in reducing the average latency of SFC requests, as well as reducing the blocking rate to provide QoE guarantee.	https://dx.doi.org/10.1109/CBD.2019.00065	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jin2022b	Research on Hybrid Intelligence Wargame Method	Wargame, as a tool to generate sample data for analysis and model training, has vast application in fields of training, command & control, and tactical research. Traditional wargame technologies greatly rely on human wisdom in the loop, impossible to generate large scale sample data. Reinforcement learning technology can generate large scale sample data, but it is not competent for the decision complexity above campaign level. This paper proposes a hybrid intelligence wargame method, which can generate large scale sample data using AI algorithms under the guidance of human wisdom. It has wide applications, which provides data analysis functions that existing wargame methods cannot provide. Prototype software has been developed based on the method, with feasibility and effectiveness verified through experiments, which has certain reference value.	https://dx.doi.org/10.1109/CCET55412.2022.9906386	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jingang2011	Using reinforcement learning for agent-based network fault diagnosis system	In the network, it is important that faults can be diagnosed at early stage before they result in serious fault. However, the situation is not optimistic, which depends on what network management software is used. Aiming to this problem, a mobile agent-based network fault diagnosis model is proposed. In the model, agent can learn by reinforcement learning (RL), which can improve fault diagnosis performance. The structure and function of model, especially the architecture and learning algorithm of diagnostic agent, is depicted. At last, compared the system performance through simulation and experiment, and results show that the model has greater advantage.	https://dx.doi.org/10.1109/ICINFA.2011.5949093	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jingjian2020	Deep reinforcement learning algorithm of voltage regulation in distribution network with energy storage system		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091843628&doi=10.3969\%2fj.issn.1000-7229.2020.03.009&partnerID=40&md5=6af7c4ee3f03b429fc4bbbeb2a6bae4e	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jokinen2022	Cognitive Modelling: From GOMS to Deep Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129773995&doi=10.1145\%2f3491101.3503771&partnerID=40&md5=2c1752fb6fc4999138e679876c057074	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jones2014	Commentary on Mactier et al. (2014): Methadone-assisted treatment and the complexity of influences on fetal development	Background and aims Individuals with methamphetamine dependence (MD) exhibit dysfunction in brain regions involved in goal maintenance and reward processing when compared with healthy individuals. We examined whether these characteristics also reflect relapse vulnerability within a sample of MD patients. Design Longitudinal, with functional magnetic resonance imaging (fMRI) and clinical interview data collected at baseline and relapse status collected at 1-year follow-up interview. Setting Keck Imaging Center, University of California San Diego, USA. Participants MD patients (n = 60) enrolled into an in-patient drug treatment program at baseline. MD participants remaining abstinent at 1-year follow-up (abstinent MD group; n = 42) were compared with MD participants who relapsed within this period (relapsed MD group; n = 18). Measurements Behavioral and neural responses to a reinforcement learning (paper-scissors-rock) paradigm recorded during an fMRI session at time of treatment. Findings The relapsed MD group exhibited greater bilateral inferior frontal gyrus (IFG) and right striatal activation than the abstinent MD group during the learning of reward contingencies (Cohen's d range: 0.60-0.83). In contrast, the relapsed MD group displayed lower bilateral striatum, bilateral insula, left IFG and left anterior cingulate activation than the abstinent MD group (Cohen's d range: 0.90-1.23) in response to winning, tying and losing feedback. Conclusions Methamphetamine-dependent individuals who achieve abstinence and then relapse show greater inferior frontal gyrus activation during learning, and relatively attenuated striatal, insular and frontal activation in response to feedback, compared with methamphetamine-dependent people who remain abstinent.	https://www.ncbi.nlm.nih.gov/pubmed/24524321	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jones2022	PLAD: Learning to Infer Shape Programs with Pseudo-Labels and Approximate Distributions	Inferring programs which generate 2D and 3D shapes is important for reverse engineering, editing, and more. Training models to perform this task is complicated because paired (shape, program) data is not readily available for many domains, making exact supervised learning infeasible. However, it is possible to get paired data by compromising the accuracy of either the assigned program labels or the shape distribution. Wake-sleep methods use samples from a generative model of shape programs to approximate the distribution of real shapes. In self-training, shapes are passed through a recognition model, which predicts programs that are treated as `pseudo-labels' for those shapes. Related to these approaches, we introduce a novel self-training variant unique to program inference, where program pseudo-labels are paired with their executed output shapes, avoiding label mismatch at the cost of an approximate shape distribution. We propose to group these regimes under a single conceptual framework, where training is performed with maximum likelihood updates sourced from either Pseudo-Labels or an Approximate Distribution (PLAD). We evaluate these techniques on multiple 2D and 3D shape program inference domains. Compared with policy gradient reinforcement learning, we show that PLAD techniques infer more accurate shape programs and converge significantly faster. Finally, we propose to combine updates from different PLAD methods within the training of a single model, and find that this approach outperforms any individual technique.	https://dx.doi.org/10.1109/CVPR52688.2022.00964	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Jordan2019	A Closed-Loop Toolchain for Neural Network Simulations of Learning Autonomous Agents	Neural network simulation is an important tool for generating and evaluating hypotheses on the structure, dynamics, and function of neural circuits. For scientific questions addressing organisms operating autonomously in their environments, in particular where learning is involved, it is crucial to be able to operate such simulations in a closed-loop fashion. In such a set-up, the neural agent continuously receives sensory stimuli from the environment and provides motor signals that manipulate the environment or move the agent within it. So far, most studies requiring such functionality have been conducted with custom simulation scripts and manually implemented tasks. This makes it difficult for other researchers to reproduce and build upon previous work and nearly impossible to compare the performance of different learning architectures. In this work, we present a novel approach to solve this problem, connecting benchmark tools from the field of machine learning and state-of-the-art neural network simulators from computational neuroscience. The resulting toolchain enables researchers in both fields to make use of well-tested high-performance simulation software supporting biologically plausible neuron, synapse and network models and allows them to evaluate and compare their approach on the basis of standardized environments with various levels of complexity. We demonstrate the functionality of the toolchain by implementing a neuronal actor-critic architecture for reinforcement learning in the NEST simulator and successfully training it on two different environments from the OpenAI Gym. We compare its performance to a previously suggested neural network model of reinforcement learning in the basal ganglia and a generic Q-learning algorithm.	https://www.ncbi.nlm.nih.gov/pubmed/31427939	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jorge2018	Reinforcement learning in real-time geometry assurance	To improve the assembly quality during production, expert systems are often used. These experts typically use a system model as a basis for identifying improvements. However, since a model uses approximate dynamics or imperfect parameters, the expert advice is bound to be biased. This paper presents a reinforcement learning agent that can identify and limit systematic errors of an expert systems used for geometry assurance. By observing the resulting assembly quality over time, and understanding how different decisions affect the quality, the agent learns when and how to override the biased advice from the expert software. (C) 2018 The Authors. Published by Elsevier B.V. Peer-review under responsibility of the scientific committee of the 51st CIRP Conference on Manufacturing Systems.	https://dx.doi.org/10.1016/j.procir.2018.03.168	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jorgensen2019	Atomistic structure learning	One endeavor of modern physical chemistry is to use bottom-up approaches to design materials and drugs with desired properties. Here, we introduce an atomistic structure learning algorithm (ASLA) that utilizes a convolutional neural network to build 2D structures and planar compounds atom by atom. The algorithm takes no prior data or knowledge on atomic interactions but inquires a first-principles quantum mechanical program for thermodynamical stability. Using reinforcement learning, the algorithm accumulates knowledge of chemical compound space for a given number and type of atoms and stores this in the neural network, ultimately learning the blueprint for the optimal structural arrangement of the atoms. ASLA is demonstrated to work on diverse problems, including grain boundaries in graphene sheets, organic compound formation, and a surface oxide structure. (C) 2019 Author(s).	https://dx.doi.org/10.1063/1.5108871	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Joshi2021	An actor-critic approach for control of residential photovoltaic-battery systems		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118136230&doi=10.1016\%2fj.ifacol.2021.08.362&partnerID=40&md5=b685cc496379909d72ae949385fc9628	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jovel2021	An Introduction to Machine Learning Approaches for Biomedical Research	Machine learning (ML) approaches are a collection of algorithms that attempt to extract patterns from data and to associate such patterns with discrete classes of samples in the data-e.g., given a series of features describing persons, a ML model predicts whether a person is diseased or healthy, or given features of animals, it predicts weather an animal is treated or control, or whether molecules have the potential to interact or not, etc. ML approaches can also find such patterns in an agnostic manner, i.e., without having information about the classes. Respectively, those methods are referred to as supervised and unsupervised ML. A third type of ML is reinforcement learning, which attempts to find a sequence of actions that contribute to achieving a specific goal. All of these methods are becoming increasingly popular in biomedical research in quite diverse areas including drug design, stratification of patients, medical images analysis, molecular interactions, prediction of therapy outcomes and many more. We describe several supervised and unsupervised ML techniques, and illustrate a series of prototypical examples using state-of-the-art computational approaches. Given the complexity of reinforcement learning, it is not discussed in detail here, instead, interested readers are referred to excellent reviews on that topic. We focus on concepts rather than procedures, as our goal is to attract the attention of researchers in biomedicine toward the plethora of powerful ML methods and their potential to leverage basic and applied research programs.	https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122020332&doi=10.3389\%2ffmed.2021.771607&partnerID=40&md5=611db75060d4185531bc03f225f68ba8	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Junges2012	Generating inspiration for agent design by reinforcement learning		https://doi.org/10.1016/j.infsof.2011.12.002	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Jungmann2014	Towards Context-Sensitive Service Composition for Service-Oriented Image Processing	Automatically composing service-based software solutions is a challenging task. Considering context information during this service composition process is even more challenging. In domains such as image processing, however, context-sensitivity is inherent and cannot be ignored when developing techniques for automatic service composition. Formal approaches tend to create ambiguous solutions, whenever the expressive power of the applied formalism is limited. For example, services may have the same formal specification, although their actual functionality depends on the concrete context. In order to satisfy individual user requests while providing data-dependent functionality, formal approaches have to be extended. We propose to incorporate Reinforcement Learning techniques and combine them with planning based composition approaches. While planning ensures formally correct solutions, learning enables the composition process to resolve ambiguity by implicitly considering context information. Preliminary results show that our combined approach adapts to a static context while still satisfying formally specified requirements.	https://dx.doi.org/10.1109/CloudCom.2014.154	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E5: Other not a paper	4
RL4SE	Jungmann2015	An approach towards adaptive service composition in markets of composed services		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938836676&doi=10.1186\%2fs13174-015-0022-8&partnerID=40&md5=0b348b595890f8caee8033aa08b1524a	Included	new_screen		4
RL4SE	Jungmann2014a	Applying Reinforcement Learning for Resolving Ambiguity in Service Composition	Automatically composing service-based software solutions is still a challenging task. Functional as well as non-functional properties have to be considered in order to satisfy individual user requests. Regarding non-functional properties, the composition process can be modeled as optimization problem and solved accordingly. Functional properties, in turn, can be described by means of a formal specification language. State-space based planning approaches can then be applied to solve the underlying composition problem. However, depending on the expressiveness of the applied formalism and the completeness of the functional descriptions, formally equivalent services may still differ with respect to their implemented functionality. As a consequence, the most appropriate solution for a desired functionality can hardly be determined without considering additional information. In this paper, we demonstrate how to overcome this lack of information by means of Reinforcement Learning. In order to resolve ambiguity, we expand state-space based service composition by a recommendation mechanism that supports decision-making beyond formal specifications. The recommendation mechanism adjusts its recommendation strategy based on feedback from previous composition runs. Image processing serves as case study. Experimental results show the benefit of our proposed solution.	https://dx.doi.org/10.1109/SOCA.2014.48	Included	new_screen		4
RL4SE	Kaiser2019	Embodied Synaptic Plasticity With Online Reinforcement Learning	The endeavor to understand the brain involves multiple collaborating research fields. Classically, synaptic plasticity rules derived by theoretical neuroscientists are evaluated in isolation on pattern classification tasks. This contrasts with the biological brain which purpose is to control a body in closed-loop. This paper contributes to bringing the fields of computational neuroscience and robotics closer together by integrating open-source software components from these two fields. The resulting framework allows to evaluate the validity of biologically-plausibe plasticity models in closed-loop robotics environments. We demonstrate this framework to evaluate Synaptic Plasticity with Online REinforcement learning (SPORE), a reward-learning rule based on synaptic sampling, on two visuomotor tasks: reaching and lane following. We show that SPORE is capable of learning to perform policies within the course of simulated hours for both tasks. Provisional parameter explorations indicate that the learning rate and the temperature driving the stochastic processes that govern synaptic learning dynamics need to be regulated for performance improvements to be retained. We conclude by discussing the recent deep reinforcement learning techniques which would be beneficial to increase the functionality of SPORE on visuomotor tasks.	https://www.ncbi.nlm.nih.gov/pubmed/31632262	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kakalou2019	Sustainable and Efficient Data Collection in Cognitive Radio Sensor Networks	Cognitive Radio is a promising technology that maximize spectrum efficiency and can apply to Wireless Sensor Networks. This paper proposes a system architecture which introduces enhancements at lower layers of a Software Defined Network of Wireless Sensors Network with Cognitive Radio capabilities for efficient sensors' power management, energy consuming channel handoffs elimination, efficient spectrum brokerage, and QoS provision in terms of data rate to the sensors' applications via the SDN flows. The large Wireless Sensors Network is divided into clusters for power efficiency - as sensor operate in lower power - which connect to a cloud-assisted Central Controller. The protocol encompasses an optimal reinforcement learning scheme for efficient spectrum utilization that enables efficient sensors' data collection, while sustainability issues are satisfied. Software Defined Wireless Sensor Network dynamically adapts to the spectrum and interference conditions on per flow basis and predicts Primary Users' traffic to totally avoid collision with the licensed users. The paper is concentrated on sustainable solutions for sensors' data collection by the cluster heads leveraging the Cognitive Radio Network facilities and taking into account the demands of the applications running on the sensors. The Cognitive Radio Sensor Network is considered as large organized on a local basis to extend networks lifetime and allow resource reuse.	https://dx.doi.org/10.1109/TSUSC.2018.2830704	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kallus2020	Confounding-robust policy evaluation in infinite-horizon reinforcement learning			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kamei2016	An Approach to the Development of a Game Agent Based on SOM and Reinforcement Learning	Recently, the researches that create agents which play board games have been studied actively. According to those studies, those agents have abilities that are comparable to the strongest experts. However, it can be said that those agents depend on the computational capability because that abilities of those agents are realized by thousands of lookahead search. On theotherhand, humanbeingshavenoadvantagescomparedwith numerical capability of computers, however, experts sometimes defeat those agents. In contrast to other approaches, our purpose is to create the agent which requires only low computational capability but is strong, like human beings. To realize our aim, we have proposed to develop the agent based on Self-Organizing Maps and reinforcement learning. From the experimental results, the agent learned by MC-learning achieved a 58\% winning rate against the adversary program, so that we have succeeded in improving the winning rate over 10\%.	https://dx.doi.org/10.1109/IIAI-AAI.2016.115	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kampert2021	Mimicking the Human Approach in the Game of Hive	While Deep Blue and AlphaGo make it seem like board games have been solved, there are still plenty of games for which no good game playing program exists. Hive is such a game. It is, combinatorically, of similar complexity as chess or Go, yet the rules of the game are such that current methods can barely beat a randomly playing agent. A major bottleneck for progress is the high branching factor of the game. We apply state of the art methods for which we develop new heuristics that are based on human domain-knowledge, attempting to improve upon the current dire state of Hive agents. Our methods have improved playing strength compared to the state of the art, although our AI still fails against actual Humans. We also find that, while in most board games, brute force or deep learning approaches work best, in Hive, an approach based on mimicking human knowledge outperforms these other approaches, including Monte Carlo Tree Search or Deep Reinforcement Learning. Future work will show if this anomalous situation is inherent to the game.	https://dx.doi.org/10.1109/SSCI50451.2021.9659999	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kamri2021	Constrained Policy Optimization for Load Balancing	To improve the bandwidth utilization in IP networks, a centralized controller splits flow aggregates over multiple paths and decides load balancing weights. Ideally, load balancing policies should anticipate the impact of their decisions on the Quality of Service (QoS). However, the embedding of accurate performance models into load balancing optimization algorithms is a challenge. In this context, we propose a Deep Reinforcement Learning (DRL) based solution that is able to learn the relationship between traffic and QoS, while providing safety to maximize throughput and avoid violating link capacity constraints. Our safe solution for QoS-aware load balancing integrates DRL algorithms with the Reward Constrained Policy Optimization algorithm. In a scenario where link delays follow the M/M/1 queuing model, we demonstrate, using a non-linear integer program, that our solution can reach a close to optimal end-to-end delay. We also show that our solution automatically learns reward parameters to meet capacity constraints.	https://dx.doi.org/10.1109/DRCN51631.2021.9477375	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kanervisto2021	Distilling Reinforcement Learning Tricks for Video Games	Reinforcement learning (RL) research focuses on general solutions that can be applied across different domains. This results in methods that RL practitioners can use in almost any domain. However, recent studies often lack the engineering steps (``tricks'') which may be needed to effectively use RL, such as reward shaping, curriculum learning, and splitting a large task into smaller chunks. Such tricks are common, if not necessary, to achieve state-of-the-art results and win RL competitions. To ease the engineering efforts, we distill descriptions of tricks from state-of-the-art results and study how well these tricks can improve a standard deep Q-learning agent. The long-term goal of this work is to enable combining proven RL methods with domain-specific tricks by providing a unified software framework and accompanying insights in multiple domains.	https://dx.doi.org/10.1109/CoG52621.2021.9618997	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kang2022	A Novel Deadline-/Interference-Aware Cooperative Data Transmission Scheduling Scheme for Optimizing AoI in Wireless Networks		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146255117&doi=10.1109\%2fTVT.2022.3231538&partnerID=40&md5=7331976ad948858e70100f91f6cf0924	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kanso2022	Engineering a Platform for Reinforcement Learning Workloads	Reinforcement Learning (RL) is an area of machine learning concerned with teaching intelligent agents to take desired actions in a specific environment. The teaching part can be performed in a simulated environment where the agent can learn how to react to the (simulated) current state in order to reach a desired state. Offering Reinforcement Learning as a service with stringent reliability and scalability requirements, entails a set of challenges at both the architectural and implementation level. In this paper we present the Bonsai platform for RL workloads. We discuss the requirements, design and implementation of the Bonsai platform. CCS CONCEPTS  Software and its engineering ? organization and properties ? Software system structures	https://dx.doi.org/10.1145/3522664.3528609	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kantor2022	Conformal bootstrap with reinforcement learning	We introduce the use of reinforcement-learning (RL) techniques to the conformal-bootstrap program. We demonstrate that suitable soft Actor-Critic RL algorithms can perform efficient, relatively cheap high-dimensional searches in the space of scaling dimensions and OPE-squared coefficients that produce sensible results for tens of CFT data from a single crossing equation. In this paper we test this approach in well-known 2D CFTs, with particular focus on the Ising and tricritical Ising models and the free compactified boson CFI. We present results of as high as 36-dimensional searches, whose sole input is the expected number of operators per spin in a truncation of the conformal-block decomposition of the crossing equations. Our study of 2D CFTs uses only the global so(2, 2) part of the conformal algebra, and our methods are equally applicable to higher-dimensional CFTs. When combined with other, already available, numerical and analytical methods, we expect our approach to yield an exciting new window into the nonperturbative structure of arbitrary (unitary or nonunitary) CFTs.	https://dx.doi.org/10.1103/PhysRevD.105.025018	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kantor2022a	Solving Conformal Field Theories with Artificial Intelligence	In this Letter, we deploy for the first time reinforcement-learning algorithms in the context of the conformal-bootstrap program to obtain numerical solutions of conformal field theories (CFI's). As an illustration, we use a soft actor-critic algorithm and find approximate solutions to the truncated crossing equations of two-dimensional CFTs, successfully identifying well-known theories like the 2D Ising model and the 2D CFT of a compactified scalar. Our methods can perform efficient high-dimensional searches that can be used to study arbitrary (unitary or nonunitary) CFTs in any spacetime dimension.	https://www.ncbi.nlm.nih.gov/pubmed/35148144	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Karam2019	Applying Convolutional Neural Networks For Image Detection	Machine Learning ``Data Science''. Its goal is to program a computer in order to use a provided data for learning from its characteristics and to create a model which used later to predict a new unseen data. Many Successful applications were developed by using Machine learning and it really acts in our life daily activities, like recommended systems, disease detection, sales predictions, robot's development, and many others that affects in our life. Machine learning uses many approaches like supervised learning, unsupervised learning and reinforcement learning, also every approach use a state of art algorithm. One of the wildly powerful approaches is the Artificial Neural Networks which use its own algorithm. During this research we will cover a part of the supervised learning which is ``CNN'' Convolutional Neural Networks, which is a subsidiary of Artificial neural networks. Convolutional neural networks are wildly used to analyze visual images. During this research we will introduce CNN architect, starting from the model construction techniques that targets best fit with the best selection of the hyper parameter for the model with optimal accuracy, targeting the best images classifications predictions.	https://dx.doi.org/10.1109/SmartNets48225.2019.9069750	Excluded	new_screen	E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for	4
RL4SE	Karangelos2012	Integrating Demand Response into agent-based models of electricity markets	This paper introduces a model for the decision-making of a Demand Response (DR) program operator as an adaptive agent participating in a competitive electricity market. The model focuses on an electricity retailer stimulating DR as a means of avoiding high balancing market prices. Nevertheless, we demonstrate the ability to extend the model to other actors who could capitalize on the value of demand flexibility - e.g. wind power producers looking to offset output variability. The model considers voluntary demand modifications whose materialization, subject to the uncertainty of consumer behavior, results in redistribution of consumption over a short time frame. As the retailer is modeled via an adaptive agent, it has the potential to learn from both consumer behavior and market outcomes. Here, we implement a reinforcement learning approach with the objective of allowing the agent to increase its profit by identifying the conditions under which DR should be stimulated. We validate the proposed agent-based model as a tool to quantify DR potential in a market setting.	https://dx.doi.org/10.1109/Allerton.2012.6483369	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Karmakar2022	Reliable Backhauling in Aerial Communication Networks Against UAV Failures: A Deep Reinforcement Learning Approach	Unmanned Aerial Vehicles (UAVs) can be utilized as aerial base stations to establish wireless communication networks in various challenging scenarios, such as emergency disaster areas and rural areas. Under large regions, the aerial communication networks would require UAVs to form wireless (backhaul) links among each other to provide end-to-end wireless services between two or more ground users (via one or more UAVs). Such UAV backhauling in aerial communication networks may be severely compromised if one or more UAVs are knocked off during the time of operation \endash it may be due to UAV hardware/software faults, limited battery, malicious attacks, etc. Deep reinforcement learning (DRL) has emerged as a powerful tool for learning tasks with large state and continuous action spaces. In this paper, we leverage emerging DRL to achieve reliable backhauling in an aerial communication network that remains functional and supports end-to-end wireless services even under various random and/or targeted UAV node failures. The proposed method (i) maximizes the reliability of UAV backhauling with joint consideration for communication coverage, (ii) learns the complex environment and its dynamics, and (iii) makes 3D positioning decisions for each UAV under the guidance of two deep neural networks. Our performance evaluation reveals that the proposed DRL approach outperforms the baseline method in terms of wireless coverage and network reliability against UAV failures.	https://dx.doi.org/10.1109/TNSM.2022.3196852	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Karpov2006	Integration and Evaluation of Exploration-Based Learning in Games	Video and computer games provide a rich platform for testing adaptive decision systems such as value-based reinforcement learning and neuroevolution. However, integrating such systems into the game environment and evaluating their performance in it is time and labor intensive. In this paper, an approach is developed for using general integration and evaluation software to alleviate these problems. In particular, the testbed for integrating and evaluating learning techniques (TIELT) is used to integrate a neuroevolution learner with an off-the-shelf computer game Unreal TournamentTM5 (Aha and Molineaux, 2004). The resulting system is successfully used to evolve artificial neural network controllers with basic navigation behavior. Our work leads to formulating a set of requirements that make a general integration and evaluation system such as TIELT a useful tool for benchmarking adaptive decision systems	https://dx.doi.org/10.1109/CIG.2006.311679	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Karthikeyan2014	An instant path planning algorithm for indoor mobile robots using adaptive dynamic programming and reinforcement learning			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Karunaratne2021	Penetrating RF Fingerprinting-based Authentication with a Generative Adversarial Attack	Physical layer authentication relies on detecting unique imperfections in signals transmitted by radio devices to isolate their fingerprint. Recently, deep learning-based authenticators have increasingly been proposed to classify devices using these fingerprints, as they achieve higher accuracies compared to traditional approaches. However, it has been shown in domains such as computer vision that adding carefully crafted perturbations to legitimate inputs can fool such classifiers. This can undermine the security provided by the authenticator. Unlike adversarial attacks applied in other domains, an adversary has no control over the propagation environment. Therefore, to investigate the severity of this type of attack in wireless communications, we consider an unauthorized transmitter attempting to have its signals classified as authorized by a deep learning-based authenticator. We demonstrate a reinforcement learning-based attack where the impersonator\emdashusing only the authenticator's binary authentication decision\emdashdistorts its signals in order to penetrate the system. Extensive simulations and experiments on a software-defined radio testbed indicate that at appropriate channel conditions and bounded by a maximum distortion level, it is possible to fool the authenticator reliably at a success rate of more than 90\%.	https://dx.doi.org/10.1109/ICC42927.2021.9500893	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Katayama2016	Ideas for a Reinforcement Learning Algorithm that Learns Programs	Conventional reinforcement learning algorithms such as Q-learning are not good at learning complicated procedures or programs because they are not designed to do that. AIXI, which is a general framework for reinforcement learning, can learn programs as the environment model, but it is not computable. AIXI has a computable and computationally tractable approximation, MC-AIXI(FAC-CTW), but it models the environment not as programs but as a trie, and still has not resolved the trade-off between exploration and exploitation within a realistic amount of computation. This paper presents our research idea for realizing an efficient reinforcement learning algorithm that retains the property of modeling the environment as programs. It also models the policy as programs and has the ability to imitate other agents in the environment. The design policy of the algorithm has two points: (1) the ability to program is indispensable for human-level intelligence, and (2) a realistic solution to the exploration/exploitation trade-off is teaching via imitation.	https://dx.doi.org/10.1007/978-3-319-41649-6_36	Excluded	conflict_resolution	E3: Only conceptual results are reported,E3: Only conceptual results are reported	4
RL4SE	Katz2021	Tunable Neural Encoding of a Symbolic Robotic Manipulation Algorithm	"We present a neurocomputational controller for robotic manipulation based on the recently developed ""neural virtual machine"" (NVM). The NVM is a purely neural recurrent architecture that emulates a Turing-complete, purely symbolic virtual machine. We program the NVM with a symbolic algorithm that solves blocks-world restacking problems, and execute it in a robotic simulation environment. Our results show that the NVM-based controller can faithfully replicate the execution traces and performance levels of a traditional non-neural program executing the same restacking procedure. Moreover, after programming the NVM, the neurocomputational encodings of symbolic block stacking knowledge can be fine-tuned to further improve performance, by applying reinforcement learning to the underlying neural architecture."	https://www.ncbi.nlm.nih.gov/pubmed/34970133	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Katz2020	Reinforcement-based Program Induction in a Neural Virtual Machine	We present a neural virtual machine that can be trained to perform algorithmic tasks. Rather than combining a neural controller with non-neural memory storage as has been done in the past, this architecture is purely neural and emulates tape-based memory via fast associative weights (one-step learning). Here we formally define the architecture, and then extend the system to learn programs using recurrent policy gradient reinforcement learning based on examples of program inputs labeled with corresponding output targets, which are compared against actual output to generate a sparse reward signal. We describe the policy gradient training procedure used, and report its empirical performance on a number of small-scale list processing tasks, such as finding the maximum list element, filtering out certain elements, and reversing the order of the elements. These results show that program induction via reinforcement learning is possible using sparse rewards and solely neural computations.	https://dx.doi.org/10.1109/IJCNN48605.2020.9207671	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Keerthana2021	Evaluating the Performance of Various Deep Reinforcement Learning Algorithms for a Conversational Chatbot	Conversational agents are the most popular AI technology in IT trends. Domain specific chatbots are now used by almost every industry in order to upgrade their customer service. The Proposed paper shows the modelling and performance of one such conversational agent created using deep learning. The proposed model utilizes NMT (Neural Machine Translation) from the TensorFlow software libraries. A BiRNN (Bidirectional Recurrent Neural Network) is used in order to process input sentences that contain large number of tokens (20-40 words). In order to understand the context of the input sentence attention model is used along with BiRNN. The conversational models usually have one drawback, that is, they sometimes provide irrelevant answer to the input. This happens quite often in conversational chatbots as the chatbot doesn't realize that it is answering without context. This drawback is solved in the proposed system using Deep Reinforcement Learning technique. Deep reinforcement Learning follows a reward system that enables the bot to differentiate between right and wrong answers. Deep Reinforcement Learning techniques allows the chatbot to understand the sentiment of the query and reply accordingly. The Deep Reinforcement Learning algorithms used in the proposed system is Q-Learning, Deep Q Neural Network (DQN) and Distributional Reinforcement Learning with Quantile Regression (QR-DQN). The performance of each algorithm is evaluated and compared in this paper in order to find the best DRL algorithm. The dataset used in the proposed system is Cornell Movie-dialogs corpus and CoQA (A Conversational Question Answering Challenge). CoQA is a large dataset that contains data collected from 8000+ conversations in the form of questions and answers. The main goal of the proposed work is to increase the relevancy of the chatbot responses and to increase the perplexity of the conversational chatbot.	https://dx.doi.org/10.1109/INCET51464.2021.9456321	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kelly2014	Genotypic versus behavioural diversity for teams of programs under the 4-v-3 keepaway soccer task			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kelly2014a	On diversity, teaming, and hierarchical policies: Observations from the Keepaway soccer task		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927643381&doi=10.1007\%2f978-3-662-44303-3_7&partnerID=40&md5=b371a886e97be7c677b81f9eb3f2049e	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kelly2018	Emergent tangled program graphs in multi-task learning			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kelly2020	A modular memory framework for time series prediction		https://doi.org/10.1145/3377930.3390216	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kelly2021	Evolving hierarchical memory-prediction machines in multi-task reinforcement learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116663584&doi=10.1007\%2fs10710-021-09418-4&partnerID=40&md5=4250b22984b589e6188b16aa2127d3dd	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kempka2016	ViZDoom: A Doom-based AI research platform for visual reinforcement learning	The recent advances in deep neural networks have led to effective vision-based reinforcement learning methods that have been employed to obtain human-level controllers in Atari 2600 games from pixel data. Atari 2600 games, however, do not resemble real-world tasks since they involve non-realistic 2D environments and the third-person perspective. Here, we propose a novel test-bed platform for reinforcement learning research from raw visual information which employs the first-person perspective in a semi-realistic 3D world. The software, called ViZDoom, is based on the classical first-person shooter video game, Doom. It allows developing bots that play the game using the screen buffer. ViZDoom is lightweight, fast, and highly customizable via a convenient mechanism of user scenarios. In the experimental part, we test the environment by trying to learn bots for two scenarios: a basic move-and-shoot task and a more complex maze-navigation problem. Using convolutional deep neural networks with Q-learning and experience replay, for both scenarios, we were able to train competent bots, which exhibit human-like behaviors. The results confirm the utility of ViZDoom as an AI research platform and imply that visual reinforcement learning in 3D realistic first-person perspective environments is feasible.	https://dx.doi.org/10.1109/CIG.2016.7860433	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kerr2003	Java resources for teaching reinforcement learning	In this paper we present a library of classes for programming reinforcement learning simulations in Java. This library is based upon the standard by Sutton and Santamaria [1], with valuable additions to simplify the implementation of the software for selected temporal-difference control algorithms and various memory models. We also present arguments for the integration of this library into the curriculum of a Java-based undergraduate course in Artificial Intelligence.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kersting2004	Logical Markov decision programs and the convergence of logical TD(?)		https://www.scopus.com/inward/record.uri?eid=2-s2.0-22944490192&doi=10.1007\%2f978-3-540-30109-7_16&partnerID=40&md5=13df158948f9bb2c5e43a96bce8651df	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kessler2017	Hierarchical Markov decision process based on DEVS formalism	Markov decision processes (MDPs) have proven useful as models of stochastic planning and decision problems. To try to propose practical implementation of MDPs, hierarchical methods are often used in MDPs or reinforcement learning to delegate the optimization of the total problem to simpler hierarchical sub-problems. The goal of the paper is to propose a generic discrete-event based software Framework allowing to use hierarchical MDPs and reinforcement learning to solve planning or decision problems. The proposed approach has been validated using the ``grid world'' typical MDP use case.	https://dx.doi.org/10.1109/WSC.2017.8247850	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Khac2021	Adaptive Optimal Control of Four-Wheel Omni Robot using Reinforcement Learning	Designing Controllers for Omni mobile robots have been widely studied, and some proposals have also mentioned that the robot model has uncertain parameters. This paper develops an optimal adaptive traction control structure based on reinforcement learning for a 4-wheel Omni robot in the condition in which a part of the model is known. An auxiliary controller with two Actor - Critic neural networks updated online was added to deal with having to create desired trajectories for all state variables in the system. Besides, the controller design also takes into account the constraint of the input signal. An example is simulated on Matlab/Simulink software to demonstrate the quality of the proposed controller.	https://dx.doi.org/10.1109/ICSSE52999.2021.9538431	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Khailany2020	Accelerating Chip Design with Machine Learning		https://doi.org/10.1145/3380446.3430690	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Khamis2013	Designing multi-agent unit tests using systematic test design patterns-(extended version)		https://doi.org/10.1016/j.engappai.2013.04.009	Excluded	new_screen	E5: Other not a paper,E1: Does not define or use a RL method	4
RL4SE	Khan2022	An Intersection-Based Routing Scheme Using Q-Learning in Vehicular Ad Hoc Networks for Traffic Management in the Intelligent Transportation System	Vehicular ad hoc networks (VANETs) create an advanced framework to support the intelligent transportation system and increase road safety by managing traffic flow and avoiding accidents. These networks have specific characteristics, including the high mobility of vehicles, dynamic topology, and frequent link failures. For this reason, providing an efficient and stable routing approach for VANET is a challenging issue. Reinforcement learning (RL) can solve the various challenges and issues of vehicular ad hoc networks, including routing. Most of the existing reinforcement learning-based routing methods are incompatible with the dynamic network environment and cannot prevent congestion in the network. Network congestion can be controlled by managing traffic flow. For this purpose, roadside units (RSUs) must monitor the road status to be informed about traffic conditions. In this paper, an intersection-based routing method using Q-learning (IRQ) is presented for VANETs. IRQ uses both global and local views in the routing process. For this reason, a dissemination mechanism of traffic information is introduced to create these global and local views. According to the global view, a Q-learning-based routing technique is designed for discovering the best routes between intersections. The central server continuously evaluates the created paths between intersections to penalize road segments with high congestion and improve the packet delivery rate. Finally, IRQ uses a greedy strategy based on a local view to find the best next-hop node in each road segment. NS2 software is used for analyzing the performance of the proposed routing approach. Then, IRQ is compared with three methods, including IV2XQ, QGrid, and GPSR. The simulation results demonstrate that IRQ has an acceptable performance in terms of packet delivery rate and delay. However, its communication overhead is higher than IV2XQ.	https://dx.doi.org/10.3390/math10203731	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Khatami2021	A reinforcement learning model to inform optimal decision paths for HIV elimination	The 'Ending the HIV Epidemic (EHE)' national plan aims to reduce annual HIV incidence in the United States from 38,000 in 2015 to 9300 by 2025 and 3300 by 2030. Diagnosis and treatment are two most effective interventions, and thus, identifying corresponding optimal combinations of testing and retention-in-care rates would help inform implementation of relevant programs. Considering the dynamic and stochastic complexity of the disease and the time dynamics of decision-making, solving for optimal combinations using commonly used methods of parametric optimization or exhaustive evaluation of pre-selected options are infeasible. Reinforcement learning (RL), an artificial intelligence method, is ideal; however, training RL algorithms and ensuring convergence to optimality are computationally challenging for large-scale stochastic problems. We evaluate its feasibility in the context of the EHE goal. We trained an RL algorithm to identify a 'sequence' of combinations of HIV-testing and retention-in-care rates at 5-year intervals over 2015-2070 that optimally leads towards HIV elimination. We defined optimality as a sequence that maximizes quality-adjusted-life-years lived and minimizes HIV-testing and care-and-treatment costs. We show that solving for testing and retention-in-care rates through appropriate reformulation using proxy decision-metrics overcomes the computational challenges of RL. We used a stochastic agent-based simulation to train the RL algorithm. As there is variability in support-programs needed to address barriers to care-access, we evaluated the sensitivity of optimal decisions to three cost-functions. The model suggests to scale-up retention-in-care programs to achieve and maintain high annual retention-rates while initiating with a high testing-frequency but relaxing it over a 10-year period as incidence decreases. Results were mainly robust to the uncertainty in costs. However, testing and retention-in-care alone did not achieve the 2030 EHE targets, suggesting the need for additional interventions. The results from the model demonstrated convergence. RL is suitable for evaluating phased public health decisions for infectious disease control.	https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114415373&doi=10.3934\%2fmbe.2021380&partnerID=40&md5=6b6e6e1a1f42f884cc8f656d44eeccdf	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kholoshnia2020	The system for finding the least resource-intensive path in two- or three-dimensional space using machine learning			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kiangala2022	An Experimental Safety Response Mechanism for an Autonomous Moving Robot in a Smart Manufacturing Environment Using Q-Learning Algorithm and Speech Recognition	The industrial manufacturing sector is undergoing a tremendous revolution moving from traditional production processes to intelligent techniques. Under this revolution, known as Industry 4.0 (I40), a robot is no longer static equipment but an active workforce to the factory production alongside human operators. Safety becomes crucial for humans and robots to ensure a smooth production run in such environments. The loss of operating moving robots in plant evacuation can be avoided with the adequate safety induction for them. Operators are subject to frequent safety inductions to react in emergencies, but very little is done for robots. Our research proposes an experimental safety response mechanism for a small manufacturing plant, through which an autonomous robot learns the obstacle-free trajectory to the closest safety exit in emergencies. We implement a reinforcement learning (RL) algorithm, Q-learning, to enable the path learning abilities of the robot. After obtaining the robot optimal path selection options with Q-learning, we code the outcome as a rule-based system for the safety response. We also program a speech recognition system for operators to react timeously, with a voice command, to an emergency that requires stopping all plant activities even when they are far away from the emergency stops (ESTOPs) button. An ESTOP or a voice command sent directly to the factory central controller can give the factory an emergency signal. We tested this functionality on real hardware from an S7-1200 Siemens programmable logic controller (PLC). We simulate a simple and small manufacturing environment overview to test our safety procedure. Our results show that the safety response mechanism successfully generates paths without obstacles to the closest safety exits from all the factory locations. Our research benefits any manufacturing SME intending to implement the initial and primary use of autonomous moving robots (AMR) in their factories. It also impacts manufacturing SMEs using legacy devices such as traditional PLCs by offering them intelligent strategies to incorporate current state-of-the-art technologies such as speech recognition to improve their performances. Our research empowers SMEs to adopt advanced and innovative technological concepts within their operations.	https://www.ncbi.nlm.nih.gov/pubmed/35161688	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kibalya2021	A Reinforcement Learning Approach for Placement of Stateful Virtualized Network Functions	Network softwarization increases network flexibility by supporting the implementation of network functions such as firewalls as software modules. However, this creates new concerns on service reliability due to failures at both software and hardware level. The survivability of critical applications is commonly assured by deploying stand-by Virtual Network Functions (VNFs) to which the service is migrated upon failure of the primary VNFs. However, it is challenging to identify the optimal Data Centers (DCs) for hosting the active and stand-by VNF instances, not only to minimize their placement cost, but also the cost of a continuous state transfer between active and stand-by instances, since a number of VNFs are stateful. This paper proposes a reinforcement learning (RL) approach for the placement of stateful VNFs that considers a joint reservation of primary and backup resources with the objective of minimizing the overall placement cost. Simulation results show that the proposed algorithm is optimized in terms of both acceptance ratio and cost, resulting in up to 27\% and 30\% improvements in terms of accepted requests and placement cost compared to a state-of-the art algorithm.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kieckbusch2019	Negotiation Approach by Reinforcement Learning for Takeoff Sequencing Decision in Airports		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076799629&doi=10.1109\%2fITSC.2019.8917497&partnerID=40&md5=442e2139c352d3408d53652018fbcae5	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kim2008	A Q-Leaning-Based On-Line Planning Approach to Autonomous Architecture Discovery for Self-managed Software		https://doi.org/10.1007/978-3-540-88875-8_65	Included	new_screen		4
RL4SE	Kim2019	The automatic frequency control based on artificial intelligence for compact particle accelerator	In this work, an advantage actor critic (A2C) based intelligent automatic frequency control (AFC) system was developed for X-band linear accelerator (LINAC). A2C is one type of reinforcement learning, which indicates how software agents should perform actions in an environment. In this paper, the A2C based AFC algorithm and its environment design, simulation result, and controller hardware and software processes are described. The objective of our design is to match LINAC and magnetron frequency, and it is implemented via reward shaping using comparison with the reflected power in the adjacent time. The simulation with the A2C algorithm was implemented in two modes, namely, periodic and White Gaussian Noise (WGN) waves, for analysis with temperature and random disturbance, respectively. In order to create artificial disturbance in the experiment, the magnetron shaft was shifted randomly every 0.5 s by using WGN with 18 degrees angle of the step motor. The standard deviation of the reflected power was 5.63 kW, and the average power was 130.9 kW. To obtain maximum reward at the beginning of the A2C training, the adjacent frequency is needed. The measured average reflected power and standard deviation were 122.8 kW and 1.75 kW, respectively, after 2000 iterations. The results show that the reflected power and standard deviation of the AFC with A2C were lower compared to open-loop with artificial disturbance. The RF station of a medical X-band LINAC was used as the test bench, and the performance was confirmed by the results of an experiment conducted at Sungkyunkwan University in Korea.	https://www.ncbi.nlm.nih.gov/pubmed/31370502	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kim2020	Autonomous Vehicle Fuel Economy Optimization with Deep Reinforcement Learning	The ever-increasing number of vehicles on the road puts pressure on car manufacturers to make their car fuel-efficient. With autonomous vehicles, we can find new strategies to optimize fuels. We propose a reinforcement learning algorithm that trains deep neural networks to generate a fuel-efficient velocity profile for autonomous vehicles given road altitude information for the planned trip. Using a highly accurate industry-accepted fuel economy simulation program, we train our deep neural network model. We developed a technique for adapting the heterogeneous simulation program on top of an open-source deep learning framework, and reduced dimension of the problem output with suitable parameterization to train the neural network much faster. The learned model combined with reinforcement learning-based strategy generation effectively generated the velocity profile so that autonomous vehicles can follow to control itself in a fuel efficient way. We evaluate our algorithm's performance using the fuel economy simulation program for various altitude profiles. We also demonstrate that our method can teach neural networks to generate useful strategies to increase fuel economy even on unseen roads. Our method improved fuel economy by 8\% compared to a simple grid search approach.	https://dx.doi.org/10.3390/electronics9111911	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kim2003	Co-operative strategy for an interactive robot soccer system by reinforcement learning method			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kim2021	On-chip trainable hardware-based deep Q-networks approximating a backpropagation algorithm	Reinforcement learning (RL) using deep Q-networks (DQNs) has shown performance beyond the human level in a number of complex problems. In addition, many studies have focused on bio-inspired hardware-based spiking neural networks (SNNs) given the capabilities of these technologies to realize both parallel operation and low power consumption. Here, we propose an on-chip training method for DQNs applicable to hardware-based SNNs. Because the conventional backpropagation (BP) algorithm is approximated, a performance evaluation based on two simple games shows that the proposed system achieves performance similar to that of a software-based system. The proposed training method can minimize memory usage and reduce power consumption and area occupation levels. In particular, for simple problems, the memory dependency can be significantly reduced given that high performance is achieved without using replay memory. Furthermore, we investigate the effect of the nonlinearity characteristics and two types of variation of non-ideal synaptic devices on the performance outcomes. In this work, thin-film transistor (TFT)-type flash memory cells are used as synaptic devices. A simulation is also conducted using fully connected neural network with non-leaky integrated-and-fire (I&F) neurons. The proposed system shows strong immunity to device variations because an on-chip training scheme is adopted.	https://dx.doi.org/10.1007/s00521-021-05699-z	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kim2018	Generating Test Input with Deep Reinforcement Learning	Test data generation is a tedious and laborious process. Search-based Software Testing (SBST) automatically generates test data optimising structural test criteria using metaheuristic algorithms. In essence, metaheuristic algorithms are systematic trial-and-error based on the feedback of fitness function. This is similar to an agent of reinforcement learning which iteratively decides an action based on the current state to maximise the cumulative reward. Inspired by this analogy, this paper investigates the feasibility of employing reinforcement learning in SBST to replace human designed metaheuristic algorithms. We reformulate the software under test (SUT) as an environment of reinforcement learning. At the same time, we present GunPowder, a novel framework for SBST which extends SUT to the environment. We train a Double Deep Q-Networks (DDQN) agent with deep neural network and evaluate the effectiveness of our approach by conducting a small empirical study. Finally, we find that agents can learn metaheuristic algorithms for SBST, achieving 100\% branch coverage for training functions. Our study sheds light on the future integration of deep neural network and SBST.		Included	new_screen		4
RL4SE	Kim2020a	Adaptive Human-Machine Evaluation Framework Using Stochastic Gradient Descent-Based Reinforcement Learning for Dynamic Competing Network	Complex problems require considerable work, extensive computation, and the development of effective solution methods. Recently, physical hardware- and software-based technologies have been utilized to support problem solving with computers. However, problem solving often involves human expertise and guidance. In these cases, accurate human evaluations and diagnoses must be communicated to the system, which should be done using a series of real numbers. In previous studies, only binary numbers have been used for this purpose. Hence, to achieve this objective, this paper proposes a new method of learning complex network topologies that coexist and compete in the same environment and interfere with the learning objectives of the others. Considering the special problem of reinforcement learning in an environment in which multiple network topologies coexist, we propose a policy that properly computes and updates the rewards derived from quantitative human evaluation and computes together with the rewards of the system. The rewards derived from the quantitative human evaluation are designed to be updated quickly and easily in an adaptive manner. Our new framework was applied to a basketball game for validation and demonstrated greater effectiveness than the existing methods.	https://dx.doi.org/10.3390/app10072558	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kim2020b	Cooperative Multi-Agent Interaction and Evaluation Framework Considering Competitive Networks with Dynamic Topology Changes	Featured Application The proposed multi-agent framework can be applied to cooperative tasks between human and machines, such as human-robot interaction, autonomous car driving, and artificial intelligence-based industrial tasks. In recent years, the problem of reinforcement learning has become increasingly complex, and the computational demands with respect to such processes have increased. Accordingly, various methods for effective learning have been proposed. With the help of humans, the learning object can learn more accurately and quickly to maximize the reward. However, the rewards calculated by the system and via human intervention (that make up the learning environment) differ and must be used accordingly. In this paper, we propose a framework for learning the problems of competitive network topologies, wherein the environment dynamically changes agent, by computing the rewards via the system and via human evaluation. The proposed method is adaptively updated with the rewards calculated via human evaluation, making it more stable and reducing the penalty incurred while learning. It also ensures learning accuracy, including rewards generated from complex network topology consisting of multiple agents. The proposed framework contributes to fast training process using multi-agent cooperation. By implementing these methods as software programs, this study performs numerical analysis to demonstrate the effectiveness of the adaptive evaluation framework applied to the competitive network problem depicting the dynamic environmental topology changes proposed herein. As per the numerical experiments, the greater is the human intervention, the better is the learning performance with the proposed framework.	https://dx.doi.org/10.3390/app10175828	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kim2022	Solving PBQP-Based Register Allocation using Deep Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128368520&doi=10.1109\%2fCGO53902.2022.9741272&partnerID=40&md5=fb65a165ee06ef967b0266be10277620	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kim2021a	Two-stage training algorithm for AI robot soccer	In multi-agent reinforcement learning, the cooperative learning behavior of agents is very important. In the field of heterogeneous multi-agent reinforcement learning, cooperative behavior among different types of agents in a group is pursued. Learning a joint-action set during centralized training is an attractive way to obtain such cooperative behavior; however, this method brings limited learning performance with heterogeneous agents. To improve the learning performance of heterogeneous agents during centralized training, two-stage heterogeneous centralized training which allows the training of multiple roles of heterogeneous agents is proposed. During training, two training processes are conducted in a series. One of the two stages is to attempt training each agent according to its role, aiming at the maximization of individual role rewards. The other is for training the agents as a whole to make them learn cooperative behaviors while attempting to maximize shared collective rewards, e.g., team rewards. Because these two training processes are conducted in a series in every time step, agents can learn how to maximize role rewards and team rewards simultaneously. The proposed method is applied to 5 versus 5 AI robot soccer for validation. The experiments are performed in a robot soccer environment using Webots robot simulation software. Simulation results show that the proposed method can train the robots of the robot soccer team effectively, achieving higher role rewards and higher team rewards as compared to other three approaches that can be used to solve problems of training cooperative multi-agent. Quantitatively, a team trained by the proposed method improves the score concede rate by 5\% to 30\% when compared to teams trained with the other approaches in matches against evaluation teams.	https://www.ncbi.nlm.nih.gov/pubmed/34616894	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kiran2020	New interactive agent based reinforcement learning approach towards smart generator bidding in electricity market with micro grid integration	In order to suit the needs of the dynamically changing electricity market, software developers have developed various tools taking in to account the need of artificial intelligence for the electricity market entities. Algorithms in artificial intelligence are often divided into either supervised, unsupervised and reinforcement learning approach. A reinforcement learning when compared to supervised and unsupervised learning makes use of agent to learn from interaction with an environment and receives rewards based on the action it takes. It either exploits or explores in finding a solution. In the deregulated power market, the GenCos are modeled as agent by which the GenCo learns the market environment as agent and explores to get profited The Multi-agent based simulation is an effective method to incorporate this sort of intelligence and for providing efficient communication among the market entities. Using Multi-agent system, the problem existing in electricity market can be reduced since each entity problem can be solved by an individual agent. Multi-agent based reinforcement learning algorithm is used to handle the electricity market data. Here an agent based computational framework named Agent Based Modeling of Electricity Systems (AMES) under Java platform is developed for the design of electricity market. Market Agents balances the supply and demand through Market Clearing Price (no congestion) and Locational Marginal Price (congestion management) by performing optimal power flow. The agent also maximizes the profit of the Generator Companies (GenCo's) through new learning strategy proposed using Variant Roth-Erev (VRE) interactive reinforcement learning method towards smart bidding among GenCo's. The congestion relieving action in the transmission line and its effects on GenCo learning is discussed in this paper. The analysis is carried out on the electricity whole sale market functioning on a day-ahead basis developed by means of location and timing of injection of power. IEEE-3 bus system and IEEE-30 bus system with microgrid considered as non-dispatchable load is considered using agent based analysis. This technique helps the GenCo's to attain possible high net earnings even with microgrid integration thus helps to relieve the congestion in the transmission lines. (C) 2020 Elsevier B.V. All rights reserved.	https://dx.doi.org/10.1016/j.asoc.2020.106762	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kitakoshi2015	Cognitive training system for dementia prevention using memory game based on the concept of human-agent interaction		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954317354&doi=10.20965\%2fjaciii.2015.p0727&partnerID=40&md5=08d99ff02af60c8fb9ab4d6f13079a1c	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kitakoshi2020	A Study on Coordination of Exercise Difficulty in Cognitive Training System for Older Adults	We have been engaging in development of a cognitive training system based on a kind of match-up game with software agent on tablet devices. Several experiments have been conducted to evaluate characteristics of the system. The results have demonstrated that the software agent in the system which employs a reinforcement learning (RL) method could promote older adults' motivation to use it for a long term and habitual engagement in the cognitive training as well, although learning settings of the system should be refined to further improve its learning/adaptive performance. This paper reviews the state settings in the agent based on the previous experimental results and the users' comments, and revise the settings so that the system can provide with more suitable difficulty of the game depending on respective users' condition such as their degree of concentration or fatigue.	https://dx.doi.org/10.1109/SCISISIS50064.2020.9322782	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kitty2008	Game engine design using data mining			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Klar2022	Scalability investigation of Double Deep Q Learning for factory layout planning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132306952&doi=10.1016\%2fj.procir.2022.04.027&partnerID=40&md5=8fce53cbb71672577e099efadccb40b4	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Klose2018	Simulated autonomous driving on realistic road networks using deep reinforcement learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059620936&doi=10.3233\%2f978-1-61499-929-4-169&partnerID=40&md5=649a90b50e14d37aa3a07d1eeeb723f2	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Knittel2006	Stochastic Reinforcement in Evolutionary Multi-Agent Game Playing of Dots-and-Boxes	An evolutionary multi-agent system is described that develops a rule-based approach to playing the game Dots and Boxes, under a probabilistic reinforcement learning paradigm. The process and behaviour using probabilistic action selection with a Boltzmann distribution is compared with an alternative technique using an Artificial Economy. The probabilistic system developed was played against a rule-based software opponent, and able to produce behaviour under a self-organising process able to perform better than the software opponent it was trained against.	https://dx.doi.org/10.1109/CIMCA.2006.201	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Knott2021	Evaluating the Robustness of Collaborative Agents			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kocak2006	A Hybrid Network Processor with Support for High-Speed Execution of CPN and Upper Layer IP Protocols	The trend in network processing is towards layer 4-7 processing in the routers and switches. In this paper, we present a hybrid network processor (NPU) architecture that supports high-speed execution of higher layer TCP/IP protocol processing. The architecture achieves this by employing optimized logic blocks for specific tasks in the processing elements. This is a unique architecture such that it can realize both task-level and packet-level parallelism. We also present an example of implementation of a fast adaptive routing algorithm, Cognitive Packet Networks (CPNs), using the proposed NPU architecture. CPN uses a neural network model with a reinforcement learning algorithm to find routes. The applicability of the CPN concept has been demonstrated through several software implementations. Through hardware implementation, we show that this new network model can sustain similar line rates as the current IP protocols.	https://dx.doi.org/10.1093/comjnl/bxh167	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kocak2002	Smart packet processor design for the cognitive packet network router	As the Internet expands significantly in numbers of users, servers, IP addresses, and routers, the IP based network architecture must evolve and change. Recently, cognitive packet networks (CPN) was proposed as an alternative packet network architecture, where there is no routing table, instead, reinforcement learning (random neural networks) is used to route smart packets. CPN routes packets based on QoS, using measurements that are constantly collected by packets and deposited in mailboxes at routers. Previously, CPN has been implemented in a software test-bed. In this paper, we present design approaches for a CPN network processor chip. Particularly, we discuss implementation details for one of the modules in the chip: the smart packet processor, which includes a neural network hardware design.	https://dx.doi.org/10.1109/MWSCAS.2002.1186911	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kocak2003	Design and implementation of smart packet processor for the Cognitive Packet Network router	As the Internet expands significantly in numbers of users, servers, IP addresses, and routers, the IP based network architecture must evolve and change. Recently, Cognitive Packet Networks (CPN) was proposed as an alternative packet network architecture, where there is no routing table, instead reinforcement learning (Random Neural Networks) is used to route smart packets [1,2]. CPN routes packets based on QoS, using measurements that are constantly collected by packets and deposited in mailboxes at routers. Previously, CPN is implemented in a software test-bed. In this paper, we present design approaches for CPN network processor chip. Particularly, we discuss implementation details for one of the modules in the chip: The smart packet processor, which includes a neural network hardware design.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kocak2003a	Design and implementation of a random neural network routing engine	Random neural network (RNN) is an analytically tractable spiked neural network model that has been implemented in software for a wide range of applications for over a decade. This paper presents the hardware implementation of the RNN model. Recently, cognitive packet networks (CPN) is proposed as an alternative packet network architecture where there is no routing table, instead the RNN based reinforcement learning is used to route packets. Particularly, we describe implementation details for the RNN based routing engine of a CPN network processor chip: the smart packet processor (SPP). The SPP is a dual port device that stores, modifies, and interprets the defining characteristics of multiple RNN models. In addition to hardware design improvements over the software implementation such as the dual access memory, output calculation step, and reduced output calculation module, this paper introduces a major modification to the reinforcement learning algorithm used in the original CPN specification such that the number of weight terms are reduced from 2n/sup 2/ to 2n. This not only yields significant memory savings, but it also simplifies the calculations for the steady state probabilities (neuron outputs in RNN). Simulations have been conducted to confirm the proper functionality for the isolated SPP design as well as for the multiple SPP's in a networked environment.	https://www.ncbi.nlm.nih.gov/pubmed/18244566	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Koch2023	Automated function development for emission control with deep reinforcement learning	The conventional automotive development process for embedded systems today is still time-and data -inefficient, and requires highly experienced software developers and calibration engineers. Consequently, it is cost-intensive and at the same time prone to sub-optimal solutions. Reinforcement Learning offers a promising approach to address these challenges. The evolved agents have proven their ability to master complex control tasks in a close-to-optimal manner without any human intervention, but the training procedures are hardly compatible with current development processes. As a result, Reinforcement Learning has rarely been used in powertrain development until now. This work describes an integration of Reinforcement Learning in the embedded system development process to automatically train and deploy agents in transient driving cycles. Using the example of exhaust gas re-circulation control for a Diesel engine, an agent is successfully trained in a fully virtualized environment, achieving emission reductions of up to 10\% in comparison to a state-of-the-art controller. Further investigations are carried out to quantify the impact of the driving cycle and ambient conditions on the agent's performance. To demonstrate the transferability between different levels of virtualization, the experienced agent is then tested in closed-loop with a real hardware controller to operate the physical actuator. By confirming the reproducibility of the learned strategy on real hardware, this article serves as proof-of-concept for a sustainable, Reinforcement Learning based path to automatically develop embedded controllers for complex control problems.	https://dx.doi.org/10.1016/j.engappai.2022.105477	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kocsis2003	Two learning algorithms for forward pruning	The article investigates two learning algorithms for forward pruning. The TS-FPV algorithm uses a tabu-search (TS) algorithm to explore the space of the forward-pruning vectors (FPVs). It focuses on critical FPVs. The RL-FPF algorithm is a reinforcement-learning (RL) algorithm for forward-pruning functions (FPFs). It uses a gradient-descent update rule. The two algorithms are tested using the chess program CRAFTY. The criteria used for evaluation are the size of the search tree and the quality of the move. The experimental results show that the two algorithms are able to tune a forward-pruning scheme that has a better overall performance than a comparable full-width search. The main result arrived at is that the FPFs obtained from RL-FPF outperform the best FPVs resulting from TS-FPV.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Koenig1992	Optimal Probabilistic and Decision-Theoretic Planning using Markovian			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E5: Other not a paper	4
RL4SE	Kogel2021	Machine learning based data and signal analysis methods for the application in failure analysis		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124232701&doi=10.31399\%2fasm.cp.istfa2021tpb1&partnerID=40&md5=e58a1172519a1fcba22e71baa314bda2	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E5: Other not a paper	4
RL4SE	Kojima2018	The avian basal ganglia are a source of rapid behavioral variation that enables vocal motor exploration	"The basal ganglia (BG) participate in aspects of reinforcement learning that require evaluation and selection of motor programs associated with improved performance. However, whether the BG additionally contribute to behavioral variation (""motor exploration"") that forms the substrate for such learning remains unclear. In songbirds, a tractable system for studying BG-dependent skill learning, a role for the BG in generating exploratory variability, has been challenged by the finding that lesions of Area X, the song-specific component of the BG, have no lasting effects on several forms of vocal variability that have been studied. Here we demonstrate that lesions of Area X in adult male zebra finches (Taeniopygia gutatta) permanently eliminate rapid within-syllable variation in fundamental frequency (FF), which can act as motor exploration to enable reinforcement-driven song learning. In addition, we found that this within-syllable variation is elevated in juveniles and in adults singing alone, conditions that have been linked to enhanced song plasticity and elevated neural variability in Area X. Consistent with a model that variability is relayed from Area X, via its cortical target, the lateral magnocellular nucleus of the anterior nidopallium (LMAN), to influence song motor circuitry, we found that lesions of LMAN also eliminate within-syllable variability. Moreover, we found that electrical perturbation of LMAN can drive fluctuations in FF that mimic naturally occurring within-syllable variability. Together, these results demonstrate that the BG are a central source of rapid behavioral variation that can serve as motor exploration for vocal learning.SIGNIFICANCE STATEMENT Many complex motor skills, such as speech, are not innately programmed but are learned gradually through trial and error. Learning involves generating exploratory variability in action (""motor exploration"") and evaluating subsequent performance to acquire motor programs that lead to improved performance. Although it is well established that the basal ganglia (BG) process signals relating to action evaluation and selection, whether and how the BG promote exploratory motor variability remain unclear. We investigated this question in songbirds, which learn to produce complex vocalizations through trial and error. In contrast with previous studies that did not find effects of BG lesions on vocal motor variability, we demonstrate that the BG are an essential source of rapid behavioral variation linked to vocal learning."	https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056429584&doi=10.1523\%2fJNEUROSCI.2915-17.2018&partnerID=40&md5=23b1d34d7ddb3e2b730c1ed4252c7a2e	Excluded	new_screen	E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for	4
RL4SE	Koley2021	Catch Me If You Learn: Real-Time Attack Detection and Mitigation in Learning Enabled CPS	Increased dependence on networked, software-based control has escalated the vulnerabilities of Cyber-Physical Systems (CPSs). Detection and monitoring components developed leveraging dynamical systems theory are often employed as lightweight security measures for protecting such safety-critical CPSs against false data injection attacks. However, existing approaches do not correlate attack scenarios with parameters of detection systems. In this work, we propose real-time attack detection and mitigation strategies for safety-critical CPSs. A Reinforcement Learning (RL) based framework is presented which adaptively sets the parameters of such detectors based on experience learned from attack scenarios. The detection system is provided with a suitable training environment to learn how to maximize the detection rate while minimizing false alarms. Along with the objective of attack detection, our framework also attempts to preserve system performance by judiciously choosing control actions based on the operating region. Our proposed method i) mathematically establishes a detection strategy for fast and accurate FDI attack detection, ii) correlates the key parameters of the detection system by learning from attack scenarios, and iii) incorporates a real-time attack mitigation strategy that uses formally synthesized fast controllers, thus creating an end-to-end defense against FDI attacks for safety-critical CPSs. We evaluate our proposed method using well-known safety-critical CPS examples.	https://dx.doi.org/10.1109/RTSS52674.2021.00023	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kollmitz2020	Learning Human-Aware Robot Navigation from Physical Interaction via Inverse Reinforcement Learning	Autonomous systems, such as delivery robots, are increasingly employed in indoor spaces to carry out activities alongside humans. This development poses the question of how robots can carry out their tasks while, at the same time, behaving in a socially compliant manner. Further, humans need to be able to communicate their preferences in a simple and intuitive way, and robots should adapt their behavior accordingly. This paper investigates force control as a natural means to interact with a mobile robot by pushing it along the desired trajectory. We employ inverse reinforcement learning (IRL) to learn from human interaction and adapt the robot behavior to its users' preferences, thereby eliminating the need to program the desired behavior manually. We evaluate our approach in a real-world experiment where test subjects interact with an autonomously navigating robot in close proximity. The results suggest that force control presents an intuitive means to interact with a mobile robot and show that our robot can quickly adapt to the test subjects' personal preferences.	https://dx.doi.org/10.1109/IROS45743.2020.9340865	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kondo2022	Service Selection for Service-Oriented Architecture using Off-line Reinforcement Learning in Dynamic Environments	Service-Oritented Architeture (SOA) is a style of system design in which the entire system is built from a combination of services, which are functional units of software. The performance of a system designed with SOA depends on the combination of services. In this research, we aim to use reinforcement learning for service selection in SOA. Service selection in SOA is characterized by its dynamic environment and inefficient collection of samples for training. We propose an offline reinforcement learning method in a dynamic environment to solve this problem. In the proposed method, transfer learning is performed by applying fine tuning and focused sampling. Experiments show that the proposed method can adapt to dynamic environments more efficiently than redoing online reinforcement learning every time the environment changes.	https://dx.doi.org/10.5220/0010872400003116	Included	conflict_resolution		4
RL4SE	Kong2022	Optimization of routing and wavelength optimization algorithm for optical transport network based on reinforcement learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143964217&doi=10.3788\%2fIRLA20220084&partnerID=40&md5=b484aeb50351769cb5911917e07de34a	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Koo2019	PySE: Automatic Worst-Case Test Generation by Reinforcement Learning	Stress testing is an important task in software testing, which examines the behavior of a program under a heavy load. Symbolic execution is a useful tool to find out the worst-case input values for the stress testing. However, symbolic execution does not scale to a large program, since the number of paths to search grows exponentially with an input size. So far, such a scalability issue has been mostly managed by pruning out unpromising paths in the middle of searching based on heuristics, but this kind of work easily eliminates the true worst case as well, providing sub-optimal one only. Another way to achieve scalability is to learn a branching policy of worst-case complexity from small scale tests and apply it to a large scale. However, use cases of such a method are restricted to programs whose worst-case branching policy has a simple pattern. To address such limitations, we propose PySE that uses symbolic execution to collect the behaviors of a given branching policy, and updates the policy using a reinforcement learning approach through multiple executions. PySE's branching policy keeps evolving in a way that the length of an execution path increases in the long term, and ultimately reaches the worst-case complexity. PySE can also learn the worst-case branching policy of a complex or irregular pattern, using an artificial neural network in a fully automatic way. Experiment results demonstrate that PySE can effectively find a path of worst-case complexity for various Python benchmark programs and scales.	https://dx.doi.org/10.1109/ICST.2019.00023	Included	new_screen		4
RL4SE	Kool2018	Competition and cooperation between multiple reinforcement learning systems		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060560963&doi=10.1016\%2fB978-0-12-812098-9.00007-3&partnerID=40&md5=c3501941af2f0e1698ab44b62cdeba5c	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Koprinkova-Hristova2022	In-silico Investigation of Human Visual System	"The paper presents in-silico investigations of the effects of brain lesions in different parts of human visual system. For this aim, a hierarchical spike timing neural network model reproducing performance of visual tasks with reinforcement learning by humans was implemented in NEST simulator. Its structure and connectivity is designed according to available information about corresponding brain areas organization. The model has two basic sub-structures: a perceptual part involved in visual information processing and perceptual decision making and basal ganglia that biases taken decisions according to received external reinforcement signal. The developed software includes also an option to introduce ""damage"" in each one of the model sub-structures thus allowing performance of in-silico investigations of effect of brain lesions. The simulations were performed feeding the model with a dynamic visual stimulus consisting of moving dots. Different scenarios including damage in single or several brain areas were prepared and model reactions were collected. Observed deterioration of visual task performance were summarized and commented."	https://dx.doi.org/10.1007/978-3-030-96638-6_25	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kosugi2020	Unpaired Image Enhancement Featuring Reinforcement-Learning-Controlled Image Editing Software	This paper tackles unpaired image enhancement, a task of learning a mapping function which transforms input images into enhanced images in the absence of input-output image pairs. Our method is based on generative adversarial networks (GANs), but instead of simply generating images with a neural network, we enhance images utilizing image editing software such as Adobe (R) Photoshop (R) for the following three benefits: enhanced images have no artifacts, the same enhancement can be applied to larger images, and the enhancement is interpretable. To incorporate image editing software into a GAN, we propose a reinforcement learning framework where the generator works as the agent that selects the software's parameters and is rewarded when it fools the discriminator. Our framework can use high-quality non-differentiable filters present in image editing software, which enables image enhancement with high performance. We apply the proposed method to two unpaired image enhancement tasks: photo enhancement and face beautification. Our experimental results demonstrate that the proposed method achieves better performance, compared to the performances of the state-of-the-art methods based on unpaired learning.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Koushik2018	A hardware testbed for learning-based spectrum handoff in cognitive radio networks	A real-time cognitive radio network (CRN) testbed is implemented by using the universal software radio peripheral (USRP) and GNU Radio to demonstrate the use of reinforcement learning and transfer learning schemes for spectrum handoff decisions. By considering the channel status (idle or occupied) and channel condition (in terms of packet error rate), the sender node performs the learning-based spectrum handoff. In reinforcement learning, the number of network observations required to achieve the optimal decisions is often prohibitively high, due to the complex CRN environment. When a node experiences new channel conditions, the learning process is restarted from scratch even when the similar channel condition has been experienced before. To alleviate this issue, a transfer learning based spectrum handoff scheme is implemented, which enables a node to learn from its neighboring node(s) to improve its performance. In transfer learning, the node searches for an expert node in the network. If an expert node is found, the node requests the Q-table from the expert node for making its spectrum handoff decisions. If an expert node cannot be found, the node learns the spectrum handoff strategy on its own by using the reinforcement learning. Our experimental results demonstrate that the machine learning based spectrum handoff performs better in the long term and effectively utilizes the available spectrum. In addition, the transfer learning requires less number of packet transmissions to achieve an optimal solution, compared to the reinforcement learning.(1)	https://dx.doi.org/10.1016/j.jnca.2017.11.003	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kovac2022	Towards intelligent compiler optimization	The future of computation is massively parallel and heterogeneous with specialized accelerator devices and instruction sets in both edge- and cluster-computing. However, software development is bound to become the bottleneck. To extract the potential of hardware wonders, the software would have to solve the following problems: heterogeneous device mapping, capability discovery, parallelization, adaptation to new ISAs, and many others. This systematic complexity will be impossible to manually tame for human developers. These problems need to be offloaded to intelligent compilers. In this paper, we present the current research that utilizes deep learning, polyhedral optimization, reinforcement learning, etc. We envision the future of compilers as consisting of empirical testing, automatic statistics collection, continual learning, device capability discovery, multiphase compiling \endash precompiling and JIT tuning, and classification of workloads. We devise a simple classification experiment to demonstrate the power of simple graph neural networks (GNNs) paired with program graphs. The test performance demonstrates the effectiveness and representational appropriateness of GNNs for compiler optimizations in heterogeneous systems. The benefits of intelligent compilers are time savings for the economy, energy savings for the environment, and greater democratization of software development.	https://dx.doi.org/10.23919/MIPRO55190.2022.9803630	Included	conflict_resolution		4
RL4SE	Kovacs2011	On the analysis and design of software for reinforcement learning, with a survey of existing systems		https://doi.org/10.1007/s10994-011-5237-8	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E5: Other not a paper	4
RL4SE	Koza1992	Automatic programming of robots using genetic programming			Excluded	conflict_resolution	E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kozak2022	Generation of Adversarial Malware and Benign Examples Using Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134588961&doi=10.1007\%2f978-3-030-97087-1_1&partnerID=40&md5=d74b0a5fc28ad335d183caceb24e18d3	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kozlov2021	Comparison of Reinforcement Learning Algorithms for Motion Control of an Autonomous Robot in Gazebo Simulator	This article compares various implementations of deep Q learning as it is one of the most efficient reinforcement learning algorithms for discrete action space systems. The efficiency of the implementations for the classical Cartpole problem ported to the Gazebo environment is investigated. Then, these algorithms are compared for a self-created bipedal robot problem. Since the creation and configuration of a real robotic system is a laborious process, the initial debugging of the robot can be performed using the appropriate software that simulates the real environment. In our case, the Gazebo simulator was used. Using the simulator allows you to conduct research without having a real robotic system. In this case, it is possible to transfer the results from the simulator to the real system. The result of the study is the conclusion about the greatest efficiency of deep Q-learning with the experience reproduction mechanism. Also, the conclusion is that even for a robot with two degrees of freedom, Q-learning algorithms are not effective enough, and a comparative study with other families of reinforcement learning algorithms is needed.	https://dx.doi.org/10.1109/ITNT52450.2021.9649145	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Krening2018	Interaction Algorithm Effect on Human Experience with Reinforcement Learning		https://doi.org/10.1145/3277904	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Krening2017	Learning From Explanations Using Sentiment and Advice in RL	In order for robots to learn from people with no machine learning expertise, robots should learn from natural human instruction. Most machine learning techniques that incorporate explanations require people to use a limited vocabulary and provide state information, even if it is not intuitive. This paper discusses a software agent that learned to play the Mario Bros. game using explanations. Our goals to improve learning from explanations were twofold: (1) to filter explanations into advice and warnings and (2) to learn policies from sentences without state information. We used sentiment analysis to filter explanations into advice of what to do and warnings of what to avoid. We developed object-focused advice to represent what actions the agent should take when dealing with objects. A reinforcement learning agent used object-focused advice to learn policies that maximized its reward. After mitigating false negatives, using sentiment as a filter was approximately 85\% accurate. object-focused advice performed better than when no advice was given, the agent learned where to apply the advice, and the agent could recover from adversarial advice. We also found the method of interaction should be designed to ease the cognitive load of the human teacher or the advice may be of poor quality.	https://dx.doi.org/10.1109/TCDS.2016.2628365	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Krijestorac2019	UAV Access Point Placement for Connectivity to a User with Unknown Location Using Deep RL	In recent years, unmanned aerial vehicles (UAVs) have been considered for telecommunications purposes as relays, caches, or IoT data collectors. In addition to being easy to deploy, their maneuverability allows them to adjust their location to optimize the capacity of the link to the user equipment on the ground or of the link to the basestation. The majority of the previous work that analyzes the optimal placement of such a UAV makes at least one of two assumptions: the channel can be predicted using a simple model or the locations of the users on the ground are known. In this paper, we use deep reinforcement learning (deep RL) to optimally place a UAV serving a ground user in an urban environment, without the previous knowledge of the channel or user location. Our algorithm relies on signal-to-interference-plus- noise ratio (SINR) measurements and a 3D map of the topology to account for blockage and scatterers. Furthermore, it is designed to operate in any urban environment. Results in conditions simulated by a ray tracing software show that with the constraint on the maximum number of iterations our algorithm has a 90\% success rate in converging to a target SINR.	https://dx.doi.org/10.1109/GCWkshps45667.2019.9024644	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kuai2021	Fair Virtual Network Function Scheduling with Deep Reinforcement Learning	Network function virtualization aims at deploying network functions on general-purpose hardwares, referred to as virtual network functions (VNFs), rather than the specialized devices. A network service can be implemented by scheduling different VNFs. In this paper, we study the VNF scheduling problem with the objective of satisfying the diversified end-to-end delay requirements while maintaining the fairness among different network services. We formulate the problem as a mixed integer nonlinear program and propose a VNF scheduling algorithm based on deep reinforcement learning, in which proximal policy optimization is adopted to optimize the policy network. Numerical results show that the proposed scheduling algorithm outperforms other canonical ones and the designed policy network can scale to fit different problem sizes.	https://dx.doi.org/10.1109/GLOBECOM46510.2021.9686006	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kuang2021	Model-free demand response scheduling strategy for virtual power plant considering risk attitude of consumer	Driven by the modern advanced information and communication technologies, distributed energy resources has the great potential for energy supply within the framework of virtual power plant(VPP). Meanwhile, demand response(DR) is becoming increasingly important for enhancing the VPP operation and mitigating the risks associated with the fluctuation of renewable energy resources(RESs). In this paper, we proposed an incentive-based DR program for VPP to minimize the deviation penalty from participating in power market. Markov decision process(MDP) with unknown transition probability is constructed from the VPP's prospective to formulate the incentive-based DR program, in which the randomness of consumer behavior and RES generation are taken into consideration. Furthermore, a value function of prospect theory(PT) is developed to characterize consumer's risk attitude and describe the psychological factors. A model-free deep reinforcement learning(DRL)-based approach is proposed to deal with the randomness existing in the model and adaptively determine the optimal DR pricing strategy for VPP, without requiring any system model information. Finally, the results of cases demonstrate the effectiveness of the proposed approach.	https://dx.doi.org/10.17775/CSEEJPES.2020.03120	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kuhlmann2019	Learning to steer nonlinear interior-point methods		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071224845&doi=10.1007\%2fs13675-019-00118-4&partnerID=40&md5=89bd924b3bf61dbeff90db5d3a9b7675	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kulkarni2020	UAV Aided Search and Rescue Operation Using Reinforcement Learning	Owing to the enhanced flexibility in deployment and decreasing costs of manufacturing, the demand for unmanned aerial vehicles (UAVs) is expected to soar in the upcoming years. In this paper, we explore a UAV aided search and rescue (SAR) operation in indoor environments, where the GPS signals might not be reliable. We consider a SAR scenario where the UAV tries to locate a victim trapped in an indoor environment by sensing the RF signals emitted from a smart device owned by the victim. To locate the victim as fast as possible, we leverage tools from reinforcement learning (RL). Received signal strength (RSS) at the UAV depends on the distance from the source, indoor shadowing and fading parameters, and antenna radiation pattern of the receiver mounted on the UAV. To make our analysis more realistic, we model two indoor scenarios with different dimensions using a commercial ray tracing software. Then, the corresponding RSS values at each possible discrete UAV location are extracted and used in a Q-learning framework. Unlike the traditional location-based navigation approach that exploits GPS coordinates, our method uses the RSS to define the states and rewards of the RL algorithm. We compare the performance of the proposed method where directional and omnidirectional antennas are used. The results reveal that the use of directional antennas provides faster convergence rates than the omnidirectional antennas.	https://dx.doi.org/10.1109/SoutheastCon44009.2020.9368285	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kumar2022	Patient-centric smart health-care systems for handling COVID-19 variants and future pandemics: Technological review, research challenges, and future directions	Information technology can play a vital role in future smart health-care systems. Using information technology, health-care services can be improved. This improvement includes shifting the specialization or department-centric health-care services to patient-centric health-care services. This shift is necessary to have better patient experiences and providing specialized health-care services to many patients with lesser resources. In information technology, the Internet of Things (IoT) is an advanced approach for ensuring this system. In IoT, industrial IoT (IIoT), Internet of nano things, Internet of robots, Internet of patients, and Internet of medical things (IoMT) are some of the concepts important to understand the functionality of smart health-care system. In addition to IoT or its variants, other IoT-associated solutions that include blockchain technology, parallel and distributed computing approaches (cloud/fog/edge), virtualization, cybersecurity, automated software development, and smart infrastructure development have shown great enhancements in recent times. The objective of this work is to explore different information-technology-based solutions that can make a patient-centric smart health-care system feasible in nearby times. In this work, recent developments of IoT that are used to interconnect health-care objects and made technical revolutions will be explored initially. Thereafter, IoT association with other technological approaches will be explored.	https://doi.org/10.1049/PBHE034E_ch10	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kumar2022a	Behavioural cloning based RL agents for district energy management		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144629012&doi=10.1145\%2f3563357.3566165&partnerID=40&md5=2dbf87f2ed83a13e0e2e65215f0e21d3	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kumari2021	Reinforcement Learning for Multiagent-based Residential Energy Management System	In Smart Grid (SG), energy management in residential houses has gained widespread popularity with the increasing population and demand for energy. Residential Energy Management System (REMS) can lead to higher efficiency and lower operating costs for the multi-carrier energy supply (i.e., gas and electricity). Several approaches exist for single carrier REMS, however, it is quite challenging to develop a multi-carrier REMS due to the dynamic consumption environment. So, this paper proposes a MultiAgent-based Residential Energy Management scheme, i.e., MA-REM for a multi-carrier energy system. It benefits each energy component, i.e., gas and electricity with different Demand Response Program (DRP), which accelerates by employing Reinforcement Learning (RL) methodology. The proposed MA-REM scheme uses Q-learning based on a dynamic pricing mechanism for optimal energy management in REMS to reduced energy cost (14.97\%) and energy consumption. The effectiveness of the proposed MA-REM scheme is evaluated by comparing it with existing approaches in terms of energy consumption and energy cost for electricity & gas.	https://dx.doi.org/10.1109/GCWkshps52748.2021.9682182	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kumari2022	Multiagent-based secure energy management for multimedia grid communication using Q-learning		https://doi.org/10.1007/s11042-021-11491-x	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kumari2013	Software module clustering using a hyper-heuristic based multi-objective genetic algorithm	This paper presents a Fast Multi-objective Hyper-heuristic Genetic Algorithm (MHypGA) for the solution of Multi-objective Software Module Clustering Problem. Multi-objective Software Module Clustering Problem is an important and challenging problem in Software Engineering whose main goal is to obtain a good modular structure of the Software System. Software Engineers greatly emphasize on good modular structure as it is easier to comprehend, develop and maintain such software systems. In recent times, the problem has been converted into a Search-based Software Engineering Problem with multiple objectives. This problem is NP hard as it is an instance of Graph Partitioning and hence cannot be solved using traditional optimization techniques. The MHypGA is a fast and effective metaheuristic search technique for suggesting software module clusters in a software system while maximizing cohesion and minimizing the coupling of the software modules. It incorporates twelve low-level heuristics which are based on different methods of selection, crossover and mutation operations of Genetic Algorithms. The selection mechanism to select a low-level heuristic is based on reinforcement learning with adaptive weights. The efficacy of the algorithm has been studied on six real-world module clustering problems reported in the literature and the comparison of the results prove the superiority of the MHypGA in terms of quality of solutions and computational time.	https://dx.doi.org/10.1109/IAdCC.2013.6514331	Included	new_screen		4
RL4SE	Kurihara1998	Multi-agent reinforcement learning system integrating exploitation- and exploration-oriented learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961359588&doi=10.1007\%2f10693067_4&partnerID=40&md5=5b6d9e18fbb5cff56c2533da619c3ded	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kurrek2020	Reinforcement Learning Lifecycle for the Design of Advanced Robotic Systems	Machine learning is a recognised technology for problem solving and accelerates the automation by enabling systems to act independently. Cyber-physical systems based on machine learning enable factories to increase the skills of robotic systems. The lack of standardised tools and workflows for artificial intelligence (AI) rises the importance of research methodologies and frameworks for industrial application. First concepts have shown potential for a combination of holistic development methodologies and AI. We present a Reinforcement Learning Lifecycle (RLL) for the development of advanced robots. The autonomous software agent can furthermore lead to the automated optimisation of the systems. The virtual-based method speeds up learning processes, improve development and operation processes by the evaluation of multiple simulation environments. We show how an AI-based methodology assists the development of advanced robots along the product lifecycle. The first implementations show potential regarding the usability and results of the approach during different development processes.	https://dx.doi.org/10.1109/ICPS48405.2020.9274698	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Kute2021	Deep Learning and Explainable Artificial Intelligence Techniques Applied for Detecting Money Laundering-A Critical Review		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107332767&doi=10.1109\%2fACCESS.2021.3086230&partnerID=40&md5=b5e30d6539fe1967b6af0d6f99c5d31a	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kutiame2022	Application of Machine Learning Algorithms in Coronary Heart Disease: A Systematic Literature Review and Meta-Analysis		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133340402&doi=10.14569\%2fIJACSA.2022.0130620&partnerID=40&md5=8eb885cea8b19a43816f3a926e916cf6	Excluded	new_screen	E5: Other not a paper,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kutz2021	Deep learning for control of nonlinear optical systems		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105968570&doi=10.1117\%2f12.2576998&partnerID=40&md5=2dba5155c6e9ced5258aecfb281773f7	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Kuznetsov2019	Automated software vulnerability testing using in-depth training methods			Included	new_screen		4
RL4SE	Kuznetsov2019a	Automated Software Vulnerability Testing Using Deep Learning Methods	The paper provides a state of current technologies, which are used for automated vulnerability testing in software. Features of fuzzing technology (which is based on making many inputs with different mutated data) are also studied. The essence of the algorithm is the selection of input data, which are more likely to cause a failure or incorrect behavior of the software product. Deep learning algorithms are used to reduce the computational complexity of the testing process. Using a simple fuzzer and Deep Reinforcement Learning algorithm shows that the number of mutations required to detect vulnerabilities is reduced by 30\%.	https://dx.doi.org/10.1109/UKRCON.2019.8879997	Included	new_screen		4
RL4SE	Kwon2020	Late Breaking Results: Reinforcement Learning-based Power Management Policy for Mobile Device Systems	This paper presents a power management policy that exploits reinforcement learning to increase power efficiency of mobile device systems. Our Q-learning-based policy predicts a system's characteristics and learns power management controls to adapt to the system's variations. Therefore, we can flexibly manage the system power regardless of the application scenario and can achieve lower energy per QoS compared to previous dynamic voltage/frequency scaling governors. To minimize the process overhead, we implemented our power management policy as hardware; the hardware-implemented policy reduced the average latency up to 40$\times$ compared to the software-implemented policy.	https://dx.doi.org/10.1109/DAC18072.2020.9218716	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E5: Other not a paper	4
RL4SE	Kwon2021	Reinforcement Learning-Based Power Management Policy for Mobile Device Systems	This paper presents a power management policy that utilizes reinforcement learning to increase the power efficiency of mobile device systems based on a multiprocessor system-on-a-chip (MPSoC). The proposed policy predicts a system's characteristics and learns power management controls to adapt to the variations in the system. We consider the behavioral characteristics of systems that run on mobile devices under diverse scenarios. Therefore, the policy can flexibly manage the system power regardless of the application scenario and achieve lower energy consumption without compromising the user satisfaction. The average energy per unit quality of service (QoS) of the proposed policy is lower than that of the previous six dynamic voltage/frequency scaling governors by 31.66\%. Furthermore, we reduce the runtime overhead by implementing the proposed policy as hardware. We implemented the policy on the field programmable gate array (FPGA) and construct a communication interface between the central processing units (CPUs) and the hardware of the proposed policy. Decision-making by the hardware-implemented policy is 3.92 times faster than by the software-implemented policy.	https://dx.doi.org/10.1109/TCSI.2021.3103503	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Labaca-Castro2021	AIMED-RL: Exploring Adversarial Malware Examples with Reinforcement Learning		https://doi.org/10.1007/978-3-030-86514-6_3	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lafontaine2019	An Open Vibration Platform to Evaluate Postural Control using a Simple Reinforcement Learning Agent	In this paper, our research team proposes an inexpensive open vibration platform built from easily available electronic components to be used as a tool by physiotherapists in order to help them in their evaluation of the postural control of individuals at risk of postural imbalance which could lead to falls. The platform has been thought to be easily reproducible and all the code necessary to make it work is made available on the researchers' websites. In addition, a simple reinforcement learning agent has been developed and tested to automatically calibrate the vibration motors for optimal stimulation. Finally, we present in this paper pilot experiments done on 7 healthy participants (40.8 years old) to validate the proper functioning of the platform. (C) 2019 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/) Peer-review under responsibility of the Conference Program Chairs.	https://dx.doi.org/10.1016/j.procs.2019.04.029	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lakshminarayanan2015	A Generalized Reduced Linear Program for Markov Decision Processes	Markov decision processes (MDPs) with large number of states are of high practical interest. However, conventional algorithms to solve MDP are computationally infeasible in this scenario. Approximate dynamic programming (ADP) methods tackle this issue by computing approximate solutions. A widely applied ADP method is approximate linear program (ALP) which makes use of linear function approximation and offers theoretical performance guarantees. Nevertheless, the ALP is difficult to solve due to the presence of a large number of constraints and in practice, a reduced linear program (RLP) is solved instead. The RLP has a tractable number of constraints sampled from the original constraints of the ALP. Though the RLP is known to perform well in experiments, theoretical guarantees are available only for a specific RLP obtained under idealized assumptions. In this paper, we generalize the RLP to define a generalized reduced linear program (GRLP) which has a tractable number of constraints that are obtained as positive linear combinations of the original constraints of the ALP. The main contribution of this paper is the novel theoretical framework developed to obtain error bounds for any given GRLP. Central to our framework are two max-norm contraction operators. Our result theoretically justifies linear approximation of constraints. We discuss the implication of our results in the contexts of ADP and reinforcement learning. We also demonstrate via an example in the domain of controlled queues that the experiments conform to the theory.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lala2022	Model Reference Tracking Control Solutions for a Visual Servo System Based on a Virtual State from Unknown Dynamics	This paper focuses on validating a model-free Value Iteration Reinforcement Learning (MFVI-RL) control solution on a visual servo tracking system in a comprehensive manner starting from theoretical convergence analysis to detailed hardware and software implementation. Learning is based on a virtual state representation reconstructed from input-output (I/O) system samples under nonlinear observability and unknown dynamics assumptions, while the goal is to ensure linear output reference model (ORM) tracking. Secondary, a competitive model-free Virtual State-Feedback Reference Tuning (VSFRT) is learned from the same I/O data using the same virtual state representation, demonstrating the framework's learning capability. A model-based two degrees-of-freedom (2DOF) output feedback controller serving as a comparisons baseline is designed and tuned using an identified system model. With similar complexity and linear controller structure, MFVI-RL is shown to be superior, confirming that the model-based design issue of poor identified system model and control performance degradation can be solved in a direct data-driven style. Apart from establishing a formal connection between output feedback control, state feedback control and also between classical control and artificial intelligence methods, the results also point out several practical trade-offs, such as I/O data exploration quality and control performance leverage with data volume, control goal and controller complexity.	https://dx.doi.org/10.3390/en15010267	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lalwani2022	Reinforcement Learning for Security of a LDPC Coded Cognitive Radio	This paper aims to enhance the effectiveness of the present spectrum and efficiency of cognitive radio and use the reinforcement learning model to enhance its security. It incorporates a concept that detects the presence of licensed primary users in a channel and assigns channels to secondary users automatically without the need for user intervention where the primary users are not present. An LDPC decoder used at the receiver's end allows for error detection and correction considering situations where noisy channels manipulate data. The LDPC decoder and software portion of cognitive radio, that is implemented using the energy detection method, are done using LabVIEW software and the reinforcement learning model which use the deep Q-learning algorithm is developed using Python.	https://dx.doi.org/10.1007/978-981-16-7167-8_64	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lan2021	Towards Pick and Place Multi Robot Coordination Using Multi-agent Deep Reinforcement Learning	Recent advances in deep reinforcement learning are enabling the creation and use of powerful multi-agent systems in complex areas such as multi-robot coordination. These show great promise to help solve many of the difficult challenges of rapidly growing domains such as smart manufacturing. In this position paper we describe our early-stage work on the use of multi-agent deep reinforcement learning to optimise coordination in a multi-robot pick and place system. Our goal is to evaluate the feasibility of this new approach in a manufacturing environment. We propose to adopt a decentralised partially observable Markov Decision Process approach and to extend an existing cooperative game work to suitably formulate the problem as a multiagent system. We describe the centralised training/decentralised execution multi-agent learning approach which allows a group of agents to be trained simultaneously but to exercise decentralised control based on their local observations. We identify potential learning algorithms and architectures that we will investigate as a base for our implementation and we outline our open research questions. Finally we identify next steps in our research program.	https://dx.doi.org/10.1109/ICARA51699.2021.9376433	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lange2017	Bringing Gaming; VR; and AR to Life with Deep Learning		https://doi.org/10.1145/3123266.3130873	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Langer2022	A reinforcement learning approach to home energy management for modulating heat pumps and photovoltaic systems	Buildings are one of the main drivers of global energy consumption and CO2 emissions. Efficient energy management systems will have to integrate renewable energy sources with heating and/or cooling to mitigate climate change. In this study, we analyze the potential of deep reinforcement learning (DRL) to control a smart home with a modulating air-to-water heat pump, a photovoltaic system, a battery energy, and a thermal storage system for floor heating and hot water supply. We transform a mixed-integer linear program (MILP) into a DRL implementation. In our numerical analysis, we compare our results based on the deep deterministic policy gradient (DDPG) algorithm to the theoretical upper bound of the model predictive control (MPC) result under full information, as well as a practice-oriented rule-based benchmark. We show that our proposed DRL implementation outperforms the rule-based approach and achieves a self-sufficiency of 75\% with only limited comfort violations. Analyzing different DRL formulations, we conclude that domain knowledge is key to formalizing an efficient decision problem with stable results. Our input data and models, developed using the Julia programming language, are available open source.	https://dx.doi.org/10.1016/j.apenergy.2022.120020	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Langlois2003	Abalearn: A program that learns how to play abalone	The article describes ABALEARN, a self-learning Abalone program capable of automatically reaching an intermediate level of play. This is achieved without expert-labelled training examples, deep searches, and exposure to competent play. Our approach is based on a reinforcement-learning algorithm that is risk seeking, since defensive players in Abalone tend to postpone a game endlessly. We show that risk sensitivity allows a successful self-play training. Moreover, we propose a set of features that seem relevant for achieving a rather skilled level of play. We evaluate our approach using a fixed heuristic opponent as a benchmark. We pit our agents against human players on-line and compare samples of our agents at different times of training.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Laroui2022	SO-VMEC: Service offloading in virtual mobile edge computing using deep reinforcement learning	Service offloading poses interesting challenges in current and next-generation networks. The classical network optimization algorithms are still painstakingly tune heuristics to get a sufficient solution. Classical approaches use data as input in order to output near-optimal solutions. These techniques show exponential computational time and deal only with small network scale. Therefore, we are motivated by replacing this tedious process with recent learning techniques to learn the behavior of the classical optimization algorithms while enhancing both the quality of service and satisfying the resources requirements of next-generation applications. Deep reinforcement learning (DRL) and machine learning (ML) can improve service offloading and network caching. An optimal service offloading in virtual mobile edge computing (SO-VMEC) use case algorithm is proposed using integer linear programming (ILP). Moreover, a service offloading protocol is presented to support the use case. We leverage software defined networking (SDN) and network function virtualization (NFV) concepts to control and virtualize network components. Then, a DRL-based offloading is proposed to deal with dense Internet of Things (IoT) networks. Extensive evaluations and comparison to state of the art techniques are carried out. Results show the efficiency of the proposed algorithms in terms of service offloading, resource utilization, and networking.	https://dx.doi.org/10.1002/ett.4211	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Latifi2020	A Self-Governed Online Energy Management and Trading for Smart Micro/Nano-Grids	Joint energy consumption and trading management is still a major challenge in smart (micro) grids. The main goal of solving such problems is to flatten the aggregate power consumption-generation curve and increase the local direct power trading among the participants as much as possible. Here, an inclusive formulation for energy management and trading of a micro/nano-grid (M/NG) is proposed in this article. Subsequently, a holistic solution to jointly optimizing the internal energy consumption management and external local energy trading for a smart grid including several M/NGs is provided. As the problem is computationally intractable, the proposed approach involves three hierarchical stages. First, a game-theoretic online stochastic energy management model is provided with a reinforcement learning solution by which the M/NGs can schedule their power consumptions. Second, an effective incentive-compatible double-auction is formulated by which the M/NGs can directly trade with each other. Third, the central controller develops an optimal power allocation program to reduce the power transmission loss and the destructive effects of local energy trading. The simulation results validate the efficiency of the proposed framework.	https://dx.doi.org/10.1109/TIE.2019.2945280	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lau2017	The many worlds hypothesis of dopamine prediction error: implications of a parallel circuit architecture in the basal ganglia	Computational models of reinforcement learning (RL) strive to produce behavior that maximises reward, and thus allow software or robots to behave adaptively [1]. At the core of RL models is a learned mapping between 'states' situations or contexts that an agent might encounter in the world and actions. A wealth of physiological and anatomical data suggests that the basal ganglia (BG) is important for learning these mappings [2,3]. However, the computations performed by specific circuits are unclear. In this brief review, we highlight recent work concerning the anatomy and physiology of BG circuits that suggest refinements in our understanding of computations performed by the basal ganglia. We focus on one important component of basal ganglia circuitry, midbrain dopamine neurons, drawing attention to data that has been cast as supporting or departing from the RL framework that has inspired experiments in basal ganglia research over the past two decades. We suggest that the parallel circuit architecture of the BG might be expected to produce variability in the response properties of different dopamine neurons, and that variability in response profile may not reflect variable functions, but rather different arguments that serve as inputs to a common function: the computation of prediction error.	https://www.ncbi.nlm.nih.gov/pubmed/28985550	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Lauer2010	Cognitive concepts in autonomous soccer playing robots	Computational concepts of cognition, their implementation in complex autonomous systems, and their empirical evaluation are key techniques to understand and validate concepts of cognition and intelligence. In this paper we want to describe computational concepts of cognition that were successfully implemented in the domain of soccer playing robots and show the interactions between cognitive concepts, software engineering and real-time application development. Beside a description of the general concepts we will focus on aspects of perception, behavior architecture, and reinforcement learning. (C) 2010 Elsevier B.V. All rights reserved.	https://dx.doi.org/10.1016/j.cogsys.2009.12.003	Excluded	new_screen	E3: Only conceptual results are reported,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lauffenburger2021	REinforcement learning to improve non-adherence for diabetes treatments by Optimising Response and Customising Engagement (REINFORCE): Study protocol of a pragmatic randomised trial	INTRODUCTION: Achieving optimal diabetes control requires several daily self-management behaviours, especially adherence to medication. Evidence supports the use of text messages to support adherence, but there remains much opportunity to improve their effectiveness. One key limitation is that message content has been generic. By contrast, reinforcement learning is a machine learning method that can be used to identify individuals' patterns of responsiveness by observing their response to cues and then optimising them accordingly. Despite its demonstrated benefits outside of healthcare, its application to tailoring communication for patients has received limited attention. The objective of this trial is to test the impact of a reinforcement learning-based text messaging programme on adherence to medication for patients with type 2 diabetes. METHODS AND ANALYSIS: In the REinforcement learning to Improve Non-adherence For diabetes treatments by Optimising Response and Customising Engagement (REINFORCE) trial, we are randomising 60 patients with suboptimal diabetes control treated with oral diabetes medications to receive a reinforcement learning intervention or control. Subjects in both arms will receive electronic pill bottles to use, and those in the intervention arm will receive up to daily text messages. The messages will be individually adapted using a reinforcement learning prediction algorithm based on daily adherence measurements from the pill bottles. The trial's primary outcome is average adherence to medication over the 6-month follow-up period. Secondary outcomes include diabetes control, measured by glycated haemoglobin A1c, and self-reported adherence. In sum, the REINFORCE trial will evaluate the effect of personalising the framing of text messages for patients to support medication adherence and provide insight into how this could be adapted at scale to improve other self-management interventions. ETHICS AND DISSEMINATION: This study was approved by the Mass General Brigham Institutional Review Board (IRB) (USA). Findings will be disseminated through peer-reviewed journals, clinicaltrials.gov reporting and conferences. TRIAL REGISTRATION NUMBER: Clinicaltrials.gov (NCT04473326).	https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121008254&doi=10.1136\%2fbmjopen-2021-052091&partnerID=40&md5=1127131d0d2d8e167fbfd272c3bb7ff9	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lawrence2022	Deep reinforcement learning with shallow controllers: An experimental application to PID tuning	"Deep reinforcement learning (RL) is an optimization-driven framework for producing control strategies for general dynamical systems without explicit reliance on process models. Good results have been reported in simulation. Here we demonstrate the challenges in implementing a state of the art deep RL algorithm on a real physical system. Aspects include the interplay between software and existing hardware; experiment design and sample efficiency; training subject to input constraints; and interpretability of the algorithm and control law. At the core of our approach is the use of a PID controller as the trainable RL policy. In addition to its simplicity, this approach has several appealing features: No additional hardware needs to be added to the control system, since a PID controller can easily be implemented through a standard programmable logic controller; the control law can easily be initialized in a ""safe'' region of the parameter space; and the final product-a well-tuned PID controller-has a form that practitioners can reason about and deploy with confidence."	https://dx.doi.org/10.1016/j.conengprac.2021.105046	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Le2021	A Reinforcement Learning-based solution for Intra-domain Egress Selection	An ingress router often has multiple potential egress points in an extensive network where it can transmit traffic to external networks. The traditional solution is choosing the closest node (with the shortest path) to the ingress node. This paper claims the drawbacks of this approach in a flexible network system and introduces our proposal called MAB-based Egress Selection. Our approach uses several Reinforcement Learning techniques, which are commonly used to resolve Multi-Armed Bandit (MAB) problem, to allow the ingress router to periodically re-pick egress point, hence optimize the long-term performance of traffic transmission. To formalize the egress selection process as a MAB problem, we use a combined score of delay and loss representing link status as a reward. However, capturing those network metrics encounters some issues due to the distributed control and restricted local view of network nodes. For this purpose, a centralized control architecture, e.g., Software-defined Network (SDN), is a promising candidate. We applied four common algorithms, ?-greedy, Softmax, UCB1 and Single Pull UCB2 (SP-UCB2) for egress selection process. The models are evaluated in two simulated network topologies with different scenarios of network traffic condition. The experimental results show that the UCB algorithms produce the best performance, especially in busy network.	https://dx.doi.org/10.1109/HPSR52026.2021.9481846	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Le2021a	Search-Based Planning and Reinforcement Learning for Autonomous Systems and Robotics		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116753799&doi=10.1007\%2f978-3-030-77939-9_14&partnerID=40&md5=79a94aad1f85d5f2f19763278a7c39a6	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	LeThi2019	A unified DC programming framework and efficient DCA based approaches for large scale batch reinforcement learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052656030&doi=10.1007\%2fs10898-018-0698-y&partnerID=40&md5=3915acfa4d80df6d8e14fff0f2250f19	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Lea1992	Fuzzy logic in autonomous orbital operations		https://www.scopus.com/inward/record.uri?eid=2-s2.0-38249014948&doi=10.1016\%2f0888-613X\%2892\%2990016-S&partnerID=40&md5=1e08e5293c5de04c61198eba9181efc6	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lecina2017	Adaptive simulations, towards interactive protein-ligand modeling	Modeling the dynamic nature of protein-ligand binding with atomistic simulations is one of the main challenges in computational biophysics, with important implications in the drug design process. Although in the past few years hardware and software advances have significantly revamped the use of molecular simulations, we still lack a fast and accurate ab initio description of the binding mechanism in complex systems, available only for up-to-date techniques and requiring several hours or days of heavy computation. Such delay is one of the main limiting factors for a larger penetration of protein dynamics modeling in the pharmaceutical industry. Here we present a game-changing technology, opening up the way for fast reliable simulations of protein dynamics by combining an adaptive reinforcement learning procedure with Monte Carlo sampling in the frame of modern multi-core computational resources. We show remarkable performance in mapping the protein-ligand energy landscape, being able to reproduce the full binding mechanism in less than half an hour, or the active site induced fit in less than 5 minutes. We exemplify our method by studying diverse complex targets, including nuclear hormone receptors and GPCRs, demonstrating the potential of using the new adaptive technique in screening and lead optimization studies.	https://www.ncbi.nlm.nih.gov/pubmed/28814780	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lee2021	Deep Q-network-based auto scaling for service in a multi-access edge computing environment	In 5G networks, it is necessary to provide services while meeting various service requirements, such as high data rates and low latency, in response to dynamic network conditions. Multi-access edge computing (MEC) is a promising concept to meet these requirements. The MEC environment enables service providers to deploy their low latency services that are composed of multiple components. However, operating a service manually and attempting to satisfy the quality of service (QoS) requirements is difficult because many factors need to be considered in an MEC scenario. In this paper, we propose an auto-scaling method using deep Q-networks (DQN), which is a reinforcement learning algorithm, to resize the number of instances assigned to service. In our evaluation, compared to other baseline methods, the proposed approach maintains the appropriate number of instances effectively in response to dynamic traffic change while satisfying QoS and minimizing the cost of operating the service in the MEC environment. The proposed method was implemented as a module running in OpenStack and published as open-source software.	https://dx.doi.org/10.1002/nem.2176	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lee2021a	Deep Q?network?based auto scaling for service in a multi?access edge computing environment		https://doi.org/10.1002/nem.2176	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lee2021b	Visualization of Deep Reinforcement Autonomous Aerial Mobility Learning Simulations	This demo abstract presents the visualization of deep reinforcement learning (DRL)-based autonomous aerial mobility simulations. In order to implement the software, Unity-RL is used and additional buildings are introduced for urban environment. On top of the implementation, DRL algorithms are used and we confirm it works well in terms of trajectory and 3D visualization.	https://dx.doi.org/10.1109/INFOCOMWKSHPS51825.2021.9484462	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E5: Other not a paper	4
RL4SE	Lee2020	Hosting AI/ML Workflows on O-RAN RIC Platform	O-RAN Alliance suggests new interfaces and architectures that can bring openness and intelligence to currently vendor-dependent Radio Access Network (RAN) environments. Newly defined O-RAN specifications guarantee the interoperability of Network Entities from different vendors to enable multivendor environments, and O-RAN Working Group 2 (WG2) focuses on AI/ML workflow architectures and specifications that enable applications of artificial intelligence (AI) and machine learning (ML) in O-RAN environments. Additionally, O-RAN Software Community (SC) realizes what O-RAN Alliance defines by developing open-source software. In this paper, we introduce an AI/ML workflow that is based on WG2 AI/ML specifications and implemented with open-source software from O-RAN SC, Acumos and Open Network Automation Platform (ONAP). Acumos Framework was used to generate and package ML models to be deployed and executed in O-RAN RAN Intelligence Controller (RIC) and components of ONAP provided monitoring and arbitration needed to operate the workflow. Along with the example of operating AI/ML workflows on O-RAN environment, we discuss shortcomings we encountered during this process and potential solutions. Finally, we suggest capabilities for RIC platform to facilitate other useful ML technologies such as reinforcement learning and federated learning as well as GPU support.	https://dx.doi.org/10.1109/GCWkshps50303.2020.9367572	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Lee2019	WISEMOVE: A Framework to Investigate Safe Deep Reinforcement Learning for Autonomous Driving	WISEMOVE is a platform to investigate safe deep reinforcement learning (DRL) in the context of motion planning for autonomous driving. It adopts a modular architecture that mirrors our autonomous vehicle software stack and can interleave learned and programmed components. Our initial investigation focuses on a state-of-the-art DRL approach from the literature, to quantify its safety and scalability in simulation, and thus evaluate its potential use on our vehicle.	https://dx.doi.org/10.1007/978-3-030-30281-8_20	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lee2020a	Flexible Reinforcement Learning Framework for Building Control using EnergyPlus-Modelica Energy Models		https://doi.org/10.1145/3427773.3427873	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lee2018	Deep Reinforcement Learning in Continuous Action Spaces: a Case Study in the Game of Simulated Curling	Many real-world applications of reinforcement learning require an agent to select optimal actions from continuous spaces. Recently, deep neural networks have successfully been applied to games with discrete actions spaces. However, deep neural networks for discrete actions are not suitable for devising strategies for games where a very small change in an action can dramatically affect the outcome. In this paper, we present a new self-play reinforcement learning framework which equips a continuous search algorithm which enables to search in continuous action spaces with a kernel regression method. Without any hand-crafted features, our network is trained by supervised learning followed by self-play reinforcement learning with a high-fidelity simulator for the Olympic sport of curling. The program trained under our framework outperforms existing programs equipped with several hand-crafted features and won an international digital curling competition.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lee2008	A Motivation-Based Action-Selection-Mechanism Involving Reinforcement Learning	An action-selection-mechanism (ASM) has been proposed to work as a fully connected finite state machine to deal with sequential behaviors as well as to allow a state in the task program to migrate to any state in the task, in which a primitive node in association with a state and its transitional conditions can be easily inserted/deleted. Also, such a primitive node can be learned by a shortest path-finding-based reinforcement learning technique. Specifically, we define a behavioral motivation as having state-dependent value as a primitive node for action selection, and then sequentially construct a network of behavioral motivations in such a way that the value of a parent node is allowed to flow into a child node by a releasing mechanism. A vertical path in a network represents a behavioral sequence. Here, such a tree for our proposed ASM can be newly generated and/or updated whenever a new behavior sequence is learned. To show the validity of our proposed ASM, experimental results of a mobile robot performing the task of pushing-a-box-into-a-goal (PBIG) will be illustrated.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lehman2019	Application of computational intelligence for command & control of unmanned air systems		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083942601&doi=10.2514\%2f6.2019-0158&partnerID=40&md5=d513672cc5fe061b7a62495fff0762e4	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lei2015	Web Service Composition Based on Reinforcement Learning	How we manage Web services depends on how we understand their variable parts and invariable parts. Studying them separately could make Web service research much easier and make our software architecture much more loose-coupled. We summarize two variable parts that affect Web service compositions: uncertain invocation results and uncertain quality of services. These uncertain factors affect success rate of service composition. Previous studies model the Web service problem as a planning problem, while this problem is considered as an uncertain planning problem in this paper. Specifically, we use Partially Observable Markov Decision Process to deal with the uncertain planning problem for service composition. According to the uncertain model, we propose a reinforcement learning method, which is an uncertainty planning method, to compose web services. The proposed method does not need to know complete information of services, instead it uses historical data and estimates the successful possibilities that services are composed together with respect to service outcomes and QoS. Simulation experiments verify the validity of the algorithm, and the results also show that our method improves the success rate of the service composition.	https://dx.doi.org/10.1109/ICWS.2015.103	Included	new_screen		4
RL4SE	Leiser2022	AI in Art: Simulating the Human Painting Process	While AI is being used more and more to generate images, the generation usually does not resemble a human painting process. However, for applications in the field of art, it is useful to simulate the human painting process-e.g. in relation to location, order, shape, color and contours of the areas being painted in each step. Such applications are for example when a robot paints a picture or a program teaches humans to paint. Consequently, in this paper we evaluate and compare different approaches to simulate the human painting process. Additionally, we present our solution for this task which is based on a combination of filters and semantic segmentation. In our survey, this approach was rated as better and more realistic than the most realistic approach for this task so far which is a reinforcement learning approach: In all surveyed categories-location, order, shape, color and contours of the areas being painted in each step-always a significant majority of the participants prefers our approach to simulate the human painting process. When we displayed two time-lapse videos with the painting process of Edvard Munch's The Scream in parallel, even 79\% found our generated process more realistic than the reinforcement learning-based process.	https://dx.doi.org/10.1007/978-3-030-95531-1_20	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Leng2008	A role-oriented BDI framework for real-time multiagent teaming			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Leng2010	A role-based cognitive architecture for multi-agent teaming		https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956313497&doi=10.1007\%2f978-3-642-13526-2_11&partnerID=40&md5=ae623a45f36a29d08fd95a92e4ce135c	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Leng2008a	A role-based framework for multi-agent teaming	Multi-agent teaming is a key research field of multi-agent systems. BDI (Belief, Desire, and Intension) architecture has been widely used to solve complex problems. The theory of joint behavior has been widely used to solve the team level optimisation problems. Due to the inherent complexity of real-time and dynamic environments, it is often extremely complex and difficult to formally specify the joint behavior of the team a priori. This paper presents a role-based BDI framework to facilitate cooperation and coordination problems. This BDI framework is extended and based on the commercial agent software development environment known as JACK Teams. A real-time 2D simulation environment known as soccerbots has been used to investigate the difficulties of multi-agent teaming. The layered architecture has been used to group the agents' competitive and cooperative behaviors, which can be learned through experience by using the reinforcement learning techniques.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Leng2022	Learning to Transmit Fresh Information in Energy Harvesting Networks	We study age of information (AoI) minimization in an ad hoc network consisting of energy harvesting transmitters that are scheduled to send status updates to their intended receivers. The transmission scheduling with power allocation problem over a communication session is first studied assuming apriori knowledge of channel state information, harvested energy, and update packet arrivals, i.e., the offline setting. The global optimal scheduling policy in this case is the solution of a mixed integer linear program which is known to be computationally hard. We propose a supervised-learning-based algorithm to mitigate the high computational complexity. A bidirectional recurrent neural network that interprets user scheduling as a time-series classification problem is trained and tested to achieve near-optimal AoI. Next, we consider online scheduling and power allocation with causal knowledge of the system state, which is an infinite-state Markov decision problem. In this case, the related reinforcement learning problem is solved by a model-free on-policy deep reinforcement learning, where the actor-critic algorithm with deep neural network function approximation is implemented. Comparable AoI to the optimal is demonstrated and faster runtime of learning solvers is observed, verifying the efficacy of learning in terms of both optimality and computational energy efficiency for AoI-focused scheduling and resource allocation problems in wireless networks.	https://dx.doi.org/10.1109/TGCN.2022.3190007	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Leonetti2010	Improving the performance of complex agent plans through reinforcement learning			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Levinson2000	Chess Neighborhoods, Function Combination, and Reinforcement Learning			Excluded	new_screen	E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for	4
RL4SE	Levy2019	Learning multi-level hierarchies with hindsight			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Levy2020	A Simple Platform for Reinforcement Learning of Simulated Flight Behaviors		https://doi.org/10.1007/978-3-030-64313-3_22	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2022	PolyRhythm: Adaptive Tuning of a Multi-Channel Attack Template for Timing Interference	As cyber-physical systems have become increasingly complex, rising computational demand has led to the ubiquitous use of multicore processors in embedded environments. Size, Weight, Power, and Cost (SWaP-C) constraints have pushed more processes onto shared platforms, including real-time tasks with deadline requirements. To prevent temporal interference among tasks running concurrently or in parallel in such systems, many operating systems provide priority-based scheduling and enforce processor reservations based on Worst-Case Execution Time (WCET) estimates. However, shared resources (both architectural components and data structures within the operating system) provide channels through which these constraints can be broken. Prior work has demonstrated that malicious execution by one or more processes can cause significant delays, leading to potential deadline misses in victim tasks. In this paper, we introduce PolyRhythm, a three-phase attack template that combines primitives across multiple architectural and kernel-based channels: (1) it uses an offline genetic algorithm to tune attack parameters based on the target hardware and OS platform; then (2) it performs an online search for regions of the attack parameter space where contention is most likely; and finally (3) it runs the attack primitives, using online reinforcement learning to adapt to dynamic execution patterns in the victim task. On a representative platform (Raspberry Pi 3B) Poly Rhythm outperforms prior work, achieving significantly more slowdown. As we show for several hardware/software platforms, Poly Rhythm also allows us to characterize the extent to which interference can occur; this helps to inform better estimates of execution times and overheads, towards preventing deadline misses in real-time systems.	https://dx.doi.org/10.1109/RTSS55097.2022.00028	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2021	Multi-Agent and Cooperative Deep Reinforcement Learning for Scalable Network Automation in Multi-Domain SD-EONs	The service provisioning in multi-domain software-defined elastic optical networks (SD-EONs) is an interesting but difficult problem to tackle, because the basic problem of lightpath provisioning, i.e., the routing and spectrum assignment (RSA), is $\mathcal {NP}$ -hard, and each domain is owned and operated by a different carrier. Therefore, even though numerous RSA heuristics have been proposed, there does not exist a universal winner that can always achieve the lowest blocking probability in all the scenarios of a multi-domain SD-EON. This motivates us to revisit the inter-domain provisioning problem in this paper by leveraging deep reinforcement learning (DRL). Specifically, we propose DeepCoop, which is an inter-domain service framework that uses multiple cooperative DRL agents to achieve scalable network automation in a multi-domain SD-EON. DeepCoop employs a DRL agent in each domain to optimize intra-domain service provisioning, while a domain-level path computation element (PCE) is introduced to obtain the sequence of the domains to go through for each lightpath request. By sharing a restricted amount of information among each other, the DRL agents can make their decisions distributedly. To ensure scalability and universality, we design the action space of each DRL agent based on well-known RSA heuristics, and architect the agents based on the soft actor-critic (SAC) scenario. We run extensive simulations to evaluate DeepCoop, and the results show that DeepCoop can adapt to the dynamic environment in a multi-domain SD-EON to always select the best RSA heuristic for minimizing blocking probability, and it outperforms the existing algorithms on inter-domain provisioning in various scenarios. Moreover, we verify that the distributed training implemented in DeepCoop ensures its universality and scalability (i.e., its training and operation do not depend on the topology of the SD-EON).	https://dx.doi.org/10.1109/TNSM.2021.3102621	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2020	DeepCoop: Leveraging Cooperative DRL Agents to Achieve Scalable Network Automation for Multi-Domain SD-EONs	We design DeepCoop to realize service provisioning in multi-domain software-defined elastic optical networks (SD-EONs) with cooperative deep reinforcement learning (DRL) agents.		Excluded	new_screen	E5: Other not a paper,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2020a	Trajectory smoothing method using reinforcement learning for computer numerical control machine tools	Tool-path codes output by computer-aided manufacturing software for high-speed machining are composed of discontinuous G01 line segments. The discontinuity of these tool movements causes computer numerical control (CNC) inefficiency. To achieve high-speed continuous motion, corner smoothing algorithms based on pre-planning methods are widely used. However, it is difficult to optimize smoothing trajectories in real-time systems. To obtain smooth trajectories efficiently, this paper proposes a neural network-based direct trajectory smoothing method. An intelligent neural network agent outputs servo commands directly based on the current tool path and running state in every cycle. To achieve direct control, motion feature and reward models were built, and reinforcement learning was used to train the neural network parameters without additional experimental data. The proposed method provides higher cutting efficiency than the local and global smoothing algorithms. Given its simple structure and low computational demands, it can easily be applied to real-time CNC systems.	https://dx.doi.org/10.1016/j.rcim.2019.101847	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2013	Research of adaptive program design algorithm based on agent			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2012	Agent-based modeling for trading wind power with uncertainty in the day-ahead wholesale electricity markets of single-sided auctions	This paper, for the first time, adopts agent-based simulation approach to investigate the bidding optimization of a wind generation company in the deregulated day-ahead electricity wholesale markets, by considering the effect of short-term forecasting accuracy of wind power generation. Two different wind penetration levels (12\% and 24\%) are investigated and compared. Based on MATPOWER 4.0 software package and the 9-bus 3-generator power system defined by Western System Coordinating Council, the agent-based models are built and run under the uniform price auction rule and locational marginal pricing mechanism. Each generation company could learn from its past experience and improves its day-ahead strategic offers by using Variant Roth-Erev reinforcement learning algorithm. The results clearly demonstrate that improving wind forecasting accuracy helps increase the net earnings of the wind generation company. Also, the wind generation company can further increase its net earnings with the adoption of learning algorithm. Besides, it is verified that increasing wind penetration level within the investigation range can help reduce the market clearing price. Furthermore, it is also demonstrated that agent-based simulation is a viable modeling tool which can provide realistic insights for the complex interactions among different market participants and various market factors. (C) 2012 Elsevier Ltd. All rights reserved.	https://dx.doi.org/10.1016/j.apenergy.2012.04.022	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2022a	Efficient Provision of Service Function Chains in Overlay Networks Using Reinforcement Learning	Software-Defined Networking (SDN) and Network Functions Virtualization (NFV) technologies facilitate deploying Service Function Chains (SFCs) at clouds in efficiency and flexibility. However, it is still challenging to efficiently chain Virtualized Network Functions (VNFs) in overlay networks without knowledge of underlying network configurations. Although there are many deterministic approaches for VNF placement and chaining, they have high complexity and depend on state information of substrate networks. Fortunately, Reinforcement Learning (RL) brings opportunities to alleviate this challenge as it can learn to make suitable decisions without prior knowledge. Therefore, in this article, we propose an RL approach for efficient SFC provision in overlay networks, where the same VNFs provided by multiple vendors are with different performance. Specifically, we first formulate the problem into an Integer Linear Programming (ILP) model for benchmarking. Then, we present the online SFC path selection into a Markov Decision Process (MDP) and propose a corresponding policy-gradient-based solution. Finally, we evaluate our proposed approach with extensive simulations with randomly generated SFC requests and a real-world video streaming dataset, and implement an emulation system for feasibility verification. Related results demonstrate that performance of our approach is close to the ILP-based method and better than deep Q-learning, random, and load-least-greedy methods.	https://dx.doi.org/10.1109/TCC.2019.2961093	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2020b	A deep reinforcement learning based approach for home energy management system		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086255941&doi=10.1109\%2fISGT45199.2020.9087647&partnerID=40&md5=e76557be91a5d735cac68b38adad1959	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2022b	Anti-collision Trajectory Planning for Satellite Formation Reconstruction Based on Deep Reinforcement Learning	This paper proposes an optimal trajectory planning method for satellite formation reconstruction based on deep reinforcement learning. To begin, the action space, state space, and reward function of satellite formation reconstruction are created, with the collision avoidance constraint taken into account. Second, the algorithm's essential parameters' learning rate and appropriate noise are determined. What's more, the Unity program is developed to create the training environment, so the real satellite dynamics model are embed in the environment. The optimal trajectory of formation satellite reconstruction obtained by this method can better meet the constraints such as collision avoidance, and the calculation speed is fast, which makes the autonomous real-time reconstruction of formation satellite possible. Finally, 1 simulation example is carried out to verify the proposed algorithm, showing that the formation reconfiguration task can be executed successfully while achieving rapid convergence.	https://dx.doi.org/10.23919/CCC55666.2022.9901660	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2022c	Automatic Driving Edge Scene Generation Method Based on Scene Dynamics and Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137139867&doi=10.19562\%2fj.chinasae.qcgc.2022.07.004&partnerID=40&md5=fa0a85decb4cf81b265545282e8ae1e4	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2020c	Mitigating Negative Impacts of Read Disturb in SSDs		https://doi.org/10.1145/3410332	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2021a	Delay-Aware VNF Scheduling: A Reinforcement Learning Approach With Variable Action Set	Software defined networking (SDN) and network function virtualization (NFV) are the key enabling technologies for service customization in next generation networks to support various applications. In such a circumstance, virtual network function (VNF) scheduling plays an essential role in enhancing resource utilization and achieving better quality-of-service (QoS). In this paper, the VNF scheduling problem is investigated to minimize the makespan (i.e., overall completion time) of all services, while satisfying their different end-to-end (E2E) delay requirements. The problem is formulated as a mixed integer linear program (MILP) which is NP-hard with exponentially increasing computational complexity as the network size expands. To solve the MILP with high efficiency and accuracy, the original problem is reformulated as a Markov decision process (MDP) problem with variable action set. Then, a reinforcement learning (RL) algorithm is developed to learn the best scheduling policy by continuously interacting with the network environment. The proposed learning algorithm determines the variable action set at each decision-making state and captures different execution time of the actions. The reward function in the proposed algorithm is carefully designed to realize delay-aware VNF scheduling. Simulation results are presented to demonstrate the convergence and high accuracy of the proposed approach against other benchmark algorithms.	https://dx.doi.org/10.1109/TCCN.2020.2988908	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2019	Reinforcement Learning Based VNF Scheduling with End-to-End Delay Guarantee	Network slicing has been recognized as a promising technology to achieve service customization for supporting various applications in fifth-generation (5G) networks. As one of its key enablers, network function virtualization (NFV) holds great potential to reduce service provisioning cost and improve resource utilization. With NFV, a service can be implemented by chaining the required virtual network functions (VNFs). In this paper, we study the scheduling of the VNFs to minimize makespan (i.e., overall completion time) of all services, while satisfying their diverse end-to-end (E2E) delay requirements. The problem is formulated as a mixed integer linear program (MILP), which is NP-hard. To address the NP-hardness of the MILP with high efficiency and high accuracy, we model the problem as a Markov decision process (MDP) with variable action sets and leverage a reinforcement learning (RL) algorithm to find its optimal scheduling policy. A Q-learning based algorithm is developed to address the challenges of variable action sets and varying action execution time of the MDP. A specific reward function is designed to realize delay-guaranteed VNF scheduling. Simulation results are provided, showing that the proposed approach outperforms the benchmark heuristic algorithms and can achieve near-optimal performance in terms of the makespan.	https://dx.doi.org/10.1109/ICCChina.2019.8855889	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2022d	Entropy-based Reinforcement Learning for computation offloading service in software-defined multi-access edge computing		https://doi.org/10.1016/j.future.2022.06.002	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2022e	Dynamic Target Following Control for Autonomous Vehicles with Deep Reinforcement Learning	Target following control is an operating condition for autonomous driving in vehicle platooning and service robots. But the model-based target following control is challenging due to the nonlinearity and uncertainty of the vehicle dynamics model. To address this problem, this paper proposes a dynamic target following control method based on deep reinforcement learning for autonomous vehicles. Its primary purpose is to make the car follow dynamic targets smoothly and reasonably. Only collected state data sets are used to learn control policy. The policy learning consists of two steps. Firstly, we describe a specific following situation in which we design a dense reward function for realizing following control. We use empirical data to pre-train with deep neural networks. Then the deep deterministic policy gradient (DDPG) algorithm is used to train the control policy to achieve continuous optimization. We conducted experiments with Carla simulation software, including the comparison with the supervised learning-based approach. The results show the effectiveness of our approach.	https://dx.doi.org/10.1109/ICARM54641.2022.9959167	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2022f	Simulation of Ground-air Cooperative Combat Based on Reinforcement Learning in Localization Environment		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132956170&doi=10.12382\%2fbgxb.2022.A005&partnerID=40&md5=61cc671d1bf8ea7caef3ab08b56ed0cb	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2022g	Associative Memory Based Experience Replay for Deep Reinforcement Learning		https://doi.org/10.1145/3508352.3549387	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2020d	Deep Reinforcement Learning-Based Vehicle Driving Strategy to Reduce Crash Risks in Traffic Oscillations	The primary objective of this study is to propose a deep reinforcement learning-based driving strategy for individual vehicles to mitigate oscillations and optimize traffic safety in stop-and-go waves. A deep deterministic policy gradient (DDPG)-based driving strategy, which requires information that is directly obtained by in-vehicle sensors, is proposed for system performance optimization. Two typical scenarios were simulated based on simulation software (SUMO): (i) the leading vehicle slowed down according to real trajectory data to produce one oscillation; (ii) the leading vehicle conducted several abrupt decelerations with various degrees of disturbance to produce multiple oscillations. The DDPG agents interacted with the SUMO platform to determine the optimal acceleration of vehicles that can reduce crash risks in various stop-and-go waves. The results showed that the proposed DDPG-based driving strategy successfully reduced the crash risk by 68.9\%-78.4\%. Scenarios with different penetration rates of DDPG agents and in various flow rates were compared to test the effect of the proposed strategy. The DDPG-based driving strategy reduced crash risk more with the increase of penetration rate and this strategy performed better when applied in the scenario with a high traffic flow rate. The proposed strategy is compared with the adaptive cruise control and jam-absorbing driving strategies. Results showed the proposed strategy outperformed other oscillation mitigating strategies in reducing crash risks.	https://dx.doi.org/10.1177/0361198120937976	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E3: Only conceptual results are reported	4
RL4SE	Li2022h	RLOps: Development Life-Cycle of Reinforcement Learning Aided Open RAN	Radio access network (RAN) technologies continue to evolve, with Open RAN gaining the most recent momentum. In the O-RAN specifications, the RAN intelligent controllers (RICs) are software-defined orchestration and automation functions for the intelligent management of RAN. This article introduces principles for machine learning (ML), in particular, reinforcement learning (RL) applications in the O-RAN stack. Furthermore, we review the state-of-the-art research in wireless networks and cast it onto the RAN framework and the hierarchy of the O-RAN architecture. We provide a taxonomy for the challenges faced by ML/RL models throughout the development life-cycle: from the system specification to production deployment (data acquisition, model design, testing and management, etc.). To address the challenges, we integrate a set of existing MLOps principles with unique characteristics when RL agents are considered. This paper discusses a systematic model development, testing and validation life-cycle, termed: RLOps. We discuss fundamental parts of RLOps, which include: model specification, development, production environment serving, operations monitoring and safety/security. Based on these principles, we propose the best practices for RLOps to achieve an automated and reproducible model development process. At last, a holistic data analytics platform rooted in the O-RAN deployment is designed and implemented, aiming to embrace and fulfil the aforementioned principles and best practices of RLOps.	https://dx.doi.org/10.1109/ACCESS.2022.3217511	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2014	Adaptive mechanism based on shared learning in multi-agent system			Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2021b	Deep Reinforcement Learning and Game Theory for Computation Offloading in Dynamic Edge Computing Markets	As a promising paradigm, computation offloading technology can offload computing tasks to multi-access edge computing (MEC) servers, which is an appealing choice for resource-constrained terminal devices to reduce their computational effort. However, due to limited resources, one crucial research challenge for computation offloading is to design the appropriate offloading policy to determine which tasks should be offloaded in some complex circumstances. In this paper, we study the offloading decision problem in a software-defined networking (SDN) driven MEC environment with multiple users and multiple servers. To ensure that end-users do not abuse the computing resources in the MEC system, we formulate the profit of MEC servers as our optimization objective. We jointly optimize the selection of MEC servers, the size of offloading data, and the price of MEC computing service to maximize the profit of MEC servers. However, considering the dynamic and stochastic of end-users, it is challenging to obtain the optimal policy in such a MEC environment. We apply deep reinforcement learning (DRL) and Game theory to our proposed approach. Specifically, we propose a proximal policy optimization (PPO) reinforcement learning framework to tackle the selection of MEC servers. Secondly, a two-step optimization problem is formulated to determine the size of offloading data and the pricing of computing services. The optimal values of those two were determined by achieving the Nash equilibrium of the strategy game between end-users. Extensive simulation results prove that our proposal has a better performance than existing solutions in convergence time and stability.	https://dx.doi.org/10.1109/ACCESS.2021.3109132	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2020e	An Attention Based Deep Reinforcement Learning Method for Virtual Network Function Placement	Network Function Virtualization (NFV) decouples network functions from the dedicated hardware and produces Virtual Network Functions (VNFs) in software. The VNFs are placed on hardware and are linked together to build a service chain. The design of an efficient VNF placement algorithm is crucial. The rapid development of machine learning, especially Deep Reinforcement Learning (Deep RL), allows us to address this problem. In this paper, we present an attention based sequence to sequence Deep RL method for VNF placement. Our approach is a policy based method optimized by REINFORCE with baseline. Our model receives physical hosts and service chain as input and produces the output sequence step by step with attention encoder and decoder. We demonstrate that our method outperforms the existing learning method and greedy heuristic.	https://dx.doi.org/10.1109/ICCC51575.2020.9345041	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2022i	A deep reinforcement learning-based approach for the residential appliances scheduling	This paper investigates the optimal real-time residential appliances scheduling of individual owner when participating in the demand response (DR) program. The proposed method is novel since we cast the optimization problem to an intelligent deep reinforcement learning (DRL) framework, which avoids solving a specific optimization model directly when facing dynamic operation conditions induced by the outdoor temperature, electricity price and resident's behavior. We consider the scheduling of power-shiftable, time-shiftable and deferrable appliances for the optimization of profit and satisfaction rate of resident. The optimization problem is first modeled as a Markov decision process and then solved by a model-free entropy-based DRL algorithm. Unlike traditional model-based methods which rely on accurate knowledge of parameters and physical models that are difficult to obtain in practice, the proposed method can develop real-time near-optimal control behavior by interacting with the environment and learning from data, which avoids the error caused by the simplification and assumption when building physical model. The proposed scheduling algorithm also achieves better tradeoff between the profit and the satisfaction rate than deterministic DRL algorithm owing to the introduction of the entropy term. Simulation results using real-world data demonstrate the effectiveness of the proposed method. (c) 2022 Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/). Peer-review under responsibility of the scientific committee of the 2021 The 2nd International Conference on Power Engineering, ICPE, 2021.	https://dx.doi.org/10.1016/j.egyr.2022.02.181	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2022j	AgentFuzz: Fuzzing for Deep Reinforcement Learning Systems	In recent years, deep reinforcement learning (DRL) technology has developed rapidly, and the application of DRL has been extended to many fields such as game gaming, au-tonomous driving, financial transactions, and robot control. As DRL applications expand and enrich, quality assurance of DRL software is increasingly important, especially in safety -critical areas. Therefore, it is necessary and urgent to adequately test DRL models to ensure the reliability and security of DRL systems. However, due to fundamental differences, traditional software testing methods cannot be directly applied to D RL systems. To bridge this gap, we introduce a new DRL system testing framework in this proposal, which aims to generate various test cases that can cause D RL systems to fail. The proposed testing framework is the first fuzzing framework for systematically testing DRL systems which we call AgentFuzz.	https://dx.doi.org/10.1109/ISSREW55968.2022.00049	Included	new_screen		4
RL4SE	Li2019a	Deep Reinforcement Learning Based Residential Demand Side Management With Edge Computing	Residential demand side management (DSM) is a promising technique to improve the stability and reduce the cost of power systems. However, residential DSM is facing challenges under the ongoing paradigm shift of computation, such as edge computing. With the proliferation of smart appliances (e.g., appliances with computing and data analysis capabilities) and high-performance computing devices (e.g., graphics processing units) in the households, we expect surging residential energy consumption caused by computation. Therefore, it is important to schedule edge computing as well as traditional energy consumption in a smart way, especially when the demand for computation and thus for electricity occurs during the peak hours of electricity consumption.In this paper, we investigate an integrated home energy management system (HEMS) who participates in a DSM program and is equipped with an edge computing server. The HEMS aims to maximize the home owner's expected total reward, defined as the reward from completing edge computing tasks minus the cost of electricity consumption, the cost of computation offloading to the cloud, and the penalty of violating the DSM requirements. The particular DSM program considered in this paper, which is a widely-adopted one, requires the household to reduce certain amount of energy consumption within a specified time window. In contrast to well-studied real-time pricing, such a DSM program results in a long-term temporal interdependency (i.e., of a few hours) and thus high-dimensional state space in our formulated Markov decision processes. To address this challenge, we use deep reinforcement learning, more specifically Deep Deterministic Policy Gradient, to solve the problem. Experiments show that our proposed scheme achieves significant performance gains over reasonable baselines.	https://dx.doi.org/10.1109/SmartGridComm.2019.8909778	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2021c	Integrating Future Smart Home Operation Platform With Demand Side Management via Deep Reinforcement Learning	Residential demand side management (DSM) is a promising technique in smart grids to improve the power system robustness and to reduce the energy cost. However, the ongoing paradigm shift of computation, such as mobile edge computing for smart home, poses a big challenge to residential DSM. Therefore, it is important to schedule the new smart home computing tasks and traditional DSM in a smart way. In this paper, we investigate an integrated home energy management system (HEMS) that participates in a DSM program and implements smart home computation tasks by offloading tasks with the help of a Smart Home Operation Platform (SHOP). The goal of HEMS is to maximize the user's expected total reward, defined as the reward from completing computing tasks minus the cost of energy consumption, execution delay, running the SHOP servers, and the penalty of violating the DSM requirements. We solve this task scheduling based DSM problem using a deep reinforcement learning method. The DSM program considered in this paper requires the household to reduce a certain amount of energy consumption within a specified time window, which, in stark contrast to the well-studied real-time pricing, results in a long-term temporal interdependence and thus a high-dimensional state space in our formulated problem. To address this challenge, we use the Deep Deterministic Policy Gradient (DDPG) method to characterize the high-dimensional state space and action space, which uses deep neural networks to estimate the state and to generate the action. Experimental results show that our proposed method achieves better performance gains over reasonable baselines.	https://dx.doi.org/10.1109/TGCN.2021.3073979	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2020f	Service Chain Mapping Algorithm Based on Reinforcement Learning	Network function virtualization integrates different types of dedicated network equipment into standard industry IT server, storage and switch equipment, enabling network functions traditionally implemented using specific equipment to use software running on IT industry standard server hardware, thereby enhancing system flexibility. Using the organic combination of NFV and software-defined network technologies, an software defined Smart grid communication network can be constructed, so that the network functions of service function chaining can be implemented on general-purpose equipment, and end-to-end services are transformed into a set of sequentially connected VNFs, which can effectively deploy and manage service function chains. Service provision and server resource utilization will be affected by SFC mapping. In order to ensure the reasonable use of network resources and the QoS of SFC, the research on SFC mapping algorithms is particularly important. In this paper, we propose a service chain mapping algorithm based on reinforcement learning, which aims to learn by the system status and the feedback value given by the mapped environment and then finally determine the actual deployment location of each virtual function node in the SFC. The comparison and analysis with other algorithms show that the SFC mapping algorithm proposed in this paper can adjust the feedback value function to optimize the load balance of the system and reduce the SFC average link delay in different network topologies.	https://dx.doi.org/10.1109/IWCMC48107.2020.9148460	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2019b	Reinforcement Learning of Code Search Sessions	Searching and reusing online code is a common activity in software development. Meanwhile, like many general-purposed searches, code search also faces the session search problem: in a code search session, the user needs to iteratively search for code snippets, exploring new code snippets that meet his/her needs and/or making some results highly ranked. This paper presents Cosoch, a reinforcement learning approach to session search of code documents (code snippets with textual explanations). Cosoch is aimed at generating a session that reveals user intentions, and correspondingly searching and reranking the resulting documents. More specifically, Cosoch casts a code search session into a Markov decision process, in which rewards measuring the relevances between the queries and the resulting code documents guide the whole session search. We have built a dataset, say CosoBe, from StackOverflow, containing 103 code search sessions with 378 pieces of user feedback. We have also evaluated Cosoch on CosoBe. The evaluation results show that Cosoch achieves an average NDCG@3 score of 0.7379, outperforming StackOverflow by 21.3\%.	https://dx.doi.org/10.1109/APSEC48747.2019.00068	Included	new_screen		4
RL4SE	Li2008	Reinforcement learning application for dynamic trust modeling in large-scale open distributed systems			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2012a	A continuous estimation of distribution algorithm by evolving graph structures using reinforcement learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866856884&doi=10.1109\%2fCEC.2012.6256481&partnerID=40&md5=72e2a6a5cb9fc464a38a8a1ec0669265	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2022k	FuzzBoost: Reinforcement Compiler Fuzzing		https://doi.org/10.1007/978-3-031-15777-6_20	Included	conflict_resolution		4
RL4SE	Li2020g	Improved Feature Learning: A Maximum-Average-Out Deep Neural Network for the Game Go		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083889058&doi=10.1155\%2f2020\%2f1397948&partnerID=40&md5=19f1a213a0700d6bbdb1e83a05277d13	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2019c	A Reinforcement Learning Model Based on Temporal Difference Algorithm	In some sense, computer game can be used as a test bed of artificial intelligence to develop intelligent algorithms. The paper proposed a kind of intelligent method: a reinforcement learning model based on temporal difference (TD) algorithm. And then the method is used to improve the playing power of the computer game of a special kind of chess. JIU chess, also called Tibetan Go chess, is mainly played in places where Tibetan tribes gather. Its play process is divided two sequential stages: preparation and battle. The layout at preparation is vital for the successive battle, even for the final winning. Studies on Tibetan JIU chess have focused on Bayesian network based pattern extraction and chess shape based strategy, which do not perform well. To address the low chess power of JIU chess from the view of artificial intelligence, we developed a reinforcement learning model based on temporal difference (TD) algorithm for the preparation stage of JIU. First, the search range was limited within a 6 $\times$ 6 area at the center of the chessboard, and the TD learning architecture was combined with chess shapes to construct an intelligent environmental feedback system. Second, optimal state transition strategies were obtained by self-play. In addition, the results of the reinforcement learning model were output as SGF files, which act as a pattern library for the battle stage. The experimental results demonstrate that this reinforcement learning model can effectively improve the playing strength of JIU program and outperform the other methods.	https://dx.doi.org/10.1109/ACCESS.2019.2938240	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2019d	A Middle Game Search Algorithm Applicable to Low-Cost Personal Computer for Go		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085505062&doi=10.1109\%2fACCESS.2019.2937943&partnerID=40&md5=0f82d373a06f1481bd08e033606f4f76	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2015	Evolving directed graphs with artificial bee colony algorithm		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940489825&doi=10.1109\%2fISDA.2014.7066282&partnerID=40&md5=e0bff19705ca3e2eaa9926bf10e51d2c	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2022l	Gobang Game Algorithm Based on Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123610605&doi=10.1007\%2f978-981-16-9247-5_36&partnerID=40&md5=287930775b6e923fc2b026735ce6599c	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2010	SRL-based trust predicting model used in multi-Agent systems			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2022m	Deep Reinforcement Learning for Penetration Testing of Cyber-Physical Attacks in the Smart Grid	The fast expansion of interconnectivity in cyber-physical critical infrastructures like smart grids has given rise to concerning exposures and vulnerabilities. Although penetration testing (PT) has been an effective approach to searching for vulnerabilities in software, devices, and networks from the attacker's view, the strong cyber-physical coupling in these large-scale infrastructures has made it challenging to manually pinpoint critical vulnerabilities, particularly at system levels due to the complexity, dimensionality, and uncertainty therein. To better protect the security of cyber-physical systems, this paper proposes a deep reinforcement learning (DRL)-based PT framework to efficiently and adaptively identify critical vulnerabilities in smart grids. Using replay attacks as an example, the paper models the attack as a Markov Decision Process with three actions - stop, record, and replay - to learn the optimal timing and ordering of replays in different operating scenarios. A cyber-physical co-simulation platform with dedicated simulators for the physical part, cyber part, control part, and attacker part of a smart distribution grid was developed as a sandbox environment to train the DRL agent. Scenarios with different levels of difficulty are tested to validate the learning capability and performance in finding critical attack paths of the DRL-based PT. The simulation results show that DRL-based PT can learn to find the optimal attack path against system stability when the grid is under high load demand, solar power generation, and weather variation. These results are promising first steps toward a highly customizable framework to pen-test complex cyber-physical systems with automatic DRL agents and various attack schemes.	https://dx.doi.org/10.1109/IJCNN55064.2022.9892584	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2021d	OpenRDW: A Redirected Walking Library and Benchmark with Multi-User, Learning-based Functionalities and State-of-the-art Algorithms	Redirected walking (RDW) is a locomotion technique that guides users on virtual paths, which might vary from the paths they physically walk in the real world. Thereby, RDW enables users to explore a virtual space that is larger than the physical counterpart with near-natural walking experiences. Several approaches have been proposed and developed; each using individual platforms and evaluated on a custom dataset, making it challenging to compare between methods. However, there are seldom public toolkits and recognized benchmarks in this field. In this paper, we introduce OpenRDW, an open-source library and benchmark for developing, deploying and evaluating a variety of methods for walking path redirection. The OpenRDW library provides application program interfaces to access the attributes of scenes, to customize the RDW controllers, to simulate and visualize the navigation process, to export multiple formats of the results, and to evaluate RDW techniques. It also supports the deployment of multi-user real walking, as well as reinforcement learning-based models exported from TensorFlow or PyTorch. The OpenRDW benchmark includes multiple testing conditions, such as walking in size varied tracking spaces or shape varied tracking spaces with obstacles, multiple user walking, etc. On the other hand, procedurally generated paths and walking paths collected from user experiments are provided for a comprehensive evaluation. It also contains several classic and state-of-the-art RDW techniques, which include the above mentioned functionalities.	https://dx.doi.org/10.1109/ISMAR52148.2021.00016	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Li2022n	Feudal Multiagent Reinforcement Learning for Interdomain Collaborative Routing Optimization		https://doi.org/10.1155/2022/1231979	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Li2019e	A Dyna-Q-Based Solution for UAV Networks Against Smart Jamming Attacks	Unmanned aerial vehicle (UAV) networks have a wide range of applications, such as in the Internet of Things (IoT), 5G communications, and so forth. However, the communications between UAVs and UAVs to ground control stations mainly use radio channels, and therefore these communications are vulnerable to cyberattacks. With the advent of software-defined radio (SDR), smart attacks that can flexibly select attack strategies according to the defender's state information are gradually attracting the attention of researchers and potential attackers of UAV networks. The smart attack can even induce the defender to take a specific defense strategy, causing even greater damage. Inspired by symmetrical thinking, a solution using a software-defined network (SDN) to combat software-defined radio was proposed. We propose a network architecture which uses dual controllers, including a UAV flight controller and SDN controller, to achieve collaborative decision-making. Built on the top of the SDN, the state information of the whole network converges quickly and is fitted to an environment model used to develop an improved Dyna-Q-based reinforcement learning algorithm. The improved algorithm integrates the power allocation and track planning of UAVs into a unified action space. The simulation data showed that the proposed communication solution can effectively avoid smart jamming attacks and has faster learning efficiency and higher convergence performance than the compared algorithms.	https://dx.doi.org/10.3390/sym11050617	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Liang2018	RLlib: Abstractions for Distributed Reinforcement Learning	Reinforcement learning (RL) algorithms involve the deep nesting of highly irregular computation patterns, each of which typically exhibits opportunities for distributed computation. We argue for distributing RL components in a composable way by adapting algorithms for top-down hierarchical control, thereby encapsulating parallelism and resource requirements within short-running compute tasks. We demonstrate the benefits of this principle through RLlib: a library that provides scalable software primitives for RE These primitives enable a broad range of algorithms to be implemented with high performance, scalability, and substantial code reuse. RLlib is available as part of the open source Ray project(1).		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Liang2006	An intelligent agent-based self-evolving maintenance and operations reasoning system	Joint Strike Fighter (JSF) autonomic logistics seeks to reduce development, production, and ownership costs for the next generation fighter aircraft by increasing system reliability, while reducing maintenance requirements to essential levels. Prognostics and health management (PHM), which enables maintenance to be planned on the basis of actual component or system health state, represents a key component within the autonomic logistics system architecture. The challenge is to develop advanced technology to integrate PHM information from a variety of different sources into a dynamically evolving knowledge base. Prototype software described herein and referred to as the self evolving maintenance and operations reasoning system (SEMOR), utilizes intelligent software agents in JADE, both model and case-based reasoners and reinforcement learning modules. The fundamental approach enables PHM reasoning to be effective in the absence of field experience through the model-based reasoning module as well as realize the benefits of case based reasoning as a PHM knowledge base grows. A reinforcement learning (RL) module is employed to evolve a maintenance integrated model (MIM), a database containing PHM and maintenance relationships and attributes. Intelligent software agents are used in their true capacity to negotiate decisions regarding database adaptation, maintenance, and logistics actions prior to human review. This paper presents the software system design, describes key technical components, provides a demonstration scenario and concludes with remarks on the technical challenges and future developments	https://dx.doi.org/10.1109/AERO.2006.1656106	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Liang2022	RLF: Directed Fuzzing based on Deep Reinforcement Learning	Fuzzy testing is widely used in the field of vulnerability mining due to its ease of use and low false positive errors. However, traditional fuzzy tests suffer from low efficiency, blind test sample generation, and huge resource consumption. In order to solve these problems, we propose RLF??a deep reinforcement learning-based directed fuzzing. RLF uses program instrumentation to obtain the execution path and calculates the distance between the path and the target. RLF will apply a deep reinforcement learning network that performs optimization to guide the selection of test samples basing on the information. Experimental results show that the method has better efficiency than other fuzzy testing tools in terms of vulnerability reproduction and significantly improves the orientation of fuzzy tests.	https://dx.doi.org/10.1109/MLCR57210.2022.00032	Included	new_screen		4
RL4SE	Liang2020	Toward Fast Platform-Aware Neural Architecture Search for FPGA-Accelerated Edge AI Applications		https://doi.org/10.1145/3400286.3418240	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Liang2011	Walking parameters design of biped robots based on reinforcement learning	Biped walking pattern is one of the most difficult problems in the humanoid robot area, and there exists no ideal algorithm for a generalized walking scenario. Recently, some methods have been proposed, including trajectory planning, passive dynamic walk. In this paper, based on the solution of inverse kinematics of a leg by combining analysis method with numerical method, trajectory planning method is used to implement the humanoid robot walking skill in a 3D simulation environment. In order to get the walking parameters automately, reinforcement learning is studied and implemented by the train system of Apollo3D program, and the training algorithm is well tested in the RoboCup3D simulation platform.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ligeiro2014	Monitoring applications		https://doi.org/10.1016/j.asoc.2014.08.021	Included	conflict_resolution		4
RL4SE	Lin2020	Modeling 3D Shapes by Reinforcement Learning		https://doi.org/10.1007/978-3-030-58607-2_32	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lin2022	Smart Underwater Pollution Detection Based on Graph-Based Multi-Agent Reinforcement Learning Towards AUV-Based Network ITS	The exploitation/utilization of marine resources and the rapid development of urbanization along coastal cities result in serious marine pollution, especially underwater diffusion pollution. It is a non-trivial task to detect the source of diffusion pollution, such that the disadvantageous effect of the pollution can be reduced. With the vision of 6G framework, we employ Autonomous Underwater Vehicle (AUV) flock and introduce the concept of AUV-based network. In particular, we utilize the Software-Defined Networking (SDN) technique to update the controllability of the AUV-based network, leading to the paradigm of SDN-enabled multi-AUVs network Intelligent Transportation Systems (SDNA-ITS). For SDNA-ITS, we utilize artificial potential field theories to model the control model. To optimize the system output, we introduce the graph-based Soft Actor-Critic (SAC) algorithm, i.e., a category of Multi-Agent Reinforcement Learning (MARL) mechanism where each AUV can be regarded as a node in a graph. In particular, we improve the optimization model based on Centralized Training Decentralized Execution (CTDE) architecture with the assistance of the SDN controller, by which each AUV can efficiently adjust its speed towards the diffusion source. Further, to achieve exact path planning for detecting the diffusion source, a dynamic detection scheme is proposed to output the united control policy to schedule the SDNA-ITS dynamically. Simulation results demonstrate that our approaches are available to detect the underwater diffusion source when the actual scenario is taken into account and perform better than some recent research products.	https://dx.doi.org/10.1109/TITS.2022.3162850	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lin2021	Blockchain and Deep Reinforcement Learning Empowered Spatial Crowdsourcing in Software-Defined Internet of Vehicles	Owing to its benefits such as flexibility, scalability, and interoperability, Software-Defined Networking (SDN) has been incorporated into Internet of Vehicles (IoV) to cope with the increasing demands of vehicular applications. The integration of SDN and IoV, namely SDN-IoV, can enrich many new applications for intelligent transportation such as traffic monitoring, smart navigation, and self-driving. The spatial crowdsourcing technology has been adopted as an effective data collection and processing method that is the premise of various SDN-IoV applications. However, as huge amounts of data are generated in spatial crowdsourcing services, the data privacy and security has become a key challenge for SDN-IoV. To overcome abovementioned challenge, a Deep Reinforcement Learning (DRL) and Blockchain empowered Spatial Crowdsourcing System (DB-SCS) is proposed. In DB-SCS, we design an improved multi-blockchain structure and a blockchain-based hierarchical task management method, which divide the spatial tasks into different categories according to the privacy requirements and the areas of the task and then decompose different categories of tasks and task receivers into sub-blockchains. While guaranteeing the data privacy, DB-SCS can also enhance the spatial crowdsourcing performance by using the proposed DRL-based management strategy to dynamically select the consensus algorithm, block size, and block generation rule. Extensive simulation experiments demonstrate that the DB-SCS can obtain high throughput, low overhead, and data privacy under various SDN-IoV scenarios.	https://dx.doi.org/10.1109/TITS.2020.3025247	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lin2021a	Edge Learning for Low-Latency Video Analytics: Query Scheduling and Resource Allocation	Low-latency and accuracy-guaranteed video analytics is essential to many delay-sensitive camera-based applications. Analyzing video frames on edge nodes in proximity can effectively reduce the response delay compared with cloud-based solutions. However, the computation and bandwidth resources on an edge node are always limited. In this paper, we design a joint video query scheduling and resource allocation problem based on an edge coordinated architecture, in order to properly accommodate real-time video queries on end cameras, the edge nodes, or the cloud. This problem is challenging in that 1) the arrivals of video queries with different resource requirements are unknown in advance and 2) the design space (of both query scheduling and resource allocation) to provision video queries varies over time. Taking the two-fold uncertainty into consideration, we formulate the query provision problem as a mix integer non-linear program which is NP-hard and not solved directly. To deal with the NP-hardness and the absence of future information, the problem is re-formulated as a Markov decision process, which can leverage historical query information to make decisions about scheduling and resource allocation. The transformed problem calls for an online solution that can efficiently adapt to the dynamic design space. Hence, we propose an edge-coordinated reinforcement learning algorithm to continuously learn from the environment, and make decisions for query scheduling and resource allocation to achieve low latency and accurate video analytics. Extensive simulation results demonstrate the advantages of the proposed algorithm in latency and accuracy.	https://dx.doi.org/10.1109/MASS52906.2021.00041	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lin2021b	Distributed Learning for Vehicle Routing Decision in Software Defined Internet of Vehicles	With the increasing number of vehicles, the traffic congestion is becoming more and more serious. In order to alleviate such a problem, this article considers transmission and inference delay of cloud centralized computing in the software defined Internet of Vehicles (SDIoV), and builds a new SDIoV architecture based on edge intelligence, for supporting real-time vehicle routing decision through distributed multi-agent reinforcement learning model. Then, a software defined device collaboration optimization method is designed to improve the efficiency of distributed training. Combined with multi-agent reinforcement learning, a distributed-learning-based vehicle routing decision algorithm (DLRD) is proposed to adaptively adjust vehicle routing online. The performed simulations show that the DLRD can successfully realize real-time routing decision for vehicles and alleviate traffic congestion with the dynamic changes of the road environment.	https://dx.doi.org/10.1109/TITS.2020.3023958	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lin2020a	A Taxonomy and Survey of Power Models and Power Modeling for Cloud Servers	Due to the increasing demand of cloud resources, the ever-increasing number and scale of cloud data centers make their massive power consumption a prominent issue today. Evidence reveals that the behaviors of cloud servers make the major impact on data centers' power consumption. Although extensive research can be found in this context, a systematic review of the models and modeling methods for the entire hierarchy (from underlying hardware components to the upper-layer applications) of the cloud server is still missing, which is supposed to cover the relevant studies on physical and virtual cloud server instances, server components, and cloud applications. In this article, we summarize a broad range of relevant studies from three perspectives: power data acquisition, power models, and power modeling methods for cloud servers (including bare-metal, virtual machine (VM), and container instances). We present a comprehensive taxonomy on the collection methods of server-level power data, the existing mainstream power models at multiple levels from hardware to software and application, and commonly used methods for modeling power consumption including classical regression analysis and emerging methods like reinforcement learning. Throughout the work, we introduce a variety of models and methods, illustrating their implementation, usability. and applicability while discussing the limitations of existing approaches and possible ways of improvement. Apart from reviewing existing studies on server power models and modeling methods, we further figure out several open challenges and possible research directions, such as the study on modeling the power consumption of lightweight virtual units like unikernel and the necessity of further explorations toward empowering server power estimation/prediction with machine learning. As power monitoring is drawing increasing attention from cloud service providers (CSPs), this survey provides useful guidelines on server power modeling and can be inspiring for further research on energy-efficient data centers.	https://dx.doi.org/10.1145/3406208	Excluded	new_screen	E5: Other not a paper,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lin2022a	Reinforcement learning-based image exposure reconstruction for homography estimation	The homography matrix plays a vital role in robotics and computer vision applications, but mainstream estimators are usually customized for specific problems and are sensitive to image quality. In response to this situation, a reinforced agent is proposed to improve image quality by sequentially reconstructing the exposure. First, the gamma correction theory is employed to design a nonlinear exposure adjustment function so that the agent's action is not bound to additional hardware or software. Then, the agent is designed as consisting of a metric network and a Q network that are trained under the reinforcement learning framework. When a black-box nondifferentiable homography estimator is given, the metric network can map the image into its corresponding embedding space, and the Q network can further determine an exposure value to produce pleasing images for it. Comprehensive experiments are conducted on homography samples generated from the public aerial DOTASet. After reconstructing the exposure of the original input, all selected estimators can obtain more accurate results. It also reveals that visually satisfactory images may not always be the best choice for homography estimation.	https://dx.doi.org/10.1007/s10489-022-04287-5	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lindner2021	Positioning of the Robotic Arm Using Different Reinforcement Learning Algorithms	Robots are programmed using either the on-line mode, in which the robot programmer manually controls the movement of the robot indicating individual trajectory points or the off mode, in which the programmer enters the program code with predefined trajectory points. Both methods are not easy to be successfully implemented in practice, which is why the research on the development of self-learning methods can be useful. In this paper, for the robot's positioning task, the four Reinforcement Learning (RL) algorithms in six combinations are investigated. At first, the basics of these algorithms are described. Then they are used in positioning control of the robot's arm model and the evaluation of positioning accuracy, motion trajectory, and the number of steps required to achieve the goal is taken into account. The simulation results are recorded. The same tests were repeated in laboratory conditions, in which the Mitsubishi robot was controlled. The simulation results are compared with results obtained in reality. Positive results that have been obtained indicate, that the RL algorithms can be successfully applied for the learning of positioning control of a robot arm.	https://dx.doi.org/10.1007/s12555-020-0069-6	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lindskog2006	Transient calcium and dopamine increase PKA activity and DARPP-32 phosphorylation	Reinforcement learning theorizes that strengthening of synaptic connections in medium spiny neurons of the striatum occurs when glutamatergic input (from cortex) and dopaminergic input (from substantia nigra) are received simultaneously. Subsequent to learning, medium spiny neurons with strengthened synapses are more likely to fire in response to cortical input alone. This synaptic plasticity is produced by phosphorylation of AMPA receptors, caused by phosphorylation of various signalling molecules. A key signalling molecule is the phosphoprotein DARPP-32, highly expressed in striatal medium spiny neurons. DARPP-32 is regulated by several neurotransmitters through a complex network of intracellular signalling pathways involving cAMP (increased through dopamine stimulation) and calcium (increased through glutamate stimulation). Since DARPP-32 controls several kinases and phosphatases involved in striatal synaptic plasticity, understanding the interactions between cAMP and calcium, in particular the effect of transient stimuli on DARPP-32 phosphorylation, has major implications for understanding reinforcement learning. We developed a computer model of the biochemical reaction pathways involved in the phosphorylation of DARPP-32 on Thr34 and Thr75. Ordinary differential equations describing the biochemical reactions were implemented in a single compartment model using the software XPPAUT. Reaction rate constants were obtained from the biochemical literature. The first set of simulations using sustained elevations of dopamine and calcium produced phosphorylation levels of DARPP-32 similar to that measured experimentally, thereby validating the model. The second set of simulations, using the validated model, showed that transient dopamine elevations increased the phosphorylation of Thr34 as expected, but transient calcium elevations also increased the phosphorylation of Thr34, contrary to what is believed. When transient calcium and dopamine stimuli were paired, PKA activation and Thr34 phosphorylation increased compared with dopamine alone. This result, which is robust to variation in model parameters, supports reinforcement learning theories in which activity-dependent long-term synaptic plasticity requires paired glutamate and dopamine inputs.	https://www.ncbi.nlm.nih.gov/pubmed/16965177	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ling1993	Learning to control dynamic systems with automatic quantization		https://doi.org/10.1007/3-540-56602-3_153	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Ling2003	Automated transit headway control via adaptive signal priority	This paper reports on a study that developed a next-generation Transit Signal Priority (TSP) strategy, Adaptive TSP, that controls adaptively transit operations of high frequency routes using traffic signals, thus automating the operations control task and relieving transit agencies of this burden. The underlying algorithm is based on Reinforcement Learning (RL), an emerging Artificial Intelligence method. The developed RL agent is responsible for determining the best duration of each signal phase such that transit vehicles can recover to the scheduled headway taking into consideration practical phase length constraints. A case study was carried out by employing the microscopic traffic simulation software Paramics to simulate transit and traffic operations at one signalized intersection along the King Streetcar route in downtown Toronto. The results show that the control policy learned by the agent could effectively reduce the transit headway deviation and causes smaller disruption to cross street traffic compared with the existing unconditional transit signal priority algorithm.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Liston2021	Entropy Based Exploration in Cognitive Radio Networks using Deep Reinforcement Learning for Dynamic Spectrum Access	This paper details the practical design of a Cognitive Radio network which uses multi-agent Deep Reinforcement Learning for dynamic spectrum access. Each network node evaluates a neural network model to determine when it can transmit and on what frequency channel. The models are trained offline in simulation to mitigate slow online training time. Furthermore, we propose the use of entropy-based-exploration to dynamically determine when more training is required in the wireless network. Unlike previous work that has only considered similar techniques in theory and simulation, we present over-the-air measurement results for the throughput and channel utilization collected in a large-scale software-defined radio testbed.	https://dx.doi.org/10.1109/WAMICON47156.2021.9444294	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Liu2021	DRL-OR: Deep Reinforcement Learning-based Online Routing for Multi-type Service Requirements	Emerging applications raise critical QoS requirements for the Internet. The improvements of flow classification technologies, software defined networks (SDN), and programmable network devices make it possible to fast identify users' requirements and control the routing for fine-grained traffic flows. Meanwhile, the problem of optimizing the forwarding paths for traffic flows with multiple QoS requirements in an online fashion is not addressed sufficiently. To address the problem, we propose DRL-OR, an online routing algorithm using multi-agent deep reinforcement learning. DRL-OR organizes the agents to generate routes in a hop-by-hop manner, which inherently has good scalability. It adopts a comprehensive reward function, an efficient learning algorithm, and a novel deep neural network structure to learn an appropriate routing policy for different types of flow requirements. To guarantee the reliability and accelerate the online learning process, we further introduce safe learning mechanism to DRL-OR. We implement DRL-OR under SDN architecture and conduct Mininet-based experiments by using real network topologies and traffic traces. The results validate that DRL-OR can well satisfy the requirements of latency-sensitive, throughput-sensitive, latency-throughput-sensitive, and latency-loss-sensitive flows at the same time, while exhibiting great adaptiveness and reliability under the scenarios of link failure, traffic change, and partial deployment.	https://dx.doi.org/10.1109/INFOCOM42981.2021.9488736	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Liu2012	AGIOT: A Model of the Internet of Things Used in Agriculture	Nowadays, more and more researchers focus on the Internet of Things (IoT), which blurs the line between virtual and real worlds. However, few of them concern how to realize this kind of applications. In order to do some further study on the software implementation, in this paper, we propose a model of Internet of things used in agriculture, named AGIOT. A new four-layer architecture of the IoT application is established and some key issues in system implementation are discussed. Further more, a new middle layer framework is presented based on service oriented architecture (SOA), which makes users access to real-time data easily through a high-level abstract interface based on web services. In order to increase the yield and improve the quality of plant, an intelligent algorithm based on reinforcement learning is introduced. The model we proposed is feasible.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Liu2022	MoveRL: To a Safer Robotic Reinforcement Learning Environment		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123430801&doi=10.1007\%2f978-3-030-93842-0_14&partnerID=40&md5=32156f2092887d3599dec1cb9ba52bd9	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Liu2022a	AI-Driven Intelligent Vehicle Behavior Decision in Software Defined Internet of Vehicle	Intelligent driving technology plays a key role in reducing road traffic accidents and ensuring driving, where the vehicle behavior decision making capability largely determines the driving performance of intelligent vehicles. At this point, more research is focused on enhancing vehicle environment identification and vehicle control capabilities, with decision-making systems receiving less attention. In order to improve the accuracy of intelligent vehicle behavior decision making and ensure the active safety of path planning, this paper firstly establishes an edge intelligence based software-defined Internet of Vehicles (ESIOV) architecture. Then, a POMDP-based intelligent vehicle behavior decision model is designed using the time-series iterative property of partially observable Markov decision process (POMDP). Finally, a reinforcement learning vehicle behaviour decision (ERVBD) algorithm based on edge intelligence is proposed to ensure the accuracy of intelligent vehicle behavior decision results and improve the decision speed. The simulation results show that ERVBD can successfully implement vehicle behavioural decision making, enabling intelligent vehicles to anticipate collision risks and adopt reasonable avoidance strategies in real time.	https://dx.doi.org/10.1109/CoDIT55151.2022.9803919	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Liu2019	Automated clash resolution of rebar design in RC joints using multi-agent reinforcement learning and BIM		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071461989&doi=10.22260\%2fisarc2019\%2f0123&partnerID=40&md5=6bdb0a07adaa13fee87306e3c12e10ed	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Liu2020	Towards automated clash resolution of reinforcing steel design in reinforced concrete frames via Q-learning and building information modeling		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078657649&doi=10.1016\%2fj.autcon.2019.103062&partnerID=40&md5=75a8e45bc1ea87845fcdec29f4b64a91	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Liu2021a	Automatic Joint Optimization of Algorithm-Level Compression and Compiler-Based Acceleration with Reinforcement Learning for DNN in Edge Devices	More accurate machine learning models often require more memory cost and more software-hardware co-adaption efforts for deployments on resource-constrained devices. Model compression techniques and deep learning compiler are developed to reduce the memory cost and latency. However, current methods require tremendous engineering efforts to optimize the model manually. This paper introduces a jointly learning based framework to perform the compression task and the acceleration task simultaneously. The joint optimization method auto-tunes the algorithm-level compression and compiler-based acceleration with reinforcement learning. The experiment results demonstrate that we compress the model by a factor of 2 or 8, and accelerate the optimization up to 30 times using our learning framework.	https://dx.doi.org/10.1109/IJCNN52387.2021.9533729	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Liu2022b	Research on Offloading and Selection Scheme Based on SWIPT Terminals of Moblie Edge Computing	This paper combines Simultaneous Wireless Information and Power Transfer technology and Mobile Edge Computing technology, and proposes a system model of multi-terminal, multi-server, and relays. Under the background of time-varying channel and time-varying servers' price, we derive the expressions of calculation rate and calculation cost of the system. The deep reinforcement learning algorithm and convex optimization algorithm are used to select the DRL offloading scheme, DRL selection scheme and time allocation. The purpose is to maximize the computing rate and minimize the computing cost when the collected energy is limited. The simulation results verify the effectiveness of the theoretical analysis and algorithm. The algorithm proposed in this paper not only reduces the program execution delay significantly, but also obtains the near optimal calculation rate and the near optimal calculation cost.	https://dx.doi.org/10.1109/ICICN56848.2022.10006487	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Liu2022c	Research on Multi-Terminal's AC Offloading Scheme and Multi-Server's AC Selection Scheme in IoT	Mobile Edge Computing (MEC) technology and Simultaneous Wireless Information and Power Transfer (SWIPT) technology are important ones to improve the computing rate and the sustainability of devices in the Internet of things (IoT). However, the system models of most relevant papers only considered multi-terminal, excluding multi-server. Therefore, this paper aims at the scenario of IoT with multi-terminal, multi-server and multi-relay, in which can optimize the computing rate and computing cost by using deep reinforcement learning (DRL) algorithm. Firstly, the formulas of computing rate and computing cost in proposed scenario are derived. Secondly, by introducing the modified Actor-Critic (AC) algorithm and convex optimization algorithm, we get the offloading scheme and time allocation that maximize the computing rate. Finally, the selection scheme of minimizing the computing cost is obtained by AC algorithm. The simulation results verify the theoretical analysis. The algorithm proposed in this paper not only achieves a near-optimal computing rate and computing cost while significantly reducing the program execution delay, but also makes full use of the energy collected by the SWIPT technology to improve energy utilization.	https://dx.doi.org/10.3390/e24101357	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Liu2020a	Automated clash resolution for reinforcement steel design in concrete frames via Q-learning and Building Information Modeling	The design of reinforcing steel bars (rebars) is critical to reinforced concrete (RC) structures. Generally, a good number of rebars are required by a design code, particularly at member connections. As such, rebar clashes (i.e., collisions and congestions) would be inevitable. It would be impractical, labor-intensive, and error-prone to avoid all possible clashes manually or even using standard design software. The building information modeling (BIM) technology has been utilized by the present architecture, engineering, and construction (ACE) industry for clash-free rebar designs. However, most existing BIM-based approaches offer the clash resolution strategy for moving components with an optimization algorithm, and are only applicable to the RC structures with regular shapes. In particular, the optimized path of rebars cannot be adjusted to avoid the obstacles, thus limiting the practical applications. Furthermore, most existing studies lack the learning from design code and constructibility constraints to realize automatic and intelligent arrangement and adjustment of rebars for avoiding the obstacles encountered in complex RC joints and frame structures. Considering these shortcomings, the authors have recently proposed an immediate reward-based multi-agent reinforcement learning (MARL) system with BIM, towards automatic clash-free rebar designs of RC joints without clashes. However, as the immediate reward is required in the MARL system for guiding the learning of a rebar design, it will not succeed in clash-free rebar designs of complex RC structures where immediate reward is often unavailable. In this study, this study further extends the previous work with Q-learning (a model-free reinforcement learning algorithm) for more realistic path planning considering both immediate and delayed rewards in clash-free rebar designs for real-world RC structures. In particular, the rebar design problem is treated as a path-planning problem of multi-agent system, where each rebar is deemed as an intelligence reinforcement learning agent. Next, by employing the Q-learning as the reinforcement learning engine, the particular form of state, action, and immediate and delayed rewards for the reinforcement MARL for automatic rebar designs considering more actual constructible constraints and design codes can be developed. Comprehensive experiments on three typical beam-column joints and a two-story RC building frame were conducted to evaluate the efficiency of the proposed method. The study results of paths of rebar designs, success rates, and average time confirm that the proposed framework with MARL and BIM is effective and efficient.	https://dx.doi.org/10.1016/j.autcon.2019.103062	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Liu2021b	Deep Reinforcement Learning based Adaptive Transmission Control in Vehicular Networks	Efficient transmission control is a challenging issue in vehicular networks due to the highly dynamic network environment. In this paper, we propose a Deep reinforcement learning based adaptive Transmission Scheduling Mechanism (DTSM), which is able to adaptively select different transmission control policies based on the current network status and the history data learning. In particular, we first introduce the adaptive transmission scheduling units (ATSU) in both Software-Defined Vehicular Networking (SDVN) controllers and the corresponding base stations. Based on this architecture, we formulate a mathematical model for optimal decision-making in SDVN controllers. Besides, in ATSUs, we proposed a deep Q-learning based transmission control method to dynamically adapt to the time-varying vehicular network scenarios. Simulation results verify that the proposed DTSM solution outperforms the single transmission control method of four existing benchmarks (e.g., TcpVegas, TcpBic, TcpWestwood, TcpVeno) in terms of average throughput and round-trip time.	https://dx.doi.org/10.1109/VTC2021-Fall52928.2021.9625318	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Liu2021c	Mask Synthesis using Machine Learning Software and Hardware Platforms	Inspired by many success stories of machine learning (ML) in a broad range of artificial intelligence (AI) applications, both industrial and academic researchers are now actively developing ML solutions for challenging problems in computational lithography. In this work, we explore the possibility of utilizing ML software and hardware platforms for mask synthesis applications. Specifically, we demonstrate a standalone mask synthesis flow that runs entirely on the TensorFlow ML platform with a reinforcement learning (RL) approach and GPU acceleration. We will describe the architecture of our ML mask synthesis framework that comprises separable and interchangeable components including neural network (NN)-based 3D mask, imaging and resist models. We will discuss the readiness of these components and present the proof-of-concept evaluation results of the proposed ML mask synthesis framework.	https://dx.doi.org/10.1117/12.2551816	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Liu2020b	Automated Clash Free Rebar Design in Precast Concrete Exterior Wall via Generative Adversarial Network and Multi-agent Reinforcement Learning	The adoption of precast concrete elements (PCEs) are becoming popular in civil infrastructures. Since quality of connections determines the structure property, design of rebar in PCEs is a mandatory stage in constructions. Due to large number of rebar, complicated shapes of PCEs and complicated rules for arrangement, it is labor-intensive and error-prone for designers to avoid all clashes even using computer software. With the aid of BIM, it is desirable to have an automated and clash-free rebar design. Taking this cue, we introduce a framework with generative adversarial network (GAN) and multi-agent reinforcement learning (MARL) for generating design and automatically avoiding clash of rebar in PCES. We use GAN to generate 2D rebar designs. Then, 2D rebar designs are transformed into digital environments for MARL. In addition, layout of rebar is modelled as path planning of agents in MARL. An illustrative example is presented to test the proposed framework.	https://dx.doi.org/10.1007/978-3-030-20454-9_54	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Liu2021d	Deep Reinforcement Learning for Communication Flow Control in Wireless Mesh Networks	Wireless mesh network (WMN) is one of the most promising technologies for Internet of Things (IoT) applications because of its self-adaptive and self-organization nature. To meet different performance requirements on communications in WMNs, traditional approaches always have to program flow control strategies in an explicit way. In this case, the performance of WMNs will be significantly affected by the dynamic properties of underlying networks in real applications. With providing a more flexible solution in mind, in this article, for the first time, we present how we can apply emerging Deep Reinforcement Learning (DRL) on communication flow control in WMNs. Moreover, different from a general DRL based networking solution, in which the network properties are pre-defined, we leverage the adaptive nature of WMNs and propose a self-adaptive DRL approach. Specifically, our method can reconstruct a WMN during the training of a DRL model. In this way, the trained DRL model can capture more properties of WMNs and achieve better performance. As a proof of concept, we have implemented our method with a self-adap-tive Deep Q-learning Network (DQN) model. The evaluation results show that the presented solution can significantly improve the communication performance of data flows in WMNs, compared to a static benchmark solution.	https://dx.doi.org/10.1109/MNET.011.2000303	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Liu2018	Energy Optimization and Fault Tolerance to Embedded System Based on Adaptive Heterogeneous Multi-Core Hardware Architecture	Energy optimization and fault recovery are significant to ensure the embedded devices to work persistently and reliably. To achieve the energy optimization, a reconfigurable heterogeneous multi-core hardware platform is designed. Based on this hardware platform, a multi-core energy-efficient scheduling mechanism using reinforcement learning algorithm to search for the optimal scheduling solution is proposed. By the above hardware and software collaborative optimization mechanism, the energy cost of embedded system can be decreased significantly. To perform the fault recovery, a multi-core control-flow fault recovery mechanism is researched. This mechanism uses the Petri net model to detect the multi-core control-flow faults, and then recover these faults by a hardware-based quick recovery technique. The experimental results showed the energy cost of EMWSN could be optimized by more than 20\% comparing to the traditional multi-core system. In addition, about 90\% multi-core control-flow faults could be recovered, and the recovery time was nearly 40\% less than the software-based recovery technique CFCSS.	https://dx.doi.org/10.1109/QRS-C.2018.00063	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Liu2019a	A New Algorithm of the Best Path Selection Based on Machine Learning	This paper proposes and designs a best path selection algorithm, which can solve the problem of path planning for intelligent driving vehicles in the case of restricted driving, traffic congestions and accidents. We tried to solve the problem under these emergency situations in path planing process for there's no driver in intelligent driving vehicle. We designed a new method of the best path selection with length priority based on the prior knowledge applied reinforcement learning strategy, and improved the search direction setting of A* shortest path algorithm in the program. This best path planing algorithm can effectively help different types of intelligent driving vehicles to select the best path in the traffic network with limited height, width and weight, accidents and traffic jams. Through simulation experiments and practical test, it is proved that the proposed algorithm has good stability, high efficiency and practicability.	https://dx.doi.org/10.1109/ACCESS.2019.2939423	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Liu2019b	New Method of the Best Path Selection with Length Priority Based on Reinforcement Learning Strategy	This paper proposes and designs a new method of the best path selection algorithm with length priority to analyze and solve the optimal path planning problem of intelligent driving vehicles in practical applications. Through the understanding and learning of the reinforcement learning algorithm, we proposed a new method of the best path selection with length priority based on the prior knowledge applied reinforcement learning strategy, and improved the search direction setting of the shortest path in the program, simplified the process of shortest path search. This path optimization method can effectively help different types of intelligent driving vehicles to smoothly select the best path in the traffic network with limited height, width and weight, accident and traffic jam. Through simulation experiments and scene experiments, it is proved that the proposed algorithm has good stability, high efficiency and practicability.	https://dx.doi.org/10.1109/ICCCN.2019.8847049	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Liu2019c	Novel approach of the best path selection based on prior knowledge reinforcement learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075633290&doi=10.1109\%2fSmartIoT.2019.00031&partnerID=40&md5=621999c86c5b3bba2ad8c4a6f428c66e	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Liu2022d	Mean line aerodynamic design of an axial compressor using a novel design approach based on reinforcement learning	This paper develops a novel design approach based on reinforcement learning, which can independently complete the mean line aerodynamic design process of the axial compressor. The approach combines Deep Deterministic Policy Gradient (DDPG) algorithm with mean line aerodynamic predicting program HARIKA to acquire the design experiences of the axial compressor. DDPG combines basic reinforcement learning algorithm with artificial neural networks to get continuous observation and give corresponding actions. After the specific modification of the DDPG, multi-objective optimization can be integrated into the design process. Under the guidance of this approach, the design and optimization processes of a 9-stage high-pressure axial compressor were completed without expert experiences. At the design point, the isentropic efficiency was 88.5\% and the surge margin was 25\%, which meets the requirement of the compressor's efficiency and stability. And there was an increase of 13.4\% and 22\%, respectively, compared to the initial design. Moreover, through the analysis of the design results, the distributions of aerodynamic parameters conform to expert experiences. To verify the approach, traditional optimization methods, multi-island genetic optimization algorithm (GA), and multi-objective particle swarm optimization algorithm (MOPSO) were used to solve the same optimization problem. The DDPG optimized efficiency was 0.2\% lower than the traditional optimization method, and the pressure ratio at the work point was, respectively, 0.6\% and 2\% higher than that of the MOPSO and GA, which proved the effectiveness of the new design approach. Furthermore, after training, this approach can give design results immediately near the specific design requirements, which is different from the traditional optimization methods. The new approach saved 93\% evaluation steps compared to the GA in the -3\% design point and finished the design process in 8 steps in the +3\% design point, where GA failed to complete.	https://dx.doi.org/10.1177/09544100211063115	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Liu2022e	Automatic Bug Triaging via Deep Reinforcement Learning	Software maintenance and evolution account for approximately 90\% of the software development process (e.g., implementation, testing, and maintenance). Bug triaging refers to an activity where developers diagnose, fix, test, and document bug reports during software development and maintenance to improve the speed of bug repair and project progress. However, the large number of bug reports submitted daily increases the triaging workload, and open-source software has a long maintenance cycle. Meanwhile, the developer activity is not stable and changes significantly during software development. Hence, we propose a novel bug triaging model known as auto bug triaging via deep reinforcement learning (BT-RL), which comprises two models: a deep multi-semantic feature (DMSF) fusion model and an online dynamic matching (ODM) model. In the DMSF model, we extract relevant information from bug reports to obtain high-quality feature representation. In the ODM model, through bug report analysis and developer activities, we use a strategy based on the reinforcement learning framework, through which we perform training while learning and recommend developers for bug reports. Extensive experiments on open-source datasets show that the BT-RL method outperforms state-of-the-art methods in bug triaging.	https://dx.doi.org/10.3390/app12073565	Included	new_screen		4
RL4SE	Liu2021e	APPLICATION OF REINFORCEMENT LEARNING IN 1-D AERODYNAMIC DESIGN OF AXIAL COMPRESSOR			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Liu2019d	Intelligent Edge Computing for IoT-Based Energy Management in Smart Cities	In recent years, green energy management systems (smart grid, smart buildings, and so on) have received huge research and industrial attention with the explosive development of smart cities. By introducing Internet of Things (IoT) technology, smart cities are able to achieve exquisite energy management by ubiquitous monitoring and reliable communications. However, long-term energy efficiency has become an important issue when using an IoT-based network structure. In this article, we focus on designing an IoT-based energy management system based on edge computing infrastructure with deep reinforcement learning. First, an overview of IoT-based energy management in smart cities is described. Then the framework and software model of an IoT-based system with edge computing are proposed. After that, we present an efficient energy scheduling scheme with deep reinforcement learning for the proposed framework. Finally, we illustrate the effectiveness of the proposed scheme.	https://dx.doi.org/10.1109/MNET.2019.1800254	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Liu2022f	Deep Residual Reinforcement Learning based Autonomous Blimp Control	Blimps are well suited to perform long-duration aerial tasks as they are energy efficient, relatively silent and safe. To address the blimp navigation and control task, in previous work we developed a hardware and software-in-the-loop framework and a PID-based controller for large blimps in the presence of wind disturbance. However, blimps have a deformable structure and their dynamics are inherently non-linear and time-delayed, making PID controllers difficult to tune. Thus, often resulting in large tracking errors. Moreover, the buoyancy of a blimp is constantly changing due to variations in ambient temperature and pressure. To address these issues, in this paper we present a learning-based framework based on deep residual reinforcement learning (DRRL), for the blimp control task. Within this framework, we first employ a PID controller to provide baseline performance. Subsequently, the DRRL agent learns to modify the PID decisions by interaction with the environment. We demonstrate in simulation that DRRL agent consistently improves the PID performance. Through rigorous simulation experiments, we show that the agent is robust to changes in wind speed and buoyancy. In real-world experiments, we demonstrate that the agent, trained only in simulation, is sufficiently robust to control an actual blimp in windy conditions. We openly provide the source code of our approach at https://github.com/robot-perception-group/AutonomousBlimpDRL. Video demonstration is provided at https://youtu.be/EMC4KnlH0yI.	https://dx.doi.org/10.1109/IROS47612.2022.9981182	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Liu2022g	Petri Nets-Based Modeling Solution for Cyber&#x2013;Physical Product Control Considering Scheduling, Deployment, and Data-Driven Monitoring		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135736619&doi=10.1109\%2fTSMC.2022.3170489&partnerID=40&md5=b285716ccfe32d9fda9d6425f206f80a	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Liu2019e	Automatic Generation of Pull Request Descriptions	Enabled by the pull-based development model, developers can easily contribute to a project through pull requests (PRs). When creating a PR, developers can add a free-form description to describe what changes are made in this PR and/or why. Such a description is helpful for reviewers and other developers to gain a quick understanding of the PR without touching the details and may reduce the possibility of the PR being ignored or rejected. However, developers sometimes neglect to write descriptions for PRs. For example, in our collected dataset with over 333K PRs, more than 34\% of the PR descriptions are empty. To alleviate this problem, we propose an approach to automatically generate PR descriptions based on the commit messages and the added source code comments in the PRs. We regard this problem as a text summarization problem and solve it using a novel sequence-to-sequence model. To cope with out-of-vocabulary words in software artifacts and bridge the gap between the training loss function of the sequence-to-sequence model and the evaluation metric ROUGE, which has been shown to correspond to human evaluation, we integrate the pointer generator and directly optimize for ROUGE using reinforcement learning and a special loss function. We build a dataset with over 41K PRs and evaluate our approach on this dataset through ROUGE and a human evaluation. Our evaluation results show that our approach outperforms two baselines by significant margins.	https://dx.doi.org/10.1109/ASE.2019.00026	Included	new_screen		4
RL4SE	Liu2023	Petri Nets-Based Modeling Solution for Cyber-Physical Product Control Considering Scheduling, Deployment, and Data-Driven Monitoring	For a complex electromechanical product that is a cyber-physical system (CPS), its dynamic behaviors are embodied in the closed-loop control between the logic process in its cyber component and actual actuators/sensors in its physical component, and thus, a well-defined model of the control is important to create a digital twin that acts as much like the real machine as possible. This article proposes a Petri nets (PNs)-based modeling solution that employs hybrid PNs (HPNs) for physics and system of sequential systems with shared resources ((SR)-R-4) nets for logic in building a hierarchical control model. We also present PNs technologies for implementing a smooth transition and bidirectional mapping from the virtual prototype to the real machine. These technologies involve a PNs integration of a reinforcement learning (RL) method for generating a workflow scheduling agent in design, an extension of PNs definitions that is compatible with the microcontroller for easy deployment in manufacturing, and an architecture of PNs execution recording for data-driven monitoring in service. A software kit is provided for the solution that includes an integrated development environment of PNs, tools for quickly building a virtual prototype, and a monitor server for remote data-driven monitoring. This solution is successfully applied in the development of a typical cyber-physical product case, namely, the chemiluminescence immunoassay (CLIA) analyzer.	https://dx.doi.org/10.1109/TSMC.2022.3170489	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Livne2020	PoPS: Policy Pruning and Shrinking for Deep Reinforcement Learning	The recent success of deep neural networks (DNNs) for function approximation in reinforcement learning has triggered the development of Deep Reinforcement Learning (DRL) algorithms in various fields, such as robotics, computer games, natural language processing, computer vision, sensing systems, and wireless networking. Unfortunately, DNNs suffer from high computational cost and memory consumption, which limits the use of DRL algorithms in systems with limited hardware resources. In recent years, pruning algorithms have demonstrated considerable success in reducing the redundancy of DNNs in classification tasks. However, existing algorithms suffer from a significant performance reduction in the DRL domain. In this article, we develop the first effective solution to the performance reduction problem of pruning in the DRL domain, and establish a working algorithm, named Policy Pruning and Shrinking (PoPS), to train DRL models with strong performance while achieving a compact representation of the DNN. The framework is based on a novel iterative policy pruning and shrinking method that leverages the power of transfer learning when training the DRL model. We present an extensive experimental study that demonstrates the strong performance of PoPS using the popular Cartpole, Lunar Lander, Pong, and Pacman environments. Finally, we develop an open source software for the benefit of researchers and developers in related fields.	https://dx.doi.org/10.1109/JSTSP.2020.2967566	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lopez2019	Dynamic Multiobjective Control for Continuous-Time Systems Using Reinforcement Learning	This paper presents an extension of the reinforcement learning algorithms to design suboptimal control sequences for multiple performance functions in continuous-time systems. The first part of the paper provides the theoretical development and studies the required conditions to obtain a state-feedback control policy that achieves Pareto optimal results for the multiobjective performance vector. Then, a policy iteration algorithm is proposed that takes into account practical considerations to allow its implementation in real-time applications for systems with partially unknown models. Finally, the multiobjective linear quadratic regulator problem is solved using the proposed control scheme and employing a multiobjective optimization software to solve the static optimization problem at each iteration.	https://dx.doi.org/10.1109/TAC.2018.2869462	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lopez2022	Q-funcT: A Reinforcement Learning Approach for Automated Black Box Functionality Testing	The steady growth of mobile applications users over the past years has resulted in increasing the workload of Software Quality Assurance teams. Regarding this, automated Android Testing has become a highlighted research subject. However, the state-of-art academic and industrial solutions available have mainly focused in exploratory or Automated Input Generation approaches, fewer works have addressed the challenge of automated functionality testing. Moreover, the proposed solutions exhibit several limitations standing out the vulnerability to app evolution and fragmentation. In this work we propose Q-funcT, a Reinforcement Learning based approach that aims to improve automated functionality testing by increasing portability. When compared with Scripted Test Cases our method takes a few minutes longer to complete the defined missions; however, regarding portability, Q-funcT shown a notably better performance.	https://dx.doi.org/10.1109/SEAI55746.2022.9832177	Included	new_screen		4
RL4SE	Louati2018	An artificial immune network to control interrupted flow at a signalized intersection	To monitor and control interrupted flow at signalized intersections, several Traffic Signal Control Systems (TSCSs) were developed based on optimization and artificial intelligence techniques. Although learning can provide intelligent ways to deal with disturbances, existing approaches still lack concepts and mechanisms that enable direct representation of knowledge and explicit learning, particularly to capture and reuse previous experiences with disturbances. This article addresses this gap by designing a new TSCS based on innovative concepts and mechanisms borrowed from biological immunity. Immune memory enables the design of a Case-Based Reasoning (CBR) System in which cases provide a direct representation of knowledge about disturbances. Immune network theory enables the design of a Reinforcement Learning (RL) mechanism to interconnect cases, capture explicit knowledge about the outcomes (success and failure) of control decisions and enable decision-making by taking advantage of previous outcomes in reaction to new occurrences of disturbances. We provide a detailed description of new learning algorithms, both to create the case-base and to interconnect cases using RL. The performance of the suggested TSCS is assessed by benchmarking it against two standard control strategies from the literature, namely fixed-time and adaptive control using the Longest Queue First - Maximal Weight Matching (LQF-MWM) algorithm. The suggested TSCS is applied on an intersection simulated using VISSIM, a state-of-the-art traffic simulation software. The results show that the suggested TSCS is able to handle different traffic scenarios with competitive performance, and that it is recommended for extreme situations involving blocked approaches and high traffic flow. (C) 2017 Elsevier Inc. All rights reserved.	https://dx.doi.org/10.1016/j.ins.2017.12.033	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lu2012	Research and application of goods vehicles matching system based on SaaS and CSCW			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lu2020	Scheduling mix-flow in SD-DCN based on Deep Reinforcement Learning with Private Link	In software-defined datacenter networks, there are bandwidth-demanding elephant flows without deadline and delay-sensitive mice flows with strict deadline. They compete with each other for limited network resources, and how to effectively schedule such mix-flow is a huge challenge. We propose DRL-PLink (deep reinforcement learning with private link) that combines software-defined network and deep reinforcement learning (DRL) to schedule mix-flow. It divides the link bandwidth and establishes some corresponding private links for different types of flows respectively to isolate them. DRL is used to adaptively allocate bandwidth resources for these private links. Furthermore, DRL-PLink introduces Clipped Double Q-learning and parameter exploration NoisyNet technology to improve the scheduling policy for overestimated value estimates and action exploration problems in DRL. The simulation results show that DRL-PLink can effectively schedule mix-flow. Compared with ECMP and pFabric, the average flow completion time of DRL-PLink has decreased by 68.87\% and 52.18\% respectively. At the same time, it maintains a high deadline meet rate (>96.6\%) close to pFabric and Karuna very much.	https://dx.doi.org/10.1109/MSN50589.2020.00071	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lu2021	Mutation Testing of Reinforcement Learning Systems		https://doi.org/10.1007/978-3-030-91265-9_8	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lu2022	Towards mutation testing of Reinforcement Learning systems		https://doi.org/10.1016/j.sysarc.2022.102701	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Luan2021	Poster: <smartTE: Partially Deployed Segment Routing for Smart Traffic Engineering with Deep Reinforcement Learning>		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112804258&doi=10.23919\%2fIFIPNetworking52078.2021.9472815&partnerID=40&md5=6d93d4735925c8e7a6f63f2e7a092567	Excluded	new_screen	E5: Other not a paper,E5: Other not a paper	4
RL4SE	Luan2021a	Poster: 	Segment Routing (SR) provides Traffic Engineering (TE) with the ability of explicit path control by steering traffic passing through specific SR routers along a desired path. However, large-scale migration from a legacy IP network to a full SR-enabled one requires prohibitive hardware replacement and software update. Therefore, network operators prefer to upgrade a subset of IP routers into SR routers during a transitional period. This paper proposes SmartTE to optimize TE performance in hybrid IP/SR networks where partially deployed SR routers coexist with legacy IP routers. We use two centrality criteria in graph theory to decide which IP routers should be upgraded into SR routers under a given upgrading ratio. SmartTE leverages Deep Reinforcement Learning (DRL) to infer the optimal traffic splitting ratio across multiple pre-defined paths between source-destination pairs. Extensive experimental results with real-world topologies show that SmartTE outperforms other baseline TE solutions in minimizing the maximum link utilization and achieves comparable performance as a full SR network by upgrading only 30\% IP routers.	https://dx.doi.org/10.23919/IFIPNetworking52078.2021.9472815	Excluded	new_screen	E5: Other not a paper,E5: Other not a paper	4
RL4SE	Luan2021b	EPC-TE: Explicit Path Control in Traffic Engineering with Deep Reinforcement Learning	Segment Routing (SR) provides Traffic Engineering (TE) with Explicit Path Control (EPC) by steering data flows passing through a list of SR routers along a desired path. However, large-scale migration from a pure IP network to a full SR one requires prohibitive hardware replacement and software update. Therefore, network operators prefer to upgrade a subset of IP routers into SR routers during a transitional period. This paper proposes EPC-TE to optimize TE performance in hybrid IP/SR networks where partially deployed SR routers coexist with legacy IP routers. We propose a concept of key nodes to achieve EPC over desired paths and a criterion to select which IP routers to upgrade first under a pre-defined upgrading ratio. EPC-TE leverages Deep Reinforcement Learning (DRL) to inference the optimal traffic splitting ratio across multiple controllable paths between source-destination pairs. EPC-TE can achieve comparable TE performance as a full SR network with an upgrading ratio less than 30\%. Extensive experimental results with real-world topologies show that EPC-TE significantly outperforms other baseline TE solutions in minimizing maximum link utilization.	https://dx.doi.org/10.1109/GLOBECOM46510.2021.9685792	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Luckow2014	Exact and approximate probabilistic symbolic execution for nondeterministic programs		https://doi.org/10.1145/2642937.2643011	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Luo2020	Blockchain-Enabled Software-Defined Industrial Internet of Things With Deep Reinforcement Learning	Recently, software-defined Industrial Internet of Things (SDIIoT), the integration of software-defined networking (SDN) and Industrial Internet of Things (IIoT), has emerged. It is perceived as an effective way to manage IIoT dynamically. Aiming to improve the scalability and flexibility of SDIIoT, multi-SDN has been applied to form a physically distributed control plane to handle a large amount of data generated by industrial devices. However, as the core of multi-SDN, reaching consensus among multiple SDN controllers is a thorny issue. To meet the required design principle, this article proposes a blockchain-enabled distributed SDIIoT to synchronize local views between distinct SDN controllers and finally reach the consensus of the global view. On the other hand, both the cryptographic operations of blockchain and the noncryptographic tasks have access to the same computational resource pool of mobile edge cloud (MEC). In order to optimize the system energy efficiency, we adaptively allocate computational resources and the batch size of the block by jointly considering the trust features of SDN controllers and the resource requirements of noncryptographic operations. To implement the truly distributed manner of blockchain, we describe our problem as a partially observable Markov decision process (POMDP) and propose a novel deep reinforcement learning (DRL) approach to solve it. In the simulation results, we compare three different protocols of blockchain and show the effectiveness of our scheme in each of them.	https://dx.doi.org/10.1109/JIOT.2020.2978516	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Luo2018	Reinforcement learning and trustworthy autonomy		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063927274&doi=10.1007\%2f978-3-319-98935-8_10&partnerID=40&md5=ea782a7880123363cce864ae64ed18af	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Luo2022	Visualizing Multi-Agent Reinforcement Learning for Robotic Communication in Industrial IoT Networks	With its mobility and flexibility, autonomous robots have received extensive attention in industrial Internet of Things (IoT). In this paper, we adopt non-orthogonal multiple access and multi-antenna technology to enhance the connectivity of sensors and the throughput of data collection through taking advantage of the power and spatial domains. For average sumrate maximization, we optimize the transmit power of sensors and the trajectories of robots jointly. To deal with uncertainty and dynamics in the industrial environment, we propose a multiagent reinforcement learning (MARL) algorithm with experience exchange. Next, we present the visualization of robotic communication and mobility to analyze the learning behavior intuitively. From the software implementation results, we observe that the proposed MARL algorithm can effectively adjust the communication strategies of sensors and control the trajectories of robots in a fully distributed manner. The code and demonstrations can be found at https://github.com/lry-bupt/Visual_MARL.	https://dx.doi.org/10.1109/INFOCOMWKSHPS54753.2022.9798273	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Luo2005	Absowtlydine: Typical unification of' volce-over-ip and the lookaside buffer			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Luo2020a	Ship Motion Trajectory and Prediction Based on Vector Analysis	For the traditional method, there is a problem that the prediction accuracy of the ship's motion trajectory is low and the prediction time is long. A method for predicting the ship motion trajectory based on vector analysis is proposed. ADAMS software is used to analyze the dynamic model of ship motion trajectory, and the visual information of ship motion is collected in the three-dimensional feature space. Combined with the best force model, the trajectory of ship is adjusted adaptively, so as to construct the stress decay characteristic analysis model for the ship based on gray target correlation analysis and guide the trajectory correction of ship. Through the reinforcement learning method of neural network, the ship motion trajectory vector analysis and prediction are carried out, and the prediction accuracy is improved.	https://dx.doi.org/10.2112/SI95-230.1	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Luria2008	The effect of workers' visibility on effectiveness of intervention programs: Supervisory-based safety interventions	Introduction: This paper discusses an organizational change intervention program targeting safety behaviors and addresses important considerations concerning the planning of organizational change. Using layout of the plant as a proxy for ease of daily leader-member interaction, the effect of workers' visibility on the effectiveness of supervisory-based safety (SBS) interventions is examined. Through a reinforcement-learning framework, it is suggested that visibility can affect supervisors' incentive to interact with subordinates regarding safety-related issues. Method: Data were collected during SBS intervention studies in five manufacturing companies. Results: Results suggest a reinforcement cycle model whereby increased visibility generates more frequent exchanges between supervisors and employees, resulting in improved safety behavior among employees. In turn, employees' safer behavior reinforces continued supervisory safety-related interaction. Conclusion and impact on industry: Visibility is an important moderator in supervisory based safety interventions, and can serve to increase workplace safety. Implications of these findings for safety are discussed. (C) 2008 National Safety Council and Elsevier Ltd. All rights reserved.	https://www.ncbi.nlm.nih.gov/pubmed/18571568	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lv2021	Research on the Fusion Pattern Recognition System Based on the Concept of Production Education Integration and Application of Generative Countermeasure Network		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115870940&doi=10.1007\%2f978-3-030-82565-2_19&partnerID=40&md5=0cc805827f9af80931b92b58469e6a8a	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Lynden2002	LEAF: A FIPA compliant software toolkit for learning based MAS			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Lyons2020	Using Taint Analysis and Reinforcement Learning (TARL) to Repair Autonomous Robot Software	It is important to be able to establish formal performance bounds for autonomous systems. However, formal verification techniques require a model of the environment in which the system operates; a challenge for autonomous systems, especially those expected to operate over longer timescales. This paper describes work in progress to automate the monitor and repair of ROS-based autonomous robot software written for an apriori partially known and possibly incorrect environment model. A taint analysis method is used to automatically extract the dataflow sequence from input topic to publish topic, and instrument that code. A unique reinforcement learning approximation of MDP utility is calculated, an empirical and non-invasive characterization of the inherent objectives of the software designers. By comparing design (a-priori) utility with deploy (deployed system) utility, we show, using a small but real ROS example, that it's possible to monitor a performance criterion and relate violations of the criterion to parts of the software. The software is then patched using automated software repair techniques and evaluated against the original off-line utility.	https://dx.doi.org/10.1109/SPW50608.2020.00045	Included	new_screen		4
RL4SE	Ma2012	The Impact of Embedded Methodologies on E-Voting Technology	Compilers and Web services, while technical in theory, have not until recently been considered compelling. After years of essential research into context-free grammar, we demonstrate the emulation of super pages, which embodies the private principles of software engineering. Despite the fact that it might seem perverse, it usually conflicts with the need to provide the partition table to cyber information. We show not only that reinforcement learning and multicast systems can connect to achieve this ambition, but that the same is true for e-commerce.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ma2021	Testing self-healing cyber-physical systems under uncertainty with reinforcement learning: an empirical study		https://doi.org/10.1007/s10664-021-09941-z	Included	new_screen		4
RL4SE	Ma2022	A Proactive Defense Strategy Against SGX Side-channel Attacks via self-checking DRL in the Cloud	Intel software guard extensions (SGX) technology allows cloud vendors to provide customers with an independent and trusted execution environment (TEE). It protects critical data confidentiality and integrity from malicious software. However, more and more SGX side-channel attacks have appeared, which seriously undermine the confidence of tenants in cloud security. The related research focuses on system hardware and SGX compiler solutions for specific attacks, which also has difficulties in deployment. Differently, we propose an intelligent-driven proactive defense strategy, which is based on live migration. To the best of our knowledge, this is the first proactive defense against SGX side-channel attacks. We adopt the Markov decision process to solve the migration programming problem. The innovative deep reinforcement learning (DRL) solves problems of the unknown state transition probability and large machine load states, which is called self-checking proximal policy optimization (SPPO). It changes the reward pattern, improving the convergence speed and stability of DRL. In prototype experiments, we deploy the strategy in the OpenStack platform agilely to prove the defense performance and low virtual machine costs.	https://dx.doi.org/10.1109/ICC45855.2022.9838400	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Maalla2021	Research on Stock Market Analysis Based on Deep Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116086813&doi=10.1109\%2fIMCEC51613.2021.9482065&partnerID=40&md5=2e00bd41dc33ac6d75b29ed630bb6862	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mabu2007	Stock trading rules using genetic network programming with actor-critic		https://www.scopus.com/inward/record.uri?eid=2-s2.0-56749101381&doi=10.1109\%2fCEC.2007.4424513&partnerID=40&md5=144fd784f94f37b4fba5ad0f66fe83ae	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mabu2006	Genetic network programming with reinforcement learning using sarsa algorithm			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mabu2006a	Genetic network programming with reinforcement learning and its application to making mobile robot behavior		https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746867121&doi=10.1541\%2fieejeiss.126.1009&partnerID=40&md5=cd519ee09ecf9df49ba48aa8a7b38a96	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mabu2007a	Trading rules on stock markets using genetic network programming with reinforcement learning and importance index		https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547146983&doi=10.1541\%2fieejeiss.127.1061&partnerID=40&md5=89f382ff4b6a6a8976a6b260842d5974	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mabu2005	Genetic network programming with actor-critic and its application			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mabu2007b	A graph-based evolutionary algorithm: Genetic Network Programming (GNP) and its extension using reinforcement learning	"This paper proposes a graph-based evolutionary algorithm called Genetic Network Programming (GNP). Our goal is to develop GNP, which can deal with dynamic environments efficiently and effectively, based on the distinguished expression ability of the graph (network) structure. The characteristics of GNP are as follows. 1) GNP programs are composed of a number of nodes which execute simple judgment/processing, and these nodes are connected by directed links to each other. 2) The graph structure enables GNP to re-use nodes, thus the structure can be very compact. 3) The node transition of GNP is executed according to its node connections without any terminal nodes, thus the past history of the node transition affects the current node to be used and this characteristic works as an implicit memory function. These structural characteristics are useful for dealing with dynamic environments. Furthermore, we propose an extended algorithm, ""GNP with Reinforcement Learning (GNPRL)"" which combines evolution and reinforcement learning in order to create effective graph structures and obtain better results in dynamic environments. In this paper, we applied GNP to the problem of determining agents' behavior to evaluate its effectiveness. Tileworld was used as the simulation environment. The results show some advantages for GNP over conventional methods."	https://www.scopus.com/inward/record.uri?eid=2-s2.0-35748930908&doi=10.1162\%2fevco.2007.15.3.369&partnerID=40&md5=e8f39d8b903316386bfd718455b5994c	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mabu2012	Adaptability analysis of genetic network programming with reinforcement learning in dynamically changing environments		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864477631&doi=10.1016\%2fj.eswa.2012.04.038&partnerID=40&md5=311830a206dd74efa9c78f5032aac0d8	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mabu2010	Evaluation on the robustness of genetic network programming with reinforcement learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751487547&doi=10.1109\%2fICSMC.2010.5642323&partnerID=40&md5=9c0b8b9bf83e4f7193c28d706f6f509c	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Machado2008	getALife - An Artificial Life Environment for the Evaluation of Agent-Based Systems and Evolutionary Algorithms for Reinforcement Learning	An Artificial Life environment - getALife - is proposed, whose major aim is to provide a framework to evaluate single and multi-agent systems and evolutionary approaches to the development of reinforcement learning algorithms. The environment is based on a predator-prey scenario, with multiple species and where individuals are mainly characterized by their decision modules and genetic information. The platform is quite powerful, flexible, modular, visually attractive, easy to program and to use, making an interesting tool both to research and teaching. Two applications based on getALife are provided: the evaluation of a Neural Network based decision module with evolutionary learning and the development of a children's game.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Maclin1996	Creating advice-taking reinforcement learners		https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029732210&doi=10.1007\%2fBF00114730&partnerID=40&md5=42f3dce0344f021b90090d26d1165179	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Macua2020	Diff-DAC: Distributed actor-critic for average multitask deep reinforcement learning			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Maeda2010	Construction of PSE system for developing reinforcement learning algorithms			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Maeda2010a	A problem solving environment that assists model development for reinforcement learning algorithms		https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952676740&doi=10.1109\%2fICCIT.2010.5711068&partnerID=40&md5=92a9061cd20ef041a5eb603e261cb91a	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Maes1993	Learning interface agents			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Maffettone2021	Gaming the beamlines-employing reinforcement learning to maximize scientific outcomes at large-scale user facilities	Beamline experiments at central facilities are increasingly demanding of remote, high-throughput, and adaptive operation conditions. To accommodate such needs, new approaches must be developed that enable on-the-fly decision making for data intensive challenges. Reinforcement learning (RL) is a domain of AI that holds the potential to enable autonomous operations in a feedback loop between beamline experiments and trained agents. Here, we outline the advanced data acquisition and control software of the Bluesky suite, and demonstrate its functionality with a canonical RL problem: cartpole. We then extend these methods to efficient use of beamline resources by using RL to develop an optimal measurement strategy for samples with different scattering characteristics. The RL agents converge on the empirically optimal policy when under-constrained with time. When resource limited, the agents outperform a naive or sequential measurement strategy, often by a factor of 100\%. We interface these methods directly with the data storage and provenance technologies at the National Synchrotron Light Source II, thus demonstrating the potential for RL to increase the scientific output of beamlines, and layout the framework for how to achieve this impact.	https://dx.doi.org/10.1088/2632-2153/abc9fc	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Magaia2021	Industrial Internet-of-Things Security Enhanced With Deep Learning Approaches for Smart Cities	The significant evolution of the Internet of Things (IoT) enabled the development of numerous devices able to improve many aspects in various fields in the industry for smart cities where machines have replaced humans. With the reduction in manual work and the adoption of automation, cities are getting more efficient and smarter. However, this evolution also made data even more sensitive, especially in the industrial segment. The latter has caught the attention of many hackers targeting Industrial IoT (IIoT) devices or networks, hence the number of malicious software, i.e., malware, has increased as well. In this article, we present the IIoT concept and applications for smart cities, besides also presenting the security challenges faced by this emerging area. We survey currently available deep learning (DL) techniques for IIoT in smart cities, mainly deep reinforcement learning, recurrent neural networks, and convolutional neural networks, and highlight the advantages and disadvantages of security-related methods. We also present insights, open issues, and future trends applying DL techniques to enhance IIoT security.	https://dx.doi.org/10.1109/JIOT.2020.3042174	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mahadevan1992	AUTOMATIC PROGRAMMING OF BEHAVIOR-BASED ROBOTS USING REINFORCEMENT LEARNING	This paper describes a general approach for automatically programming a behavior-based robot. New behaviors are learned by trial and error using a performance feedback function as reinforcement. Two algorithms for behavior learning are described that combine Q learning, a well-known scheme for propagating reinforcement values temporally across actions, with statistical clustering and Hamming distance. two ways of propagating reinforcement values spatially across states. A real behavior-based robot called OBELIX is described that learns several component behaviors in an example task involving pushing boxes. A simulator for the box pushing task is also used to gather data on the learning techniques. A detailed experimental study using the real robot and the simulator suggests two conclusions. (1) The learning techniques are able to learn the individual behaviors, sometimes outperforming a handcoded program. (2) Using a behavior-based architecture speeds up reinforcement learning by converting the problem of learning a complex task into that of learning a simpler set of special-purpose reactive subtasks.	https://dx.doi.org/10.1016/0004-3702(92)90058-6	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mahardhika2018	Botnet Detection Using On-line Clustering with Pursuit Reinforcement Competitive Learning (PRCL)	Botnet is a malicious software that often occurs at this time, and can perform malicious activities, such as DDoS, spamming, phishing, keylogging, clickfraud, steal personal information and important data. Botnets can replicate themselves without user consent. Several systems of botnet detection has been done by using classification methods. Classification methods have high precision, but it needs more effort to determine appropiate classification model. In this paper, we propose reinforced approach to detect botnet with Online Clustering using Reinforcement Learning. Reinforcement Learning involving interaction with the environment and became new paradigm in machine learning. The reinforcement learning will be implemented with some rule detection, because botnet ISCX dataset is categorized as unbalanced dataset which have high range of each number of class. Therefore we implemented Reinforcement Learning to Detect Botnet using Pursuit Reinforcement Competitive Learning (PRCL) with additional rule detection which has reward and punisment rules to achieve the solution. Based on the experimental result, PRCL can detect botnet in real time with high accuracy (100\% for Neris, 99.9\% for Rbot, 78\% for SMTP_Spam, 80.9\% for Nsis, 80.7\% for Virut, and 96.0\% for Zeus) and fast processing time up to 176 ms. Meanwhile the step of CPU and memory usage which are 78 \% and 4.3 GB for pre-processing, 34\% and 3.18 GB for online clustering with PRCL, and 23\% and 3.11 GB evaluation. The proposed method is one solution for network administrators to detect botnet which has unpredictable behavior in network traffic.	https://dx.doi.org/10.24003/emitter.v6i1.207	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mahmoodzadeh2021	Trustable Reinforcement Learning for Asset Integrity Management	"SUMMARY & CONCLUSIONSSignificant recent accomplishments and immense future potentials of Reinforcement Learning (RL) are behind the surge in interest in this class of decision-making algorithms within the Artificial Intelligence domain. However, RL application to real-world problems such as technical asset integrity management has remained very limited. One of the main obstacles to the real-world deployment of RL can be broadly characterized as the lack of trust. Assets operators are reluctant to deploy policies derived from RL-based software because they are not currently designed to earn the user confidence. This research is an effort to address the ""trustability"" concerns in RL for safety and asset integrity management type of problems. We propose a methodology that sets up multiple risk barriers in the RL output policy and provides an estimation of the RL policy's performance prior to deployment. Our proposed method is compatible with any RL algorithm. The methodology was tested on a scaled-down integrity management case-study in a simulated pipeline environment and successfully derived a safer and superior policy for the pipe's corrosion-related maintenance management."	https://dx.doi.org/10.1109/RAMS48097.2021.9605744	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mainland2005	Decentralized, adaptive resource allocation for sensor networks			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Maino2022	Project and Development of a Reinforcement Learning Based Control Algorithm for Hybrid Electric Vehicles	Hybrid electric vehicles are, nowadays, considered as one of the most promising technologies for reducing on-road greenhouse gases and pollutant emissions. Such a goal can be accomplished by developing an intelligent energy management system which could lead the powertrain to exploit its maximum energetic performances under real-world driving conditions. According to the latest research in the field of control algorithms for hybrid electric vehicles, Reinforcement Learning has emerged between several Artificial Intelligence approaches as it has proved to retain the capability of producing near-optimal solutions to the control problem even in real-time conditions. Nevertheless, an accurate design of both agent and environment is needed for this class of algorithms. Within this paper, a detailed plan for the complete project and development of an energy management system based on Q-learning for hybrid powertrains is discussed. An integrated modular software framework for co-simulation has been developed and it is thoroughly described. Finally, results have been presented about a massive testing of the agent aimed at assessing for the change in its performance when different training parameters are considered.	https://dx.doi.org/10.3390/app12020812	Excluded	new_screen	E3: Only conceptual results are reported,E2: Software engineering is not the problem RL is used for	4
RL4SE	Majumdar2022	The Changing Landscape of AI-driven System Optimization for Complex Combinatorial Optimization	With the unprecedented success of modern machine learning in areas like computer vision and natural language processing, a natural question is where can it have maximum impact in real life. At Intel Labs, we are actively investing in research that leverages the robustness and generalizability of deep learning to solve system optimization problems. Examples of such systems include individual hardware modules like memory schedulers and power management units on a chip, automated compiler and software design tools as well as broader problems like chip design. In this talk, I will address some of the open challenges in systems optimization and how Intel and others in the research community are harnessing the power of modern reinforcement learning to address those challenges. A particular aspect of problems in the domain of chip design is the very large combinatorial complexity of the solution space. For example, the number of possible ways to place standard cells and macros on a canvas for even small to medium sized netlists can approach 10 100 to 10 1000 . Importantly, only a very small subset of these possible outcomes are actually valid and performant.	https://dx.doi.org/10.1109/MLCAD55463.2022.9900092	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mallozzi2017	Combining Machine-Learning with Invariants Assurance Techniques for Autonomous Systems	Autonomous Systems are systems situated in some environment and are able of taking decision autonomously. The environment is not precisely known at design-time and it might be full of unforeseeable events that the autonomous system has to deal with at run-time. This brings two main problems to be addressed. One is that the uncertainty of the environment makes it difficult to model all the behaviours that the autonomous system might have at the design-time. A second problem is that, especially for safety-critical systems, maintaining the safety requirements is fundamental despite the system's adaptations. We address such problems by shifting some of the assurance tasks at run-time. We propose a method for delegating part of the decision making to agent-based algorithms using machine learning techniques. We then monitor at run-time that the decisions do not violate the autonomous system's safety-critical requirements and by doing so we also send feedback to the decision-making process so that it can learn. We plan to implement this approach using reinforcement learning for decision making and predictive monitoring for checking at run-time the preservation and/or violation of invariant properties of the system. We also plan to validate it using ROS as software middleware and miniaturized vehicles and real vehiclesas hardware.	https://dx.doi.org/10.1109/ICSE-C.2017.40	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mallozzi2019	A Runtime Monitoring Framework to Enforce Invariants on Reinforcement Learning Agents Exploring Complex Environments	"Without prior knowledge of the environment, a software agent can learn to achieve a goal using machine learning. Model-free Reinforcement Learning (RL) can be used to make the agent explore the environment and learn to achieve its goal by trial and error. Discovering effective policies to achieve the goal in a complex environment is a major challenge for RL. Furthermore, in safety-critical applications, such as robotics, an unsafe action may cause catastrophic consequences in the agent or in the environment. In this paper, we present an approach that uses runtime monitoring to prevent the reinforcement learning agent to perform ""wrong"" actions and to exploit prior knowledge to smartly explore the environment. Each monitor is de?ned by a property that we want to enforce to the agent and a context. The monitors are orchestrated by a meta-monitor that activates and deactivates them dynamically according to the context in which the agent is learning. We have evaluated our approach by training the agent in randomly generated learning environments. Our results show that our approach blocks the agent from performing dangerous and safety-critical actions in all the generated environments. Besides, our approach helps the agent to achieve its goal faster by providing feedback and shaping its reward during learning."	https://dx.doi.org/10.1109/RoSE.2019.00011	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mallozzi2018	MoVEMo: A Structured Approach for Engineering Reward Functions	Reinforcement learning (RL) is a machine learning technique that has been increasingly used in robotic systems. In reinforcement learning, instead of manually pre-program what action to take at each step, we convey the goal a software agent in terms of reward functions. The agent tries different actions in order to maximize a numerical value, i.e. the reward. A misspecified reward function can cause problems such as reward hacking, where the agent finds out ways that maximize the reward without achieving the intended goal. As RL agents become more general and autonomous, the design of reward functions that elicit the desired behaviour in the agent becomes more important and cumbersome. In this paper, we present a technique to formally express reward functions in a structured way; this stimulates a proper reward function design and as well enables the formal verification of it. We start by defining the reward function using state machines. In this way, we can statically check that the reward function satisfies certain properties, e.g., high-level requirements of the function to learn. Later we automatically generate a runtime monitor - which runs in parallel with the learning agent - that provides the rewards according to the definition of the state machine and based on the behaviour of the agent. We use the UPPAAL model checker to design the reward model and verify the TCTL properties that model high-level requirements of the reward function and LARVA to monitor and enforce the reward model to the RL agent at runtime.	https://dx.doi.org/10.1109/IRC.2018.00053	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Malone2012	Implementation of an embodied general reinforcement learner on a serial link manipulator	BECCA (a Brain-Emulating Cognition and Control Architecture software package) was developed in order to perform general reinforcement learning, that is, to enable unmodeled embodied systems operating in unstructured environments to perform unfamiliar tasks. It accomplishes this through automatic paired feature creation and reinforcement learning algorithms. This paper describes an implementation of BECCA on a seven Degree of Freedom (DoF) Barrett Whole Arm Manipulator (WAM) undergoing a series of experiments designed to test the reinforcement learner's ability to adapt to the WAM hardware. In the experiments, the following is demonstrated, 1) learning to transition the WAM between states, 2) learning to perform at near optimal levels on one, two and three dimensional navigation tasks, 3) applying learning in simulation to hardware performance, 4) learning under inconsistent, human-generated reward, and 5) combining the reinforcement learner with Probabilistic Roadmap Methods (PRM) to improve scalability. The goal of the paper is to demonstrate both the scalability of the BECCA reinforcement learning approach using different formulations of the state space and to show the approach in this paper operating on complex physical hardware.	https://dx.doi.org/10.1109/ICRA.2012.6225154	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Malviya2021	Various Machine Learning Techniques for Software Defect Prediction			Excluded	new_screen	E5: Other not a paper,E5: Other not a paper	4
RL4SE	Mammadli2020	Static Neural Compiler Optimization via Deep Reinforcement Learning	The phase-ordering problem of modern compilers has received a lot of attention from the research community over the years, yet remains largely unsolved. Various optimization sequences exposed to the user are manually designed by compiler developers. In designing such a sequence developers have to choose the set of optimization passes, their parameters and ordering within a sequence. Resulting sequences usually fall short of achieving optimal runtime for a given source code and may sometimes even degrade the performance when compared to unoptimized version. In this paper, we employ a deep reinforcement learning approach to the phase-ordering problem. Provided with sub-sequences constituting LLVM's O3 sequence, our agent learns to outperform the O3 sequence on the set of source codes used for training and achieves competitive performance on the validation set, gaining up to 1.32x speedup on previously-unseen programs. Notably, our approach differs from autotuning methods by not depending on one or more test runs of the program for making successful optimization decisions. It has no dependence on any dynamic feature, but only on the statically-attainable intermediate representation of the source code. We believe that the models trained using our approach can be integrated into modern compilers as neural optimization agents, at first to complement, and eventually replace the handcrafted optimization sequences.	https://dx.doi.org/10.1109/LLVMHPCHiPar51896.2020.00006	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mamun2020	Intra- and Inter-Server Smart Task Scheduling for Profit and Energy Optimization of HPC Data Centers	Servers in a data center are underutilized due to over-provisioning, which contributes heavily toward the high-power consumption of the data centers. Recent research in optimizing the energy consumption of High Performance Computing (HPC) data centers mostly focuses on consolidation of Virtual Machines (VMs) and using dynamic voltage and frequency scaling (DVFS). These approaches are inherently hardware-based, are frequently unique to individual systems, and often use simulation due to lack of access to HPC data centers. Other approaches require profiling information on the jobs in the HPC system to be available before run-time. In this paper, we propose a reinforcement learning based approach, which jointly optimizes profit and energy in the allocation of jobs to available resources, without the need for such prior information. The approach is implemented in a software scheduler used to allocate real applications from the Princeton Application Repository for Shared-Memory Computers (PARSEC) benchmark suite to a number of hardware nodes realized with Odroid-XU3 boards. Experiments show that the proposed approach increases the profit earned by 40\% while simultaneously reducing energy consumption by 20\% when compared to a heuristic-based approach. We also present a network-aware server consolidation algorithm called Bandwidth-Constrained Consolidation (BCC), for HPC data centers which can address the under-utilization problem of the servers. Our experiments show that the BCC consolidation technique can reduce the power consumption of a data center by up-to 37\%.	https://dx.doi.org/10.3390/jlpea10040032	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mandow2020	Architectural planning with shape grammars and reinforcement learning: Habitability and energy efficiency	This paper describes the generation of sketches of small single-family dwellings that satisfy habitability requirements and are energy efficient. The proposed approach considers three stages in the generation process, and each one is based on a combination of shape grammars and reinforcement learning. First a set of very simple shape grammar rules is defined that are capable of generating a great variety of sketches. In order to guarantee the generation of sketches that are both suitable for habitation and energy efficient, a reinforcement learning process is applied on this set. Then the grammar so trained is used to generate only ``good'' sketches. More precisely, the learning process applies positive rewards to sketches that satisfy desired habitability and energy efficiency guidelines. As a result, sequences of grammar rules that lead to good sketches are identified. In this paper we present the general approach followed to develop the system and describe in detail the procedure applied in the reinforcement learning process. Experimental results are also presented, to show convergence of the learning process, and to compare the obtained results with those of real designs. A standard energy simulation program is used to validate the approach.	https://dx.doi.org/10.1016/j.engappai.2020.103909	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mandyam2021	COP-E-CAT: Cleaning and Organization Pipeline for EHR Computational and Analytic Tasks	In order to ensure that analyses of complex electronic healthcare record (EHR) data are reproducible and generalizable, it is crucial for researchers to use comparable preprocessing, filtering, and imputation strategies. We introduce COP-E-CAT: Cleaning and Organization Pipeline for EHR Computational and Analytic Tasks, an open-source processing and analysis software for MIMIC-IV, a ubiquitous benchmark EHR dataset. COP-E-CAT allows users to select filtering characteristics and preprocess covariates to generate data structures for use in downstream analysis tasks. This user-friendly approach shows promise in facilitating reproducibility and comparability among studies that leverage the MIMIC-IV data, and enhances EHR accessibility to a wider spectrum of researchers than current data processing methods. We demonstrate the versatility of our workflow by describing three use cases: ensemble prediction, reinforcement learning, and dimension reduction. The software is available at: https://github.com/eyeshoe/cop-e-cat.	https://dx.doi.org/10.1145/3459930.3469536	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mao2021	A Flow Control Method Based on Deep Deterministic Policy Gradient		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093124158&doi=10.1007\%2f978-981-15-8462-6_40&partnerID=40&md5=b7cb7222a93774423b00da236c1577f8	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mao2020	Online Fault-tolerant VNF Chain Placement: A Deep Reinforcement Learning Approach	Since Network Function Virtualization (NFV) decouples network functions (NFs) from the underlying dedicated hardware and realizes them in the form of software called Virtual Network Functions (VNFs), they are enabled to run in any resource-sufficient virtual machines (VMs) and offer diverse network services by service function chains (SFCs). Given the complexity and unpredictability of the network state, we propose a deep reinforcement learning (DRL) based online SFC placement method named DDQP (Double Deep Q-networks Placement). Meanwhile, VNFs are vulnerable to various faults such as software failures. Thus, we backup standby instances to enhance the fault tolerance of our model, and DDQP automatically deploys both active and standby instances in real-time. Specifically, we use DNN (Deep Neural Networks) to deal with large continuous network state space. In the case of stateful VNFs, we offer constant generated state updates from active instances to standby instances to guarantee seamless redirection after failures. With the goal of balancing the waste of resources and ensuring service reliability, we introduce five progressive schemes of resource reservations to meet different customer needs. Our experimental results demonstrate that DDQP responds rapidly to arriving requests and reaches near-optimal performance. Specifically, DDQP outweighs the state-of-the-art method by 16.30\% higher acceptance ratio with 82x speedup on average.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mao2014	A Two-Layer Approach to Developing Self-Adaptive Multi-Agent Systems in Open Environment		https://doi.org/10.4018/ijats.2014010104	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mao2014a	An Integrated Approach to Developing Self-Adaptive Software	One of the main challenges of developing self-adaptive systems in open environment comes from uncertain self-adaptation requirements due to the unpredictability of environment changes and its co-existence with well-defined self-adaptation requirements in self-adaptive systems. This paper presents an integrated approach that combines off-line and on-line self-adaptation together in a unified technical framework to support the development and running of such systems. We consider self-adaptive system as a multi-agent organization and propose a novel dynamic binding self-adaptation mechanism inspired from organization metaphors to specify and analyze self-adaptation. A description language, SADL, is designed to program well-defined self-adaptation logic at design-time and implement off-line self-adaptation. In order to deal with uncertain self-adaptation, a reinforcement learning method is incorporated with the dynamic binding mechanism, which enables software agents to make decisions on self-adaptation at run-time and implement on-line self-adaptation. Our approach provides a unified frame-work to accommodate off-line and on-line approaches and a general-purpose methodology to develop complex self-adaptive systems in a systematic way. A supported platform called SADE+ is developed and a case is studied to illustrate the proposed approach.		Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mariani2012	AutoBlackTest: Automatic Black-Box Testing of Interactive Applications	Automatic test case generation is a key ingredient of an efficient and cost-effective software verification process. In this paper we focus on testing applications that interact with the users through a GUI, and present AutoBlackTest, a technique to automatically generate test cases at the system level. AutoBlackTest uses reinforcement learning, in particular Q-Learning, to learn how to interact with the application under test and stimulate its functionalities. The empirical results show that AutoBlackTest can execute a relevant portion of the code of the application under test, and can reveal previously unknown problems by working at the system level and interacting only through the GUI.	https://dx.doi.org/10.1109/ICST.2012.88	Included	new_screen		4
RL4SE	Marinescu2019	Optimising residential electric vehicle charging under renewable energy: Multi-agent learning in software simulation and hardware-in-the-loop evaluation	The integration of intermittent renewable energy sources coupled with the increasing demand of electric vehicles (EVs) poses new challenges to the electrical grid. To address this, many solutions based on demand response have been presented. These solutions are typically tested only in software-based simulations. In this paper, we present the application in hardware-in-the-loop (HIL) of a recently proposed algorithm for decentralised EV charging, prediction-based multi-agent reinforcement learning (P-MARL), to the problem of optimal EV residential charging under intermittent wind power and variable household baseload demands. P-MARL is an approach that can address EV charging objectives in a demand response aware manner, to avoid peak power usage while maximising the exploitation of renewable energy sources. We first train and test our algorithm in a residential neighbourhood scenario using GridLAB-D, a software power network simulator. Once agents learn optimal behaviour for EV charging while avoiding peak power demand in the software simulator, we port our solution to HIL while emulating the same scenario, in order to decrease the effects of agent learning on power networks. Experimental results carried out in a laboratory microgrid show that our approach makes full use of the available wind power, and smooths grid demand while charging EVs for their next day's trip, achieving a peak-to-average ration of 1.67, down from 2.24 in the baseline case. We also provide an analysis of the additional demand response effects observed in HIL, such as voltage drops and transients, which can impact the grid and are not observable in the GridLAB-D software simulation.	https://dx.doi.org/10.1002/er.4559	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Marks2006	Chapter 27 Market Design Using Agent-Based Models		https://www.scopus.com/inward/record.uri?eid=2-s2.0-66049140790&doi=10.1016\%2fS1574-0021\%2805\%2902027-7&partnerID=40&md5=f7f49d2b317283a37c34265cd03c12b8	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Martignoni2004	Behavior construction and refinement from high-level specifications	Mobile robots are excellent examples of systems that need to show a high level of autonomy. Often robots are loosely supervised by humans who are not intimately familiar with the inner workings of the robot. We cannot,generally predict exact environmental conditions in which the robot will operate in advance. This means that the behavior must be adapted in the field. Untrained individuals cannot (and probably should not) program the robot to effect these changes. We need a system that will (a) allow re-tasking, and (b) allow adaptation of the behavior to the specific conditions in the field. In this paper we concentrate on (b). VVe will describe how to assemble controllers, based on high-level descriptions of the behavior. We will show how the behavior can be tuned by the human, despite not knowing how the code is put together. We will also show how this can be done automatically, using reinforcement learning, and point out the problems that must be overcome for this approach to work.	https://dx.doi.org/10.1117/12.568865	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Martinez-Plumed2013	Learning with configurable operators and RL-based heuristics		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875846081&doi=10.1007\%2f978-3-642-37382-4_1&partnerID=40&md5=17b45a3b7534c0534d6c7ec7b8cfdea6	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Martinez-Tenor2018	Towards a common implementation of reinforcement learning for multiple robotic tasks	Mobile robots are increasingly being employed for performing complex tasks in dynamic environments. Those tasks can be either explicitly programmed by an engineer or learned by means of some automatic learning method, which improves the adaptability of the robot and reduces the effort of setting it up. In this sense, reinforcement learning (RI.) methods are recognized as a promising tool for a machine to learn autonomously how to do tasks that are specified in a relatively simple manner. However, the dependency between these methods and the particular task to learn is a well-known problem that has strongly restricted practical implementations in robotics so far. Breaking this barrier would have a significant impact on these and other intelligent systems; in particular, having a, core method that requires little tuning effort for being applicable to diverse tasks would boost their autonomy in learning and self adaptation capabilities. In this paper we present such a practical core implementation of RL, which enables the learning process for multiple robotic tasks with minimal per-task tuning or none. Based on value iteration methods, we introduce a novel approach for action selection, called Q-biased softmax regression (QBIASSR), that takes advantage of the structure of the state space by attending the physical variables involved (e.g., distances to obstacles, robot pose, etc.), thus experienced sets of states accelerate the decision-making process of unexplored or rarely-explored states. Intensive experiments with both real and simulated robots, carried out with the software framework also introduced here, show that our implementation is able to learn different robotic tasks without tuning the earning method. They also suggest that the combination of true online SARSA(lambda) (TOSL) with QBIASSE can outperform the existing RL core algorithms in low-dimensional robotic tasks. All of these are promising results towards the possibility of learning much more complex tasks autonomously by a robotic agent. (C) 2017 Elsevier Ltd. All rights reserved.	https://dx.doi.org/10.1016/j.eswa.2017.11.011	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Martin-Guerrero2007	Validation of a reinforcement learning policy for dosage optimization of erythropoietin			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Matthieu2022	Adapting Quality of Service of EnergyHarvesting IoT Devices	This chapter tackles the software and hardware design of energyharvesting sensor nodes. It presents the problem of managing energy in energyharvesting sensor networks, as well as a state of the art of energy managers. The chapter details the operation of an energy manager by focusing on two recent and promising examples: energy manager relying on fuzzy logic theory and the principle of Reinforcement Learning. It then presents an energy management application using a concrete case: the LoRa longrange radio standard which is part of the Low Power Wide Area Network family, more and more acclaimed thanks to the compromise that it offers between range and energy consumption. Many technologies aimed at the Internet of Things market have been proposed in recent years, as much by the academic community as by manufacturers. Among these, the standard LoRaWAN, which uses LoRa modulation, enables longrange communications with a limited energy consumption.	http://ieeexplore.ieee.org/document/9980478	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mazied2019	The wireless control plane: An overview and directions for future research	Software-defined networking (SDN), which has been successfully deployed in the management of complex data centers, has recently been incorporated into a myriad of 5G networks to intelligently manage a wide range of heterogeneous wireless devices, software systems, and wireless access technologies. Thus, the SDN control plane needs to communicate wirelessly with the wireless data plane either directly or indirectly. The uncertainties in the wireless SDN control plane (WCP) make its design challenging. Both WCP schemes (direct WCP, D-WCP, and indirect WCP, I-WCP) have been incorporated into recent 5G networks; however, a discussion of their design principles and their design limitations is missing. This paper introduces an overview of the WCP design (I-WCP and D-WCP) and discusses its intricacies by reviewing its deployment in recent 5G networks. Furthermore, to facilitate synthesizing a robust WCP, this paper proposes a generic WCP framework using deep reinforcement learning (DRL) principles and presents a roadmap for future research.	https://dx.doi.org/10.1016/j.jnca.2018.09.017	Excluded	new_screen	E3: Only conceptual results are reported,E2: Software engineering is not the problem RL is used for	4
RL4SE	McDermid2020	Safety of artificial intelligence: A collaborative model			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	McGovern2002	Building a Basic Block Instruction Scheduler with Reinforcement Learning and Rollouts		https://doi.org/10.1023/A:1017976211990	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Medina2012	Traffic signal control using reinforcement learning and the max-plus algorithm as a coordinating strategy	This paper explores the performance of decentralized reinforcement learning agents with communication capabilities for the operation of traffic signals in an oversaturated network. An explicit coordinating mechanism is implemented as part of the reward structure of the agent using the max-plus algorithm, aiming at improving the network-wide performance. Results from a simulated network with realistic features showed that Q-learning agents could process a greater number of vehicles than optimized signal timings from state-of-practice simulation software TRANSYT7F, even under varying oversaturation conditions. The effect of adding the max-plus algorithm was limited, but towards improved performance in terms of both total throughput and reduced number of stops per vehicle. Ongoing research evaluates potential conditions where the coordination should be emphasized to further enhance results, as well as alternative implementations of the max-plus algorithm.	https://dx.doi.org/10.1109/ITSC.2012.6338911	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Medina2010	Arterial traffic control using reinforcement learning agents and information from adjacent intersections in the state and reward structure	An application that uses reinforcement learning (RL) agents for traffic control along an arterial under high traffic volumes is presented. RL agents were trained using Q learning and a modified version of the state representation that included information on the occupancy of the links from neighboring intersections. The proposed structure also includes a reward that considers potential blockage from downstream intersections (due to saturated conditions), as well as pressure to coordinate the signal response with the future arrival of traffic from upstream intersections. Experiments using microscopic simulation software were conducted for an arterial with 5 intersections under high conflicting volumes, and results were compared with the best settings of coordinated pre-timed phasing. Data showed lower delays and less number of stops with RL agents, as well as a more balanced distribution of the delay among all vehicles in the system. Evidence of coordinated-like behavior was found as the number of stops to traverse the 5 intersections was on average lower than 1.5, and also since the distribution of green times from all intersections was very similar. As traffic approached to capacity, however, delays with the pre-timed phasing were lower than with RL agents, but the agents produced lower maximum delay times and lower maximum number of stops per vehicle. Future research will analyze variable coefficients in the state and reward structures for the system to better cope with a wide variety of traffic volumes, including transitions from oversaturation to undersaturation and vice versa.	https://dx.doi.org/10.1109/ITSC.2010.5624977	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mehrabi2022	Using deep reinforcement learning to search reachability properties in systems specified through graph transformation		https://doi.org/10.1007/s00500-022-06815-4	Included	new_screen		4
RL4SE	Mehta2005	Motion Alphabet Augmentation Based on Past Experiences	"Multi-modal control is a commonly used design tool for breaking up complex control tasks into sequences of simpler tasks. It has previously been shown that rapidly-exploring randomized trees (as well as other viable approaches) can be used for reachability computations given a set of modes, and reinforcement learning can be performed over the reachable set to obtain the optimal control sequence. In this paper, we investigate the problem of adding new modes to a motion description language in a structured manner. We formalize an approach for augmenting the motion alphabet by adding new modes to reduce the complexity of the control program. In particular, we show a general technique for combining recurring mode sequences into one smooth ""meta-mode"". This problem is solved using a variational approach and numerical examples illustrate the feasibility of the proposed method."	https://dx.doi.org/10.1109/CDC.2005.1582139	Excluded	new_screen	E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mehta2006	An optimal control approach to mode generation in hybrid systems	This paper presents a solution to the problem of constructing control programs, i.e. sequences of control modes, from a given motion alphabet. In particular, techniques are developed that enable reinforcement learning to act directly at the mode level, and hence make learning applicable to continuous time control systems in a straightforward manner. Moreover, given such a control program, the issue of improving the system performance through the addition of new control laws is addressed as an optimal control problem. In fact, this is achieved through an optimal combination of recurring mode strings. A number of examples are provided that illustrate the viability of the proposed methods. (c) 2005 Elsevier Ltd. All rights reserved.	https://dx.doi.org/10.1016/j.na.2005.07.044	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Meldgaard2020	Structure prediction of surface reconstructions by deep reinforcement learning	We demonstrate how image recognition and reinforcement learning combined may be used to determine the atomistic structure of reconstructed crystalline surfaces. A deep neural network represents a reinforcement learning agent that obtains training rewards by interacting with an environment. The environment contains a quantum mechanical potential energy evaluator in the form of a density functional theory program. The agent handles the 3D atomistic structure as a series of stacked 2D images and outputs the next atom type to place and the atomic site to occupy. Agents are seen to require 1000-10 000 single point density functional theory evaluations, to learn by themselves how to build the optimal surface reconstructions of anatase TiO2(001)-(1 x 4) and rutile SnO2(110)-(4 x 1).	https://www.ncbi.nlm.nih.gov/pubmed/32434171	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Melnik2020	Hybrid Intellectual Scheme for Scheduling of Heterogeneous Workflows based on Evolutionary Approach and Reinforcement Learning	Scheduling of workload in distributed computing systems is a well-known optimization proble. A workload may include single independent software packages. However, the computational process in scientific and industrial fields is often organized as composite applications or workflows which are represented by collection of interconnected computing packages that solve a common problem. We identified three common computing modes: batch, stream and iterative. The batch mode is a classic way for one-time execution of software packages with an initially specified set of input data. Stream mode corresponds to launch of a software package for further continuous processing of active data in real time. Iterative mode is a launching of a distributed application with global synchronization at each iteration. Each computing mode has its own specifics for organization of computation process. But at the moment, there are new problems that require organization of interaction between computing modes (batch, stream, iterative) and to develop optimization algorithms for this complex computations that leads to formation of heterogeneous workflows. In this work, we present a novel developed hybrid intellectual scheme for organizing and scheduling of heterogeneous workflows based on evolutionary computing and reinforcement learning methods.	https://dx.doi.org/10.5220/0010112802000211	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Meng2019	Reinforcement Learning in Financial Markets	Recently there has been an exponential increase in the use of artificial intelligence for trading in financial markets such as stock and forex. Reinforcement learning has become of particular interest to financial traders ever since the program AlphaGo defeated the strongest human contemporary Go board game player Lee Sedol in 2016. We systematically reviewed all recent stock/forex prediction or trading articles that used reinforcement learning as their primary machine learning method. All reviewed articles had some unrealistic assumptions such as no transaction costs, no liquidity issues and no bid or ask spread issues. Transaction costs had significant impacts on the profitability of the reinforcement learning algorithms compared with the baseline algorithms tested. Despite showing statistically significant profitability when reinforcement learning was used in comparison with baseline models in many studies, some showed no meaningful level of profitability, in particular with large changes in the price pattern between the system training and testing data. Furthermore, few performance comparisons between reinforcement learning and other sophisticated machine/deep learning models were provided. The impact of transaction costs, including the bid/ask spread on profitability has also been assessed. In conclusion, reinforcement learning in stock/forex trading is still in its early development and further research is needed to make it a reliable method in this domain.	https://dx.doi.org/10.3390/data4030110	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Meng2019a	Research on Resource Allocation Method of Space Information Networks Based on Deep Reinforcement Learning	The space information networks (SIN) have a series of characteristics, such as strong heterogeneity, multiple types of resources, and difficulty in management. Aiming at the problem of resource allocation in SIN, this paper firstly establishes a hierarchical and domain-controlled SIN architecture based on software-defined networking (SDN). On this basis, the transmission, caching, and computing resources of the whole network are managed uniformly. The Asynchronous Advantage Actor-Critic (A3C) algorithm in deep reinforcement learning is introduced to model the process of resource allocation. The simulation results show that the proposed scheme can effectively improve the expected benefits of unit resources and improve the resource utilization efficiency of the SIN.	https://dx.doi.org/10.3390/rs11040448	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mera-Gomez2018	A Multi-Agent Elasticity Management Based on Multi-Tenant Debt Exchanges	A multi-tenant Software as a Service (SaaS) application is a highly configurable software that allows its owner to serve multiple tenants, each with their own workflows, workloads and Service Level Objectives (SLOs). Tenants are usually organizations that serve several users and the application appears to be a different one for each tenant. However, in practice, multi-tenant SaaS applications limit the diversity of tenants by clustering them in a few categories (e.g. premium, standard) with predefined SLOs. Additionally, this coarse-grained clustering reduces the advantage of these multi-tenant ecosystems over single tenant architectures to share dynamically virtual resources between tenants based on their own workload profile and elasticity adaptation decisions. To address this limitation, we propose a multi-agent elasticity management where each tenant is represented by a reinforcement learning agent that performs elasticity adaptations based on a new technical debt perspective, and make use of debt attributes (i.e. amnesty, interest) to form autonomous coalitions that minimise the effect of the unavoidable imperfections in any elasticity management approach. We extended CloudSim and Burlap to evaluate our approach. The simulation results indicate that our debt-aware multi-agent elasticity management preserves the diversity of tenants and reduces SLO violations without affecting the aggregate utility of the application owner.	https://dx.doi.org/10.1109/SASO.2018.00014	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Merrick2013	A game theoretic framework for incentive-based models of intrinsic motivation in artificial systems	An emerging body of research is focusing on understanding and building artificial systems that can achieve open-ended development influenced by intrinsic motivations. In particular, research in robotics and machine learning is yielding systems and algorithms with increasing capacity for self-directed learning and autonomy. Traditional software architectures and algorithms are being augmented with intrinsic motivations to drive cumulative acquisition of knowledge and skills. Intrinsic motivations have recently been considered in reinforcement learning, active learning and supervised learning settings among others. This paper considers game theory as a novel setting for intrinsic motivation. A game theoretic framework for intrinsic motivation is formulated by introducing the concept of optimally motivating incentive as a lens through which players perceive a game. Transformations of four well-known mixed-motive games are presented to demonstrate the perceived games when players' optimally motivating incentive falls in three cases corresponding to strong power, affiliation and achievement motivation. We use agent-based simulations to demonstrate that players with different optimally motivating incentive act differently as a result of their altered perception of the game. We discuss the implications of these results both for modeling human behavior and for designing artificial agents or robots.	https://www.ncbi.nlm.nih.gov/pubmed/24198797	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Metzger2022	Realizing self-adaptive systems via online reinforcement learning and feature-model-guided exploration	A self-adaptive system can automatically maintain its quality requirements in the presence of dynamic environment changes. Developing a self-adaptive system may be difficult due to design time uncertainty; e.g., anticipating all potential environment changes at design time is in most cases infeasible. To realize self-adaptive systems in the presence of design time uncertainty, online machine learning, i.e., machine learning at runtime, is increasingly used. In particular, online reinforcement learning is proposed, which learns suitable adaptation actions through interactions with the environment at runtime. To learn about its environment, online reinforcement learning has to select actions that were not selected before, which is known as exploration. How exploration happens impacts the performance of the learning process. We focus on two problems related to how adaptation actions are explored. First, existing solutions randomly explore adaptation actions and thus may exhibit slow learning if there are many possible adaptation actions. Second, they are unaware of system evolution, and thus may explore new adaptation actions introduced during evolution rather late. We propose novel exploration strategies that use feature models (from software product line engineering) to guide exploration in the presence of many adaptation actions and system evolution. Experimental results for two realistic self-adaptive systems indicate an average speed-up of the learning process of 33.7\% in the presence of many adaptation actions, and of 50.6\% in the presence of evolution.	https://dx.doi.org/10.1007/s00607-022-01052-x	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Metzger2020	Feature Model-Guided Online Reinforcement Learning for Self-Adaptive Services		https://doi.org/10.1007/978-3-030-65310-1_20	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mhaisen2020	Real-Time Scheduling for Electric Vehicles Charging/Discharging Using Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085520711&doi=10.1109\%2fICIoT48696.2020.9089471&partnerID=40&md5=cf25737e304bd7552289dfda56624acb	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mi2022	AutoDefense: Reinforcement Learning Based Autoreactive Defense Against Network Attacks	Attributed to the programmability and visibility provided by Software Defined Network (SDN) technologies, more flexible and complex functions can be performed on network at-tacks. However, identifying the attack traffic accurately for attack mitigation is a challenge. Most existing solutions leverage traffic characteristics to achieve this goal. Recent attacks characteristics have become more complex and indistinguishable from legitimate traffic. In this paper, we propose AutoDefense, a novel frame-work that leverages reinforcement learning techniques to deploy defense policies dynamically and adaptively based on the signals collected from the data plane. While we seek to achieve the same goal with the existing efforts where the network/server resources the attackers control should be limited, we allow more legitimate flows to enter the network, rather than relinquish bandwidth when attacks happen. Through evaluations, we demonstrate that AutoDefense could reduce 39\% of the attack traffic and allow 48.6 \% more legitimate flows in the network. AutoDefense also improves the average flow completion time by 42.7\% for the flows with a long tail latency.	https://dx.doi.org/10.1109/CNS56114.2022.9947232	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mieth2021	Learning-Enabled Residential Demand Response: Automation and Security of Cyberphysical Demand Response Systems		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102442125&doi=10.1109\%2fMELE.2020.3047470&partnerID=40&md5=4601a9cbac15cbc025df9d950691b6d3	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mike2021	Teaching Machine Learning to Computer Science Preservice Teachers: Human vs. Machine Learning		https://doi.org/10.1145/3408877.3439550	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mileti?2022	Impact of Connected Vehicles on Learning based Adaptive Traffic Control Systems	Adaptive Traffic Signal Control (ATSC) systems can be implemented to reduce travel times at urban intersections by changing the signal program according to real-time traffic situations. Modern approaches to ATSC are based on Reinforcement Learning (RL) which can allow the controller to learn the control policy independently. By including the concept of Connected Vehicles (CVs), the RL-based ATSC system can use data gathered from CVs instead of traditional traffic sensors. In this paper, the impact of varying CV penetration rate on RL-based ATSC is implemented and evaluated in a simulated environment. Obtained results show that with a sufficient CVs penetration rate the RL-based ATSC systems can significantly reduce the delay of all vehicles in the traffic network.	https://dx.doi.org/10.1109/SMC53654.2022.9945071	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mileti?2021	Combining Neural Gas and Reinforcement Learning for Adaptive Traffic Signal Control	Travel time of vehicles in urban traffic networks can be reduced by using Adaptive Traffic Signal Control (ATSC) to change the signal program according to the current traffic situation. Modern ATSC approaches based on Reinforcement Learning (RL) can learn the optimal signal control policy. While there are multiple RL based ATSC implementations available, most suffer from high state-action complexity leading to slow convergence and long training time. In this paper, the state-action complexity of ATSC based RL is reduced by implementing Growing Neural Gas learning structure as an integral part of RL, leading to high convergence rate and system stability. The presented approach is evaluated on a simulated signalized intersection, and compared with self-organizing map RL-based ATSC systems. Obtained results prove that the reduction of state-action complexity in this manner improves the effectiveness of RL based ATSC not needing to have an a priory analysis of needed number of neurons for state representation.	https://dx.doi.org/10.1109/ELMAR52657.2021.9550948	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mileti?2020	State Complexity Reduction in Reinforcement Learning based Adaptive Traffic Signal Control	The throughput of a signalized intersection can be increased by appropriate adjustment of the signal program using Adaptive Traffic Signal Control (ATSC). One possible approach is to use Reinforcement Learning (RL). It enables model-free learning of the control law for the reduction of the negative impacts of traffic congestion. RL based ATSC achieves good results but requires many learning iterations to train optimal control policy due to high state-action complexity. In this paper, a novel approach for state complexity reduction in RL by using Self-Organizing Maps (SOM) is presented. With SOM, the convergence rate of RL and system stability in the later stages of learning is increased. The proposed approach is evaluated against the traditional RL approach that uses Q-Learning on a simulated isolated intersection calibrated according to realistic traffic data. Presented simulation results prove the effectiveness of the proposed approach regarding learning stability and traffic measures of effectiveness.	https://dx.doi.org/10.1109/ELMAR49956.2020.9219024	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Millan2014	An intelligent tool to assist architecture students in the early stages of design			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Miller2017	Parlai: A dialog research software platform		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055714428&doi=10.18653\%2fv1\%2fd17-2014&partnerID=40&md5=acb0a4fcf5daf95ceca17707b7f82c9b	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Miller2022	Feedback control approaches for restoration of power grids from blackouts	The automated restoration of power systems with variable energy resources is a timely problem to tackle. Automated restoration advice can support operators in deciding on strategic actions to restore power grids from a blackout with a mix of conventional and renewable generation resources. To this end, this paper frames the restoration process of power grids with solar resources as a nonlinear dynamic model with algebraic constraints in discrete time which is steered by feedback control loops. We discuss two feedback-control strategies based on greedy and reinforcement learning algorithms, and contrast their performance with restoration plans generated by a mixed-integer linear program. We found that the reinforcement learning algorithm infers restoration actions faster than the greedy one. However, the tuning process of the reinforcement learning parameters is slower than for the greedy one.	https://dx.doi.org/10.1016/j.epsr.2022.108414	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Milliez2014	Simulating Human-Robot Interactions for Dialogue Strategy Learning		https://doi.org/10.1007/978-3-319-11900-7_6	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mills1991	Reinforcement learning using back-propagation as a building block	A novel unsupervised reinforcement learning rule is introduced, based on the use of the supervised backpropagation algorithm as a component building block. The learning rule is easy to understand and implement in software and builds on the accumulated experience of researchers using backpropagation. Unlike most reinforcement learning systems, the new rule can operate with either continuously valued or binary outputs. It is very tolerant with respect to a wide variety of performance measures and is unrestricted in range and variability. The technique should find application in most reinforcement learning situations but should have particular benefit for learning control systems.<>	https://dx.doi.org/10.1109/IJCNN.1991.170625	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ming2000	The Application of Mystery Shopping in an Apparel Retailing Chain in China		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064734035&doi=10.1108\%2fRJTA-04-01-2000-B007&partnerID=40&md5=c2fa644fe1b8a113daa94bd926faed18	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mirzaei2022	Reinforcement Learning Reward Function for Test Case Prioritization in Continuous Integration	Given that software systems are constantly changing at a fast rate during the software development process, Continuous integration testing that a cost-effective software development practice is characterized by constantly evolving test cases, rapid return, and limited performance time. Regression test is performed after any change in the software, so we must seek to optimize the regression test methods. One of the methods that have attracted a lot of attention today is the prioritization of the test using reinforcement learning. The purpose of this study was to survey and compare reinforcement learning reward functions, which are used to test case prioritization. We divided these reward functions into two categories: reward functions that depend on current information and reward functions which depend on historical information. The current information based reward functions rely only on the information of a previous implementation and the historical information-based reward functions rely on the whole information of the past. Each of these functions has the strengths and weaknesses that we have discussed in this study. Among the reward functions of these two categories, the HFCW reward function has better performance both in terms of fault detection and usage time. Keywords\emdash Continuous integration, Test case prioritization, Machine learning, Reinforcement learning, Reward function	https://dx.doi.org/10.1109/CFIS54774.2022.9756464	Included	new_screen		4
RL4SE	Mitchell2021	Motor adaptation via distributional learning	Objective. Both artificial and biological controllers experience errors during learning that are probabilistically distributed. We develop a framework for modeling distributions of errors and relating deviations in these distributions to neural activity. Approach. The biological system we consider is a task where human subjects are required to learn to minimize the roll of an inverted T-shaped object with an unbalanced weight (i.e. one side of the object is heavier than the other side) during lift. We also collect BOLD activity during this process. For our experimental setup, we define the state of the system to be the maximum magnitude roll of the object after lift onset and give subjects the goal of achieving the zero state. Main Results. We derive a model for this problem from a variant of Temporal Difference Learning. We then combine this model with Distributional Reinforcement Learning (DRL), a framework that involves defining a value distribution by treating the reward as stochastic. This model transforms the goal of the controller from achieving a target state, to achieving a distribution over distances from the target state. We call it a Distributional Temporal Difference Model (DTDM). The DTDM allows us to model errors in unsuccessfully minimizing object roll using deviations in the value distribution when the center of mass of the unbalanced object is changed. We compute deviations in global neural activity and show that they vary continuously with deviations in the value distribution. Different aspects might contribute to this global shift or signal difference, including a difference in grasp and lift force at lift onset, as well as sensory feedback of error/roll after lift onset. We predict that there exists a coordinated, global response to errors that incorporates all of this information, which is encoding the DTDM objective and used on subsequent trials enabling success. We validate the utility of the DTDM as a model for biological adaptation by using it to engineer a robotic controller to solve a similar problem. Significance. We develop a novel theoretical framework and show that it can be used to model a non-trivial motor learning task. Because this theoretical framework is consistent with state-of-the-art reinforcement learning, we can also use it to program a robot to perform a similar task. These results suggest a way to model the multiple subsystems composing global neural activity in a way that transfers well to engineering artificial intelligence.	https://www.ncbi.nlm.nih.gov/pubmed/32674091	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mitchell2003	Using Markov-k Memory for Problems with Hidden-state			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mitriakov2022	An open-source software framework for reinforcement learning-based control of tracked robots in simulated indoor environments	A simulation framework based on the open-source robotic software Gazebo and the Robot Operating System is presented for articulated tracked robots, designed for reinforcement-learning-based (RL) control skill acquisition. In particular, it is destined to serve as a research tool in the development and evaluation of methods in the domain of mobility learning for articulated tracked robots, in 3D indoor environments. Its architecture allows to interchange between different RL libraries and algorithm implementations, while learning can be customized to endow specific properties within a control skill. To demonstrate its utility, we focus on the most demanding case of staircase ascent and descent using depth image data, while respecting safety via reward function shaping and incremental, domain randomization-based, end-to-end learning.	https://dx.doi.org/10.1080/01691864.2022.2076570	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mitsis2019	Intelligent Dynamic Data Offloading in a Competitive Mobile Edge Computing Market	Software Defined Networks (SDN) and Mobile Edge Computing (MEC), capable of dynamically managing and satisfying the end-users computing demands, have emerged as key enabling technologies of 5G networks. In this paper, the joint problem of MEC server selection by the end-users and their optimal data offloading, as well as the optimal price setting by the MEC servers is studied in a multiple MEC servers and multiple end-users environment. The flexibility and programmability offered by the SDN technology enables the realistic implementation of the proposed framework. Initially, an SDN controller executes a reinforcement learning framework based on the theory of stochastic learning automata towards enabling the end-users to select a MEC server to offload their data. The discount offered by the MEC server, its congestion and its penetration in terms of serving end-users' computing tasks, and its announced pricing for its computing services are considered in the overall MEC selection process. To determine the end-users' data offloading portion to the selected MEC server, a non-cooperative game among the end-users of each server is formulated and the existence and uniqueness of the corresponding Nash Equilibrium is shown. An optimization problem of maximizing the MEC servers' profit is formulated and solved to determine the MEC servers' optimal pricing with respect to their offered computing services and the received offloaded data. To realize the proposed framework, an iterative and low-complexity algorithm is introduced and designed. The performance of the proposed approach was evaluated through modeling and simulation under several scenarios, with both homogeneous and heterogeneous end-users.	https://dx.doi.org/10.3390/fi11050118	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mittal2020	Optimization of Energy Management Strategy for Range-Extended Electric Vehicle Using Reinforcement Learning and Neural Network		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083858098&doi=10.4271\%2f2020-01-1190&partnerID=40&md5=de299fba9f63075dd74ae52eec4b0289	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Miyamoto2021	Network Topology-Traceable Fault Recovery Framework with Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105913767&doi=10.1007\%2f978-3-030-75100-5_34&partnerID=40&md5=0de2f1184f74d714d8b3008ce3e285c1	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Miyazaki2002	Improvements of the penalty avoiding rational policy making algorithm and an application to the othello game		https://www.scopus.com/inward/record.uri?eid=2-s2.0-18444391484&doi=10.1527\%2ftjsai.17.548&partnerID=40&md5=b576a934d83bda87468bc3d8f8d3276a	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mlika2021	Massive IoT Access With NOMA in 5G Networks and Beyond Using Online Competitiveness and Learning	This article studies the problem of online user grouping, scheduling, and power allocation for massive Internet of Things (IoT) access in beyond 5G networks using nonorthogonal multiple access (NOMA). NOMA has been identified as a promising technology to accommodate a large number of devices using a limited number of radio resources. In this work, the objective is to maximize the number of served devices while allocating their transmission powers such that their real-time requirements as well as their limited operating energy are respected. First, we formulate the problem as a mixed-integer nonlinear program (MINLP) that can be transformed to MILP for some special cases. Second, we study its NP-hardness in different cases. Then, by dividing the problem into multiple NOMA grouping and scheduling subproblems, an efficient online competitive algorithm is proposed to solve each subproblem. Next, we show how to use the proposed online algorithm as a black box and how to combine the obtained solutions to each subproblem in a reinforcement learning setting to obtain the power allocation for each NOMA group. Our analyses are supplemented by simulation results to illustrate the performance of the proposed algorithms in comparison to optimal and state-of-the-art methods.	https://dx.doi.org/10.1109/JIOT.2021.3068061	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Modi2016	Reinforcement Learning with Neural Networks: A Survey	Reinforcement learning (RL) comes from the self-learning theory. RL can autonomously get optional results with the knowledge obtained from various conditions by interacting with dynamic environment. It allows machines and software agents to automatically determine the ideal behavior within a specific context, in order to maximize its performance. Neural network reinforcement learning is most popular algorithm. Advantage of using neural network is that it regulates RL more efficient in real life applications. In this paper, we firstly survey reinforcement learning theory and model. Then we present various main RL algorithms. Then we discuss different neural network RL algorithms. Finally we introduce some application of RL and outline some future research of RL with NN.	https://dx.doi.org/10.1007/978-3-319-30933-0_47	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Moeinizade2022	A reinforcement Learning approach to resource allocation in genomic selection		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129393233&doi=10.1016\%2fj.iswa.2022.200076&partnerID=40&md5=1c901049a387d55a747a131fbaefece0	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Moghadam2019	Machine learning-assisted performance testing		https://doi.org/10.1145/3338906.3342484	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E5: Other not a paper	4
RL4SE	Moghadam2013	Urban traffic control using adjusted reinforcement learning in a multi-agent sytem		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884297557&doi=10.19026\%2frjaset.6.3676&partnerID=40&md5=ec77c12c80c7a723aefef245b57312ac	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Moghadam2018	Adaptive runtime response time control in PLC-based real-time systems using reinforcement learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051555083&doi=10.1145\%2f3194133.3194153&partnerID=40&md5=90badcbe8ebcd12fd1c95db008938f9d	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Moghadam2018a	Learning-Based Response Time Analysis in Real-Time Embedded Systems: A Simulation-Based Approach	Response time analysis is an essential task to verify the behavior of real-time systems. Several response time analysis methods have been proposed to address this challenge, particularly for real-time systems with different levels of complexity. Static analysis is a popular approach in this context, but its practical applicability is limited due to the high complexity of the industrial real-time systems, as well as many unpredictable runtime events in these systems. In this work-in-progress paper, we propose a simulation-based response time analysis approach using reinforcement learning to find the execution scenarios leading to the worst-case response time. The approach learns how to provide a practical estimation of the worst-case response time through simulating the program without performing static analysis. Our initial study suggests that the proposed approach could be applicable in the simulation environments of the industrial real-time control systems to provide a practical estimation of the execution scenarios leading to the worst-case response time.		Excluded	conflict_resolution	E5: Other not a paper,E2: Software engineering is not the problem RL is used for	4
RL4SE	Moghadam2018b	Learning-Based Self-Adaptive Assurance of Timing Properties in a Real-Time Embedded System	Providing an adaptive runtime assurance technique to meet the performance requirements of a real-time system without the need for a precise model could be a challenge. Adaptive performance assurance based on monitoring the status of timing properties can bring more robustness to the underlying platform. At the same time, the results or the achieved policy of this adaptive procedure could be used as feedback to update the initial model, and consequently for producing proper test cases. Reinforcement-learning has been considered as a promising adaptive technique for assuring the satisfaction of the performance properties of software-intensive systems in recent years. In this work-in-progress paper, we propose an adaptive runtime timing assurance procedure based on reinforcement learning to satisfy the performance requirements in terms of response time. The timing control problem is formulated as a Markov Decision Process and the details of applying the proposed learning-based timing assurance technique are described.	https://dx.doi.org/10.1109/ICSTW.2018.00031	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Moghadam2020	Poster: Performance Testing Driven by Reinforcement Learning	Performance testing remains a challenge, particularly for complex systems. Different application-, platform- and workload-based factors can influence the performance of software under test. Common approaches for generating platform- and workload-based test conditions are often based on system model or source code analysis, real usage modeling and use-case based design techniques. Nonetheless, creating a detailed performance model is often difficult, and also those artifacts might not be always available during the testing. On the other hand, test automation solutions such as automated test case generation can enable effort and cost reduction with the potential to improve the intended test criteria coverage. Furthermore, if the optimal way (policy) to generate test cases can be learnt by testing system, then the learnt policy can be reused in further testing situations such as testing variants, evolved versions of software, and different testing scenarios. This capability can lead to additional cost and computation time saving in the testing process. In this research, we present an autonomous performance testing framework which uses a model-free reinforcement learning augmented by fuzzy logic and self-adaptive strategies. It is able to learn the optimal policy to generate platform- and workload-based test conditions which result in meeting the intended testing objective without access to system model and source code. The use of fuzzy logic and self-adaptive strategy helps to tackle the issue of uncertainty and improve the accuracy and adaptivity of the proposed learning. Our evaluation experiments show that the proposed autonomous performance testing framework is able to generate the test conditions efficiently and in a way adaptive to varying testing situations.	https://dx.doi.org/10.1109/ICST46399.2020.00048	Included	conflict_resolution		4
RL4SE	Moghadam2022	An autonomous performance testing framework using self-adaptive fuzzy reinforcement learning		https://doi.org/10.1007/s11219-020-09532-z	Included	new_screen		4
RL4SE	Moghaddam2019	Design of marketplaces for smart manufacturing services		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082746889&doi=10.1016\%2fj.promfg.2020.01.312&partnerID=40&md5=66ae032d57832564f5de539b3a136097	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Mohammadkhani2018	A new method for behavioural-based malware detection using reinforcement learning	Malware is - the abbreviation for malicious software - a comprehensive term for software that is deliberately created to perform an unauthorised and often harmful function. Viruses, backdoors, key-loggers, Trojans, password thieves' software, spyware, adwares are number of malware samples. Previously, calling something a virus or Trojan was enough. However, methods of contamination are developed, the term virus and other malware definition was not satisfactory for all types of malicious programs. This research focus on clustering the malware according to the malware features. To avoid the dangers of malware, some applications have been created to track them down. This paper presents a new method for detection of malware using reinforcement learning. The result demonstrates that the proposed method can detect the malware more accurate.	https://dx.doi.org/10.1504/IJDMMM.2018.095372	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	MohdAzmin2022	Smart OTA Scheduling for Connected Vehicles using Prescriptive Analytics and Deep Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138823153&doi=10.4271\%2f2022-01-1045&partnerID=40&md5=a153c96be9f4a60c614bfa62de87617b	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mohseni2022	FMI real-time co-simulation-based machine deep learning control of HVAC systems in smart buildings: Digital-twins technology	As heating, ventilation, and air conditioning (HVAC) systems have become one of the most contributing systems in energy consumption in the world, the control of these large-scale systems remains a challenging duty due to the decoupling effects of control variables. Accordingly, the penetration of these types of systems in all-smart buildings has increased in recent years. Furthermore, the application of digital twin as a fast-growing concept is being developed. In HVAC systems, independent and accurate control of temperature and humidity of the indoor air has been playing an undeniable role in reducing energy consumption. In this paper, to have cost-effective energy management in a single-zone HVAC system, a new reliable digital twin proximal policy optimization (PPO)-based model-independent nonsingular terminal sliding-mode control (MINTSMC) methodology has been proposed. Moreover, due to the nonlinear characteristics of HVAC systems, MINTSMC tends to handle the un-modeled system dynamics and disturbances. For regulating parameters of proposed control, an efficient PPO algorithm has been developed due to its actor-critic-based reinforcement learning. Extensive examinations and comparative analyses with particle swarm optimization designed sliding-mode control and proportional-integral-derivative controller have been made using digital twin of the proposed controller to show the importance, accuracy, and application of this method in the comfort and energy management achievement of HVAC control systems. A digital signal processor computing device has been utilized for implementation by utilizing hardware-in-loop (HIL) in the concept of the digital twin. To determine the interface between established models, software-in-loop, and HIL, the Functional Mock-up Interface has been utilized. The outcomes revealed a superior performance of suggested digital twin-based controller than the compared control methodologies in the compensation of unknown uncertainties, fast-tracking, and smooth response.	https://dx.doi.org/10.1177/01423312221119635	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mollaysa2020	Goal-directed generation of discrete structures with conditional generative models			Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mondal2022	An Economic and Non-cooperative Load-balancing Framework among Federated Cloudlets		https://doi.org/10.1016/j.comnet.2022.108847	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mora2012	Intelligent power saving technique for mobile devices	Power saving in mobile devices has become a hot topic nowadays. Enhancing user experience through intelligent techniques that help to extend battery lifetime is today's goal for many manufacturers and developers. Different approaches to improve power consumption have focused on improving the hardware used, the operating system, and/or the applications performance; however, this study's focusisto design an intelligent software framework. After a throughout evaluation of the power consumption in different modules of the phone, in order to identify which modules consume power the most,a reinforcement learning method is proposed to effectively deal with this issue by granting/denying accessto the user of executing battery-draining tasks.	https://dx.doi.org/10.1109/APCC.2012.6388189	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Morinelly2016	Dual MPC with Reinforcement Learning	An adaptive optimal control algorithm for system with uncertain dynamics is formulated under a Reinforcement Learning framework. An embedded exploratory component, is included explicitly in the objective function of an output feedback receding horizon Model Predictive Control problem. The optimization is formulated as a Quadratically Constrained Quadratic Program and it is solved to epsilon-global optimality. The iterative interaction between the action specified by the optimal solution and the approximation of cost functions balances the exploitation of current knowledge and the need for exploration. The proposed method is shown to converge to the optimal policy for a controllable discrete time linear plant with unknown output parameters. (C) 2016, IFAC (International Federation of Automatic Control) Hosting by Elsevier Ltd. All rights reserved.	https://dx.doi.org/10.1016/j.ifacol.2016.07.276	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Morris2019	Explainable Anomaly and Intrusion Detection Intelligence for Platform Information Technology Using Dimensionality Reduction and Ensemble Learning	Intrusion Detection Systems (IDS) for Platform Information Technology (PIT) Systems are deficient in capturing information, studies, assessments, research, and data for effectiveness. An IDS utilizes machine learning techniques to monitor cyber health, learn from malicious versus normal types of traffic, and implement automated protection measures for PIT systems. Ensemble learning is used for anomaly detection, malware detection, intrusion detection, and subtle or substantial changes in hardware or software. Ensemble learning can be used to differentiate static and dynamic commands, directives, and processes that are normal or abnormal by clustering similarities. This method assists with identification of abnormalities and grouping them. Dimensionality reduction minimizes the number of features currently being evaluated for selection. Principal Component Analysis (PCA) and Independent Component Analysis (ICA) is used to highlight the feature that is atypical in the test systems hardware and software baseline. These methods can be used for predictive and cyber health monitoring. Anomaly detection, identification, and discovery of PIT software resources, commands, and directives that have malicious intent. Ensemble learning and dimensionality reduction provide the foundation for an adaptable IDS that changes based on known (supervised) and unknown (unsupervised) environments, processes, programs, data, labels, and traffic. This paper proposes PCA, ICA, and ensemble learning methods for systematic use with PIT systems anomaly and intrusion detection intelligence. The trained IDS and cyber health model provides a software baseline for similar PIT systems. The IDS and preventative cyber health monitoring will be agnostic of the PIT system focusing on automatic test technology. Training the IDS and cyber health monitoring software on multiple PIT systems will provide research, data, and information that can be applied to these unique systems. The blending of normal and malicious behavior, processes, resources, and traffic provides information and data for changes in a PIT systems baseline configuration for even subtle deviations or changes in a systems hardware, software, or firmware. The continuous training, learning, and testing of the common agnostic intelligent anomaly detection and prevention tools provide system readiness to automatic test technology, cybersecurity health monitoring, and thorough detection of the slightest changes in normal system behavior. The application is expanded to include types of artificial neural networks, reinforcement learning, policy iteration, and value iteration in the future to advance the research and products for PIT systems.	https://dx.doi.org/10.1109/AUTOTESTCON43700.2019.8961052	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mortensen2020	Atomistic structure learning algorithm with surrogate energy model relaxation	The recently proposed atomistic structure learning algorithm (ASLA) builds on neural network enabled image recognition and reinforcement learning. It enables fully autonomous structure determination when used in combination with a first-principles total energy calculator, e.g., a density functional theory (DFT) program. To save on the computational requirements, ASLA utilizes the DFT program in a single-point mode, i.e., without allowing for relaxation of the structural candidates according to the force information at the DFT level. In this work, we augment ASLA to establish a surrogate energy model concurrently with its structure search. This enables approximative but computationally cheap relaxation of the structural candidates before the single-point energy evaluation with the computationally expensive DFT program. We demonstrate a significantly increased performance of ASLA for building benzene while utilizing a surrogate energy landscape. Further, we apply this model-enhanced ASLA in a thorough investigation of the c(4x8) phase of the Ag(111) surface oxide. ASLA successfully identifies a surface reconstruction which has previously only been guessed on the basis of scanning tunneling microscopy images.	https://dx.doi.org/10.1103/PhysRevB.102.075427	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Moura2019	Automatic Quality of Experience Management for WLAN Networks using Multi-Armed Bandit	Providing acceptable Quality of Experience (QoE) in Wireless Local Area Network (WLAN) is very difficult: home networks are managed by non-technical people, and the proprietary management solutions of enterprise networks usually do not incorporate QoE mechanisms. Due to these difficulties, automatic QoE management mechanisms are welcome. This paper presents control loops capable of changing the power and transmission channels in the WLAN, based on Software Defined Wireless Networks and reinforcement learning, in order to improve user satisfaction for Web applications. A prototype evaluates our proposal in three case studies with a web browsing application, in which several access points are controlled by a central controller or by independent controllers. Our results show that the control loop can improve the Mean Opinion Score (MOS) by at least 4 \% in the worst case, and 167 \% in the best case, thus benefiting the user. Further, the control loop also reduced in page load time by 25 \% in the worst case, and 233 \% in the best case.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Moura2020	Wireless control using reinforcement learning for practical web QoE	Wireless networks show several challenges not found in wired networks, due to the dynamics of data transmission. Besides, home wireless networks are managed by non-technical people, and providers do not implement full management services because of the difficulties of manually managing thousands of devices. Thus, automatic management mechanisms are desirable. However, such control mechanisms are hard to achieve in practice because we do not always have a model of the process to be controlled, or the behavior of the environment is dynamic. Thus, the control must adapt to changing conditions, and it is necessary to identify the quality of the control executed from the perspective of the user of the network service. This article proposes a control loop for transmission power and channel selection, based on Software Defined Networking and Reinforcement Learning (RL), and capable of improving Web Quality of Experience metrics, thus benefiting the user. We evaluate a prototype in which some Access Points are controlled by a single controller or by independent controllers. The control loop uses the predicted Mean Opinion Score (MOS) as a reward, thus the system needs to classify the web traffic. We proposed a semi-supervised learning method to classify the web sites into three classes (light, average and heavy) that groups pages by their complexity, i.e. number and size of page elements. These classes define the MOS predictor used by the control loop. The proposed web site classifier achieves an average score of 87\%+/- 1\%, classifying 500 unlabeled examples with only fifteen known examples, with a sub-second runtime. Further, the RL control loop achieves higher Mean Opinion Score (up to 167\% in our best result) than the baselines. The page load time of clients browsing heavy web sites is improved by up to 6.6x.	https://dx.doi.org/10.1016/j.comcom.2020.02.032	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Moy2019	Decentralized Spectrum Learning for IoT Wireless Networks Collision Mitigation	This paper describes the principles and implementation results of reinforcement learning algorithms on IoT devices for radio collision mitigation in ISM unlicensed bands. Learning is here used to improve both the IoT network capability to support a larger number of objects as well as the autonomy of IoT devices. We first illustrate the efficiency of the proposed approach in a proof-of-concept based on USRP software radio platforms operating on real radio signals. It shows how collisions with other RF signals present in the ISM band are diminished for a given IoT device. Then we describe the first implementation of learning algorithms on LoRa devices operating in a real LoRaWAN network, that we named IoTligent. The proposed solution adds neither processing overhead so that it can be ran in the IoT devices, nor network overhead so that no change is required to LoRaWAN. Real life experiments have been done in a realistic LoRa network and they show that IoTligent device battery life can be extended by a factor 2 in the scenarios we faced during our experiment.	https://dx.doi.org/10.1109/DCOSS.2019.00117	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Moy2020	Decentralized spectrum learning for radio collision mitigation in ultra-dense IoT networks: LoRaWAN case study and experiments	This paper describes the theoretical principles and experimental results of reinforcement learning algorithms embedded into IoT devices (Internet of Things), in order to tackle the problem of radio collision mitigation in ISM unlicensed bands. Multi-armed bandit (MAB) learning algorithms are used here to improve both the IoT network capability to support the expected massive number of objects and the energetic autonomy of the IoT devices. We first illustrate the efficiency of the proposed approach in a proof-of-concept, based on USRP software radio platforms operating on real radio signals. It shows how collisions with other RF signals are diminished for IoT devices that use MAB learning. Then we describe the first implementation of such algorithms on LoRa devices operating in a real LoRaWAN network at 868 MHz. We named this solution IoTligent. IoTligent does not add neither processing overhead, so it can be run into the IoT devices, nor network overhead, so that it requires no change to LoRaWAN protocol. Real-life experiments done in a real LoRa network show that IoTligent devices' battery life can be extended by a factor of 2, in the scenarios we faced during our experiment. Finally we submit IoTligent devices to very constrained conditions that are expected in the future with the growing number of IoT devices, by generating an artificial IoT massive radio traffic in anechoic chamber. We show that IoTligent devices can cope with spectrum scarcity that will occur at that time in unlicensed bands.	https://dx.doi.org/10.1007/s12243-020-00795-y	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Moy2021	An OpenAI-OpenDSS framework for reinforcement learning on distribution-level microgrids	This paper introduces a Python framework built upon open-source software for electric grids (OpenDSS) and deep learning (Open AI) for the study of further applications of reinforcement learning on distribution grid networks. This paper describes the framework and applies it to a 13-bus, grid-tied microgrid system, training and using a reinforcement learning agent to optimally control capacitor banks to maintain system voltage under changing loads. The performance of the agent is then compared to known optimal control, as well as to the performance of a capacitor controller built-in to OpenDSS, and a supervised learning-trained neural network.	https://dx.doi.org/10.1109/PESGM46819.2021.9638106	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mukherjee2011	A reinforcement learning approach with spline-fit object tracking for AIBO Robot's high level decision making		https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960937845&doi=10.1007\%2f978-3-642-22288-7_14&partnerID=40&md5=7a865a7a825182e63aaf3ba6e1908564	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Mukherjee2011a	Reinforcement learning approach to AIBO robot's decision making process in Robosoccer's goal keeper problem		https://www.scopus.com/inward/record.uri?eid=2-s2.0-81255138618&doi=10.1109\%2fSNPD.2011.39&partnerID=40&md5=b7e4ca8823ba7f1dd8790b54c5200bc6	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Muldrey2019	Mixed Signal Design Validation Using Reinforcement Learning Guided Stimulus Generation for Behavior Discovery	High operating speeds and use of aggressive fabrication technologies necessitate validation of mixed-signal electronic systems at every stage of top-down design: behavioral to netlist to physical design to silicon. At each step, design validation establishes the equivalence of lower level design descriptions against their higher level specifications. Prior research has leveraged state reachability analysis, nonconvex optimization, or performance specifications in order to generate tests. In contrast, we reformulate the systems under validation as a Markov decision process and examine the use of reinforcement-learning to provide a globally convergent solution, a means of ``storing'' the valuable information created during stimulus generation, and low-cost iterated generation. The integration of the proposed design validation methodology with deep-Q learning software and the suite of Cadence simulation tools is presented, validation results for selected design bugs in representative designs are analyzed, and the quality and efficiency of the proposed design validation methodology is discussed.	https://dx.doi.org/10.1109/VTS.2019.8758673	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Murakami2017	Design of incentive-based demand response programs using inverse optimization	An incentive design method is proposed for incentive-based demand response programs targeting residential consumers. Consumers are modelled as decision-makers and their models represent, unlike existing models, dynamical nature of power consumption behaviors. The design is done based on inverse optimization. The degree of freedom that exists in the solution can be effectively utilized to make the demand response program acceptable for consumers and economically efficient for power suppliers. Simulation tests using reinforcement learning have shown that the designed incentive works as expected.	https://dx.doi.org/10.1109/SMC.2017.8123043	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Murdock2001	Meta-case-Based Reasoning: Using Functional Models to Adapt Case-Based Agents			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E3: Only conceptual results are reported	4
RL4SE	Murillo-Morales2020	Automatic assistance to cognitive disabled web users via reinforcement learning on the browser		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091535619&doi=10.1007\%2f978-3-030-58805-2_8&partnerID=40&md5=84c0a44a700f6df52189f58e1308934a	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Musavi2018	Route selection over clustered cognitive radio networks: An experimental evaluation	Cognitive radio (CR) is the next-generation wireless communication system that allows unlicensed users (or secondary users, SUs) to explore and exploit the underutilized licensed spectrum (or white spaces) owned by licensed users (or primary users, PUs) in an opportunistic manner. This paper proposes a route selection scheme over a clustered cognitive radio network (CRN) that enables SUs to form clusters, and a SU source node to search for a route to its destination node. An intrinsic characteristic of CRN is the dynamicity of operating environment in which network conditions (i.e., PUs' activities) change as time goes by. Based on the network conditions, SUs form clusters whose cluster sizes are based on the number of available common channels in a cluster, select a common operating channel for each cluster, and search for a route over a clustered CRN using an artificial intelligence approach called reinforcement learning. Majority of the research related to CRNs has been limited to theoretical and simulation studies, and testbed investigation focusing on physical and data link layers. This investigation is a proof of concept focusing on the network layer of a route selection scheme over a clustered CRN in a universal software radio peripheral (USRP)/ GNU radio platform. Experimental results show that the proposed route selection scheme improves cluster stability by reducing the number of route breakages caused by route switches, and network scalability by reducing the number of clusters in the network without significant deterioration of quality of service, including throughput, packet delivery rate, and end-to-end delay.	https://dx.doi.org/10.1016/j.comcom.2018.07.035	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Na2020	Accelerate Personalized IoT Service Provision by Cloud-Aided Edge Reinforcement Learning: A Case Study on Smart Lighting		https://doi.org/10.1007/978-3-030-65310-1_6	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Naeem2021	A Generative Adversarial Network Enabled Deep Distributional Reinforcement Learning for Transmission Scheduling in Internet of Vehicles	The Cognitive Internet of Vehicles (CIoV) is an intelligent network that embeds the cognitive mechanism in the Internet of Vehicles (IoV) to sense the environment and observe the network states to learn the optimal policies adaptively. However, one of the key challenges in CIoV systems is to design a smart agent that can smartly schedule the packet transmission for ultra-reliable low latency communication (URLLC) under extreme random and noisy network conditions. We propose a software defined network (SDN) based scheduling algorithm that leverages generative adversarial network (GAN) based deep distributional Q-network (GAN-DDQN) for learning the action-value distribution for intelligent transmission scheduling. A reward-clipping technique is proposed for stabilizing the training of GAN-DDQN against the effect of broadly spanning utility values. The extensive simulation results verify that GAN-Scheduling achieves higher spectral efficiency (SE), service level agreement (SLA), system throughput, transmission packet rate with lower transmission delay, and power consumption compared to the existing reinforcement learning algorithms.	https://dx.doi.org/10.1109/TITS.2020.3033577	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Nair2018	Learning Fast Optimizers for Contextual Stochastic Integer Programs	We present a novel reinforcement learning (RL) approach to learning a fast and highly scalable solver for a two-stage stochastic integer program in the large-scale data setting. Mixed integer programming solvers do not scale to large datasets for this problem class. Additionally, they solve each instance independently, without any knowledge transfer across instances. We address these limitations with a learnable local search solver that jointly learns two policies, one to generate an initial solution and another to iteratively improve it with local moves. The policies use contextual features for a problem instance as input, which enables learning across instances and generalization to new ones. We also propose learning a policy to compute a bound on the objective using dual decomposition. Benchmark results show that on test instances our approach rapidly achieves approximately 30\% to 2000\% better objective value, which a state of the art integer programming solver (SCIP) requires more than an order of magnitude more running time to match. Our approach also achieves better solution quality on seven out of eight benchmark problems than standard baselines such as Tabu Search and Progressive Hedging.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Nakamoto2020	Toward autonomous adaptive embedded systems for sustainable services using reinforcement learning (WiP report)	A connected space comprises embedded systems that are attached to the physical space and cloud systems through the Internet. Using the connected space, various services can be continuously provided. These services can be dynamic and flexible based on user requirements and usage environments. The systems need to adapt to various changes in the need of users, service providers, and environments. However, embedded systems that implement elements of the connected space have resource constraints and difficulty in updating software; this is a significant challenge for embedded systems in providing dynamic and flexible services continuously. To tackle this challenge, this study considers reinforcement learning technologies for autonomous adaptive embedded systems for sustainable usage. We discuss the requirements of embedded systems and the rationale of the selected reinforcement learning method.	https://dx.doi.org/10.1109/CANDARW51189.2020.00063	Excluded	new_screen	E5: Other not a paper,E5: Other not a paper	4
RL4SE	Nanduri2011	Application of reinforcement learning-based algorithms in CO2 allowance and electricity markets	Climate change is one of the most important challenges faced by the world this century. In the U.S., the electric power industry is the largest emitter of CO2, contributing to the climate crisis. Federal emissions control bills in the form of cap-and-trade programs are currently idling in the U.S. Congress. In the mean time, ten states in the northeastern U.S. have adopted a regional cap-and-trade program to reduce CO2 levels and also to increase investments in cleaner technologies. Many of the states in which the cap-and-trade programs are active operate under a restructured market paradigm, where generators compete to supply power. This research presents a bi-level game-theoretic model to capture competition between generators in cap-and-trade markets and restructured electricity markets. The solution to the game-theoretic model is obtained using a reinforcement learning based algorithm.	https://dx.doi.org/10.1109/ADPRL.2011.5967367	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Nanduri2009	A reinforcement learning algorithm for obtaining the Nash equilibrium of multi-player matrix games	With the advent of e-commerce, the contemporary marketplace has evolved significantly toward competition-based trading of goods and services. Competition in many such market scenarios can be modeled as matrix games. This paper presents a computational algorithm to obtain the Nash equilibrium of n-player matrix games. The algorithm uses a stochastic-approximation-based Reinforcement Learning (RL) approach and has the potential to solve n-player matrix games with large player-action spaces. The proposed RL algorithm uses a value-iteration-based approach, which is well established in the Markov decision processes/semi-Markov decision processes literature. To emphasize the broader impact of our solution approach for matrix games, we discuss the established connection of matrix games with discounted and average reward stochastic games, which model a much larger class of problems. The solutions from the RL algorithm are extensively benchmarked with those obtained from an openly available software (GAMBIT). This comparative analysis is performed on a set of 16 matrix games with up to four players and 64 action choices. We also implement our algorithm on practical examples of matrix games that arise due to strategic bidding in restructured electric power markets.	https://dx.doi.org/10.1080/07408170802369417	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Nanduri2012	Economic impact assessment and operational decision making in emission and transmission constrained electricity markets	Carbon constrained electricity markets are a reality in 10 northeastern states and California in the US, as well as the European Union. Close to a Billion US Dollars have been spent by entities (mainly generators) in the Regional Greenhouse Gas Initiative in procuring CO2 allowances to meet binding emissions restrictions. In the near future, there are expected to be significant impacts due to the cap-and-trade program, especially when the cap stringency increases. In this research we develop a bilevel, complete-information, matrix game-theoretic model to assess the economic impact and make operational decisions in carbon-constrained restructured electricity markets. Our model is solved using a reinforcement learning approach, which takes into account the learning and adaptive nature of market participants. Our model also accounts for all the power systems constraints via a DC-OPF problem. We demonstrate the working of the model and compute various economic impact indicators such as supply shares, cost pass-through, social welfare, profits, allowance prices, and electricity prices. Results from a 9-bus power network are presented. (C) 2011 Elsevier Ltd. All rights reserved.	https://dx.doi.org/10.1016/j.apenergy.2011.12.012	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Narayanankutty2021	Self-Adapting Model-Based SDSec For IoT Networks Using Machine Learning	IoT networks today face a myriad of security vulnerabilities in their infrastructure due to its wide attack surface. Large-scale networks are increasingly adopting a Software-Defined Networking approach, it allows for simplified network control and management through network virtualization. Since traditional security mechanisms are incapable of handling virtualized environments, SDSec or Software-Defined Security is introduced as a solution to support virtualized infrastructure, specifically aimed at providing security solutions to SDN frameworks. To further aid large scale design and development of SDN frameworks, Model-Driven Engineering (MDE) has been proposed to be used at the design phase, since abstraction, automation and analysis are inherently key aspects of MDE. This provides an efficient approach to reducing large problems through models that abstract away the complex technicality of the total system. Making adaptations to these models to address security issues faced in IoT networks, largely reduces cost and improves efficiency. These models can be simulated, analysed and supports architecture model adaptation; model changes are then reflected back to the real system. We propose a model-driven security approach for SDSec networks that can self-adapt using machine learning to mitigate security threats. The overall design time changes can be monitored at run time through machine learning techniques (e.g. deep, reinforcement learning) for real time analysis. This approach can be tested in IoT simulation environments, for instance using the CAPS IoT modeling and simulation framework. Using self-adaptation of models and advanced machine learning for data analysis would ensure that the SDSec architecture adapts and improves over time. This largely reduces the overall attack surface to achieve improved end-to-end security in IoT environments.	https://dx.doi.org/10.1109/ICSA-C52384.2021.00023	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E5: Other not a paper	4
RL4SE	Nasereddin2019	Hybrid Robotic Reinforcement Learning for Inspection/Correction Tasks	The ability to rapidly program robots for complex tasks is an important precursor to wider adoption of robotics in industry. Robot programming is often time consuming and brittle to unanticipated variations in processing. Automated robot task learning is a solution to this problem. Reinforcement Learning (RL) is a commonly used approach for a robot to autonomously learn simple tasks. In RL, rewards are used to guide the robot towards learning an optimal plan or control policy. RL, however, has proven to be of limited value for problems with large-state spaces and considerable environmental variability. In this paper, we investigate formulation of the RL approach for inspect/correct types of tasks, specifically a misplaced block in a simple grid-world environment (requiring searching the gird world to identify a missing block and returning the missing block back to the target). We use a hybrid method, combining the SARSA algorithm and a model of the environment. The model of the environment is used as a reference model to reduce the state space, avoiding unnecessary exploration of the environment. A main focus of this research is the impact of task variability on RL performance. (C) 2019 The Authors. Published by Elsevier Ltd.	https://dx.doi.org/10.1016/j.promfg.2020.01.384	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Natafgi2018	Smart Traffic Light System Using Machine Learning	In Lebanon, traffic problems are a major concern for the population. The rising number of cars that exceeds the capacity of the roads, the inefficiency of public transportation infrastructures and the non-adaptive traffic light systems are contributors to the traffic crisis. Most roads in Lebanon suffer from traffic jams due to the traditional static green and red times allocations that are inconsiderate to the current state of the traffic. A solution to this problem is a system that adapts to the variations of the traffic dynamically and updates the traffic signal phases accordingly. In this paper, an adaptive traffic light system is implemented using reinforcement learning and tested using real data from Lebanese traffic. For training and testing the system, a software simulation tool is used. This tool can simulate the traffic intersection and allows the neural network to interact with it. Compared with the actual traffic light system, the proposed model displayed a reduction in average queue lengths by 62.82\% and in average queuing time by 56.37\%.	https://dx.doi.org/10.1109/IMCET.2018.8603041	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Nauck1994	NEFCON-I: an X-Window based simulator for neural fuzzy controllers	In this paper we present NEFCON-I, a graphical simulation environment for building and training neural fuzzy controllers based on the NEFCON model. NEFCON-I is an X-Window based software that allows the user to specify the initial fuzzy sets, fuzzy rules and rule based fuzzy error. The neural fuzzy controller is trained by a reinforcement learning procedure which is derived from the fuzzy error backpropagation algorithm for fuzzy perceptrons. NEFCON-I communicates with an external process where a dynamical system is simulated.<>	https://dx.doi.org/10.1109/ICNN.1994.374401	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Neal2021	Reinforcement Learning Based Penetration Testing of a Microgrid Control Algorithm	Microgrids (MGs) are small-scale power systems which interconnect distributed energy resources and loads within clearly defined regions. However, the digital infrastructure used in an MG to relay sensory information and perform control commands can potentially be compromised due to a cyberattack from a capable adversary. An MG operator is interested in knowing the inherent vulnerabilities in their system and should regularly perform Penetration Testing (PT) activities to prepare for such an event. PT generally involves looking for defensive coverage blind spots in software and hardware infrastructure, however the logic in control algorithms which act upon sensory information should also be considered in PT activities. This paper demonstrates a case study of PT for an MG control algorithm by using Reinforcement Learning (RL) to uncover malicious input which compromises the effectiveness of the controller. Through trial-and-error episodic interactions with a simulated MG, we train an RL agent to find malicious input which reduces the effectiveness of the MG controller.	https://dx.doi.org/10.1109/CCWC51732.2021.9376126	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ngo2013	Confidence-based progress-driven self-generated goals for skill acquisition in developmental robots	A reinforcement learning agent that autonomously explores its environment can utilize a curiosity drive to enable continual learning of skills, in the absence of any external rewards. We formulate curiosity-driven exploration, and eventual skill acquisition, as a selective sampling problem. Each environment setting provides the agent with a stream of instances. An instance is a sensory observation that, when queried, causes an outcome that the agent is trying to predict. After an instance is observed, a query condition, derived herein, tells whether its outcome is statistically known or unknown to the agent, based on the confidence interval of an online linear classifier. Upon encountering the first unknown instance, the agent queries the environment to observe the outcome, which is expected to improve its confidence in the corresponding predictor. If the environment is in a setting where all instances are known, the agent generates a plan of actions to reach a new setting, where an unknown instance is likely to be encountered. The desired setting is a self-generated goal, and the plan of action, essentially a program to solve a problem, is a skill. The success of the plan depends on the quality of the agent's predictors, which are improved as mentioned above. For validation, this method is applied to both a simulated and real Katana robot arm in its blocks-world environment. Results show that the proposed method generates sample-efficient curious exploration behavior, which exhibits developmental stages, continual learning, and skill acquisition, in an intrinsically-motivated playful agent.	https://www.ncbi.nlm.nih.gov/pubmed/24324448	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Nguyen2022	Reinforcement learning coupled with finite element modeling for facial motion learning	BACKGROUND AND OBJECTIVE: Facial palsy patients or patients with facial transplantation have abnormal facial motion due to altered facial muscle functions and nerve damage. Computer-aided system and physics-based models have been developed to provide objective and quantitative information. However, the predictive capacity of these solutions is still limited to explore the facial motion patterns with emerging properties. The present study aims to couple the reinforcement learning and the finite element modeling for facial motion learning and prediction. METHODS: A novel modeling workflow for learning facial motion was developed. A physically-based model of the face within the Artisynth modeling platform was used. Information exchange protocol was proposed to link reinforcement learning and rigid multi-bodies dynamics outcomes. Two reinforcement learning algorithms (deep deterministic policy gradient (DDPG) and Twin-delayed DDPG (TD3)) were used and implemented to drive the simulations of symmetry-oriented and smile movements. Numerical outcomes were compared to experimental observations (Bosphorus database) for evaluation and validation purposes. RESULTS: As result, after more than 100 episodes of exploring the environment, the agent starts to learn from previous trials and can find the optimal policy after more than 300 episodes of training. Regarding the symmetry-oriented motion, the muscle excitations predicted by the trained agent help to increase the value of reward from R = -2.06 to R = -0.23, which counts for approximately 89\% improvement of the symmetry value of the face. For smile-oriented motion, two points at the edge of the mouth move up 0.35 cm, which is within the range of movements estimated from the Bosphorus database (0.4 +/- 0.32 cm). CONCLUSIONS: The present study explored the muscle excitation patterns by coupling reinforcement learning with a detailed finite element model of the face. We developed, for the first time, a novel coupling scheme to integrate the finite element simulation into the reinforcement learning process for facial motion learning. As perspectives, this present workflow will be applied for facial palsy and facial transplantation patients to guide and optimize the functional rehabilitation program.	https://doi.org/10.1016/j.cmpb.2022.106904	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Nguyen2021	Model Predictive Control for Micro Aerial Vehicles: A Survey	This paper presents a review of the design and application of model predictive control strategies for Micro Aerial Vehicles and specifically multirotor configurations such as quadrotors. The diverse set of works in the domain is organized based on the control law being optimized over linear or nonlinear dynamics, the integration of state and input constraints, possible fault-tolerant design, if reinforcement learning methods have been utilized and if the controller refers to free-flight or other tasks such as physical interaction or load transportation. A selected set of comparison results are also presented and serve to provide insight for the selection between linear and nonlinear schemes, the tuning of the prediction horizon, the importance of disturbance observer-based offset-free tracking and the intrinsic robustness of such methods to parameter uncertainty. Furthermore, an overview of recent research trends on the combined application of modern deep reinforcement learning techniques and model predictive control for multirotor vehicles is presented. Finally, this review concludes with explicit discussion regarding selected open-source software packages that deliver off-the-shelf model predictive control functionality applicable to a wide variety of Micro Aerial Vehicle configurations.	https://dx.doi.org/10.23919/ECC54610.2021.9654841	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E5: Other not a paper	4
RL4SE	Nguyen2021a	Optimizing the resource usage of actor-based systems		https://doi.org/10.1016/j.jnca.2021.103143	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Nguyen2021b	Scaling UPF Instances in 5G/6G Core With Deep Reinforcement Learning	In the 5G core and the upcoming 6G core, the User Plane Function (UPF) is responsible for the transportation of data from and to subscribers in Protocol Data Unit (PDU) sessions. The UPF is generally implemented in software and packed into either a virtual machine or container that can be launched as a UPF instance with a specific resource requirement in a cluster. To save resource consumption needed for UPF instances, the number of initiated UPF instances should depend on the number of PDU sessions required by customers, which is often controlled by a scaling algorithm. In this paper, we investigate the application of Deep Reinforcement Learning (DRL) for scaling UPF instances that are packed in the containers of the Kubernetes container-orchestration framework. We propose an approach with the formulation of a threshold-based reward function and adapt the proximal policy optimization (PPO) algorithm. Also, we apply a support vector machine (SVM) classifier to cope with a problem when the agent suggests an unwanted action due to the stochastic policy. Extensive numerical results show that our approach outperforms Kubernetes's built-in Horizontal Pod Autoscaler (HPA). DRL could save 2.7\endash3.8\% of the average number of Pods, while SVM could achieve 0.7\endash4.5\% saving compared to HPA.	https://dx.doi.org/10.1109/ACCESS.2021.3135315	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Nguyen2022a	Towards designing a generic and comprehensive deep reinforcement learning framework		https://doi.org/10.1007/s10489-022-03550-z	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Nguyen2020	Representation learning for software engineering and programming languages		https://doi.org/10.1145/3416506.3423581	Excluded	conflict_resolution	E5: Other not a paper,E5: Other not a paper	4
RL4SE	Nickles2012	A system for the use of answer set programming in reinforcement learning		https://doi.org/10.1007/978-3-642-33353-8_40	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E5: Other not a paper	4
RL4SE	Nicola2022	Improvement of the Control of a Grid Connected Photovoltaic System Based on Synergetic and Sliding Mode Controllers Using a Reinforcement Learning Deep Deterministic Policy Gradient Agent	This article presents the control of a grid connected PV (GC-PV) array system, starting from a benchmark. The control structure used in this article was a cascade-type structure, in which PI or synergetic (SYN) controllers were used for the inner control loop of i(d) and i(q) currents and PI or sliding mode control (SMC) controllers were used for the outer control loop of the u(dc) voltage from the DC intermediate circuit. This paper presents the mathematical model of the PV array together with the main component blocks: simulated inputs for the PV array; the PV array itself; the MPPT algorithm; the DC-DC boost converter; the voltage and current measurements for the DC intermediate circuit; the load and connection to power grid; the DC-AC converter; and the power grid. It also presents the stages of building and training the reinforcement learning (RL) agent. To improve the performance of the control system for the GC-PV array system without using controllers with a more complicated mathematical description, the advantages provided by the RL agent on process controls could also be used. This technique does not require exact knowledge of the mathematical model of the controlled system or the type of uncertainties. The improvement in the control system performance for the GC-PV array system, both when using simple PI-type controllers or complex SMC- and SYN-type controllers, was achieved using an RL agent based on the Deep Deterministic Policy Gradient (DDPG). The variant of DDPG used in this study was the Twin-Delayed (TD3). The improvement in performance of the control system were obtained by using the correction command signals provided by the trained RL agent, which were added to the command signals u(d), u(q) and i(dref). The parametric robustness of the proposed control system based on SMC and SYN controllers for the GC-PV array system was proven in the case of a variation of 30\% caused by the three-phase load. Moreover, the results of the numerical simulations are shown comparatively and the validation of the synthesis of the proposed control system was obtained. This was achieved by comparing the proposed system with a software benchmark for the control of a GC-PV array system performed in MATLAB Simulink. The numerical simulations proved the superiority of the performance of control systems that use the RL-TD3 agent.	https://dx.doi.org/10.3390/en15072392	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Nie2022	Poster Abstract: Human-Centric Data-Driven Optimization and Recommendation in EV-Interfaced Grid at City Scale		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144621734&doi=10.1145\%2f3563357.3567752&partnerID=40&md5=69f400733f93de80fafc9193aad10caf	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E5: Other not a paper	4
RL4SE	Nikanjam2022	Faults in deep reinforcement learning programs: a taxonomy and a detection approach		https://doi.org/10.1007/s10515-021-00313-x	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Nikiruy2020	Spike-Timing-Dependent and Spike-Shape-Independent Plasticities with Dopamine-Like Modulation in Nanocomposite Memristive Synapses	In hardware neuromorphic systems (NSs), memristors are used as synaptic connections. In such systems, spike-timing-dependent plasticity (STDP) is a promising local learning rule. Herein, STDP is studied in a system composed of a pair of hardware or software neurons connected by a (CoFeB)(x)(LiNbO3)(100-x) nanocomposite-based memristor. The dopamine-like modulation of memristor-based STDP is implemented simply by the change in the polarity of spikes generated by artificial neurons operating in the inhibitory or excitatory mode. This modulation method is shown to be compatible with hardware neurons, different spike shapes, and is used in spiking NSs with bioinspired dopamine-like reinforcement learning.	https://dx.doi.org/10.1002/pssa.201900938	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Nikookar2022	Guided Task Planning Under Complex Constraints	Creating a plan, i.e., composing a sequence of items to achieve a task is inherently complex if done manually. This requires not only finding a sequence of relevant items but also understanding user requirements and incorporating them as constraints. For instance, in course planning, items are core and elective courses, and degree requirements capture their complex dependencies as constraints. In trip planning, items are points of interest (POIs) and constraints represent time and monetary budget, two user-specified requirements. Most importantly, a plan must comply with the ideal interleaving of items to achieve a goal such as enhancing students' skills towards the broader learning goal of an education program, or in the travel scenario, improving the overall user experience. We study the Task Planning Problem (TPP) with the goal of generating a sequence of items that optimizes multiple objectives while satisfying complex constraints. TPP is modeled as a Constrained Markov Decision Process, and we adapt weighted Reinforcement Learning to learn a policy that satisfies complex dependencies between items, user requirements, and satisfaction. We present a computational framework RL-Planner for TPP. RL-Planner requires minimal input from domain experts (academic advisors for courses, or travel agents for trips), yet produces personalized plans satisfying all constraints. We run extensive experiments on datasets from university programs and from travel agencies. We compare our solutions with plans drafted by human experts and with fully automated approaches. Our experiments corroborate that existing automated solutions are not suitable to solve TPP and that our plans are highly comparable to expensive handcrafted ones.	https://dx.doi.org/10.1109/ICDE53745.2022.00067	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Nitti2015	Planning in discrete and continuous Markov Decision Processes by probabilistic programming			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Noelle2011	Interacting Complementary Learning Systems in Brains and Machines	There is growing evidence for multiple complementary learning systems in the human brain. The midbrain dopamine system, along with its projections to the striatum and throughout cortex, can be seen as implementing a form of reinforcement learning. The prefrontal cortex, supported by loops through the thalamus, provides critical mechanisms for working memory, actively maintaining task relevant control information. The hippocampus, and parahippocampal cortical areas, provide support for the persistent retention of rapidly acquired information, such as memories of life episodes. Forming the foundation of all three of these systems are fundamental processes of synaptic plasticity. These complementary learning systems continuously interact, producing learning performance that transcends the capabilities of any one mechanism. Taking inspiration from this neural architecture, I present a multicomponent learning mechanism for software agents that learns from sparse rewards but may also benefit from situated instruction. This learning process is demonstrated in a simple maze learning task.		Excluded	new_screen	E5: Other not a paper,E1: Does not define or use a RL method	4
RL4SE	Noori2017	Glucose level control using Temporal Difference methods	Control theory has been widely used in various fields; one of these areas is medical issues. Diabetes is one of the new topics of interest in control. Obtaining the rates for the injection of insulin automatically always been a concern of physicians. The purpose of the control and treatment of diabetes, is keeping blood glucose in the normal range as possible. In this paper, we used Sarsa method - which is an on-policy Temporal Difference (TD) technique - for insulin delivery rate. TD methods are the most known methods for solving reinforcement learning problem. Because TD methods don't require a precise model of environment dynamics; they have absorbed interests in medical applications during recent years. Although temporal difference methods don't require a mathematical model of the environment, but for simulating an environment, we used Palumbo mathematical model instead of real patients. Since patients' medical parameters vary from person to person, for controlling the disease we should have different drug schedules, in other word, we should have different controller for each patient. While RL methods, by interacting with their environment, automatically define suitable doses for each person. If we want less trial and error on real patients and therefore reduce the side effects of changes in dose on the patient; according to the parameters of a patient, we design a controller which estimate the appropriate insulin injection rate. Then the drug program can be applied to other real patients. At this stage controller (applies Sarsa algorithm) with less trial and error, determines the appropriate dose for real patient. The results of the simulations, represents the efficiency of the proposed method.	https://dx.doi.org/10.1109/IranianCEE.2017.7985166	Excluded	new_screen	E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for	4
RL4SE	Nootyaskool2018	Reinforcement Learning Applied to Scrum Team towards Large-Scale Global Optimization	Large-scale problems have size of problem over a thousand dimensions in finding a best solution that uses long computation times. In this work, we use an idea of scrum methodology that is a well-known in software development companies, to create an optimization algorithm. The scrum methodology describing about the team organization likes as rugby team management that player have expert in game. The proposed algorithm is developed based on concept of the evolutionary computation by this work added agent specifics in leaning environment of the problem. The specific of the agent is reinforcement learning by taking an action and getting reward. The proposed algorithm was experimented on a large-scale global optimization finding optimum point of numerical function, comparing between with and without reinforcement learning. The experiment result showed that the usage of reinforcement learning has good results.	https://dx.doi.org/10.1109/ICSP.2018.8652286	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Nouri2007	Reactive power planning in distribution systems using a reinforcement learning method	This work presents a new reinforcement learning (RL) algorithm for capacitor allocation in distribution feeders. The problem formulation considers two distinct objectives related to total cost of power loss and total cost of capacitors including the purchase and installation costs. The formulation is a multi-objective and non-differentiable optimization problem. The proposed method of this article uses RL procedure for sizing and siting of capacitors in radial distribution feeders. The proposed method has been implemented in a software package and its effectiveness has been verified through a 9-bus radial distribution feeder and also a 34-bus radial distribution feeder for the sake of conclusions supports. A comparison has been made between the proposed method of this paper and similar methods in other research works to show its effectiveness for solving optimum capacitor planning problem.	https://dx.doi.org/10.1109/ICIAS.2007.4658366	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Nowakowski2022	Deep reinforcement learning coupled with musculoskeletal modelling for a better understanding of elderly falls	Reinforcement learning (RL) has been used to study human locomotion learning. One of the current challenges in healthcare is our understanding of and ability to slow the decline due to muscle ageing and its effect on human falls. The purpose of this study was to investigate reinforcement learning for human movement strategies when modifying muscle parameters to account for age-related changes. In particular, human falls with modified physiological factors were modelled and simulated to determine the effect of muscle descriptors for ageing on kinematic behaviour and muscle force control. A 3D musculoskeletal model (8 DoF and 22 muscles) of the human body was used. The deep deterministic policy gradient (DDPG) method was implemented. Different muscle descriptors for ageing were integrated, including changes in maximum isometric force, contraction velocity, the deactivation time constant and passive muscle strain. Additionally, the effects of isometric force reductions of 10, 20 and 30\% were also considered independently. An environment for the simulation was developed using the opensim-rl package for Python with the training process completed on Google Compute Engine. The simulation outcomes for healthy young adult and elderly falls under modified muscle behaviours were compared to experimental observations for validation. The result of our elderly simulation for multiple ageing-related factors (M_all) produced a walking speed of 0.26 m/s for the two steps taken prior to the fall. The over activation of the hip extensors and inactivation of knee extensors led to a backward fall for this elderly simulation. The inactivated rectus femoris and right tibialis are main actors of the forward fall. Our simulation outcomes are consistent with experimental observations through the comparison of kinematic features and motion history evolution. We showed in the present study, for the first time, that RL can be used as a strategy to explore the effect of ageing muscle physiological factors on kinematics and muscle control during falls. Our findings show that the elderly fall model for the M_all condition more closely resembles experimental elderly fall data than our simulations which considered age-related reductions of force alone. As future perspectives, the behaviour preceding a fall will be studied to establish the strategies used to avoid falls or fall with minimal consequence, leading to the identification of patient-specific rehabilitation programmes for elderly people.	https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128769424&doi=10.1007\%2fs11517-022-02567-3&partnerID=40&md5=f18390c13a50616bebe31211d9d97e07	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	O2021	Deep Reinforcement Learning based control algorithms: Training and validation using the ROS Framework in CARLA Simulator for Self-Driving applications	This paper presents a Deep Reinforcement Learning (DRL) framework adapted and trained for Autonomous Vehicles (AVs) purposes. To do that, we propose a novel software architecture for training and validating DRL based control algorithms that exploits the concepts of standard communication in robotics using the Robot Operating System (ROS), the Docker approach to provide the system with portability, isolation and flexibility, and CARLA (CAR Learning to Act) as our hyper-realistic open-source simulation platform. First, the algorithm is introduced in the context of Self-Driving and DRL tasks. Second, we highlight the steps to merge the proposed algorithm with ROS, Docker and the CARLA simulator, as well as how the training stage is carried out to generate our own model, specifically designed for the AV paradigm. Finally, regarding our proposed validation architecture, the paper compares the trained model with other state-of-the-art traditional control approaches, demonstrating the full strength of our DL based control algorithm, as a preliminary stage before implementing it in our real-world autonomous electric car.	https://dx.doi.org/10.1109/IV48863.2021.9575616	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Obi2021	A Lifetime-Aware Centralized Routing Protocol for Wireless Sensor Networks using Reinforcement Learning	This paper presents the design of a Lifetime-Aware Centralized Q-routing Protocol (LACQRP) for Wireless Sensor Network (WSN) to maximize the network lifetime. This is achieved by implementing Q-learning on the sink of the WSN, which also acts as a controller that has global knowledge of the network topology as enabled by Software-Defined WSN (SDWSN). The controller generates all possible distance-based minimum spanning trees (MSTs), which form the set of routing tables (RTs). The maximization of the network lifetime is achieved by the controller learning the routing table that minimizes the maximum of the sensor nodes' consumption energies using Reinforcement Learning (RL). The simulation results show that the LACQRP learns the best RT that maximizes the network lifetime and has a better network lifetime performance when compared with recent distributed RL routing protocols for lifetime optimization, which are Reinforcement Learning-Based Routing (RLBR) and Reinforcement Learning for Lifetime Optimization (R2LTO).	https://dx.doi.org/10.1109/WiMob52687.2021.9606390	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Oddi2020	Integrating Open-Ended Learning in the Sense-Plan-Act Robot Control Paradigm	This paper presents the achievements obtained from a study performed within the IMPACT (Intrinsically Motivated Planning Architecture for Curiosity-driven roboTs) Project funded by the European Space Agency (ESA). The main contribution of the work is the realization of an innovative robotic architecture in which the well-known three-layered architectural paradigm (decisional, executive, and functional) for controlling robotic systems is enhanced with autonomous learning capabilities. The architecture is the outcome of the application of an interdisciplinary approach integrating Artificial Intelligence (AI), Autonomous Robotics, and Machine Learning (ML) techniques. In particular, state-of-the-art AI planning systems and algorithms were integrated with Reinforcement Learning (RL) algorithms guided by intrinsic motivations (curiosity, exploration, novelty, and surprise). The aim of this integration was to: (i) develop a software system that allows a robotic platform to autonomously represent in symbolic form the skills autonomously learned through intrinsic motivations; (ii) show that the symbolic representation can be profitably used for automated planning purposes, thus improving the robot's exploration and knowledge acquisition capabilities. The proposed solution is validated in a test scenario inspired by a typical space exploration mission involving a rover.	https://dx.doi.org/10.3233/FAIA200373	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Oddi2020a	Integrating open-ended learning in the sense-plan-act robot control paradigm1		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091736014&doi=10.3233\%2fFAIA200373&partnerID=40&md5=4181a73a161201aaa03190bdd80febe4	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Oh2022	A reinforcement learning-based demand response strategy designed from the Aggregator's perspective	The demand response (DR) program is a promising way to increase the ability to balance both supply and demand, optimizing the economic efficiency of the overall system. This study focuses on the DR participation strategy in terms of aggregators who offer appropriate DR programs to customers with flexible loads. DR aggregators engage in the electricity market according to customer behavior and must make decisions that increase the profits of both DR aggregators and customers. Customers use the DR program model, which sends its demand reduction capabilities to a DR aggregator that bids aggregate demand reduction to the electricity market. DR aggregators not only determine the optimal rate of incentives to present to the customers but can also serve customers and formulate an optimal energy storage system (ESS) operation to reduce their demands. This study formalized the problem as a Markov decision process (MDP) and used the reinforcement learning (RL) framework. In the RL framework, the DR aggregator and each customer are allocated to each agent, and the agents interact with the environment and are trained to make an optimal decision. The proposed method was validated using actual industrial and commercial customer demand profiles and market price profiles in South Korea. Simulation results demonstrated that the proposed method could optimize decisions from the perspective of the DR aggregator.	https://dx.doi.org/10.3389/fenrg.2022.957466	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Oh2022a	A multi-use framework of energy storage systems using reinforcement learning for both price-based and incentive-based demand response programs		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135819508&doi=10.1016\%2fj.ijepes.2022.108519&partnerID=40&md5=8c59cd582c94f0ec4a2a4917264bdf27	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ohta2001	Gemini in RoboCup-2000			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ojand2022	Q-Learning-Based Model Predictive Control for Energy Management in Residential Aggregator		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122500432&doi=10.1109\%2fTASE.2021.3091334&partnerID=40&md5=b038b070a4eda87dd3bb82697b697529	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Okamura2011	Application of Reinforcement Learning to Software Rejuvenation	Software rejuvenation is a preventive and proactive maintenance solution that is particularly useful for counteracting the phenomenon of software aging. Hence, it should be ideally triggered adaptively without the complete knowledge on system failure (degradation) time distribution in operational phase. In this paper we consider an operational software system with multiple degradation levels and derive the optimal software rejuvenation policy maximizing the steady-state system availability, via the semi-Markov decision process. We develop a statistically non-parametric algorithm to estimate the optimal software rejuvenation schedule. Then, the reinforcement learning algorithm, called Q learning, is used for developing an on-line adaptive algorithm. A numerical example is presented to investigate asymptotic behavior of the resulting on-line adaptive algorithm.	https://dx.doi.org/10.1109/ISADS.2011.92	Included	new_screen		4
RL4SE	Ole2019	Towards Concept Based Software Engineering for Intelligent Agents	The development of AI and machine learning applications at an industry mature level while maintaining quality and productivity goals is one of today's major challenges. Research in the field of intelligent agents has achieved many successes in recent years, especially due to various reinforcement learning techniques, and promises a high benefit in times of automation and autonomous systems. Bringing them into production, however, requires optimization against many other criteria than just accuracy. This leads to the emerging field of machine teaching. We already know many of the objectives used there from software engineering research, which has led to many well-established principles in recent decades. One of them is the component-based development whose idea finds an interesting counterpart in hierarchical reinforcement learning. We show that both areas can benefit from each other and introduce our approach of Concept Based Software Engineering, which is focused on supporting productivity and quality goals during the development of such systems.	https://dx.doi.org/10.1109/RAISE.2019.00015	Included	new_screen		4
RL4SE	Oliehoek2017	The MADP Toolbox: An Open Source Library for Planning and Learning in (Multi-)Agent Systems	This article describes the Multiagent Decision Process (MADP) Toolbox, a software library to support planning and learning for intelligent agents and multiagent systems in uncertain environments. Key features are that it supports partially observable environments and stochastic transition models; has unified support for single- and multiagent systems; provides a large number of models for decision-theoretic decision making, including one-shot and sequential decision making under various assumptions of observability and cooperation, such as Dec-POMDPs and POSGs; provides tools and parsers to quickly prototype new problems; provides an extensive range of planning and learning algorithms for single- and multiagent systems; is released under the GNU GPL v3 license; and is written in C++ and designed to be extensible via the object-oriented paradigm. Keywords: software, decision-theoretic planning, reinforcement learning, multiagent systems		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Oliveira2019	Difference Based Metrics for Deep Reinforcement Learning Algorithms	Although state-of-the-art deep reinforcement learning often achieves superhuman levels on some tasks, the authors still struggle to analyze, compare or report the obtained results due to the unstable nature of the algorithms and the diversity of metrics used in the literature. Furthermore, these metrics fail to show some characteristics of the learning process, leading to misinterpretations by the analyst. The objective of this paper is to propose, implement and analyze a difference based evaluation metric that highlight different aspects of the learning process, allowing for more detailed results and analysis that can be used by automated software or analysts, experienced or not. A possible applicability for the proposed metric is to create automated evaluation systems, that can detect anomalies during the training process automatically.	https://dx.doi.org/10.1109/ACCESS.2019.2945879	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Oliveira2022	Assessing Policy, Loss and Planning Combinations in Reinforcement Learning Using a New Modular Architecture	The model-based reinforcement learning paradigm, which uses planning algorithms and neural network models, has recently achieved unprecedented results in diverse applications, leading to what is now known as deep reinforcement learning. These agents are quite complex and involve multiple components, factors that create challenges for research and development of new models. In this work, we propose a new modular software architecture suited for these types of agents, and a set of building blocks that can be easily reused and assembled to construct new model-based reinforcement learning agents. These building blocks include search algorithms, policies, and loss functions (Code available at https://github.com/GaspTO/Modular_MBRL). We illustrate the use of this architecture by combining several of these building blocks to implement and test agents that are optimized to three different test environments: Cartpole, Minigrid, and Tictactoe. One particular search algorithm, made available in our implementation and not previously used in reinforcement learning, which we called averaged minimax, achieved good results in the three tested environments. Experiments performed with our implementation showed the best combination of search, policy, and loss algorithms to be heavily problem dependent.	https://dx.doi.org/10.1007/978-3-031-16474-3_35	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Oliveira2022a	Assessing Policy, Loss and Planning Combinations in Reinforcement Learning Using a New Modular Architecture		https://doi.org/10.1007/978-3-031-16474-3_35	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Olowononi2022	Deep Reinforcement Learning for Deception in IRS-assisted UAV Communications	Research into the optimization of wireless communications continues to be of interest due to its prominent role in Internet-of-things and cyber physical systems (CPS) related applications. With emerging technologies like federated learning (FL) and software defined networks deployed over the air, the design of wireless networks must be rethought to align well with technological advancements. Intelligent Reflective Surfaces (IRS), a technology that enhances the controllability of the channel between the transmitter and the receiver has demonstrated a great potential to enhance communications especially in challenging terrains. In this research, as a case study, we consider the deployment of IRS with unmanned aerial vehicles to enhance wireless communications for battlefield scenarios. We also study the security concerns in such deployment using a deep reinforcement learning (DRL) coupled with defensive deception approach. Specifically, data-driven power allocation in communication channels using RL is leveraged upon to obfuscate the attack surface, lure jammers into designated channels and ultimately mitigate attempted denial-of-service attacks. Simulation experiments carried out attest to the veracity of the proposed approach.	https://dx.doi.org/10.1109/MILCOM55135.2022.10017780	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Olyaei2019	Implement deep SARSA in grid world with changing obstacles and testing against new environment		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054307344&doi=10.1007\%2f978-981-10-8672-4_20&partnerID=40&md5=889c5075de0813a693c655edc9c5bfa6	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Omri2022	Learning to Rank for Test Case Prioritization	In Continuous Integration (CI) environments, the productivity of software engineers depends strongly on the ability to reduce the round-trip time between code commits and feedback on failed test cases. Test case prioritization is popularly used as an optimization mechanism for ranking tests by their likelihood in revealing failures. However, existing techniques are usually time and resource intensive making them not suitable to be applied within CI cycles. This paper formulates the test case prioritization problem as an online learn-to-rank model using reinforcement learning techniques. Our approach minimizes the testing overhead and continuously adapts to the changing environment as new code and new test cases are added in each CI cycle. We validated our approach on an industrial case study showing that over 95\% of the test failures are still reported back to the software engineers while only 40\% of the total available test cases are being executed.	https://dx.doi.org/10.1145/3526072.3527525	Included	new_screen		4
RL4SE	Omwenga2021	ScanCloud: Holistic GPR Data Analysis for Adaptive Subsurface Object Detection	The conventional ground penetrating radar (GPR) data analysis methods, which use piecemeal approaches in processing the GPR data formulated in variant formats such as A-Scan, B-Scan, and C-Scan, fail to provide a global view of underground objects on the fly to adapt the operations of GPR systems in the field. To bridge the gap, in this paper, we propose a novel GPR data analysis approach termed ``ScanCloud'' which is focused on the whole in situ GPR dataset rather than on individual A-Scans, B-Scans or C-Scans. We also study the integration of ScanCloud and a deep reinforcement learning method called deep deterministic policy gradient (DDPG) to adapt the operation of GPR system. The proposed method is evaluated using GPR modeling software called GprMax. Simulation results show the efficacy of ScanCloud and the adaptive GPR system enabled by the integration of ScanCluod and DDPG.	https://dx.doi.org/10.1109/IRI51335.2021.00027	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Omwenga2021a	Cognitive GPR for Subsurface Object Detection Based on Deep Reinforcement Learning	Ground penetrating radars (GPRs) carried by mobile platforms, such as vehicles and drones, have been applied in various applications, for instance, subsurface utility detection, structural health inspection, and autonomous driving. However, existing GPR systems are not able to operate autonomously and adaptively due to several challenges, including the lack of intelligence, uncertain and dynamic nature of sensing environments, and huge state and action spaces. To overcome these challenges, in this article, we propose an autonomous cognitive GPR (AC-GPR) enabled by a deep reinforcement learning (DRL) approach. Specifically, the operation of the proposed AC-GPR is first formulated as a sequential decision process. A novel reward function is developed for the DRL model by defining and combining two different types of entropy-based rewards resulting from object detection and recognition, respectively. A deep Q-learning network (DQN) is developed to address the extreme curse of dimensionality in the state space and learn a policy directing the actions of the AC-GPR. The AC-GPR is evaluated using software called GprMax by combining DRL with GPR modeling and simulation. Results show that our proposed DRL-based AC-GPR outperforms other GPR systems using different approaches in terms of detection accuracy and operating time.	https://dx.doi.org/10.1109/JIOT.2021.3059281	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Orhan2021	Connection Management xAPP for O-RAN RIC: A Graph Neural Network and Reinforcement Learning Approach	Connection management is an important problem for any wireless network to ensure smooth and well-balanced operation throughout. Traditional methods for connection management (specifically user-cell association) consider sub-optimal and greedy solutions such as connection of each user to a cell with maximum receive power. However, network performance can be improved by leveraging machine learning (ML) and artificial intelligence (AI) based solutions. The next generation software defined 5G networks defined by the Open Radio Access Network (O-RAN) alliance facilitates the inclusion of ML/AI based solutions for various network problems. In this paper, we consider intelligent connection management based on the O-RAN network architecture to optimize user association and load balancing in the network. We formulate connection management as a combinatorial graph optimization problem. We propose a deep reinforcement learning (DRL) solution that uses the underlying graph to learn the weights of the graph neural networks (GNN) for optimal user-cell association. We consider three candidate objective functions: sum user throughput, cell coverage, and load balancing. Our results show up to 10\% gain in throughput, 45-140\% gain cell coverage, 20-45\% gain in load balancing depending on network deployment configurations compared to baseline greedy techniques.	https://dx.doi.org/10.1109/ICMLA52953.2021.00154	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Orozov2021	Rule-based system against reinforcement learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117607164&doi=10.1145\%2f3472410.3472437&partnerID=40&md5=1f0b41ad6907259ffcb5e3d0a43928aa	Excluded	new_screen	E5: Other not a paper,E2: Software engineering is not the problem RL is used for	4
RL4SE	Orseau2014	Teleporting universal intelligent agents		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905814917&doi=10.1007\%2f978-3-319-09274-4_11&partnerID=40&md5=790e058f08f7a6b8603c7a63a7b9255e	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ouattara2018	Duality Approach to Bilevel Programs with a Convex Lower Level	Bilevel programs are optimization problems where some variables are solutions to optimization problems themselves, and they arise in a variety of control applications, including: control of vehicle traffic networks, inverse reinforcement learning and inverse optimization, and robust control for human-automation systems. This paper develops a duality-based approach to solving bilevel programs where the lower level problem is convex. Our approach is to use partial dualization to construct a new dual function that is differentiable, unlike the Lagrangian dual that is only directionally differentiable. We use our dual to define a duality-based reformulation of bilevel programs, prove equivalence of our reformulation with the original bilevel program, and then introduce regularization to ensure constraint qualification holds. These technical results about our new dual and regularized duality-based reformulation are used to provide theoretical justification for an algorithm we construct for solving bilevel programs with a convex lower level, and we conclude by demonstrating the efficacy of our algorithm by solving two practical instances of bilevel programs.	https://dx.doi.org/10.23919/ACC.2018.8431802	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ouyang2018	Audio-Visual Emotion Recognition with Capsule-like Feature Representation and Model-Based Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055559609&doi=10.1109\%2fACIIAsia.2018.8470316&partnerID=40&md5=cb9e2582bea7cba97ee80281b0ed046a	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ozaln2019	An Implementation of Vision Based Deep Reinforcement Learning for Humanoid Robot Locomotion	Deep reinforcement learning (DRL) exhibits a promising approach for controlling humanoid robot locomotion. However, only values relating sensors such as IMU, gyroscope, and GPS are not sufficient robots to learn their locomotion skills. In this article, we aim to show the success of vision based DRL. We propose a new vision based deep reinforcement learning algorithm for the locomotion of the Robotis-op2 humanoid robot for the first time. In experimental setup, we construct the locomotion of humanoid robot in a specific environment in the Webots software. We use Double Dueling Q Networks (D3QN) and Deep Q Networks (DQN) that are a kind of reinforcement learning algorithm. We present the performance of vision based DRL algorithm on a locomotion experiment. The experimental results show that D3QN is better than DQN in that stable locomotion and fast training and the vision based DRL algorithms will be successfully able to use at the other complex environments and applications.	https://dx.doi.org/10.1109/INISTA.2019.8778209	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ozeloglu2022	Deep reinforcement learning-based autonomous parking design with neural network compute accelerators	We describe the design and implementation of an autonomous prototype vehicle which finds an empty parking slot in a parking area, and parks itself in the empty parking slot, using neural networks based on deep reinforcement learning (RL). To perform an autonomous parking procedure for our prototype vehicle, two different artificial neural networks (ANNs) are trained using a deep RL Algorithm in a simulation environment and embedded into the computing platform of the prototype car. One of the ANNs enables the vehicle to drive autonomously in the parking environment. At the same time, an image processing algorithm is used to determine whether a parking slot is empty. When the image processing algorithm finds a suitable parking slot, a different ANN is activated and performs a safe parking procedure. However, ANN-based machine learning techniques require high processing power and impose a high computational burden on embedded CPU and GPU platforms. To alleviate the computational burden, one can achieve higher performance and less power consumption using an application-specific hardware design, where logic resources are fully exploited according to the algorithm of interest, in an energy-efficient manner. In this article, hardware accelerators for our ANN models are designed and generated via the Vivado high-level synthesis (HLS) tool, targeting an ARM based programmable SoC platform, ZedBoard. Our ANN accelerators have achieved a speedup of 17x as compared to an ARM software implementation. For deeper fully-connected layers used in deep RL-based solutions, function-level parallelism (Vivado's dataflow) is employed to improve the computational efficiency. Our proposed stage-level description for fully connected layers outperforms recent studies in terms of computation time.	https://dx.doi.org/10.1002/cpe.6670	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Padberg2005	On the potential of process simulation in software project schedule optimization	In this paper, we highlight the application potential of process simulation techniques for software cybernetics research. Software engineering has seen many fruitful applications of simulation when modeling, understanding, and improving the software development process. In particular, process simulation has proven to be a valuable and efficient tool in our own software cybernetics research, having helped us to understand how scheduling policies actually behave in our discrete-time Markov decision process model for software projects. We outline how to advance the use of process simulation in our model to a much higher level: when computing optimal scheduling policies, simulation can be applied in the optimization step of the dynamic programming algorithms in order to save computation time. This approach resembles optimization techniques from the field of reinforcement learning, providing further evidence of the potential of simulation in software cybernetics.	https://dx.doi.org/10.1109/COMPSAC.2005.115	Excluded	conflict_resolution	E5: Other not a paper,E5: Other not a paper	4
RL4SE	Padberg2011	Model-Based Scheduling Analysis for Software Projects	"We show how to compute optimal policies for the scheduling of software development projects under uncertainty. Our approach is based on a stochastic scheduling model that explicitly captures the strong feedback between the tasks in software development (""ripple effects""). We apply reinforcement learning to the optimization problem. For a selected sample project, we compute the optimal policy, simulate the project, and analyze the task assignments that are made by the optimal policy. From the analysis of the simulated schedules, we derive tentative, generic scheduling guidelines. The guidelines prioritize certain tasks based on the characteristics of the software architecture, and assign tasks according to the past performance of the developers."	https://dx.doi.org/10.1109/COMPSACW.2011.57	Included	new_screen		4
RL4SE	Padberg2011a	Optimal Scheduling of Software Projects Using Reinforcement Learning	"We compute optimal scheduling policies for software development projects. We use reinforcement learning as the optimization technique. Our approach is based on a formal, stochastic scheduling model that explicitly captures the strong feedback between the tasks in software development (""ripple effects""). For sample projects, we compute the optimal policy, simulate the project, and analyze the task assignments that are made by the optimal policy. We find that optimal policies typically assign tasks according to the past performance of the developers and the characteristics of the software design. In particular, we address the problem of when to schedule large or strongly coupled components. We also sketch approaches to the optimization of large projects."	https://dx.doi.org/10.1109/APSEC.2011.39	Included	new_screen		4
RL4SE	Paduraru2021	Task Distribution and Human Resource Management Using Reinforcement Learning	The process of assigning tasks in large companies is a costly expenditure of human resources. Usually, many people are employed to distribute tasks as best as possible among the people involved in the projects. While there are software applications that support this effort, they are limited, and the people who make the decisions about where to send the various tasks considering load balancing, evaluating the capabilities of the possible solvers and many other factors are still handled manually. In this paper, we propose a solution using reinforcement learning to train an automatic agent capable of managing the process itself, thus reducing human effort and cost. Our method first attempts to learn from existing datasets and then improve itself in an unsupervised manner. The results are promising and validate our original idea that using an automated agent to address the observed gap can be a valuable addition to existing task management applications.	https://dx.doi.org/10.1109/ASEW52652.2021.00029	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Paduraru2021a	RiverFuzzRL - an open-source tool to experiment with reinforcement learning for fuzzing	Combining fuzzing techniques and reinforcement learning could be an important direction in software testing. However, there is a gap in support for experimentation in this field, as there are no open-source tools to let academia and industry to perform experiments easily. The purpose of this paper is to fill this gap by introducing a new framework, named RiverFuzzRL, on top of our already mature frame-work for AI-guided fuzzing, River. We provide out-of-the-box implementations for users to choose from or customize for their test target. The work presented here is performed on testing binaries and does not require access to the source code, but it can be easily adapted to other types of software testing as well. We also discuss the challenges faced, opportunities, and factors that are important for performance, as seen in the evaluation.	https://dx.doi.org/10.1109/ICST49551.2021.00055	Included	new_screen		4
RL4SE	Paggi2014	Use of a Semantic Language to Reduce the Indeterminacy in Agents Communication	In the field of agent communications uncertainty and vagueness in the message content and in the achievable results play a primordial role when two agents (human or artificial) communicate. Even though the importance of vagueness and uncertainty has been recognized long ago, only recently mechanisms related to the communications' semantics that allow a practical approach have been designed; more specifically, the development of tools such as agent programming languages and frameworks, which is a field of intensive research. On the other hand, recent theoretical ideas, drawn from situation semantics theory and the works of Sutton on semantic information, support this work. This paper applies these ideas to the field of multi-agent systems (MAS) and sketches how one can reduce the impact of vagueness and uncertainty present in the communication between software agents by means of context information, collaboration and basic reinforcement learning using a language designed for agent communication: the Sematic Agent Programming Language (S-APL).	https://dx.doi.org/10.1109/MCSI.2014.64	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Pagliarini2014	ALife for Real and Virtual Audio-Video Performances	"MAG (i.e.: an Italian acronym which stands for Musical Genetic Algorithms) is an electronic art piece in which a multifaceted software attempts to ""translate"" musical expression into a corresponding static or animated graphical expressions. The mechanism at the base of such ""translation"" consists in a quite complex and articulated algorithm that, in short, is based on artificial learning. Indeed, MAG implements different learning techniques to allow artificial agents to learn about music flow by developing an adaptive behaviour. In our specific case, such a technique consists of a population of neural networks - one dimensional artificial agents that populate their two dimensional artificial world, and which are served by a simple input output control system - that can use both genetic and reinforcement learning algorithms to evolve appropriate behavioural answers to an impressively large shapes of inputs, through both a fitness formula based genetic pressure, and, eventually, a user-machine based feedbacks. More closely, in the first version of MAG algorithm the agents' control system is a perceptron; the world of the agents is a two dimensional grid that changes its dimensions accordingly to the host-screen; the most important input artificial agents get (i.e. not necessarily the only one) is the musical wave that any given musical file produces, run-time; the output is the behavioural answer that agents produce by moving, and thereby drawing on to a computer screen, therefore graphical. The combination of artificial evolution and the flows of a repeated song or different musical tunes make it possible for the software to obtain a special relationship between sound waves and the aesthetics of consequent graphical results. Further, we started to explore the concept of run-time creation of both music and graphical expression. Recently, we developed a software by which it is possible to allow any user to create new song versions of popular music with the MusicTiles app simply by connecting musical building blocks. This creation of musical expression can happen as a performance (i.e. run-time). When connecting the MusicTiles app to the MAG software, we provide the connection and the possibility to melt both musical expression and graphical expression in parallel and at run-time, and therefore creating a audio-video performance that is always unique."		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Palmieri2020	Dataset of active avoidance in Wistar-Kyoto and Sprague Dawley rats: Experimental data and reinforcement learning model code and output	"Data were collected from 40 Wistar-Kyoto (WKY) and 40 Sprague Dawley (SD) rats during an active escape-avoidance experiment. Footshock could be avoided by pressing a lever during a danger period prior to onset of shock. If avoidance did not occur, a series of footshocks was administered, and the rat could press a lever to escape (terminate shocks). For each animal, data were simplified to the presence or absence of lever press and stimuli in each 12-second time frame. Using the pre-processed dataset, a reinforcement learning (RL) model, based on an actor-critic architecture, was utilized to estimate several different model parameters that best characterized each rat's behaviour during the experiment. Once individual model parameters were determined for all 80 rats, behavioural recovery simulations were run using the RL model with each animal's ""best-fit"" parameters; the simulated behaviour generated avoidance data (percent of trials avoided during a given experimental session) that could be compared across simulated rats, as is customarily done with empirical data. The datasets representing both the experimental data and the model-generated data can be interpreted in various ways to gain further insight into rat behaviour during avoidance and escape learning. Furthermore, the estimated parameters for each individual rat can be compared across groups. Thus, possible between-strain differences in model parameters can be detected, which might provide insights into strain differences in learning. The software implementing the RL model can also be applied to or serve as a template for other experiments involving acquisition learning. Reference for Co-Submission: K.M. Spiegler, J. Palmieri, K.C.H. Pang, C.E. Myers, A reinforcement-learning model of active avoidance behavior: Differences between Sprague-Dawley and Wistar-Kyoto rats. Behay. Brain Res. (2020 Jun 22[epub ahead of print]) doi: 10.1016/j.bbr.2020.112784 (C) 2020 The Authors. Published by Elsevier Inc."	https://www.ncbi.nlm.nih.gov/pubmed/32904157	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Palombarini2012	SmartGantt - An interactive system for generating and updating rescheduling knowledge using relational abstractions	Generating and updating rescheduling knowledge that can be used in real time has become a key issue in reactive scheduling due to the dynamic and uncertain nature of industrial environments and the emergent trend towards cognitive systems in production planning and execution control. Disruptive events have a significant impact on the feasibility of plans and schedules. In this work, the automatic generation and update through learning of rescheduling knowledge using simulated transitions of abstract schedule states is proposed. An industrial example where a current schedule must be repaired in response to unplanned events such as the arrival of a rush order, raw material delay, or an equipment failure which gives rise to the need for rescheduling is discussed. A software prototype (SmartGantt) for interactive schedule repair in real-time is presented. Results demonstrate that responsiveness is dramatically improved by using relational reinforcement learning and relational abstractions to develop a repair policy. (C) 2012 Elsevier Ltd. All rights reserved.	https://dx.doi.org/10.1016/j.compchemeng.2012.06.021	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Pamuklu2021	Reinforcement Learning Based Dynamic Function Splitting in Disaggregated Green Open RANs	With the growing momentum around Open RAN (O-RAN) initiatives, performing dynamic Function Splitting (FS) in disaggregated and virtualized Radio Access Networks (vRANs), in an efficient way, is becoming highly important. An equally important efficiency demand is emerging from the energy consumption dimension of the RAN hardware and software. Supplying the RAN with Renewable Energy Sources (RESs) promises to boost the energy-efficiency. Yet, FS in such a dynamic setting, calls for intelligent mechanisms that can adapt to the varying conditions of the RES supply and the traffic load on the mobile network. In this paper, we propose a reinforcement learning (RL)based dynamic function splitting (RLDFS) technique that decides on the function splits in an O-RAN to make the best use of RES supply and minimize operator costs. We also formulate an operational expenditure minimization problem. We evaluate the performance of the proposed approach on a real data set of solar irradiation and traffic rate variations. Our results show that the proposed RLDFS method makes effective use of RES and reduces the cost of an MNO. We also investigate the impact of the size of solar panels and batteries which may guide MNOs to decide on proper RES and battery sizing for their networks.	https://dx.doi.org/10.1109/ICC42927.2021.9500721	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Pan2020	Additional planning with multiple objectives for reinforcement learning	Most control tasks have multiple objectives that need to be achieved simultaneously, while the reward definition is the weighted combination of all objects to determine one optimal policy. This configuration has a limitation in exploration flexibility and presents difficulty in reaching a satisfied terminate condition. Although some multi-objective reinforcement learning (MORL) methods have been presented recently, they concentrate on obtaining a set of compromising options rather than one best-performed strategy. On the other hand, the existing policy-improve methods have rarely emphasized on solving multiple objectives circumstances. Inspired by the enhanced policy search methods, an additional planning technique with multiple objectives for reinforcement learning is proposed in this paper, which is denoted as RLAP-MOP. This method provides opportunities to evaluate parallel requirements at the same time and suggests several optimal feasible actions to improve long-term performance further. Meanwhile, the short-term planning adopted in this paper has advantages in maintaining safe trajectories and building more accurate approximate models, which contributes to accelerating the training program. For comparison, an RLAP with single-objective optimization is also introduced in theoretical and experimental studies. The proposed techniques are investigated on a multi-objective cartpole environment and a soft robotic palpation task. The superiorities in the improved return values and learning stability prove that the multiple objectives based additional planning is a promising assistant to improve reinforcement learning. (c) 2019 Elsevier B.V. All rights reserved.	https://dx.doi.org/10.1016/j.knosys.2019.105392	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Pan2022	EDAML 2022 Keynote Speaker: Machine Learning for Agile, Intelligent and Open-Source EDA	This talk will present some recent results and trends toward agile, intelligent and open-source design automation for digital/analog/mixed-signal ICs, in particular leveraging AI/machine learning with domainspecific customizations. Placement is a fundamental EDA problem. I will first show how we leverage deep learning hardware and software to develop an open-source VLSI placement engine, DREAMPlace [DAC'19 and TCAD'21 Best Paper Awards], which is around 40x faster than the previous state-of-the-art academic global placer. DREAMPlace has been further developed to tackle detailed placement acceleration and region constraints, and used together with reinforcement learning for macro placement to achieve superhuman results. I will then present the DARPA-funded project MAGICAL which leverages both machine and human intelligence to produce fully automated analog layout from netlists to GDSII, including automatic layout constraint generation, placement, and routing. MAGICAL 1.0 has been open-sourced, and validated with silicon tape-outs. The talk will conclude with some future research directions.	https://dx.doi.org/10.1109/IPDPSW55747.2022.00193	Excluded	new_screen	E5: Other not a paper,E5: Other not a paper	4
RL4SE	Pan2021	Artificial neural network for defect detection in CT images of wood		https://doi.org/10.1016/j.compag.2021.106312	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Pan2015	Study and Discussion on Professional Ability Training of Mechanical Specialty of Common Colleges	"Since China's reform and opening up, the education concept of the higher education has taken place the larger change. In our country, it is gradually changing from ""made in China"" to ""designed in China"		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Pan2021a	GreenTE.ai: Power-Aware Traffic Engineering via Deep Reinforcement Learning	"Power-aware traffic engineering via coordinated sleeping is usually formulated into Integer Programming problems, which are generally NP-hard with unbounded computation time for large-scale networks. This results in delayed control decision making in dynamic network environments. Motivated by advances in deep Reinforcement Learning, we consider building intelligent systems that learn to adaptively change router/switch's power state according to changing network conditions. Neural network's forward propagation can greatly speed up power on/off decision making. Generally, conducting RL requires a learning agent to iteratively explore and perform the ""good"" actions based on the feedback from the environment. By coupling Software-Defined Networking for performing centrally calculated actions to the environment and In-band Network Telemetry for collecting feedback from the environment, we develop GreenTE.ai, a closed-loop control/training system to automate power-aware traffic engineering. Furthermore, we propose novel techniques to enhance the learning ability and reduce the learning complexity. With both energy efficiency and traffic load balancing considered, GreenTE.ai can generate reasonable power saving actions within 276ms under a network testbed of 11 software P4 switches."	https://dx.doi.org/10.1109/IWQOS52092.2021.9521281	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Pandala2022	Robust Predictive Control for Quadrupedal Locomotion: Learning to Close the Gap Between Reduced- and Full-Order Models		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130445454&doi=10.1109\%2fLRA.2022.3176105&partnerID=40&md5=cea83d70805aad67ec37fc4677cacbc0	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Pandey2013	Learning algorithms For intelligent agents based e-learning system	Requirements are the basic entity which makes the project to be successfully implemented. In the development of the software, theses requirements must be measured accurately and correctly. The requirements of the end users may be changed with respect to different individual personality. As different requirements are given by different customers, all of them cannot be processed by single software system. In such a case, it is essential to make the changes in the software systems automatically which fulfills all the requirements of the user. Such condition is met by the systems with intelligent agents. In this paper, algorithms of intelligent agents adviser agent, personalization agent and content managing agent are proposed to automatically gather the requirements of the students and fulfill them. The algorithms are structured using reinforcement learning. Theses algorithm makes the agents to sense the needs of the students and evolve in the course of their operation. These intelligent agents are applied after the deployment of the e-learning software. All the algorithms have been implemented successfully.	https://dx.doi.org/10.1109/IAdCC.2013.6514369	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Pandey2020	Q-Learning based SFC deployment on Edge Computing Environment	Reinforcement learning (RL) has been used in various path finding applications including games, robotics and autonomous systems. Deploying Service Function Chain (SFC) with optimal path and resource utilization in edge computing environment is an important and challenging problem to solve in Software Defined Network (SDN) paradigm. In this paper we used RL based Q-Learning algorithm to find an optimal SFC deployment path in edge computing environment with limited computing and storage resources. To achieve this, our deployment scenario uses a hierarchical network structure with local, neighbor and datacenter servers. Our Q-Learning algorithm uses an intuitive reward function which does not only depend on the optimal path but also considers edge computing resource utilization and SFC length. We defined regret and empirical standard deviation as evaluation parameters. We evaluated our results by making 1200 test cases with varying SFC-length, edge resources and Virtual Network Function's (VNF) resource demand. The computation time of our algorithm varies between 0.03~0.6 seconds depending on the SFC length and resource requirement.	https://dx.doi.org/10.23919/APNOMS50412.2020.9236981	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Panerati2021	Learning to Fly&#x2014;a Gym Environment with PyBullet Physics for Reinforcement Learning of Multi-agent Quadcopter Control		https://doi.org/10.1109/IROS51168.2021.9635857	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Panerati2021a	Learning to Flyemdasha Gym Environment with PyBullet Physics for Reinforcement Learning of Multi-agent Quadcopter Control	Robotic simulators are crucial for academic research and education as well as the development of safety-critical applications. Reinforcement learning environments\emdash simple simulations coupled with a problem specification in the form of a reward function\emdashare also important to standardize the development (and benchmarking) of learning algorithms. Yet, full-scale simulators typically lack portability and paral-lelizability. Vice versa, many reinforcement learning environments trade-off realism for high sample throughputs in toy-like problems. While public data sets have greatly benefited deep learning and computer vision, we still lack the software tools to simultaneously develop\emdashand fairly compare\emdashcontrol theory and reinforcement learning approaches. In this paper, we propose an open-source OpenAI Gym-like environment for multiple quadcopters based on the Bullet physics engine. Its multi-agent and vision-based reinforcement learning interfaces, as well as the support of realistic collisions and aerodynamic effects, make it, to the best of our knowledge, a first of its kind. We demonstrate its use through several examples, either for control (trajectory tracking with PID control, multi-robot flight with downwash, etc.) or reinforcement learning (single and multi-agent stabilization tasks), hoping to inspire future research that combines control theory and machine learning.	https://dx.doi.org/10.1109/IROS51168.2021.9635857	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Panerati2021b	Learning to Fly-a Gym Environment with PyBullet Physics for Reinforcement Learning of Multi-agent Quadcopter Control	Robotic simulators are crucial for academic research and education as well as the development of safety-critical applications. Reinforcement learning environments-simple simulations coupled with a problem specification in the form of a reward function-are also important to standardize the development (and benchmarking) of learning algorithms. Yet, full-scale simulators typically lack portability and parallelizability. Vice versa, many reinforcement learning environments trade-off realism for high sample throughputs in toy-like problems. While public data sets have greatly benefited deep learning and computer vision, we still lack the software tools to simultaneously develop-and fairly compare-control theory and reinforcement learning approaches. In this paper, we propose an open-source OpenAI Gym-like environment for multiple quadcopters based on the Bullet physics engine. Its multi-agent and vision-based reinforcement learning interfaces, as well as the support of realistic collisions and aerodynamic effects, make it, to the best of our knowledge, a first of its kind. We demonstrate its use through several examples, either for control (trajectory tracking with PID control, multi-robot flight with downwash, etc.) or reinforcement learning (single and multi-agent stabilization tasks), hoping to inspire future research that combines control theory and machine learning.	https://dx.doi.org/10.1109/IROS51168.2021.9635857	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Papahristou2011	Training Neural Networks to Play Backgammon Variants Using Reinforcement Learning	Backgammon is a board game that has been studied considerably by computer scientists. Apart from standard backgammon, several yet unexplored variants of the game exist, which use the same board, number of checkers, and dice but may have different rules for moving the checkers, starting positions and movement direction. This paper studies two popular variants in Greece and neighboring countries, named Fevga and Plakoto. Using reinforcement learning and Neural Network function approximation we train agents that learn a game position evaluation function for these games. We show that the resulting agents significantly outperform the open-source program Tavli3D.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Paragliola2018	A Reinforcement Learning-Based Approach for the Risk Management of e-Health Environments: A Case Study	Modern medical software systems are often classified as medical devices and governed by regulations which require stringent risk safety activities to be implemented to minimize the occurrence of risky events. This paper proposes a Reinforcement Learning (RL based approach for training a software agent for risk management of medical software systems. The goal of the RL agent is to avoid that a patient enters in dangerous and undesirable states. At the same time, the agent must be able to reach on a safe state or an exit in a minimum interval of time.	https://dx.doi.org/10.1109/SITIS.2018.00114	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Paragliola2019	Risk management for nuclear medical department using reinforcement learning algorithms		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065998144&doi=10.1007\%2fs40860-019-00084-z&partnerID=40&md5=0418f30d720a490a8e2207124daa14ce	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Parisi2015	Reinforcement learning vs human programming in tetherball robot games	Reinforcement learning of motor skills is an important challenge in order to endow robots with the ability to learn a wide range of skills and solve complex tasks. However, comparing reinforcement learning against human programming is not straightforward. In this paper, we create a motor learning framework consisting of state-of-the-art components in motor skill learning and compare it to a manually designed program on the task of robot tetherball. We use dynamical motor primitives for representing the robot's trajectories and relative entropy policy search to train the motor framework and improve its behavior by trial and error. These algorithmic components allow for high-quality skill learning while the experimental setup enables an accurate evaluation of our framework as robot players can compete against each other. In the complex game of robot tetherball, we show that our learning approach outperforms and wins a match against a high quality hand-crafted system.	https://dx.doi.org/10.1109/IROS.2015.7354296	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Park2022	Deep Reinforcement Learning Based Dynamic Proportional-Integral (PI) Gain Auto-Tuning Method for a Robot Driver System	To meet the growing trend of stringent fuel economy regulations, automakers around the world are designing modules such as engines, motors, transmissions and batteries to be as efficient as possible. In order to verify the effect of these designs on the overall fuel efficiency of the vehicle, the vehicle equipped with each module is placed on the chassis dynamometer, driven to follow the target vehicle speed, and actual fuel efficiency is measured. These tests are traditionally performed by human operators, but are now being replaced by robots (physical or software) to ensure the accuracy and reliability of test results. Although the conventionally proposed proportional integral (PI)-based controller has a simple structure and is easy to implement, it requires the process of finding the optimal gain whenever the test conditions such as vehicle or drive cycle change, which is difficult and time consuming. In this study, we propose a proportional integral controller gain adjustment algorithm using deep reinforcement learning. The reinforcement learning agent learns to dynamically modify the PI gain value of the acceleration/deceleration pedal to better follow the target vehicle in a simulation environment. The perturbation is used in each training episode to reduce the difference between the simulation and real testing environment. Upon completion of the training process, the trained agent performs an adjustment process that generates a reference gain table. We then use this reference gain table to perform a real test. The performance of the proposed system was evaluated using Hyundai Tucson HEV (NX4) on an AVL chassis dynamometer. We also compared the performance of our proposed algorithm to traditional fuzzy logic-based PI controllers. The obtained experimental results show that the proposed control system achieved a performance improvement of aounrd 46.8\% compared to the conventional PI control system in terms of root mean square error.	https://dx.doi.org/10.1109/ACCESS.2022.3159785	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Park2022a	Tetris Bot using Deep Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143615249&doi=10.5302\%2fJ.ICROS.2022.22.0140&partnerID=40&md5=cb9e6daf7dfdc6dde310cdbe8b2c091b	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Parra-Ullauri2022	Event-driven temporal models for explanations - ETeMoX: explaining reinforcement learning		https://doi.org/10.1007/s10270-021-00952-4	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Partalas2008	Reinforcement learning with classifier selection for focused crawling		https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053390538&doi=10.3233\%2f978-1-58603-891-5-759&partnerID=40&md5=a96d4d6745be4d63247c6373d526ff30	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Pasqualini2020	Pseudo Random Number Generation through Reinforcement Learning and Recurrent Neural Networks	A Pseudo-Random Number Generator (PRNG) is any algorithm generating a sequence of numbers approximating properties of random numbers. These numbers are widely employed in mid-level cryptography and in software applications. Test suites are used to evaluate the quality of PRNGs by checking statistical properties of the generated sequences. These sequences are commonly represented bit by bit. This paper proposes a Reinforcement Learning (RL) approach to the task of generating PRNGs from scratch by learning a policy to solve a partially observable Markov Decision Process (MDP), where the full state is the period of the generated sequence, and the observation at each time-step is the last sequence of bits appended to such states. We use Long-Short Term Memory (LSTM) architecture to model the temporal relationship between observations at different time-steps by tasking the LSTM memory with the extraction of significant features of the hidden portion of the MDP's states. We show that modeling a PRNG with a partially observable MDP and an LSTM architecture largely improves the results of the fully observable feedforward RL approach introduced in previous work.	https://dx.doi.org/10.3390/a13110307	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Pasqualini2020a	Pseudo Random Number Generation: a Reinforcement Learning approach	Pseudo-Random Numbers Generators (PRNGs) are algorithms produced to generate long sequences of statistically uncorrelated numbers, i.e. Pseudo-Random Numbers (PRNs). These numbers are widely employed in mid-level cryptography and in software applications. Test suites are used to evaluate PRNGs quality by checking statistical properties of the generated sequences. Machine learning techniques are often used to break these generators, i.e. approximating a certain generator or a certain sequence using a neural network. But what about using machine learning to generate PRNs generators? This paper proposes a Reinforcement Learning (RL) approach to the task of generating PRNGs from scratch by learning a policy to solve an N-dimensional navigation problem. In this context, N is the length of the period of the sequence to generate and the policy is iteratively improved using the average score of an appropriate test suite run over that period. Aim of this work is to demonstrate the feasibility of the proposed approach, to compare it with classical methods, and to lay the foundation of a research path which combines RL and PRNGs. (C) 2020 The Authors. Published by Elsevier B.V.	https://dx.doi.org/10.1016/j.procs.2020.03.057	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Pasunuru2021	Dual reinforcement-based specification generation for image de-rendering			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Patchaiammal2019	Software fault prediction exploration using machine learning techniques			Excluded	conflict_resolution	E3: Only conceptual results are reported,E3: Only conceptual results are reported	4
RL4SE	Pattie1996	Skinnerbots	Instrumental (or operant) conditioning, a form of animal learning, is similar to reinforcement learning in that it allows an agent to adapt its actions to gain maximally from the environment while only being rewarded for correct performance. But animals learn much more complicated behaviors through instrumental conditioning than robots presently acquire through reinforcement learning. We describe a new computational model of the conditioning process; our discussion focuses on a training technique called chaining. Four aspects of our model distinguish it from simple reinforcement learning: conditional reinforcers, shifting reinforcement contingencies, explicit action sequencing, and state space refinement. We apply our model to a task commonly used to study working memory in rats and monkeys: the DMTS (Delayed Match to Sample) task. Animals learn this task in stages. Our model also acquires the task in stages, in a similar manner. We have also used our learning program to control a B21 robot.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Paul2020	A Reinforcement Learning Agent based on Genetic Programming and Universal Search	Universal search can serve as an asymptotically optimal agent for machine inversion and time-limited optimization problems. The optimality is independent of problem size, but search space has an exponential dependency on solution size. Reinforcement learning with gradient ascent can dampen this search space. However, in many scenarios in large state spaces, the gradient information becomes nonexistent for a long time which slows down learning. Genetic programming merged with universal search is proposed and build a reinforcement learning agent to alleviate this problem. The universal search is implemented using a functional dataflow graph-based programming model with equivalent program pruning and gradient ascent based incremental learning. The genetic programming naturally fits into the universal search with implicit crossover and mutation operators and without any need of problem-specific population initialization. The agent is experimented on two problem environments and outperformed state of the art method.	https://dx.doi.org/10.1109/ICICCS48265.2020.9121014	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Paul2022	On Solving Heterogeneous Tasks with Microservices		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116911827&doi=10.1007\%2fs40031-021-00676-5&partnerID=40&md5=db45d4af1c45da1bf352b04796c52c6c	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Pavlov2022	Developing a Learning Algorithm for a Multiagent Control System of an Electrical-Engineering Facility for an Oil- and Gas-Production Enterprise with Distributed Generation		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146273719&doi=10.3103\%2fS1068371222110086&partnerID=40&md5=1fd9683397cca3be498f20a479c1cefa	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Peirelinck2018	Using reinforcement learning for optimizing heat pump control in a building model in Modelica		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050237865&doi=10.1109\%2fENERGYCON.2018.8398832&partnerID=40&md5=28e3a26d77d904e2f5f1723a735a86ba	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Peled2015	Semantic locality and context-based prefetching using reinforcement learning	Most modern memory prefetchers rely on spatio-temporal locality to predict the memory addresses likely to be accessed by a program in the near future. Emerging workloads, however, make increasing use of irregular data structures, and thus exhibit a lower degree of spatial locality. This makes them less amenable to spatio-temporal prefetchers. In this paper, we introduce the concept of Semantic Locality, which uses inherent program semantics to characterize access relations. We show how, in principle, semantic locality can capture the relationship between data elements in a manner agnostic to the actual data layout, and we argue that semantic locality transcends spatio-temporal concerns. We further introduce the context-based memory prefetcher, which approximates semantic locality using reinforcement learning. The prefetcher identifies access patterns by applying reinforcement learning methods over machine and code attributes, that provide hints on memory access semantics. We test our prefetcher on a variety of benchmarks that employ both regular and irregular patterns. For the SPEC 2006 suite, it delivers speedups as high as 2.8$\times$ (20\% on average) over a baseline with no prefetching, and outperforms leading spatio-temporal prefetchers. Finally, we show that the context-based prefetcher makes it possible for naive, pointer-based implementations of irregular algorithms to achieve performance comparable to that of spatially optimized code.	https://dx.doi.org/10.1145/2749469.2749473	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Peng2003	Optimizing mobile electronic program guide by adaptive learning			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Peng2019	Learning deep decentralized policy network by collective rewards for real-time combat game			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Peng2016	R-Learning and Gaussian Process Regression Algorithm for Cloud Job Access Control	Reinforcement learning is an area of machine learning inspired by behaviorist psychology, concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Recently reinforcement learning has been given abroad attention, but when it is applied to solve problems with large-scale discrete or contiguous state space environments, the results are likely to be unsatisfactory and even fail to find optimal policies. In order to solve this problem, we establish a new generative model about the value function and use Gaussian Process Regression to approximate the state-action pairs which were never or seldom visited. We testify to the performance of the proposed algorithm by an access-control queuing job in a cloud computing environment. The computational results demonstrate the scheme can balance the exploration and exploitation in the learning process and accelerate the convergence to a certain extent.	https://dx.doi.org/10.1109/CSCloud.2016.15	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Perez2010	Multi-objective Reinforcement Learning for Responsive Grids	Grids organize resource sharing, a fundamental requirement of large scientific collaborations. Seamless integration of Grids into everyday use requires responsiveness, which can be provided by elastic Clouds, in the Infrastructure as a Service (IaaS) paradigm. This paper proposes a model-free resource provisioning strategy supporting both requirements. Provisioning is modeled as a continuous action-state space, multi-objective reinforcement learning (RL) problem, under realistic hypotheses; simple utility functions capture the high level goals of users, administrators, and shareholders. The model-free approach falls under the general program of autonomic computing, where the incremental learning of the value function associated with the RL model provides the so-called feedback loop. The RL model includes an approximation of the value function through an Echo State Network. Experimental validation on a real data-set from the EGEE Grid shows that introducing a moderate level of elasticity is critical to ensure a high level of user satisfaction.	https://dx.doi.org/10.1007/s10723-010-9161-0	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Perez2009	Responsive elastic computing		https://doi.org/10.1145/1555301.1555311	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Perli2008	Developing applications for lego robots using multiple platforms			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Pervej2020	Dynamic Power Allocation and Virtual Cell Formation for Throughput-Optimal Vehicular Edge Networks in Highway Transportation	This paper investigates highly mobile vehicular networks from users' perspectives in highway transportation. Particularly, a centralized software-defined architecture is introduced in which centralized resources can be assigned, programmed, and controlled using the anchor nodes (ANs) of the edge servers. Unlike the legacy networks, where a typical user is served from only one access point (AP), in the proposed system model, a vehicle user is served from multiple APs simultaneously. While this increases the reliability and the spectral efficiency of the assisted users, it also necessitates an accurate power allocation in all transmission time slots. As such, a joint user association and power allocation problem is formulated to achieve enhanced reliability and weighted user sum rate. However, the formulated problem is a complex combinatorial problem, remarkably hard to solve. Therefore, fine-grained machine learning algorithms are used to efficiently optimize joint user associations and power allocations of the APs in a highly mobile vehicular network. Furthermore, a distributed single-agent reinforcement learning algorithm, namely SARL-MARL, is proposed which obtains nearly identical genie-aided optimal solutions within a nominal number of training episodes than the baseline solution. Simulation results validate that our solution outperforms existing schemes and can attain genie-aided optimal performances.	https://dx.doi.org/10.1109/ICCWorkshops49005.2020.9145348	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Pervej2020a	Eco-Vehicular Edge Networks for Connected Transportation: A Distributed Multi-Agent Reinforcement Learning Approach	This paper introduces an energy-efficient, software-defined vehicular edge network for the growing intelligent connected transportation system. A joint user-centric virtual cell formation and resource allocation problem is investigated to bring eco-solutions at the edge. This joint problem aims to combat against the power-hungry edge nodes while maintaining assured reliability and data rate. More specifically, by prioritizing the downlink communication of dynamic eco-routing, highly mobile autonomous vehicles are served with multiple low-powered access points (APs) simultaneously for ubiquitous connectivity and guaranteed reliability of the network. The formulated optimization is exceptionally troublesome to solve within a polynomial time, due to its complicated combinatorial structure. Hence, a distributed multi-agent reinforcement learning (D-MARL) algorithm is proposed for eco-vehicular edges, where multiple agents cooperatively learn to receive the best reward. First, the algorithm segments the centralized action space into multiple smaller groups. Based on the model-free distributed Q learner, each edge agent takes its actions from the respective group. Also, in each learning state, a software-defined controller chooses the global best action from individual bests of the distributed agents. Numerical results validate that our learning solution achieves near-optimal performances within a small number of training episodes as compared with existing baselines.	https://dx.doi.org/10.1109/VTC2020-Fall49728.2020.9348507	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Petousis2019	Using sequential decision making to improve lung cancer screening performance	Globally, lung cancer is responsible for nearly one in five cancer deaths. The National Lung Screening Trial (NLST) demonstrated the efficacy of low-dose computed tomography (LDCT) to identify early-stage disease, setting the basis for widespread implementation of lung cancer screening programs. However, the specificity of LDCT lung cancer screening is suboptimal, with a significant false positive rate. Representing this imaging-based screening process as a sequential decision making problem, we combined multiple machine learning-based methods to learn a partially-observable Markov decision process that simultaneously optimizes lung cancer detection while enhancing test specificity. Using NLST data, we trained a dynamic Bayesian network as an observational model and used inverse reinforcement learning to discover a rewards function based on experts' decisions. Our resultant predictive model decreased the false positive rate while maintaining a high true positive rate at a level comparable to human experts. Our model also detected a number of lung cancers earlier.	https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087808861&doi=10.1109\%2fACCESS.2019.2935763&partnerID=40&md5=89f1d50c6ba3e8b3a8d1357d82e68ad6	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Petty2022	Modeling cyberattacks with extended Petri nets		https://doi.org/10.1145/3476883.3520209	Excluded	new_screen	E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for	4
RL4SE	Pezze2022	Machine learning and natural language processing for automating software testing (tutorial)		https://doi.org/10.1145/3540250.3569451	Excluded	new_screen	E5: Other not a paper,E5: Other not a paper	4
RL4SE	Pfau2017	Automated Game Testing with ICARUS: Intelligent Completion of Adventure Riddles via Unsupervised Solving		https://doi.org/10.1145/3130859.3131439	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Pham2019	Deep reinforcement learning based qos-aware routing in knowledge-defined networking		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070979326&doi=10.1007\%2f978-3-030-14413-5_2&partnerID=40&md5=d6d13c82718abea1d619b8e6f17af63c	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Phan2019	Control Strategy of a Hybrid Renewable Energy System Based on Reinforcement Learning Approach for an Isolated Microgrid	Due to the rising cost of fossil fuels and environmental pollution, renewable energy (RE) resources are currently being used as alternatives. To reduce the high dependence of RE resources on the change of weather conditions, a hybrid renewable energy system (HRES) is introduced in this research, especially for an isolated microgrid. In HRES, solar and wind energies are the primary energy resources while the battery and fuel cells (FCs) are considered as the storage systems that supply energy in case of insufficiency. Moreover, a diesel generator is adopted as a back-up system to fulfill the load demand in the event of a power shortage. This study focuses on the development of HRES with the combination of battery and hydrogen FCs. Three major parts were considered including optimal sizing, maximum power point tracking (MPPT) control, and the energy management system (EMS). Recent developments and achievements in the fields of machine learning (ML) and reinforcement learning (RL) have led to new challenges and opportunities for HRES development. Firstly, the optimal sizing of the hybrid renewable hydrogen energy system was defined based on the Hybrid Optimization Model for Multiple Energy Resources (HOMER) software for the case study in an island in the Philippines. According to the assessment of EMS and MPPT control of HRES, it can be concluded that RL is one of the most emerging optimal control solutions. Finally, a hybrid perturbation and observation (P&O) and Q-learning (h-POQL) MPPT was proposed for a photovoltaic (PV) system. It was conducted and validated through the simulation in MATLAB/Simulink. The results show that it showed better performance in comparison to the P&O method.	https://dx.doi.org/10.3390/app9194001	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Phan2021	Resilient Multi-Agent Reinforcement Learning with Adversarial Value Decomposition	We focus on resilience in cooperative multi-agent systems, where agents can change their behavior due to udpates or failures of hardware and software components. Current state-of-the-art approaches to cooperative multi-agent reinforcement learning (MARL) have either focused on idealized settings without any changes or on very specialized scenarios, where the number of changing agents is fixed, e.g., in extreme cases with only one productive agent. Therefore, we propose Resilient Adversarial value Decomposition with Antagonist-Ratios (RADAR). RADAR offers a value decomposition scheme to train competing teams of varying size for improved resilience against arbitrary agent changes. We evaluate RADAR in two cooperative multi-agent domains and show that RADAR achieves better worst case performance w.r.t. arbitrary agent changes than state-of-the-art MARL.		Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Philipp2017	Reinforcement Learning for Multi-Step Expert Advice			Included	conflict_resolution		4
RL4SE	Pierrot2019	Learning compositional neural programs with recursive tree search and planning			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Pierzcha?a2019	Machine Learning-Based Open Framework for Multiresolution Multiagent Simulation		https://doi.org/10.1007/978-3-030-43890-6_17	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Piette2016	Patient-Centered Pain Care Using Artificial Intelligence and Mobile Health Tools: Protocol for a Randomized Study Funded by the US Department of Veterans Affairs Health Services Research and Developme	"BACKGROUND: Cognitive behavioral therapy (CBT) is one of the most effective treatments for chronic low back pain. However, only half of Department of Veterans Affairs (VA) patients have access to trained CBT therapists, and program expansion is costly. CBT typically consists of 10 weekly hour-long sessions. However, some patients improve after the first few sessions while others need more extensive contact. OBJECTIVE: We are applying principles from ""reinforcement learning"" (a field of artificial intelligence or AI) to develop an evidence-based, personalized CBT pain management service that automatically adapts to each patient's unique and changing needs (AI-CBT). AI-CBT uses feedback from patients about their progress in pain-related functioning measured daily via pedometer step counts to automatically personalize the intensity and type of patient support. The specific aims of the study are to (1) demonstrate that AI-CBT has pain-related outcomes equivalent to standard telephone CBT, (2) document that AI-CBT achieves these outcomes with more efficient use of clinician resources, and (3) demonstrate the intervention's impact on proximal outcomes associated with treatment response, including program engagement, pain management skill acquisition, and patients' likelihood of dropout. METHODS: In total, 320 patients with chronic low back pain will be recruited from 2 VA healthcare systems and randomized to a standard 10 sessions of telephone CBT versus AI-CBT. All patients will begin with weekly hour-long telephone counseling, but for patients in the AI-CBT group, those who demonstrate a significant treatment response will be stepped down through less resource-intensive alternatives including: (1) 15-minute contacts with a therapist, and (2) CBT clinician feedback provided via interactive voice response calls (IVR). The AI engine will learn what works best in terms of patients' personally tailored treatment plans based on daily feedback via IVR about their pedometer-measured step counts, CBT skill practice, and physical functioning. Outcomes will be measured at 3 and 6 months post recruitment and will include pain-related interference, treatment satisfaction, and treatment dropout. Our primary hypothesis is that AI-CBT will result in pain-related functional outcomes that are at least as good as the standard approach, and that by scaling back the intensity of contact that is not associated with additional gains in pain control, the AI-CBT approach will be significantly less costly in terms of therapy time. RESULTS: The trial is currently in the start-up phase. Patient enrollment will begin in the fall of 2016 and results of the trial will be available in the winter of 2019. CONCLUSIONS: This study will evaluate an intervention that increases patients' access to effective CBT pain management services while allowing health systems to maximize program expansion given constrained resources."	https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047730597&doi=10.2196\%2fresprot.4995&partnerID=40&md5=68fd2890d6960aa9ee6d6244de1766bd	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Piette2022	Patient-Centered Pain Care Using Artificial Intelligence and Mobile Health Tools A Randomized Comparative Effectiveness Trial	IMPORTANCE Cognitive behavioral therapy for chronic pain (CBT-CP) is a safe and effective alternative to opioid analgesics. Because CBT-CP requires multiple sessions and therapists are scarce, many patients have limited access or fail to complete treatment. OBJECTIVES To determine if a CBT-CP program that personalizes patient treatment using reinforcement learning, a field of artificial intelligence (AI), and interactive voice response (IVR) calls is noninferior to standard telephone CBT-CP and saves therapist time. DESIGN, SETTING, AND PARTICIPANTS This was a randomized noninferiority, comparative effectiveness trial including 278 patients with chronic back pain from the Department of Veterans Affairs health system (recruitment and data collection from July 11, 2017-April 9. 2020). More patients were randomized to the AI-CBT-CP group than to the control (1.4:1) to maximize the system's ability to learn from patient interactions. INTERVENTIONS All patients received 10 weeks of CBT-CP. For the AI-CBT-CP group, patient feedback via daily IVR calls was used by the AI engine to make weekly recommendations for either a 45-minute or 15-minute therapist-delivered telephone session or an individualized IVR-delivered therapist message. Patients in the comparison group were offered 10 therapist-delivered telephone CBT-CP sessions (45 minutes/session). MAIN OUTCOMES AND MEASURES The primary outcome was the Roland Morris Disability Questionnaire (RMDQ; range 0-24), measured at 3 months (primary end point) and 6 months. Secondary outcomes included pain intensity and pain interference. Consensus guidelines were used to identify clinically meaningful improvements for responder analyses (eg, a 30\% improvement in RMDQ scores and pain intensity). Data analyses were performed from April 2021 to May 2022. RESULTS The study population included 278 patients (mean [SD] age, 63.9 [12.2] years; 248 [892\%] men; 225 [81.8\%] White individuals). The 3-month mean RMDQ score difference between AI-CBT-CP and standard CBT-CP was -0.72 points (95\% CI, -2.06 to 0.62) and the 6-month difference was -1.24 (95\% CI, -2.48 to 0); noninferiority criterion were met at both the 3- and 6-month end points (P < .001 for both). A greater proportion of patients receiving AI-CBT-CP had clinically meaningful improvements at 6 months as indicated by RMDQ (37\% vs 19\%; P = .01) and pain intensity scores (29\% vs 17\%; P = .03). There were no significant differences in secondary outcomes. Pain therapy using AI-CBT-CP required less than half of the therapist time as standard CBT-CP. CONCLUSIONS AND RELEVANCE The findings of this randomized comparative effectiveness trial indicated that AI-CBT-CP was noninferior to therapist-delivered telephone CBT-CP and required substantially less therapist time. Interventions like AI-CBT-CP could allow many more patients to be served effectively by CBT-CP programs using the same number of therapists.	https://www.ncbi.nlm.nih.gov/pubmed/35939288	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Pinciroli2020	Agent-based modeling and reinforcement learning for optimizing energy systems operation and maintenance: the pathmind solution		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107303778&doi=10.3850\%2f978-981-14-8593-0_5863-cd&partnerID=40&md5=5cfd0dd569b2ae62f7e15923344f86c1	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Pinto2011	Improving policy gradient estimates with influence information			Excluded	new_screen	E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for	4
RL4SE	Pinyoanuntapong2019	Distributed Multi-Hop Traffic Engineering via Stochastic Policy Gradient Reinforcement Learning	Multi-hop networks (e.g., mesh, ad-hoc, and sensor networks) are important and cost-efficient communication backbones. Over the last few years wireless data traffic has drastically increased due to the changes in the way today's society creates, shares, and consumes information. This demands the efficient and intelligent utilization of limited network resources to optimize network performance. Traffic engineering (TE) optimizes network performance and enables optimal forwarding and routing rules to meet the quality of service (QoS) requirements for a large volume of traffic flows. This paper proposes a distributed model-free TE solution based on stochastic policy gradient reinforcement learning (RL), which aims to learn a stochastic routing policy for each router so that each router can send a packet to the next-hop router according to the learned optimal probability. The proposed policy-gradient solution naturally leads to multi-path TE strategies, which can effectively distribute the high traffic loads among all available routing paths to minimize the E2E delay. Moreover, a distributed software-defined networking architecture is proposed, which enables the fast prototyping of the proposed multi-agent actor-critic TE (MA-AC TE) algorithm and in-nature supports automated TE through multi-agent RL learning.	https://dx.doi.org/10.1109/GLOBECOM38437.2019.9013134	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Pipe2002	Experiments on a Pittsburgh-style fuzzy classifier system for mobile robotics			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Plessen2018	Encoding Motion Primitives for Autonomous Vehicles Using Virtual Velocity Constraints and Neural Network Scheduling	Within the context of trajectory planning for autonomous vehicles this paper proposes methods for efficient encoding of motion primitives in neural networks on top of model-based and gradient-free reinforcement learning. It is distinguished between 5 core aspects: system model, network architecture, training algorithm, training tasks selection and hardware/software implementation. For the system model, a kinematic (3-states-2-controls) and a dynamic (16-states-2-controls) vehicle model are compared. For the network architecture, 3 feedforward structures are compared including weighted skip connections. For the training algorithm, virtual velocity constraints and network scheduling are proposed. For the training tasks, different feature vector selections are discussed. For the implementation, aspects of gradient-free learning using 1 GPU and the handling of perturbation noise therefore are discussed. The effects of proposed methods are illustrated in experiments encoding up to 14625 motion primitives. The capabilities of tiny neural networks with as few as 10 scalar parameters when scheduled on vehicle velocity are emphasized.	https://dx.doi.org/10.1109/ICMLA.2018.00038	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Polese2022	ColO-RAN: Developing Machine Learning-based xApps for Open RAN Closed-loop Control on Programmable Experimental Platforms	Cellular networks are undergoing a radical transformation toward disaggregated, fully virtualized, and programmable architectures with increasingly heterogeneous devices and applications. In this context, the open architecture standardized by the O-RAN Alliance enables algorithmic and hardware-independent Radio Access Network (RAN) adaptation through closed-loop control. O-RAN introduces Machine Learning (ML)-based network control and automation algorithms as socalled xApps running on RAN Intelligent Controllers. However, in spite of the new opportunities brought about by the Open RAN, advances in ML-based network automation have been slow, mainly because of the unavailability of large-scale datasets and experimental testing infrastructure. This slows down the development and widespread adoption of Deep Reinforcement Learning (DRL) agents on real networks, delaying progress in intelligent and autonomous RAN control. In this paper, we address these challenges by discussing insights and practical solutions for the design, training, testing, and experimental evaluation of DRLbased closed-loop control in the Open RAN. To this end, we introduce ColO-RAN, the first publicly-available large-scale O-RAN testing framework with software-defined radios-in-the-loop. Building on the scale and computational capabilities of the Colosseum wireless network emulator, ColO-RAN enables ML research at scale using O-RAN components, programmable base stations, and a wireless data factory. Specifically, we design and develop three exemplary xApps for DRL-based control of RAN slicing, scheduling and online model training, and evaluate their performance on a cellular network with 7 softwarized base stations and 42 users. Finally, we showcase the portability of ColO-RAN to different platforms by deploying it on Arena, an indoor programmable testbed. The lessons learned from the ColO-RAN implementation and the extensive results from our first-of-its-kind large-scale evaluation highlight the importance of experimental frameworks for the development of end-toend intelligent RAN control pipelines, from data analysis to the design and testing of DRL agents. They also provide insights on the challenges and benefits of DRL-based adaptive control, and on the trade-offs associated to training on a live RAN. ColO-RAN and the collected large-scale dataset are publicly available to the research community.	https://dx.doi.org/10.1109/TMC.2022.3188013	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Poltronieri2021	Reinforcement Learning for value-based Placement of Fog Services	Optimal service and resource management in Fog Computing is an active research area in academia. In fact, to fulfill the promise to enable a new generation of immersive, adaptive, and context-aware services, Fog Computing requires novel solutions capable of better exploiting the available computational and network resources at the edge. Resource management in Fog Computing could particularly benefit from self-* approaches capable of learning the best resource allocation strategies to adapt to the ever changing conditions. In this context, Reinforcement Learning (RL), a technique that allows to train software agents to learn which actions maximize a reward, represents a compelling solution to investigate. In this paper, we explore RL as an optimization method for the value-based management of Fog services over a pool of Fog nodes. More specifically, we propose FogReinForce, a solution based on Deep Q-Network (DQN) algorithm that learns to select the allocation for service components that maximizes the value-based utility provided by those services.		Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Polymenakos2020	SafePILCO: A Software Tool for Safe and Data-Efficient Policy Synthesis		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097081808&doi=10.1007\%2f978-3-030-59854-9_3&partnerID=40&md5=31875b5b9ec185d8485730328994f8dd	Excluded	new_screen	E1: Does not define or use a RL method,E1: Does not define or use a RL method	4
RL4SE	Ponce2015	A Case Study in Hybrid Multi-threading and Hierarchical Reinforcement Learning Approach for Cooperative Multi-agent Systems	This paper describes a case study about a multi-agent system for cooperative tasks, i.e. a mixing color task given three different sources of color. A reinforcement learning approach was performed by the agents, however, this type of learning exploits exponentially when the number of states in the environment is very large. In that sense, the paper proposes to use the MaxQ-Q hierarchical reinforcement learning algorithm to obtain a suitable policy for agents in order to minimize the time process to achieve the goal, and to reduce the state space. In addition, since the multi-agent system runs in a software application, a multi-threading paradigm was proposed to use. Experimental results show that this multi-agent system can reduce the time process and still maintain independence of agents.	https://dx.doi.org/10.1109/MICAI.2015.20	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Pope2022	Hierarchical Reinforcement Learning for Air Combat At DARPA&#x0027;s AlphaDogfight Trials		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142845508&doi=10.1109\%2fTAI.2022.3222143&partnerID=40&md5=4efba5b982a99121d432c1b7fef7be49	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Pope2022a	Hierarchical Reinforcement Learning for Air Combat At DARPA's AlphaDogfight Trials	Autonomous control in high-dimensional, continuous state spaces is a persistent and important challenge in the fields of robotics and artificial intelligence. Because of high risk and complexity, the adoption of AI for autonomous combat systems has been a long-standing difficulty. In order to address these issues, DARPA's AlphaDogfight Trials (ADT) program sought to vet the feasibility of and increase trust in AI for autonomously piloting an F-16 in simulated air-to-air combat. Our submission to ADT solves the high-dimensional, continuous control problem using a novel hierarchical deep reinforcement learning approach consisting of a high-level policy selector and a set of separately trained low-level policies specialized for excelling in specific regions of the state space. Both levels of the hierarchy are trained using off-policy, maximum entropy methods with expert knowledge integrated through reward shaping. Our approach outperformed human expert pilots and achieved a second-place rank in the ADT championship event. Impact Statement\endash Significant performance milestones in reinforcement learning have been achieved in recent years, with autonomous agents demonstrating super-human performance across a wide variety of tasks. Before these algorithms can be extensively deployed in real-world defense applications, a greater level of trust must first be achieved. ADT was an important step towards developing the trust necessary to operationalize these algorithms, by demonstrating their effectiveness on a foundational yet relevant problem in a high-fidelity simulation environment. Developed for the program, our hierarchical reinforcement learning agent was designed alongside of and competed against active fighter pilots, and ultimately defeated a graduate of the United States Air Force's F-16 Weapons Instructor Course in match play.	https://dx.doi.org/10.1109/TAI.2022.3222143	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Prado2021	Automated Design Space Exploration for Optimized Deployment of DNN on Arm Cortex-A CPUs	The spread of deep learning on embedded devices has prompted the development of numerous methods to optimize the deployment of deep neural networks (DNNs). Works have mainly focused on: 1) efficient DNN architectures; 2) network optimization techniques, such as pruning and quantization; 3) optimized algorithms to speed up the execution of the most computational intensive layers; and 4) dedicated hardware to accelerate the data flow and computation. However, there is a lack of research on cross-level optimization as the space of approaches becomes too large to test and obtain a globally optimized solution. Thus, leading to suboptimal deployment in terms of latency, accuracy, and memory. In this work, we first detail and analyze the methods to improve the deployment of DNNs across the different levels of software optimization. Building on this knowledge, we present an automated exploration framework to ease the deployment of DNNs. The framework relies on a reinforcement learning search that, combined with a deep learning inference framework, automatically explores the design space and learns an optimized solution that speeds up the performance and reduces the memory on embedded CPU platforms. Thus, we present a set of results for state-of-the-art DNNs on a range of Arm Cortex-A CPU platforms achieving up to $4\times $ improvement in performance and over $2\times $ reduction in memory with negligible loss in accuracy with respect to the BLAS floating-point implementation.	https://dx.doi.org/10.1109/TCAD.2020.3046568	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Pretorius2020	A game-theoretic analysis of networked system control for common-pool resource management using multi-agent reinforcement learning			Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Prigent2022	A Methodology to Build Decision Analysis Tools Applied to Distributed Reinforcement Learning	As Artificial Intelligence-based applications become more and more complex, speeding up the learning phase (which is typically computation-intensive) becomes more and more necessary. Distributed machine learning (ML) appears adequate to address this problem. Unfortunately, ML also brings new development frameworks, methodologies and high-level program-ming languages that do not fit to the regular high-performance computing design flow. This paper introduces a methodology to build a decision making tool that allows ML experts to arbitrate between different frameworks and deployment configurations, in order to fulfill project objectives such as the accuracy of the resulting model, the computing speed or the energy consumption of the learning computation. The proposed methodology is applied to an industrial-grade case study in which reinforcement learning is used to train an autonomous steering model for a cargo airdrop system. Results are presented within a Pareto front that lets ML experts choose an appropriate solution, a framework and a deployment configuration, based on the current operational situation. While the proposed approach can effortlessly be applied to other machine learning problems, as for many decision making systems, the selected solutions involve a trade-off between several antagonist evaluation criteria and require experts from different domains to pick the most efficient solution from the short list. Nevertheless, this methodology speeds up the development process by clearly discarding, or, on the contrary, including combinations of frameworks and configurations, which has a significant impact for time and budget-constrained projects.	https://dx.doi.org/10.1109/IPDPSW55747.2022.00173	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Pritchard2022	Automating Staged Rollout with Reinforcement Learning	Staged rollout is a strategy of incrementally releasing software updates to portions of the user population in order to accelerate defect discovery without incurring catastrophic outcomes such as system wide outages. Some past studies have examined how to quantify and automate staged rollout, but stop short of simultaneously considering multiple product or process metrics explicitly. This paper demonstrates the potential to automate staged rollout with multi-objective reinforcement learning in order to dynamically balance stakeholder needs such as time to deliver new features and downtime incurred by failures due to latent defects. CCS CONCEPTS  Software and its engineering ? Software testing and debugging.	https://dx.doi.org/10.1145/3510455.3512782	Included	conflict_resolution		4
RL4SE	Priya2021	5GhNet: an intelligent QoE aware RAT selection framework for 5G-enabled healthcare network	The COVID-19 outbreak has stimulated the digital transformation of antiquated healthcare system to a smart hospital, enabling the personalised and remote healthcare services. To augment the functionalities of these intelligent healthcare systems, 5G & B5G heterogeneous network has emerged as a robust and reliable solution. But the pivotal challenge for 5G & B5G connectivity solutions is to ensure flexible and agile service orchestration with acknowledged Quality of Experience (QoE). However, the existing radio access technology (RAT) selection strategies are incapacitated in terms of QoE provisioning and Quality of Service (QoS) maintenance. Therefore, an intelligent QoE aware RAT selection architecture based on software-defined wireless networking (SDWN) and edge computing has been proposed for 5G-enabled healthcare network. The proposed model leverages the principles of invalid action masking and multi-agent reinforcement learning to allow faster convergence to QoE optimised RAT selection policy. The analytical evaluation validates that the proposed scheme outperforms the other existing schemes in terms of enhancing personalised user-experience with efficient resource utilisation.	https://www.ncbi.nlm.nih.gov/pubmed/34849173	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Priya2021a	QAAs: QoS provisioned artificial intelligence framework for AP selection in next-generation wireless networks		https://doi.org/10.1007/s11235-020-00710-9	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Qian2020	Guest Editors' Introduction to the Special Issue on Machine Learning Architectures and Accelerators	The twelve papers in this special section focus on machine learning architectures and accelerators. Deep learning or deep neural networks (DNNs), as one of the most powerful machine learning techniques, has achieved extraordinary performance in computer vision and surveillance, speech recognition and natural language processing, healthcare and disease diagnosis, etc. Various forms of DNNs have been proposed, including Convolutional Neural Networks, Recurrent Neural Networks, Deep Reinforcement Learning, Transformer model, etc. Deep learning exhibits an offline training phase to derive the weight parameters from an excessive training dataset, as well as an online inference phase to perform classification/prediction/perception/ control tasks based on the trained model. The paper in this section aim to find a convergence of software and hardware/architecture. It aims at DNN algorithms, parallel computing, and compiler code generation techniques that are hardware/architecture friendly, as well as computer architectures that are universal and consistently highly performant on a wide range of DNN algorithms and applications. In this co-design and co-optimization framework we can mitigate the limitation of investigating in only a single direction, shedding some light on the future of embedded, ubiquitous artificial intelligence.	https://dx.doi.org/10.1109/TC.2020.2997574	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E5: Other not a paper	4
RL4SE	Qian2022	Obstacle avoidance planning of autonomous vehicles using deep reinforcement learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143631625&doi=10.1177\%2f16878132221139661&partnerID=40&md5=7aa1f6bda0e8e756ba645f93909c4044	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Qian2020a	A Workflow-Aided Internet of Things Paradigm with Intelligent Edge Computing	In this article, we propose a workflow-aided Internet of things (WIoT) paradigm with intelligent edge computing (IEC) to automate the execution of IoT applications with dependencies. Our design primarily targets at reducing the latency of the IoT systems from two perspectives. To reduce the latency from an application perspective, we develop a WIoT paradigm to orchestrate various IoT applications in a programming way. To reduce the latency from a computation perspective, we propose a novel IEC framework to execute latency-sensitive IoT tasks at the edge network. We put forth a deep reinforcement learning algorithm to adaptively allocate the edge resources to the dynamic requests, aiming to provide the best quality of service for terminal users in real-time. Furthermore, we design a software platform to implement the proposed WIoT with IEC. Experimental results demonstrate that WIoT with IEC can significantly reduce the service latency and improve the network throughput, compared with the traditional cloud-based IoT systems.	https://dx.doi.org/10.1109/MNET.001.1900665	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Qian2019	Survey on Reinforcement Learning Applications in Communication Networks	In recent years, intelligent communication has drawn huge research efforts in both academia and industry. With the advent of 5G technology, intelligent wireless terminals and intelligent communication networks are increasingly under intensive study. Artificial intelligence enhances the network capability with automatic and adaptive adjustment. Reinforcement learning (RL) and deep reinforcement learning (DRL) are two powerful techniques in artificial intelligence which can learn the optimal decision according to the environment feedback. In this paper, we focus on the latest research progress on RL and DRL applied in three emerging technologies including mobile edge computing (MEC), software defined network (SDN) and network virtualization in 5G. The prospect of further research and development in the future is preliminarily forecasted.	https://dx.doi.org/10.23919/JCIN.2019.8917870	Excluded	new_screen	E5: Other not a paper,E2: Software engineering is not the problem RL is used for	4
RL4SE	Qiao2011	Inverse reinforcement learning with Gaussian process	We present new algorithms for inverse reinforcement learning (IRL, or inverse optimal control) in convex optimization settings. We argue that finite-space IRL can be posed as a convex quadratic program under a Bayesian inference framework with the objective of maximum a posteriori estimation. To deal with problems in large or even infinite state space, we propose a Gaussian process model and use preference graphs to represent observations of decision trajectories. Our method is distinguished from other approaches to IRL in that it makes no assumptions about the form of the reward function and yet it retains the promise of computationally manageable implementations for potential real-world applications. In comparison with an establish algorithm on small-scale numerical problems, our method demonstrated better accuracy in apprenticeship learning and a more robust dependence on the number of observations.	https://dx.doi.org/10.1109/ACC.2011.5990948	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Qiao2018	A Reinforcement Learning Solution to Cold-Start Problem in Software Crowdsourcing Recommendations	"Recommendation is one key functionality of software crowdsourcing platforms, which is responsible for recommending developers appropriate software projects, or vice versa. Meanwhile, software crowdsourcing recommendation in practice usually faces a cold-start problem: a platform has not yet gathered sufficient information, and thus its recommendations can be imprecise or unbalanced.To tackle this problem, this paper introduces reinforcement learning into crowdsourcing recommendations, and presents ClusterUCBscRec, a novel project recommending approach to learn user feedbacks actively. ClusterUCBscRec adopts the ""explore & exploit"" strategy to improve the recommending performance continuously, and therefore goes quickly through the cold-start stage. Besides the project models, developer models built from multiple aspects, including developer profile, preferences and skills are introduced into recommendation. Developers and projects are clustered to speed up training and recommending processes to further improve the performance.We have evaluated ClusterUCBscRec on Jointforce. Experimental results show that the novel approach significantly improves the performance of crowdsourcing recommendations and can solve the cold-start problem effectively, compared with COFIBA and BiUCB."	https://dx.doi.org/10.1109/PIC.2018.8706279	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Qin2021	Intelligent traffic light under fog computing platform in data control of real-time traffic flow	As the global economy develops rapidly, traffic congestion has become a major problem for first-tier cities in various countries. In order to address the problem of failed real-time control of the traffic flow data by the traditional traffic light control as well as malicious attack and other security problems faced by the intelligent traffic light (ITL) control system, a multi-agent distributed ITL control method was proposed based on the fog computing platform and the Q learning algorithm used for the reinforcement learning in this study, and the simulation comparison was conducted by using the simulation platform jointly constructed based on the VISSIM-Excel VBA-MATLAB software. Subsequently, on the basis of puzzle difficulty of the computational Diffie-Helleman (CDH) and Hash Collision, the applicable security control scheme of ITL under the fog computing was proposed. The results reveal that the proposed intelligent control system prolongs the time of green light properly when the number of vehicles increases, thereby reducing the delay time and retention rate of vehicles; the security control scheme of ITL based on the puzzle of CDH is less efficient when the vehicle density increases, while that based on the puzzle of Hash collision is very friendly to the fog equipment. In conclusion, the proposed control method of ITL based on the fog computing and Q learning algorithm can alleviate the traffic congestion effectively, so the proposed method has high security.	https://dx.doi.org/10.1007/s11227-020-03443-3	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Qin2020	Enabling Multicast Slices in Edge Networks		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092148953&doi=10.1109\%2fJIOT.2020.2991107&partnerID=40&md5=330b889e597fc29007edd151e1ac9b40	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Qiu2019	A novel QoS-enabled load scheduling algorithm based on reinforcement learning in software-defined energy internet		https://doi.org/10.1016/j.future.2018.09.023	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Qu2022	Priority-awareness VNF migration method based on deep reinforcement learning		https://doi.org/10.1016/j.comnet.2022.108866	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Qu2021	Reliable Service Function Chain Deployment Method Based on Deep Reinforcement Learning	Network function virtualization (NFV) is a key technology to decouple hardware device and software function. Several virtual network functions (VNFs) combine into a function sequence in a certain order, that is defined as service function chain (SFC). A significant challenge is guaranteeing reliability. First, deployment server is selected to place VNF, then, backup server is determined to place the VNF as a backup which is running when deployment server is failed. Moreover, how to determine the accurate locations dynamically with machine learning is challenging. This paper focuses on resource requirements of SFC to measure its priority meanwhile calculates node priority by current resource capacity and node degree, then, a novel priority-awareness deep reinforcement learning (PA-DRL) algorithm is proposed to implement reliable SFC dynamically. PA-DRL determines the backup scheme of each VNF, then, the model jointly utilizes delay, load balancing of network as feedback factors to optimize the quality of service. In the experimental results, resource efficient utilization, survival rate, and load balancing of PA-DRL were improved by 36.7\%, 35.1\%, and 78.9\% on average compared with benchmark algorithm respectively, average delay was reduced by 14.9\%. Therefore, PA-DRL can effectively improve reliability and optimization targets compared with other benchmark methods.	https://www.ncbi.nlm.nih.gov/pubmed/33924460	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Quek2021	Deep Q-network implementation for simulated autonomous vehicle control	Deep reinforcement learning is poised to be a revolutionised step towards newer possibilities in solving navigation and autonomous vehicle control tasks. Deep Q-network (DQN) is one of the more popular methods of deep reinforcement learning that allows the agent that controls the vehicle to learn through its mistakes based on its actions and interactions with the environment. This paper presents the implementation of DQN to an autonomous self-driving vehicle control in two different simulated environments; first environment is in Python which is a simple 2D environment and then advanced to Unity software separately which is a 3D environment. Based on the scores and pixel inputs, the agent in the vehicle learns and adapts to its surrounding. It develops the best solution strategy to direct itself in the environment where its task is to manoeuvre the vehicle from point to point on a simulated highway scenario. The implemented DQN technique approximates the action value function with convolutional neural network. This evaluates the Q-function for the Q-learning architecture and updates the action value function. This paper shows that DQN is an effective learning method for the agent of an autonomous vehicle. In both simulated environments, the autonomous vehicle gradually learnt the manoeuvre operations and progressively gained the ability to successfully navigate itself and avoid obstacles without prior information of the surrounding.	https://dx.doi.org/10.1049/itr2.12067	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Quintana2022	Bio-plausible digital implementation of a reward modulated STDP synapse	Reward-modulated Spike-Timing-Dependent Plasticity (R-STDP) is a learning method for Spiking Neural Network (SNN) that makes use of an external learning signal to modulate the synaptic plasticity produced by Spike-Timing-Dependent Plasticity (STDP). Combining the advantages of reinforcement learning and the biological plausibility of STDP, online learning on SNN in real-world scenarios can be applied. This paper presents a fully digital architecture, implemented on an Field-Programmable Gate Array (FPGA), including the R-STDP learning mechanism in a SNN. The hardware results obtained are comparable to the software simulations results using the Brian2 simulator. The maximum error is of 0.083 when a 14-bits fix-point precision is used in realtime. The presented architecture shows an accuracy of 95\% when tested in an obstacle avoidance problem on mobile robotics with a minimum use of resources.	https://dx.doi.org/10.1007/s00521-022-07220-6	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Radac2022	Learning Model-Free Reference Tracking Control with Affordable Systems		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135526933&doi=10.1007\%2f978-3-031-09928-1_10&partnerID=40&md5=bf55a20990db05bed12aa41c8e7d5e06	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Radha2022	The general design of the automation for multiple fields using reinforcement learning algorithm		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122006491&doi=10.11591\%2fijeecs.v25.i1.pp481-487&partnerID=40&md5=13894d9b0cd40f03a31cd35ecd1b4f3d	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Radovi?2021	Hardware implementation of the upper confidence-bound algorithm for reinforcement learning		https://doi.org/10.1016/j.compeleceng.2021.107537	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Rahman2020	A Vision to Mitigate Bioinformatics Software Development Challenges	Developers construct bioinformatics software to automate crucial analysis and research related to biological science. However, challenges while developing bioinformatics software can prohibit advancement in biological science research. Through a human-centric systematic analysis, we can identify challenges related to bioinformatics software development and envision future research directions. From our qualitative analysis with 221 Stack Overflow questions, we identify six categories of challenges: file operations, searching genetic entities, defect resolution, configuration management, sequence alignment, and translation of genetic information. To mitigate the identified challenges we envision three research directions that require synergies between bioinformatics and automated software engineering: (i) automated configuration recommendation using optimization algorithms, (ii) automated and comprehensive defect categorization, and (iii) intelligent task assistance with active and reinforcement learning.	https://dx.doi.org/10.1145/3417113.3422155	Included	new_screen		4
RL4SE	Rahman2022	On Efficient Operation of a V2G-Enabled Virtual Power Plant		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144601813&doi=10.1145\%2f3563357.3564067&partnerID=40&md5=d2602ba19fcf15965735204538822b09	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Rahmani2021	Machine learning (Ml) in medicine: Review, applications, and challenges		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119902780&doi=10.3390\%2fmath9222970&partnerID=40&md5=9bf3faa87a0466bf02804dbc18f907e5	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Raja2020	Intelligent Reward-Based Data Offloading in Next-Generation Vehicular Networks	A massive increase in the number of mobile devices and data-hungry vehicular network applications creates a great challenge for mobile network operators (MNOs) to handle huge data in cellular infrastructure. However, due to fluctuating wireless channels and high mobility of vehicular users, it is even more challenging for MNOs to deal with vehicular users within a licensed cellular spectrum. Data offloading in the vehicular environment plays a significant role in offloading the vehicle's data traffic from congested cellular network's licensed spectrum to the free unlicensed WiFi spectrum with the help of roadside units (RSUs). In this article, an intelligent reward-based data offloading in the next generation vehicular networks (IR-DON) architecture is proposed for dynamic optimization of data traffic and selection of intelligent RSU. Within the IR-DON architecture, an intelligent access network discovery and selection function (I-ANDSF) module with Q-learning, a reinforcement learning algorithm is designed. The I-ANDSF is modeled under a software-defined network (SDN) controller to solve the dynamic optimization problem by performing an efficient offloading. This increases the overall system throughput by choosing an optimal and intelligent RSU in the network selection process. The simulation results have shown the accurate network traffic classification, optimal network selection, guaranteed quality of service, reduced delay, and higher throughput achieved by the I-ANDSF module.	https://dx.doi.org/10.1109/JIOT.2020.2974631	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Raje2012	Software service selection by multi-level matching and reinforcement learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869594563&doi=10.1007\%2f978-3-642-32615-8_31&partnerID=40&md5=0cd9062b73a1835a5e2626173a36dbc9	Included	new_screen		4
RL4SE	Ram2022	Hardware Accelerator for Capsule Network based Reinforcement Learning	Convolutional neural networks are widely used in reinforcement learning. Capsule networks are gaining popularity over the traditional convolutional neural networks in many classification tasks. A capsule is a multidimensional activity vector consisting of neurons that represent the features of a specific type of entity such as an object or part of an object. In this paper, we explore the capability of a Capsule Network for deep reinforcement learning-based applications. Our proposed capsule network architecture with the same number of parameters as that of a convolutional neural network for reinforcement learning takes on average nine times lesser number of network update iterations than that of a convolutional neural network. We also propose a hardware accelerator for deep Q-learning that uses the capsule network as a deep Q-network instead of a convolutional neural network. We have implemented a capsule network-based deep Q-learning architecture for inference on the Xilinx Kintex UltraScale field-programmable gate array. We have tested the network on pygame based environments. Our hardware implementation achieves an overall speedup of 77.45x as compared to the software implementation of the capsule network for deep reinforcement learning on Intel Xeon CPU E5-1607, 4 core, @3.1GHz and 10.86x as compared to implementation on Nvidia Ge-Force GTX1080 GPU.	https://dx.doi.org/10.1109/VLSID2022.2022.00041	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Raman2021	Reinforcement Learning-Based Home Energy Management System for Resiliency	With increase in the frequency of natural disasters such as hurricanes that disrupt the supply from the grid, there is a greater need for resiliency in electric supply. Rooftop solar photovoltaic (PV) panels along with batteries can provide resiliency to a house in a blackout due to a natural disaster. Our previous work showed that intelligence can reduce the size of a PV+battery system for the same level of post-blackout service compared to a conventional system that does not employ intelligent control. The intelligent controller proposed is based on model predictive control (MPC), which has two main challenges. One, it requires simple yet accurate models as it involves real-time optimization. Two, the discrete actuation for residential loads (on/off) makes the underlying optimization problem a mixed-integer program (MIP) which is challenging to solve. An attractive alternative to MPC is reinforcement learning (RL) as the real-time control computation is both model-free and simple. These points of interest accompany certain trade-offs; RL requires computationally expensive offline learning, and its performance is sensitive to various design choices. In this work, we propose an RL-based controller. We compare its performance with the MPC controller proposed in our prior work and a non-intelligent baseline controller. The RL controller is found to provide a resiliency performance - by commanding critical loads and batteries-similar to MPC with a significant reduction in computational effort.	https://dx.doi.org/10.23919/ACC50511.2021.9483162	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Randrianasolo2014	Q-Learning: From Computer Network Security to Software Security	Reinforcement learning techniques become more popular in computer network security. The same reinforcement learning techniques developed for network security can be applied to software security as well. This research summarizes a work in progress attempt to incorporate Q-learning algorithm in software security. The Q-learning method is embedded as part of the software itself to provide a security mechanism that has ability to learn by itself to develop a temporary repair mechanism. The results of the experiment express that given the right parameters and the right setting the Q-learning approach rapidly learns to block all malicious actions. Data analysis on the Q-values produced by the software can provide security diagnostic as well. A larger scale experiment is expected to be seen in the future work.	https://dx.doi.org/10.1109/ICMLA.2014.47	Included	conflict_resolution		4
RL4SE	Rangel-Martinez2021	Machine learning on sustainable energy: A review and outlook on renewable energy systems, catalysis, smart grid and energy storage		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113758502&doi=10.1016\%2fj.cherd.2021.08.013&partnerID=40&md5=5e9293b99ca5cb51491935cbe4e93701	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E5: Other not a paper	4
RL4SE	Rathnayaka2021	Cognitive Rehabilitation based Personalized Solution for Dementia Patients using Reinforcement Learning	Dementia is one of the most challenging health problems faced globally with the increase in the ageing population. The estimated current prevalence of dementia is 47.5 million worldwide. This number will nearly double in every 20 years globally. Dementia is basically, a syndrome which cannot be cured by medicine, but non-pharmacological therapy can be used to treat Dementia patients, this is known as Cognitive Rehabilitation Therapy. According to the recommendations of the doctors, the use of a brain training application could be better than traditional approaches. There are number of Brain training mobile applications in the world that could be useful in improving human concentration, attention and all sorts of brain activities but there isn't any customized software solution that has games or activities. Patients can be in different stages of Dementia. Therefore, for a better cognitive rehabilitation they need personalized therapies with the games and activities. Accordingly, developing this application is an actual global requirement for dementia patients. The world is evolving with new technologies and this application includes the mind games based on such technologies as Reinforcement Learning which predict the next level for patients based on user behavior. And there are some activities on speech recognition using Deep Neural Network as well. Patients, caregivers and doctors can view the progress reports of the patients. All the games have designed along with the supervision and recommendation from a Consultant Psychiatrist in Sri Lanka. The main objective is to help the Dementia patients in cognitive rehabilitation to improve the quality of life with best suited personalized games and activities.	https://dx.doi.org/10.1109/SysCon48628.2021.9447133	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Reddy2022	Agent-Driven Traffic Light Sequencing System Using Deep Q-Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133548513&doi=10.1007\%2f978-981-16-9113-3_32&partnerID=40&md5=2a1e1e67b21d706e0b6001fa3196b1bc	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Reddy2020	Quickly Generating Diverse Valid Test Inputs with Reinforcement Learning	Property-based testing is a popular approach for validating the logic of a program. An effective property-based test quickly generates many diverse valid test inputs and runs them through a parameterized test driver. However, when the test driver requires strict validity constraints on the inputs, completely random input generation fails to generate enough valid inputs. Existing approaches to solving this problem rely on whitebox or greybox information collected by instrumenting the input generator and/or test driver. However, collecting such information reduces the speed at which tests can be executed. In this paper, we propose and study a black-box approach for generating valid test inputs. We first formalize the problem of guiding random input generators towards producing a diverse set of valid inputs. This formalization highlights the role of a guide which governs the space of choices within a random input generator. We then propose a solution based on reinforcement learning (RL), using a tabular, on-policy RL approach to guide the generator. We evaluate this approach, RLCheck, against pure random input generation as well as a state-of-the-art greybox evolutionary algorithm, on four real-world benchmarks. We find that in the same time budget, RLCheck generates an order of magnitude more diverse valid inputs than the baselines.		Included	new_screen		4
RL4SE	Reichstaller2018	Risk-Based Testing of Self-Adaptive Systems Using Run-Time Predictions	Devising test strategies for specific test goals relies on predictions of the run-time behavior of the software system under test (SuT) based on specifications, models, or the code. For a system following a single strategy as run-time behavior, the test strategy can be fixed at design time. For an adaptive system, which may choose from several strategies due to environment changes, a combination of test strategies has to be found, which still can be achieved at design time provided that all system strategies and the switching policy are predictable. Self-adaptive systems, also adapting their system strategies and strategy switches according to the environmental dynamics, render such design-time predictions futile, but there also the test strategies have to be adapted at run time. We characterize the necessary interplay between system strategy adaptation of the SuT and test strategy adaptation of the tester as a Stochastic Game. We argue that the tester's part, formalized by means of a Markov Decision Process, can be automatically solved by the use of Reinforcement Learning methods where we discuss both model-based and model-free variants. Finally, we propose a particular framework inspired by Direct Future Prediction which, given a simulation of the SuT and its environment, autonomously finds good test strategies w.r.t. imposed quanti?able goals. While these goals, in general, can be initialized arbitrarily, our evaluation concentrates on risk-based goals rewarding the detection of hazardous failures.	https://dx.doi.org/10.1109/SASO.2018.00019	Included	conflict_resolution		4
RL4SE	Remani2019	Residential Load Scheduling with Renewable Generation in the Smart Grid: A Reinforcement Learning Approach		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051384249&doi=10.1109\%2fJSYST.2018.2855689&partnerID=40&md5=ff1cdb685a0cdae8d4ace71063d719a8	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ren2021	Technologies of virtual scenario construction for intelligent driving testing		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100307393&doi=10.11834\%2fjig.200469&partnerID=40&md5=e0ec0086195734290cd7d125b358b217	Excluded	new_screen	E5: Other not a paper,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ren2021a	Blockchain-Based VEC Network Trust Management: A DRL Algorithm for Vehicular Service Offloading and Migration	To meet the execution requirements of delay-sensitive services in vehicular edge computing (VEC) networks, vehicular services need to be offloaded to edge computing nodes. For complex, large-scale services, the services need to be migrated if the services are not completed before the vehicles leave the coverage of edge computing nodes. Trust and resource matching between areas thus become major problems. This paper studies the decision model of vehicular service offloading and migration. First, software-defined network (SDN) technology is introduced into the traditional network architecture, and a two-layer distributed SDN-controlled VEC network architecture is designed, which is divided into a domain control layer and an area control layer. In this framework, we use the consortium blockchain as a carrier to share network topology information between SDN controllers to prevent information leakage. We then established a service offloading and migration optimization problem model to minimize service execution delay, reduce energy consumption and maximize the throughput of the blockchain system. We describe the problem model as a Markov Decision Process (MDP), introduce a deep reinforcement learning (DRL) algorithm named asynchronous advantage actor-critic (A3C) and design a dynamic service offloading and migration algorithm (DSOMA) based on A3C to solve the problem. Simulation results show that DSOMA can increase the throughput of the blockchain system, and DSOMA is superior to the deep Q-learning (DQN) algorithm and greedy offloading algorithm in reducing service execution delay and system energy consumption.	https://dx.doi.org/10.1109/TVT.2021.3092346	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ren2020	Vehicular Network Edge Intelligent Management : A Deep Deterministic Policy Gradient Approach for Service Offloading Decision	The development of edge computing has alleviated the problem of limited vehicular computing capabilities in VANET. The vehicular edge computing (VEC) provide resources for the implementation of multiple intelligent services. However, the mobility of vehicles and the diversity of edge computing nodes pose huge challenges for service offloading. Deep reinforcement learning (DRL) in artificial intelligence (AI) is an effective technology to solve such challenges. Based on this scenario, we first introduce a software-defined vehicular networks (SDV) architecture that takes full advantage of the characteristics of SDN technology and can effectively and dynamically obtain a global view in VANET to facilitate the management of resources in the network. Then, we propose a new intelligent service offloading decision model, which introduces the Deep Deterministic Policy Gradient (DDPG) algorithm in DRL to solve the joint optimization of service offloading with multiple constraints. Simulation results show that the DDPG-based service offloading model has better performance and better stability than similar algorithms.	https://dx.doi.org/10.1109/IWCMC48107.2020.9148507	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ren2020a	Research on Signal Control Method of Single Intersection Based on Reinforcement Learning			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Restuccia2020	DeepWiERL: Bringing Deep Reinforcement Learning to the Internet of Self-Adaptive Things	"Recent work has demonstrated that cutting-edge advances in deep reinforcement learning (DRL) may be leveraged to empower wireless devices with the much-needed ability to ""sense"" current spectrum and network conditions and ""react"" in real time by either exploiting known optimal actions or exploring new actions. Yet, understanding whether real-time DRL can be at all applied in the resource-challenged embedded IoT domain, as well as designing IoT-tailored DRL systems and architectures, still remains mostly uncharted territory. This paper bridges the existing gap between the extensive theoretical research on wireless DRL and its system-level applications by presenting Deep Wireless Embedded Reinforcement Learning (DeepWiERL), a general-purpose, hybrid software/hardware DRL framework specifically tailored for embedded IoT wireless devices. DeepWiERL provides abstractions, circuits, software structures and drivers to support the training and real-time execution of state-of-the-art DRL algorithms on the device's hardware. Moreover, DeepWiERL includes a novel supervised DRL model selection and bootstrap (S-DMSB) technique that leverages transfer learning and high-level synthesis (HLS) circuit design to orchestrate a neural network architecture that satisfies hardware and application throughput constraints and speeds up the DRL algorithm convergence. Experimental evaluation on a fully-custom software-defined radio testbed (i) proves for the first time the feasibility of real-time DRL-based algorithms on a real-world wireless platform with multiple channel conditions; (ii) shows that DeepWiERL supports 16x data rate and consumes 14x less energy than a software-based implementation, and (iii) indicates that S-DMSB may improve the DRL convergence time by 6x and increase the obtained reward by 45\% if prior channel knowledge is available."	https://dx.doi.org/10.1109/INFOCOM41043.2020.9155461	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Reza2021	The MOOClet Framework: Unifying Experimentation, Dynamic Improvement, and Personalization in Online Courses		https://doi.org/10.1145/3430895.3460128	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Rezaei2021	Energy-efficient parking analytics system using deep reinforcement learning		https://doi.org/10.1145/3486611.3486660	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Rieker2012	An intelligent agent for optimal river-reservoir system management	A generalized software package is presented for developing an intelligent agent for stochastic optimization of complex river-reservoir system management and operations. Reinforcement learning is an approach to artificial intelligence for developing a decision-making agent that learns the best operational policies without the need for explicit probabilistic models of hydrologic system behavior. The agent learns these strategies experientially in a Markov decision process through observational interaction with the environment and simulation of the river-reservoir system using well-calibrated models. The graphical user interface for the reinforcement learning process controller includes numerous learning method options and dynamic displays for visualizing the adaptive behavior of the agent. As a case study, the generalized reinforcement learning software is applied to developing an intelligent agent for optimal management of water stored in the Truckee river-reservoir system of California and Nevada for the purpose of streamflow augmentation for water quality enhancement. The intelligent agent successfully learns long-term reservoir operational policies that specifically focus on mitigating water temperature extremes during persistent drought periods that jeopardize the survival of threatened and endangered fish species.	https://dx.doi.org/10.1029/2012WR011958	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Rihani2018	A Neural Network Based Handover for Multi-RAT Heterogeneous Networks with Learning Agent	The wireless communication networks continue to evolve with multiple radio access technologies (LTE, WIFI, WIMAX ...) included in the same device. Thus, the end-nodes have the flexibility to decide which radio technology to select according to their availability and location. In this case, they must be made intelligent and autonomous enough in order to select automatically the best available wireless technology. Neural Networks represent a promising machine learning technique to be used to make such decisions. The proposed system is implemented on a System on Chip (SoC) integrating both processors and FPGA logic fabrics on the same chip with fast interconnection between them. This allows designing Software/Hardware systems with optimized performance. The adopted SoC offers the partial reconfiguration (PR) feature on the FPGA, which adds more flexibility to these high performance devices. In this paper, we propose a Neural Network based system for heterogeneous networks. The system is based on reinforcement learning that aims to make the system intelligent enough when deciding to perform vertical handover between multiple wireless systems. The switching between the wireless systems standards is implemented using PR technique.	https://dx.doi.org/10.1109/ReCoSoC.2018.8449382	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Rijsdijk2021	Reinforcement Learning-Based Design of Side-Channel Countermeasures		https://doi.org/10.1007/978-3-030-95085-9_9	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Rijsdijk2022	Reinforcement Learning-Based Design of Side-Channel Countermeasures	Deep learning-based side-channel attacks are capable of breaking targets protected with countermeasures. The constant progress in the last few years makes the attacks more powerful, requiring fewer traces to break a target. Unfortunately, to protect against such attacks, we still rely solely on methods developed to protect against generic attacks. The works considering the protection perspective are few and usually based on the adversarial examples concepts, which are not always easy to translate to real-world hardware implementations. In this work, we ask whether we can develop combinations of countermeasures that protect against side-channel attacks. We consider several widely adopted hiding countermeasures and use the reinforcement learning paradigm to design specific countermeasures that show resilience against deep learning-based side-channel attacks. Our results show that it is possible to significantly enhance the target resilience to a point where deep learning-based attacks cannot obtain secret information. At the same time, we consider the cost of implementing such countermeasures to balance security and implementation costs. The optimal countermeasure combinations can serve as development guidelines for real-world hardware/software-based protection schemes.	https://dx.doi.org/10.1007/978-3-030-95085-9_9	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Rikhtegar2021	DeepRLB: A deep reinforcement learning-based load balancing in data center networks	Data center networks (DCNs) are facing challenging control problems such as flow scheduling, congestion control, load balancing, and bandwidth allocation when dynamically handling heterogeneous mice and elephant flows. This paper focuses on the load balancing in DCNs as an online decision-making problem with high complexity (NP-hard problem). Traditional load balancing approaches are usually the heuristic algorithms relying on the operator's viewpoint and a short-term knowledge of the traffic conditions and network environment. This paper proposes DeepRLB, a deep reinforcement learning (DRL)-based load balancing approach for software-defined networking-based DCNs which exploits the deep deterministic policy gradient (DDPG) algorithm to adaptively learn the link-weight values by observing the traffic flow characteristics. The actor and critic neural networks in the DDPG model have been designed based on two learning models: fully connected (DeepRLB-Full) and convolutional (DeepRLB-Conv). The output of the model is further used by the controller to determine the forwarding paths for traffic flows accordingly. We evaluated the performance of DeepRLB by comparing it with ECMP as well as a per-packet load balancing algorithm in three different data center topologies under 30 heterogeneous traffic demand matrices. The obtained results showed that the DeepRLB algorithm outperforms the ECMP with respect to studied load balancing metrics considerably (DeepRLB-Conv achieved the best performance). As a sample, DeepRLB-Full and DeepRLB-Conv were able to reach 19.14\% and 24.46\% reduction in the total number of over-utilized and under-utilized network links compared to ECMP, respectively.	https://dx.doi.org/10.1002/dac.4912	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Rivas-Blanco2018	Smart Cable-Driven Camera Robotic Assistant	This paper describes the mechanical design and a cognition system for a novel concept-of-camera robotic assistant. The system combines the advantages of intra-abdominal devices and autonomous camera navigation. The robotic assistant is composed of a magnetic intra-abdominal camera robot with two internal cable-driven degrees of freedom and an external robot that handles an external magnet. The intelligence of the robot is implemented in a cognitive architecture based on a long-term memory that stores the robot's knowledge and learning capabilities to improve the robot's behavior. The navigation strategy combines a reactive control based on instrument tracking and a proactive control based on predefined behaviors, depending on the actual state of the task. The robot's learning capabilities include a semantic customization, to adapt the camera's behavior to the surgeon's preferences, and reinforcement learning, to improve the camera navigation strategy. This paper details both the hardware implementation of the system and the software implementation in a robotic operating system architecture. The cognition system and the performance of the cable-driven mechanism have been validated with a set of in vitro experiments. Moreover, the camera robot has been evaluated through an in-vivo experiment in a pig.	https://dx.doi.org/10.1109/THMS.2017.2767286	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Robinson2014	Modified reinforcement learning for sequential action behaviors and its application to robotics	When developing a robot or other automaton, the efficacy of the agent is highly dependent on the performance of the behaviors which underpin the control system. Especially in the case of agents which must act in real world or disorganized environments, the design of robust behaviors can be both difficult and time consuming, and often requires the use of sensitive tuning. In response to this need, we present a behavioral, goal-oriented, reinforcement-based machine learning strategy which is flexible, simple to implement, and designed for application in real-world environments, but with the capability of software-based training. In this paper, we will explain our design paradigms, the formal implementation thereof, and the algorithm proper. We will show that the algorithm is able to emulate standard reinforcement learning within comparable training time, and to extend the capabilities thereof as well. We also demonstrate extension of learning beyond the scope of training examples, and present an example of a physical robot which learns a sequential action behavior by experimentation.	https://dx.doi.org/10.1109/SECON.2014.6950737	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Robinson2011	Co-learning segmentation in marketplaces		https://doi.org/10.1007/978-3-642-28499-1_1	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Rodrigues2016	Simulation and implementation of cognitive radio algorithms for satellite communications			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Rodrigues-Filho2022	Hatch: Self-distributing systems for data centers		https://doi.org/10.1016/j.future.2022.02.008	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Rodriguez-Gutiez2016	MARL-Ped plus Hitmap: Towards Improving Agent-Based Simulations with Distributed Arrays	Multi-agent systems allow the modelling of complex, heterogeneous, and distributed systems in a realistic way. MARL-Ped is a multi-agent system tool, based on the MPI standard, for the simulation of different scenarios of pedestrians who autonomously learn the best behavior by Reinforcement Learning. MARL-Ped uses one MPI process for each agent by design, with a fixed fine-grain granularity. This requirement limits the performance of the simulations for a restricted number of processors that is lesser than the number of agents. On the other hand, Hitmap is a library to ease the programming of parallel applications based on distributed arrays. It includes abstractions for the automatic partition and mapping of arrays at runtime with arbitrary granularity, as well as functionalities to build flexible communication patterns that transparently adapt to the data partitions. In this work, we present the methodology and techniques of granularity selection in Hitmap, applied to the simulations of agent systems. As a first approximation, we use the MARL-Ped multi-agent pedestrian simulation software as a case of study for intra-node cases. Hitmap allows to transparently map agents to processes, reducing oversubscription and intra-node communication overheads. The evaluation results show significant advantages when using Hitmap, increasing the flexibility, performance, and agent-number scalability for a fixed number of processing elements, allowing a better exploitation of isolated nodes.	https://dx.doi.org/10.1007/978-3-319-49956-7_17	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Rodriguez-Gutiez2016a	MARL-Ped+Hitmap: Towards improving agent-based simulations with distributed arrays		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007295252&doi=10.1007\%2f978-3-319-49956-7_17&partnerID=40&md5=44ae96c406c55d79f21a8d3de75e77ba	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E3: Only conceptual results are reported	4
RL4SE	Romdhana2022	IFRIT: Focused Testing through Deep Reinforcement Learning	Software is constantly changing as developers add new features or make changes. This directly impacts the effectiveness of the test suite associated with that software, especially when the new modifications are in an area where no test case exists. This article addresses the issue of developing a high-quality test suite to repeatedly cover a given point in a program, with the ultimate goal of exposing faults affecting the given program point. Our approach, IFRIT, uses Deep Reinforcement Learning to generate diverse inputs while keeping a high level of reachability of the desired program point. IFRIT achieves better results than state-of-the-art and baseline tools, improving reachability, diversity and fault detection.	https://dx.doi.org/10.1109/ICST53961.2022.00013	Included	new_screen		4
RL4SE	Romero-Hdz2018	A Reinforcement Learning Based Approach for Welding Sequence Optimization	We develop and implement a Q-learning based Reinforcement Learning (RL) algorithm for Welding Sequence Optimization (WSO) where structural deformation is used to compute reward function. We utilize a thermomechanical Finite Element Analysis (FEA) method to predict deformation. We run welding simulation experiment using well-known Simufact (R) software on a typical widely used mounting bracket which contains eight welding beads. RL based welding optimization technique allows the reduction of structural deformation up to similar to 66\%. RL based approach substantially speeds up the computational time over exhaustive search.	https://dx.doi.org/10.1007/978-981-10-7043-3_2	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Rossi2020	Geo-distributed efficient deployment of containers with Kubernetes	Software containers are changing the way applications are designed and executed. Moreover, in the last few years, we see the increasing adoption of container orchestration tools, such as Kubernetes, to simplify the management of multi-container applications. Kubernetes includes simple deployment policies that spread containers on computing resources located in the cluster and automatically scale them out or in based on some cluster-level metrics. As such, Kubernetes is not well-suited for deploying containers in a geo-distributed computing environment and dealing with the dynamism of application workload and computing resources. To tackle the problem, in this paper we present ge-kube (Geo-distributed and Elastic deployment of containers in Kubernetes), an orchestration tool that relies on Kubernetes and extends it with self-adaptation and network-aware placement capabilities. Ge-kube introduces flexible and decentralized control loops that can be easily equipped with different deployment policies. Specifically, we propose a two-step control loop, in which a model-based reinforcement learning approach dynamically controls the number of replicas of individual containers on the basis of the application response time, and a network-aware placement policy allocates containers on geo-distributed computing resources. To address the placement issue, we propose an optimization problem formulation and a network-aware heuristic, which explicitly take into account the non-negligible network delays among computing resources so to satisfy Quality of Service requirements of latency-sensitive applications. Using a surrogate CPU-intensive application and a real application (i.e., Redis), we conducted an extensive set of experiments, which show the benefits arising from the combination of elasticity and placement policies, as well as the advantages of using network-aware placement solutions.	https://dx.doi.org/10.1016/j.comcom.2020.04.061	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Rossi2020a	Hierarchical Scaling of Microservices in Kubernetes	In the last years, we have seen the increasing adoption of the microservice architectural style where applications satisfy user requests by invoking a set of independently deployable services. Software containers and orchestration tools, such as Kubernetes, have simplified the development and management of microservices. To manage containers' horizontal elasticity, Kubernetes uses a decentralized threshold-based policy that requires to set thresholds on system-oriented metrics (i.e., CPU utilization). This might not be well-suited to scale latency-sensitive applications, which need to express requirements in terms of response time. Moreover, being a fully decentralized solution, it may lead to frequent and uncoordinated application reconfigurations. In this paper, we present me-kube (Multi-level Elastic Kubernetes), a Kubernetes extension that introduces a hierarchical architecture for controlling the elasticity of microservice-based applications. At higher level, a centralized per-application component coordinates the run-time adaptation of subordinated distributed components, which, in turn, locally control the adaptation of each microservice. Then, we propose novel proactive and reactive hierarchical control policies, based on queuing theory. To show that me-kube provides general mechanisms, we also integrate reinforcement learning-based scaling policies. Using me-kube, we perform a large set of experiments, aimed to show the advantages of a hierarchical control over the default Kubernetes autoscaler.	https://dx.doi.org/10.1109/ACSOS49614.2020.00023	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Rossi2019	Horizontal and Vertical Scaling of Container-Based Applications Using Reinforcement Learning	"Software containers are changing the way distributed applications are executed and managed on cloud computing resources. Interestingly, containers offer the possibility of handling workload fluctuations by exploiting both horizontal and vertical elasticity ""on the fly"". However, most of the existing control policies consider horizontal and vertical scaling as two disjointed control knobs. In this paper, we propose Reinforcement Learning (RL) solutions for controlling the horizontal and vertical elasticity of container-based applications with the goal to increase the flexibility to cope with varying workloads. Although RL represents an interesting approach, it may suffer from a possible long learning phase, especially when nothing about the system is known a-priori. To speed up the learning process and identify better adaptation policies, we propose RL solutions that exploit different degrees of knowledge about the system dynamics (i.e., Q-learning, Dyna-Q, and Model-based). We integrate the proposed policies in Elastic Docker Swarm, our extension that introduces self-adaptation capabilities in the container orchestration tool Docker Swarm. We demonstrate the effectiveness and flexibility of model-based RL policies through simulations and prototype-based experiments."	https://dx.doi.org/10.1109/CLOUD.2019.00061	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Rothmann2022	FAQ: A Flexible Accelerator for Q-Learning with Configurable Environment	Reinforcement Learning is an area of machine learning that is concerned with optimizing the behavior of an agent in an environment by maximizing cumulative rewards. This can be done with classical reinforcement learning algorithms such as Q-Learning and SARSA. This paper presents FAQ, a flexible FPGA-based accelerator for the Q-Learning algorithm. The architecture of the accelerator can be configured in multiple ways, like adjusting the bit width of Q-values or changing the number of pipeline stages. The evaluation shows that FAQ achieves 249\% higher throughput than state-of-the-art FPGA implementations while decreasing DSP and BRAM utilization. Additionally, a software-configurable environment was implemented, and the whole system was tested on an Ultra96-V2 development board utilizing the PYNQ framework. Compared to a CPU implementation, FAQ is more than 13 times faster, including communication overhead caused by transferring the environment onto the FPGA and reading the resulting Q-table.	https://dx.doi.org/10.1109/ASAP54787.2022.00026	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Rouzbahani2021	A review on virtual power plant for energy management	A Virtual Power Plant (VPP) is a practical concept that aggregates various Renewable Energy Sources (RESs) to improve energy management efficiency and facilitate energy trading. Operation scheduling for all energy components in VPPs plays a vital role from an energy management perspective. Technical and economic constraints and uncertainties that significantly affect the scheduling program must be considered simultaneously. This paper provides a comprehensive review of the scheduling problem in the VPP concept, following Kitchenham's guidelines, to address questions such as: What are the most frequent scheduling techniques in VPPs? How technical and economic aspects of scheduling have been considered to optimize the problem? Moreover, how to deal with different types of uncertainties? To that end, all previous studies on this topic have been extracted and analyzed, focusing on the scheduling algorithm's necessity. Several optimal scheduling methods are investigated that show learning-based approaches have not been well studied. Then, joint technical and economic limitations and dealing with various types of uncertainties are appraised. Contribute to better knowledge for future studies on VPPs, the research gaps regarding optimization techniques, joint technoeconomic, and various kinds of uncertainties have been introduced. Finally, this paper also suggests utilizing a Deep Reinforcement Learning (DRL)-based technique to address the mentioned concerns due to generalization, scalability, and feature extraction, which are originated from a combination of reinforcement learning and deep learning.	https://dx.doi.org/10.1016/j.seta.2021.101370	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Rovbo2019	Hierarchical Control Architecture for a Learning Robot Based on Heterogenic Behaviors	The paper describes a hierarchical control architecture for robotic systems with learning that allows combining various goal-directed algorithms. A top-level control algorithm is proposed that switches control between base algorithms: Q-learning, random walk and a rule-based planning. The algorithm is implemented as a software module and is verified by the example of the task of finding a given door in a building of complex planning. The task is considered as a reinforcement learning problem in two distinct cases: with a goal fixed between the episodes and the goal changing from episode to episode. The simulation showed that the proposed method is more stable for different variants of the task than each of the basic ones separately, although it does not give the best result for each individual case.	https://dx.doi.org/10.1007/978-3-030-30763-9_4	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Roy2021	Clinical Efficacy and Psychological Mechanisms of an App-Based Digital Therapeutic for Generalized Anxiety Disorder: Randomized Controlled Trial	Background: Current treatments for generalized anxiety disorder (GAD) often yield suboptimal outcomes, partly because of insufficient targeting of underlying psychological mechanisms (eg, avoidance reinforcement learning). Mindfulness training (MT) has shown efficacy for anxiety; yet, widespread adoption has been limited, partly because of the difficulty in scaling in-person-based delivery. Digital therapeutics are emerging as potentially viable treatments; however, very few have been empirically validated. Objective: The aim of this study is to test the efficacy and mechanism of an app-delivered MT that was designed to target a potential mechanism of anxiety (reinforcement learning), based on which previous studies have shown concern regarding feedback and the perpetuation of anxiety through negative reinforcement. Methods: Individuals with GAD were recruited using social media advertisements and randomized during an in-person visit to receive treatment as usual (n=33) or treatment as usual+app-delivered MT (Unwinding Anxiety; n=32). The latter was composed of 30 modules to be completed over a 2-month period. Associated changes in outcomes were assessed using self-report questionnaires 1 and 2 months after treatment initiation. Results: We randomized 65 participants in this study, and a modified intent-to-treat approach was used for analysis. The median number of modules completed by the MT group was 25.5 (IQR 17) out of 30; 46\% (13/28) of the participants completed the program. In addition, the MT group demonstrated a significant reduction in anxiety (GAD-7) compared with the control group at 2 months (67\% vs 14\%; median change in GAD-7: -8.5 [IQR 6.5] vs -1.0 [IQR 5.0]; P<.001; 95\% CI 6-10). Increases in mindfulness at 1 month (nonreactivity subscale) mediated decreases in worry at 2 months (Penn State Worry Questionnaire; P=.02) and decreases in worry at 1 month mediated reductions in anxiety at 2 months (P=.03). Conclusions: To our knowledge, this is the first report on the efficacy and mechanism of an app-delivered MT for GAD. These findings demonstrate the clinical efficacy of MT as a digital therapeutic for individuals with anxiety (number needed to treat=1.6). These results also link recent advances in our mechanistic understanding of anxiety with treatment development, showing that app-delivered MT targets key reinforcement learning pathways, resulting in tangible, clinically meaningful reductions in worry and anxiety. Evidence-based, mechanistically targeted digital therapeutics have the potential to improve health at a population level at a low cost.	https://www.ncbi.nlm.nih.gov/pubmed/34860673	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ruan2021	A reinforcement learning-based algorithm for the aircraft maintenance routing problem	With recent developments in the airline industry worldwide, the competition among the industry has increased largely with many key players in the market. In order to generate profits, the industry has paid much attention to generate optimal routes that are maintenance feasible. The main aim of operational aircraft maintenance routing problem (OAMRP) is to generate these optimal routes for each aircraft that are maintenance feasible and follow the constraints defined by the Federal Aviation Administration (FAA). In this paper, the OAMRP is studied with two main objectives. First, to propose a formulation of a network flow-based Integer Linear Programming (ILP) framework for the OAMRP that considers three main maintenance constraints simultaneously: maximum flyinghour, limit on the number of take-offs between two consecutive maintenance checks and the work-force capacity. Second, to develop a new reinforcement learning-based algorithm which can be used to solve the problem, quickly and efficiently, as compared to commonly available optimization software. Finally, the evaluation of the proposed algorithm on real case datasets obtained from a major airline located in the Middle East verifies that the algorithm generates high-quality solutions quickly for both medium and large-scale flight schedule dataset.	https://dx.doi.org/10.1016/j.eswa.2020.114399	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Rundo2019	Deep LSTM with Reinforcement Learning Layer for Financial Trend Prediction in FX High Frequency Trading Systems	High-frequency trading is a method of intervention on the financial markets that uses sophisticated software tools, and sometimes also hardware, with which to implement high-frequency negotiations, guided by mathematical algorithms, that act on markets for shares, options, bonds, derivative instruments, commodities, and so on. HFT strategies have reached considerable volumes of commercial traffic, so much so that it is estimated that they are responsible for most of the transaction traffic of some stock exchanges, with percentages that, in some cases, exceed 70\% of the total. One of the main issues of the HFT systems is the prediction of the medium-short term trend. For this reason, many algorithms have been proposed in literature. The author proposes in this work the use of an algorithm based both on supervised Deep Learning and on a Reinforcement Learning algorithm for forecasting the short-term trend in the currency FOREX (FOReign EXchange) market to maximize the return on investment in an HFT algorithm. With an average accuracy of about 85\%, the proposed algorithm is able to predict the medium-short term trend of a currency cross based on the historical trend of this and by means of correlation data with other currency crosses using techniques known in the financial field with the term arbitrage. The final part of the proposed pipeline includes a grid trading engine which, based on the aforementioned trend predictions, will perform high frequency operations in order to maximize profit and minimize drawdown. The trading system has been validated over several financial years and on the EUR/USD cross confirming the high performance in terms of Return of Investment (98.23\%) in addition to a reduced drawdown (15.97 \%) which confirms its financial sustainability.	https://dx.doi.org/10.3390/app9204460	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Russo2022	Logic-Based Machine Learning: Recent Advances and Their Role in Neuro-Symbolic AI	Over the last two decades there has been a growing interest in logic-based machine learning, where the goal is to learn a logic program, called a hypothesis, that together with a given background knowledge explains a set of examples. Although logic-based machine learning has traditionally addressed the task of learning definite logic programs (with no negation), our logic-based machine learning approaches have extended this field to a wider class of formalisms for knowledge representation, captured by the answer set programming (ASP) semantics. The ASP formalism is truly declarative and due to its non-monotonicity it is particularly well suited to commonsense reasoning. It allows constructs such as choice rules, hard and weak constraints, and support for default inference and default assumptions. Choice rules and weak constraints are particularly useful for modelling human preferences, as the choice rules can represent the choices available to the user, and the weak constraints can specify which choices a human prefers. In the recent years we have made fundamental contributions to the field of logic-based machine learning by extending it to the learning of the full class of ASP programs and the first part of this talk provides an introduction to these results and to the general field of learning under the answer set semantics, referred here as learning from answer sets (LAS). To be applicable to real-world problems, LAS has to be tolerant to noise in the data, scalable over large search spaces, amenable to user-defined domain-specific optimisation criteria and capable of learning interpretable knowledge from structured and unstructured data. The second part of this talk shows how these problems are addressed by our recently proposed FastLAS approach for learning Answer Set Programs, which is targeted at solving restricted versions of observational and non-observational predicate learning from answer sets tasks. The advanced features of our family of LAS systems have made it possible to solve a variety of real-world problems in a manner that is data efficient, scalable and robust to noise. LAS can be combined with statistical learning methods to realise neuro-symbolic solutions that perform both fast, low-level prediction from unstructured data, and high-level logic-based learning of interpretable knowledge. The talk concludes with presenting two such neuro-symbolic solutions for respectively solving image classification problems in the presence of distribution shifts, and discovering sub-goal structures for reinforcement learning agents.		Excluded	new_screen	E1: Does not define or use a RL method,E5: Other not a paper	4
RL4SE	Saad2011	Bridging the gap between reinforcement learning and knowledge representation: A logical off- and on-policy framework		https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960121720&doi=10.1007\%2f978-3-642-22152-1_40&partnerID=40&md5=e4867ca482eb57d5193cbe1c5872af44	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Saad2011a	Learning to act optimally in partially observable Markov decision processes using hybrid probabilistic logic programs			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sabbadin2005	Dynamic reserve site selection under contagion risk of deforestation			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Saber2021	Autonomous GUI Testing using Deep Reinforcement Learning	Automating software testing looks forward to speeding up testing processes and ensuring possible replication of discovered software bugs. However, Automating the GUI testing process is highly challenging due to the need for human intervention to determine actions and assess outcomes. We introduce a novel approach to fully automate GUI testing using deep reinforcement learning. Our deep reinforcement learning model discovers all system states and determines possible testing sequences. The automated testing agent starts with exploring the tested environment to learn the most efficient paths for reaching maximum coverage while discovering GUI bugs. In this case, testers could focus more on functionality testing to improve the overall software quality. We evaluated the developed model on a couple of industry products, and it showed a substantial increase in coverage than random testing.	https://dx.doi.org/10.1109/ICENCO49852.2021.9715282	Included	new_screen		4
RL4SE	Sabzehzar2015	An Improved eXtended Classifier System for the Real-time-input Real-time-output (XCSRR) Stability Control of a Biped Robot	In this paper a revised reinforcement learning method is presented for stability control problems with real-value inputs and outputs. The revised eXtended Classifier System for Real-input and Real-output (XCSRR) controller is designed, which is capable of working at fully real-value environment such as stability control of robots. XCSRR is a novel approach to enhance the performance of classifier systems for more practical problems than systems with merely binary behaviour. As a case study, we use XCSRR to control the stability of a biped robot, which is subjected to unknown external forces that would disturb the robot equilibrium. The external forces and the dynamics of the upper body of the biped robot are modelled in MATLAB software to train the XCSRR controller. Theoretical and experimental results of the learning behaviour and the performance of stability control on the robot demonstrate the strength and efficiency of the proposed new approach. (C) 2015 The Authors. Published by Elsevier B.V.	https://dx.doi.org/10.1016/j.procs.2015.09.198	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sacco2020	A distributed reinforcement learning approach for energy and congestion-aware edge networks		https://doi.org/10.1145/3386367.3431670	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sachio2021	Simultaneous Process Design and Control Optimization using Reinforcement Learning	The performance of a chemical plant is highly affected by its design and control. A design cannot be accurately evaluated without its controls and vice versa. To optimally address design and control simultaneously, one must formulate a bi-level mixed-integer nonlinear program with a dynamic optimization problem as the inner problem; this is intractable. However, by computing an optimal policy using reinforcement learning, a controller with a closed-form expression can be computed and embedded into the mathematical program. In this work, an approach that uses a policy gradient method to compute the optimal policy, which is then embedded into the mathematical program is proposed. The approach is tested in a tank design case study and the performance of the controller is evaluated. It is shown that the proposed approach outperforms current state-of-the-art control strategies. This opens a whole new range of possibilities to address the simultaneous design and control of engineering systems. Copyright (C) 2021 The Authors.	https://dx.doi.org/10.1016/j.ifacol.2021.08.293	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sachio2022	Integrating process design and control using reinforcement learning	To create efficient-high performing processes, one must find an optimal design with its corresponding controller that ensures optimal operation in the presence of uncertainty. When comparing different process designs, for the comparison to be meaningful, each design must involve its optimal operation. Therefore, to optimize a process' design, one must address design and control simultaneously. For this, one can formulate a bilevel optimization problem, with the design as the outer problem in the form of a mixed-integer nonlinear program (MINLP) and a stochastic optimal control as the inner problem. This is intractable by most approaches. In this paper we propose to compute the optimal control using reinforcement learning, and then embed this controller into the design problem. This allows to decouple the solution procedure, while having the same optimal result as if solving the bilevel problem. The approach is tested in two case studies and the performance of the controller is evaluated. The case studies indicate that the proposed approach outperforms current stateof-the-art simultaneous design and control strategies. This opens a new avenue to address simultaneous design and control of engineering systems. (c) 2021 Published by Elsevier B.V. on behalf of Institution of Chemical Engineers.	https://dx.doi.org/10.1016/j.cherd.2021.10.0320263-8762	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Saenz-Aguirre2019	Artificial Neural Network Based Reinforcement Learning for Wind Turbine Yaw Control	This paper introduces a novel data driven yaw control algorithm synthesis method based on Reinforcement Learning (RL) for a variable pitch variable speed wind turbine. Yaw control has not been extendedly studied in the literature; in fact, most of the currently considered developments in the scope of the wind energy are oriented to the pitch and speed control. The most important drawbacks of the yaw control are the very large time constants and the strict yaw angle change rate constraints due to the high mechanical loads when the wind turbine angle is changed in order to adequate it to the wind speed orientation. An optimal yaw control algorithm needs to be designed in order to adapt the rotor orientation depending on the wind turbine dynamics and the local wind speed regime. Consequently, the biggest challenge of the yaw control algorithm is to decide the moment and the quantity of the wind turbine orientation variation to achieve the highest quantity of power at each instant, taking into account the constraints derived from the mechanical limitations of the yawing system and the mechanical loads. In this paper, a novel based algorithm based on the RL Q-Learning algorithm is introduced. The first step is to obtain a model of the power generated by the wind turbine (a real onshore wind turbine in this paper) through a power curve, that in conjunction with a conventional proportional regulator will be used to obtain a dataset that explains the actual behaviour of the real wind turbine when a variety of different yaw control commands are imposed. That knowledge is then used to learn the best control action for each different state of the wind turbine with respect to the wind direction represented by the yaw angle, storing that knowledge in a matrix Q(s,a). The last step is to model that matrix through a MultiLayer Perceptron with BackPropagation (MLP-BP) Artificial Neural Network (ANN) to avoid large matrix management and quantification problems. Once that the optimal yaw controller has been synthetized, its performance has been assessed using a number of wind speed realizations obtained using the software application TurbSim, in order to analyze how the introduced novel algorithm deals with different wind speed scenarios.	https://dx.doi.org/10.3390/en12030436	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Saha2022	Weakly Supervised Neuro-Symbolic Module Networks for Numerical Reasoning over Text			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sahai2019	Learning to Communicate with Limited Co-design	In this work we examine the problem of learning to cooperate in the context of wireless communication. We consider the two agent setting where agents must learn modulation and demodulation schemes that enable them to communicate with each other in the presence of a power-constrained additive white Gaussian noise (AWGN) channel. We investigate whether learning is possible under different levels of information sharing between distributed agents that are not necessarily co-designed. We make use of the ``Echo'' protocol, a learning protocol where an agent hears, understands, and repeats (echoes) back the message received from another agent. Each agent uses what it sends and receives to train itself to communicate. To capture the idea of cooperation between agents that are ``not necessarily co-designed,'' we use two different populations of function approximators - neural networks and polynomials. In addition to diverse learning agents, we include non-learning agents that use fixed standardized modulation protocols such as QPSK and 16QAM. This is used to verify that the Echo approach to learning to communicate works independent of the inner workings of the agents, and that learning agents can not only learn to match the communication expectations of others, but can also collaboratively invent a successful communication approach from independent random initializations. In addition to simulation-based experiments, we implement the Echo protocol in physical software-defined radio experiments to verify that it can work with real radios. To explore the continuum between tight co-design of learning agents and independently designed agents, we study how learning is impacted by different levels of information sharing - including sharing training symbols, sharing intermediate loss information, and sharing full gradient information. The resulting learning techniques span supervised learning and reinforcement learning. We find that in general, co-design (increased information sharing) accelerates learning and that this effect becomes more pronounced as the communication task becomes harder.	https://dx.doi.org/10.1109/ALLERTON.2019.8919749	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sahba2022	An Intelligent System for Safely Managing Traffic Flow of Connected Autonomous Vehicles at Multilane Intersections in Smart Cities	A Smart city as a complex system includes many different subsystems interacting together. Smart transportation is one of the subsystems in a Smart city which consist of roads, vehicles, intersections, roadside units, traffic control systems, and so on. Many of these components needs to be connected to provide a smooth safe traffic in a smart city. The performance of these components largely depends on the computational latency of algorithms running on local or central processors. Hence, providing an optimized solution to minimize this delay is much needed. In this work, we propose a way to properly manage the flow of self-driving vehicle traffic at road junctions, taking into account pedestrian traffic. Self-driving vehicles are able to communicate with each other and smart devices along the road. However, surveillance cameras are needed to observe pedestrians'' traffic at the intersection. Therefore, we use cameras, smart sensors, processors, and communication equipment embedded in self-driving vehicles and roadside smart devices to collect data, process it, and generate proper instructions to manage self-driving vehicles traffic flow at intersections. In this research we have used Simulation of Urban Mobility software to simulate traffic behaviors resulting from the use of the proposed solution. Although the simulation shows a smooth flow of traffic in a simple junction, a deep reinforcement learning approach is then proposed to manage traffic flow in multiple lane junctions. To decrease the risk of collision in intersections, a deep reinforcement approach that utilizes safety parameters for its reward function developed. The results illustrate noticeable decrease in collision at intersections.	https://dx.doi.org/10.1109/CCWC54503.2022.9720878	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sahoo2021	MemOReL: A <u>Mem</u>ory-oriented <u>O</u>ptimization Approach to <u>Re</u>inforcement <u>L</u>earning on FPGA-based Embedded Systems		https://doi.org/10.1145/3453688.3461533	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Saka2018	Reinforcement Learning for Reliability Optimisation	Software Reliability Optimization problem is aimed at bridging the reliability gap in an optimal way. In an industrial setting, focussed testing at the component level is the most favored solution exercised to fill the reliability gap. However, with the increased complexity in the software systems coupled with limited testing timing constraints finding an optimal set of test suite to bridge the reliability gap has become an area of intense research. Furthermore, the stochastic nature of the reliability improvement estimates associated with each test suite manifolds the complexity. Here, we propose Reinforcement Learning (RL), as a mechanism to find an optimal solution. We have shown how an interactive learning is used to estimate the true outcome of the selection and the action selection policy so as to maximise the long term reward. The estimation methodology and the selection policy is inspired by Multi-armed bandit solution strategies. Firstly, we employ a sample average estimation technique for deriving the true outcomes. Secondly, a variant of simple greedy algorithm coined as epsilon-greedy algorithm is considered for action selection policy. These two steps are iteratively exercised until the selection criteria converges. The efficacy of the proposed approach is illustrated on a real time case study.		Included	conflict_resolution		4
RL4SE	Sakuma2020	Airflow Direction Control of Air Conditioners Using Deep Reinforcement Learning	Achieving a uniform comfort within an indoor housing environment is important for health and productivity while saving the energy consumption of a Heating, Ventilation, and Air Conditioners (HVAC) device. The optimal control of an HVAC system is a well-studied area. While many works explore the optimal temperature set-point, a few works consider effective airflow direction control. This work proposes an airflow direction control method that aims uniform comfort of the indoor environment using a deep reinforcement learning (DRL) approach. We implemented our proposed DRL framework using computational fluid dynamics (CFD) simulation software. Our proposed method was evaluated for comfort and energy consumption. The experimental results show the improvements for our proposed method in comfort by 21.3 \% while reducing energy consumption by 34.5 \% for the average than the baseline method.	https://dx.doi.org/10.23919/SICEISCS48470.2020.9083565	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Salden2005	Sustainable cybernetics systems: Backbones of ambient intelligent environments		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889955160&doi=10.1007\%2f0-387-22991-4_11&partnerID=40&md5=71c5ce12335d50c356991fc6ccd63205	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E3: Only conceptual results are reported	4
RL4SE	Samadi2021	Q-Learning-Oriented Distributed Energy Management of Grid-Connected Microgrid	In this paper11This work is supported by Niroo Research Institute (NRI)., reinforcement learning (RL) is used for energy management of agent based microgrid (MG). The Grid connected MG that contains wind turbine, fuel cell (FC), diesel generator and electric vehicle (EV) to supply its demands, is modeled as a multi-agent system (MAS). The DER and customer are considered as self-interested agents that try to maximize their profits and optimize their behavior. These agents use RL to interact with each other in distributed manner without any direct communication. The market operator of MG is responsible to gather agents' data that have been submitted and clears the market to meet the desired goals. Modeling the stochastic nature of wind power generation and demand fluctuation of customer agents, implementing demand side management program for customer agents, besides taking into account the technical constraint of diesel generator, FC and EV agent are the main strengths of this paper. The simulation results confirm the efficiency of the proposed approach.	https://dx.doi.org/10.1109/ICEE52715.2021.9544152	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Samir2020	Age of Information Aware Trajectory Planning of UAVs in Intelligent Transportation Systems: A Deep Learning Approach	Unmanned aerial vehicles (UAVs) are envisioned to play a key role in intelligent transportation systems to complement the communication infrastructure in future smart cities. UAV-assisted vehicular networking research typically adopts throughput and latency as the main performance metrics. These conventional metrics, however, are not adequate to reflect the freshness of the information, an attribute that has been recently identified as a critical requirement to enable services such as autonomous driving and accident prevention. In this paper, we consider a UAV-assisted single-hop vehicular network, wherein sensors (e.g., LiDARs and cameras) on vehicles generate time sensitive data streams, and UAVs are used to collect and process this data while maintaining a minimum age of information (AoI). We aim to jointly optimize the trajectories of UAVs and find scheduling policies to keep the information fresh under minimum throughput constraints. The formulated optimization problem is shown to be mixed integer non-linear program (MINLP) and generally hard to be solved. Motivated by the success of machine learning (ML) techniques particularly deep learning in solving complex problems with low complexity, we reformulate the trajectories and scheduling policies problem as a Markov decision process (MDP) where the system state space considers the vehicular network dynamics. Then, we develop deep reinforcement learning (DRL) to learn the vehicular environment and its dynamics in order to handle UAVs' trajectory and scheduling policy. In particular, we leverage Deep Deterministic Policy Gradient (DDPG) for learning the trajectories of the deployed UAVs to efficiently minimize the Expected Weighted Sum AoI (EWSA). Simulations results demonstrate the effectiveness of the proposed design and show the deployed UAVs adapt their velocities during the data collection mission in order to minimize the AoI.	https://dx.doi.org/10.1109/TVT.2020.3023861	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sampedro2019	A Fully-Autonomous Aerial Robot for Search and Rescue Applications in Indoor Environments using Learning-Based Techniques		https://doi.org/10.1007/s10846-018-0898-1	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sancho-Tomas2017	Extending no-MASS: Multi-agent stochastic simulation for demand response of residential appliances		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085032036&doi=10.26868\%2f25222708.2017.056&partnerID=40&md5=c4b7fdfb5d7a37f8b5606e4a530847ea	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Santos2021	Availability-aware and energy-aware dynamic SFC placement using reinforcement learning		https://doi.org/10.1007/s11227-021-03784-7	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Saoda2022	An Energy Supervisor Architecture for Energy-Harvesting Applications	Energy-harvesting designs typically include highly entangled app-lication-level and energy-management subsystems that span both hardware and software. This tight integration makes developing sophisticated energy-harvesting systems challenging, as developers have to consider both embedded system development and intermit-tent energy management simultaneously. Even when successful, solutions are often monolithic, produce suboptimal performance, and require substantial effort to translate to a new design. Instead, we propose a new energy-harvesting power management architecture, Altair that offloads all energy-management operations to the power supply itself while making the power supply programmable. Altair introduces an energy supervisor and a standard interface to enable an abstraction layer between the power supply hardware and the running application, making both replaceable and recon-figurable. To ensure minimal resource conflict on the application processor, while running resource-hungry optimization techniques in the supervisor, we implement the Altair design in a lower power microcontroller that runs in parallel with the application. We also develop a programmable power supply module and a software library for seamless application development with Altair. We evaluate the versatility of the proposed architecture across a spectrum of IoT devices and demonstrate the generality of the plat-form. We also design and implement an online energy-management technique using reinforcement learning on top of the platform and compare the performance against fixed duty-cycle baselines. Results indicate that sensors running the online energy-manager perform similar to continuously powered sensors, have a l0x higher event generation rate than the intermittently powered ones, 1.8-7x higher event detection accuracy, experience 50\% fewer power failures, and are 44\% more available than the sensors that maintain a constant duty-cycle.	https://dx.doi.org/10.1109/IPSN54338.2022.00033	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sapio2022	Developing and Testing a New Reinforcement Learning Toolkit with Unreal Engine		https://doi.org/10.1007/978-3-031-05643-7_21	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sartea2017	A Monte Carlo tree search approach to active malware analysis			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sasaki2017	Experimental study on behavior acquisition of mobile robot by deep Q-Network		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032825242&doi=10.20965\%2fjaciii.2017.p0840&partnerID=40&md5=0e3c9c9098131999bc2cf7a180c8170f	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sato2019	Swarm Reinforcement Learning for Operational Planning of Energy Plants for Small and Mid-Sized Building Energy Management Systems	This paper proposes operation planning of energy plants by swarm reinforcement learning in order to realize successful BEMS for small and mid-sized buildings. It usually takes many man-hours to develop an evolutionary computation based program and develop a model considering facility characteristics and so on for an energy management system, while engineering man-hours can be reduced and appropriate operational planning can be expected to be realized by a versatile program of swarm reinforcement learning without consideration of facility characteristics and so on. Moreover, the results of the proposed methods are compared with those of a basic Q learning based method and a basic particle swarm optimization (PSO) based method. It is verified that energy cost can be more reduced by one of the proposed methods (PSO-Q based method) than those by the original Q-learning based method. Since the rates to the whole cost are large in case of small and mid-sized buildings, the proposed swarm reinforcement learning based methods can contribute to successful BEMS for small and mid-sized buildings.	https://dx.doi.org/10.1109/ICAIIC.2019.8668985	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sato2001	Parameterized logic programs where computing meets learning	In this paper, we describe recent attempts to incorporate learning into logic programs as a step toward adaptive software that can learn from an environment. Although there are a. variety of types of learning, we focus on parameter learning of logic programs, one for statistical learning by the EM algorithm and the other for reinforcement learning by learning automatons. Both attempts are not full-fledged yet, but in the former case, thanks to the general framework and an efficient EM learning algorithm combined with a tabulated search, we have obtained very promising results that open up the prospect of modeling complex symbolic-statistical phenomena.		Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sato2016	Automatic generation of specification-based test cases by applying genetic algorithms in reinforcement learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962510553&doi=10.1007\%2f978-3-319-31220-0_5&partnerID=40&md5=3e1ed65e48970a50cae55d6eabdf15fd	Included	conflict_resolution		4
RL4SE	Sawant2022	Bridging the gap between QP-based and MPC-based Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142232780&doi=10.1016\%2fj.ifacol.2022.07.600&partnerID=40&md5=5717c08bb501e077e1eb78b98df896a0	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Schaff2022	Soft Robots Learn to Crawl: Jointly Optimizing Design and Control with Sim-to-Real Transfer	"This work provides a complete framework for the simulation, co-optimization, and sim-to-real transfer of the design and control of soft legged robots. The compliance of soft robots provides a form of ""mechanical intelligence""-the ability to passively exhibit behaviors that would otherwise be difficult to program. Exploiting this capacity requires careful consideration of the coupling between mechanical design and control. Co-optimization provides a promising means to generate sophisticated soft robots by reasoning over this coupling. However, the complex nature of soft robot dynamics makes it difficult to achieve a simulation environment that is both sufficiently accurate to allow for sim-to-real transfer and fast enough for contemporary co-optimization algorithms. In this work, we describe a modularized model order reduction algorithm that significantly improves the efficiency of finite element simulation, while preserving the accuracy required to successfully learn effective soft robot design-control pairs that transfer to reality. We propose a reinforcement learning-based framework for co-optimization and demonstrate successful optimization, construction, and zero-shot sim-to-real transfer of several soft crawling robots. Our learned robot outperforms an expert-designed crawling robot, showing that our approach can generate novel, high-performing designs even in well-understood domains."		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Schalow1996	Electromyographic identification of spinal oscillator patterns and recouplings in a patient with incomplete spinal cord lesion: Oscillator formation training as a method to improve motor activities	A patient with a strongly lesioned spinal cord, sub C5, relearned running, besides improving other movements, by an oscillator formation training (rhythmic, dynamic, stereotyped exercise). After 45 days of jumping on a springboard and other rhythm trainings, the patient was able to run 90 m in 4ls (7.9 km/h) (even 9.3 km/h 3 years after the lesion) besides marching (5.7 km/h), cycling, playing tennis and skiing. FF-type (alpha(1)) (f = 8.3-11.4 Hz) and FR-type (alpha(2)) (f = 6.7 Hz) motor unit firings were identified by electromyography (EMG) with surface electrodes by their oscillatory firing patterns in this patient. In EMG literature, the alpha(2)-oscillatory firing is called ''myokymic discharging'' Alternating long and short oscillation periods were measured in FF-type motor units, with changing focus (change from long/short to short/long oscillation periods). The alternating mean period durations differred by approximately 10 ms. Transient synchronization of oscillatory firing FF-type motor units was observed with up to two phase relations per oscillation cycle. In recumbent position, the phase change in synchronization of two oscillatory firing motor units in the soleus muscle of one leg correlated with the change from alternating to symmetrical oscillatory firing of a third motor unit in the soleus muscle of the other leg. This measurement indicates that the alternating oscillatory firing of premotor neuronal networks is correlated with synchronization of oscillatory firing neuronal subnetworks, i.e. with coupling changes of oscillators, and is not due to reciprocal inhibition of half-centre oscillators as suggested by the change from alternating to symmetrical oscillatory firing. Coupling changes of oscillatory firing subnetworks to generate macroscopic (integrative) network functions are therefore a general organization form of the central nervous system (CNS), and are not related to rhythmic movements like walking or running only. It is proposed that synchronization of spinal oscillators, phase changes in synchronization, changes from alternating to symmetrical firing and backwards, and changes in the focus of alternating oscillatory firing are, among others, physiologic coupling rules of the human CNS to generate, by ongoing coupling changes of oscillatory firing subnetworks, integrative functions such as rhythmic and non-rhythmic movements. One phase relation between two oscillatory firing alpha(1)-motor units was preserved from one volitional leg muscle activation (isometric contraction) to the subsequent one. Since running times improved upon successive runs for 90 m, the spinal cord seems to be able to store pattern organization for seconds up to minutes. Controlled and uncontrolled oscillatory firing of alpha(1)-motor units in volitionally activated leg muscles were observed in this patient, which indicated that there still were pathologic recruitments of subnetworks after re-learning running and other movements. During walking, running, and jumping on a springboard, the activation patterns of the vastus lateralis, hamstrings, tibialis anterior, peronaeus longus, peronaeus brevis and soleus muscles were recorded (surface electromyography) to be still pathologic in accordance with partly still pathologic joint rotation angles measured kinematically. Especially upon running, the left knee joint flexion was reduced in swine by a rather permanent activity of the rectus femoris combined with an extra burst of the vastus lateralis in mid-swing. The recorded abnormalities are due to modification of the motor program rather than to muscle weakness per se. A further improvement of the movements of the patient seems possible by improving the motor program, i.e. by improving the functioning of the spinal pattern generators. By comparing the phasic EMG activity upon walking, running and jumping on a springboard, the motor program turned out to be best for jumping and running, at least with respect to the activity of the left peronaeus longus muscle. This indicates that during the more rhythmic, dynamic, stereotyped movements (like jumping or running) more physiologic spinal motor programs were activated by the remaining supraspinal drive. Jumping on a springboard generated the most physiologic movement pattern, probably by ''Mitbewegung'' (co-movement) activated by the synchronization of both legs once per jumping cycle, which induced stronger synchronization of right and left movement pattern generators by a shared afferent input and cycle resettings of oscillatory firing subnetworks of the left and right pattern generators. It is proposed that at least partly, the spinal cord generates stereotyped movements by coupling changes of oscillatory firing subnetworks. The main cause for the movement disorders (and spasticity) occurring following spinal cord lesion is the pathologic organization of the functionally deteriorated neuronal subunits below the lesion (because of nonuse as one reason) in combination with a lesion- and degeneration-induced unbalanced supraspinal and afferent drive, respectively for self-organization of spinal networks. On the basis of our successful therapy trial and measurements on normal, brain-dead, completely (paraplegic) and incompletely spinal cord lesioned (tetraparetic) individuals, it is proposed that the training-induced plasticity of the human CNS to re-preformate neuronal networks for up to minutes and permanently has been underestimated. The oscillator formation training was successful, since the rhythm training fitted the rhythmic organization of the CNS (at least with respect to the premotor alpha(1), alpha(2) alpha(3)-oscillators), as the rhythmic, stereotyped, dynamic leg movements are mainly located in the lumbosacral enlargement of the cord, which was not lesioned in this patient (lesion sub C5), and could be activated by the little remaining supraspinal drive, and since co-movement (''Mitbewegung'') was induced once per jumping cycle by the simultaneous afferent input. This improvement in movements was made possible by re-preformation of spinal neuronal networks for improved self-organization by rhythmic movement-induced afferent input, and by rebalanced lesion-adapted drive of supraspinal networks (by lesion-adapted reorganization). The type-related single motor axon firing patterns were partly verified by telemetrically obtained rhythmic EMG patterns. The self-organization of spinal oscillators, including coupling rules between oscillators, was compared with mathematically derived coupling features of populations of interacting nonlinear biological oscillators, and is discussed with respect to generation of spinal pattern generators in man and animals. Spasticity, ''Mitbewegungen'' (co-movement), ''reafference principle'', supraspinal control and putative descending systems are analysed with respect to the pathologic organization of the human nervous system following incomplete spinal cord lesion. The oscillator formation Graining is compared with the Bobath therapy, reinforcement learning and treadmill walking. Eight 0 to 5 days old naked infants, born in week 36 to 41 of gestation, showed primary automatic stepping with a mean frequency of 0.2 Hz upon natural simulation of the soles of their feet. The incompletely spinal cord lesioned patient was walking, jumping and running with frequencies of 0.8 Hz, 0.9 Hz and 1.5 Hz respectively. It is discussed that the genetically predetermined preformation of spinal neuronal networks in the infants was immature. It seems that the early movements of infants are precursors of later locomotion. Since preformations of neuronal networks are not fixed, the primary stepping with its variations is a system designed to learn to walk or run from interactions with the periphery. Based on the similarities between the innervation during ontogenesis and lesion-induced reinnervation of muscle fibres by different types of motor axons in the frog, the neural maturation model and the system theory of infant motor development are compared to the reorganization of the CNS following spinal cord lesions especially with respect to micturition and stepping. In comparison to the Vojta physiotherapy, the oscillator formation training puts emphasis on the human-specific bipedal locomotion which is genetically predetermined. The rhythm training fits the self-organization of spinal neuronal networks for intergrative functions by changing rhythm couplings of oscillatory firing functional subnetworks (biological oscillators). Only little supraspinal drive is needed for the rhythmic stereotyped dynamic movements. Tetrapedal locomotion, not prominent after the birth, seems to be overstressed in the physiotherapy of spinal cord lesioned adult patients.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Schaul2013	A video game description language for model-based or interactive learning	We propose a powerful new tool for conducting research on computational intelligence and games. `PyVGDL' is a simple, high-level description language for 2D video games, and the accompanying software library permits parsing and instantly playing those games. The streamlined design of the language is based on defining locations and dynamics for simple building blocks, and the interaction effects when such objects collide, all of which are provided in a rich ontology. It can be used to quickly design games, without needing to deal with control structures, and the concise language is also accessible to generative approaches. We show how the dynamics of many classical games can be generated from a few lines of PyVGDL. The main objective of these generated games is to serve as diverse benchmark problems for learning and planning algorithms; so we provide a collection of interfaces for different types of learning agents, with visual or abstract observations, from a global or first-person viewpoint. To demonstrate the library's usefulness in a broad range of learning scenarios, we show how to learn competent behaviors when a model of the game dynamics is available or when it is not, when full state information is given to the agent or just subjective observations, when learning is interactive or in batch-mode, and for a number of different learning algorithms, including reinforcement learning and evolutionary search.	https://dx.doi.org/10.1109/CIG.2013.6633610	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Scheiderer2020	Simulation-as-a-Service for Reinforcement Learning Applications by Example of Heavy Plate Rolling Processes	In the production industry, the digital transformation enables a significant optimization potential. The concept of reinforcement learning offers a suitable approach to train agents on learning control strategies, further advancing automation. While applications training directly on real-world processes are rare due to economical and safety constraints, simulations offer a way to develop and evaluate agents prior to deployment. With the rise of service-based business models, the simulation owner and the machine learning expert are likely to be different stakeholders in a joint project. Due to different requirements for both simulations and reinforcement-learning agents, the stakeholders may be reluctant or unable to grant full access to the respective software. This poses a serious impediment to the potential of the digital transformation. In this paper, a distributed architecture is proposed, which allows the remote training of reinforcement learning agents on a simulation. It is shown that this architecture allows the cooperation between two stakeholders by exposing a suitable technical interface to the simulation. The proposed architecture is implemented for a simulation of the multi-step metal forming process of heavy plate rolling. Furthermore, the implemented architecture is used to successfully train a reinforcement-learning agent on the task of designing optimal parameter schedules. (C) 2020 The Authors. Published by Elsevier Ltd.	https://dx.doi.org/10.1016/j.promfg.2020.10.126	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Scheidt2012	Remembering forward: Neural correlates of memory and prediction in human motor adaptation	"We used functional MR imaging (FMRI), a robotic manipulandum and systems identification techniques to examine neural correlates of predictive compensation for spring-like loads during goal-directed wrist movements in neurologically-intact humans. Although load changed unpredictably from one trial to the next, subjects nevertheless used sensorimotor memories from recent movements to predict and compensate upcoming loads. Prediction enabled subjects to adapt performance so that the task was accomplished with minimum effort. Population analyses of functional images revealed a distributed, bilateral network of cortical and subcortical activity supporting predictive load compensation during visual target capture. Cortical regions--including prefrontal, parietal and hippocampal cortices--exhibited trial-by-trial fluctuations in BOLD signal consistent with the storage and recall of sensorimotor memories or ""states"" important for spatial working memory. Bilateral activations in associative regions of the striatum demonstrated temporal correlation with the magnitude of kinematic performance error (a signal that could drive reward-optimizing reinforcement learning and the prospective scaling of previously learned motor programs). BOLD signal correlations with load prediction were observed in the cerebellar cortex and red nuclei (consistent with the idea that these structures generate adaptive fusimotor signals facilitating cancelation of expected proprioceptive feedback, as required for conditional feedback adjustments to ongoing motor commands and feedback error learning). Analysis of single subject images revealed that predictive activity was at least as likely to be observed in more than one of these neural systems as in just one. We conclude therefore that motor adaptation is mediated by predictive compensations supported by multiple, distributed, cortical and subcortical structures."	https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054093131&doi=10.1016\%2fj.neuroimage.2011.07.072&partnerID=40&md5=0b17ec1a16e612a4c17c24c2bcfea5a4	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Scheiermann2022	AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time	Recently, the seminal algorithms AlphaGo and AlphaZero have started a new era in game learning and deep reinforcement learning. While the achievements of AlphaGo and AlphaZero \endash playing Go and other complex games at super human level \endash are truly impressive, these architectures have the drawback that they require high computational resources. Many researchers are looking for methods that are similar to AlphaZero, but have lower computational demands and are thus more easily reproducible. In this paper, we pick an important element of AlphaZero \endash the Monte Carlo Tree Search (MCTS) planning stage \endash and combine it with temporal difference (TD) learning agents. We wrap MCTS for the first time around TD n-tuple networks and we use this wrapping only at test time to create versatile agents that keep at the same time the computational demands low. We apply this new architecture to several complex games (Othello, ConnectFour, Rubik's Cube) and show the advantages achieved with this AlphaZero-inspired MCTS wrapper. In particular, we present results that this agent is the first one trained on standard hardware (no GPU or TPU) to beat the very strong Othello program Edax up to and including level 7 (where most other learning-from-scratch algorithms could only defeat Edax up to level 2).	https://dx.doi.org/10.1109/TG.2022.3206733	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Schmidhuber2005	Completely self-referential optimal reinforcement learners			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Schmidl2020	Reinforcement learning for energy reduction of conveying and handling systems		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100845624&doi=10.1016\%2fj.procir.2020.05.240&partnerID=40&md5=92dffac20928b3271c76f3e122e4cf47	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Schmidt2020	Control architecture for embedding reinforcement learning frameworks on industrial control hardware		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081082436&doi=10.1145\%2f3378184.3378198&partnerID=40&md5=5a19bb95bbf9f331b25061811a89ae25	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Schmiedt2022	Target State Optimization: Drivability Improvement for Vehicles with Dual Clutch Transmissions	Vehicles with dual clutch transmissions (DCT) are well known for their comfortable drivability since gear shifts can be performed jerklessly. The ability of blending the torque during gear shifts from one clutch to the other, making the type of automated transmission a perfect alternative to torque converters, which also comes with a higher efficiency. Nevertheless, DCT also have some drawbacks. The actuation of two clutches requires an immense control effort, which is handled in the implementation of a wide range of software functions on the transmission control unit (TCU). These usually contain control parameters, which makes the behavior adaptable to different vehicle and engine platforms. The adaption of these parameters is called calibration, which is usually an iterative time-consuming process. The calibration of the embedded software solutions in control units is a widely known problem in the automotive industry. The calibration of any vehicle subsystem (e.g., engine, transmission, suspension, driver assistance systems for autonomous driving, etc.) requires costly test trips in different ambient conditions. To reduce the calibration effort and the accompanying use of professionals, several approaches to automize the calibration process are proposed. Due to the fact that a solution is desired which can optimize different calibration problems, a generic metaheuristic approach is aimed. Regardless, the scope of the current research is the optimization of the launch behavior for vehicles equipped with DCT since, particularly at low speeds, the transmission behavior must meet the intention of the driver (drivers tend to be more perceptive at low speeds). To clarify the characteristics of the launch, several test subject studies are performed. The influence factors, such as engine sound, maximal acceleration, acceleration build-up (mean jerk), and the reaction time, are taken into account. Their influence on the evaluation of launch with relation to the criteria of sportiness, comfort, and jerkiness, are examined based on the evaluation of the test subject studies. According to the results of the study, reference values for the optimization of the launch behavior are derived. The research contains a study of existing approaches for optimizing driving behavior with metaheuristics (e.g., genetic algorithms, reinforcement learning, etc.). Since the existing approaches have different drawbacks (in scope of the optimization problem) a new approach is proposed, which outperforms existing ones. The approach itself is a hybrid solution of reinforcement learning (RL) and supervised learning (SL) and is applied in a software in the loop environment, and in a test vehicle.	https://dx.doi.org/10.3390/app122010283	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Schreck2019	Learning Retrosynthetic Planning through Simulated Experience	The problem of retrosynthetic planning can be framed as a one-player game, in which the chemist (or a computer program) works backward from a molecular target to simpler starting materials through a series of choices regarding which reactions to perform. This game is challenging as the combinatorial space of possible choices is astronomical, and the value of each choice remains uncertain until the synthesis plan is completed and its cost evaluated. Here, we address this search problem using deep reinforcement learning to identify policies that make (near) optimal reaction choices during each step of retrosynthetic planning according to a user-defined cost metric. Using a simulated experience, we train a neural network to estimate the expected synthesis cost or value of any given molecule based on a representation of its molecular structure. We show that learned policies based on this value network can outperform a heuristic approach that favors symmetric disconnections when synthesizing unfamiliar molecules from available starting materials using the fewest number of reactions. We discuss how the learned policies described here can be incorporated into existing synthesis planning tools and how they can be adapted to changes in the synthesis cost objective or material availability.	https://www.ncbi.nlm.nih.gov/pubmed/31263756	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Schuitema2010	The design of LEO: A 2D bipedal walking robot for online autonomous Reinforcement Learning	Real robots demonstrating online Reinforcement Learning (RL) to learn new tasks are hard to find. The specific properties and limitations of real robots have a large impact on their suitability for RL experiments. In this work, we derive the main hardware and software requirements that a RL robot should fulfill, and present our biped robot LEO that was specifically designed to meet these requirements. We verify its aptitude in autonomous walking experiments using a pre-programmed controller. Although there is room for improvement in the design, the robot was able to walk, fall and stand up without human intervention for 8 hours, during which it made over 43; 000 footsteps.	https://dx.doi.org/10.1109/IROS.2010.5650765	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Schwung2018	On-line Energy Optimization of Hybrid Production Systems Using Actor-Critic Reinforcement Learning	This paper presents a novel approach for energy optimization in large scale industrial production systems based on an actor-critic reinforcement learning (ACRL) framework. The objective of the on-line capable self-learning algorithm is the optimization of the energy consumption of the whole production process. Our central ACRL framework is realized by artificial neural network (ANN) function approximation using Gaussian radial-basis functions (RBF) for the critic and the actor, respectively, and gives the opportunity to cover not only a discrete but also continuous state and action space, which is necessary for hybrid systems where discrete and continuous actuator behavior is combined. For testing and validation purposes we develop a software model of our bulk good laboratory plant as application example for the developed ACRL algorithm. The model is based on mass-flow equations for a continuous bulk good supply whereas the energy consumption is modeled by functions dependent on the actuators behavior. The capability of our machine learning (ML) approach for energy optimization is underlined by simulation results for the task of supplying bulk good to a subsequent dosing section.	https://dx.doi.org/10.1109/IS.2018.8710466	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Scott2020	BanditFuzz: A Reinforcement-Learning Based Performance Fuzzer for SMT Solvers		https://doi.org/10.1007/978-3-030-63618-0_5	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Scurto2021	Designing Deep Reinforcement Learning for Human Parameter Exploration		https://doi.org/10.1145/3414472	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sefidgar2022	Landing System Development Based on Inverse Homography Range Camera Fusion (IHRCF)	The Unmanned Aerial Vehicle (UAV) is one of the most remarkable inventions of the last 100 years. Much research has been invested in the development of this flying robot. The landing system is one of the more challenging aspects of this system's development. Artificial Intelligence (AI) has become the preferred technique for landing system development, including reinforcement learning. However, current research is more focused is on system development based on image processing and advanced geometry. A novel calibration based on our previous research had been used to ameliorate the accuracy of the AprilTag pose estimation. With the help of advanced geometry from camera and range sensor data, a process known as Inverse Homography Range Camera Fusion (IHRCF), a pose estimation that outperforms our previous work, is now possible. The range sensor used here is a Time of Flight (ToF) sensor, but the algorithm can be used with any range sensor. First, images are captured by the image acquisition device, a monocular camera. Next, the corners of the landing landmark are detected through AprilTag detection algorithms (ATDA). The pixel correspondence between the image and the range sensor is then calculated via the calibration data. In the succeeding phase, the planar homography between the real-world locations of sensor data and their obtained pixel coordinates is calculated. In the next phase, the pixel coordinates of the AprilTag-detected four corners are transformed by inverse planar homography from pixel coordinates to world coordinates in the camera frame. Finally, knowing the world frame corner points of the AprilTag, rigid body transformation can be used to create the pose data. A CoppeliaSim simulation environment was used to evaluate the IHRCF algorithm, and the test was implemented in real-time Software-in-the-Loop (SIL). The IHRCF algorithm outperformed the AprilTag-only detection approach significantly in both translational and rotational terms. To conclude, the conventional landmark detection algorithm can be ameliorated by incorporating sensor fusion for cameras with lower radial distortion.	https://www.ncbi.nlm.nih.gov/pubmed/35271018	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sengupta2014	Learning distributed caching strategies in small cell networks	Caching has emerged as a vital tool in modern communication systems for reducing peak data rates by allowing popular files to be pre-fetched and stored locally at end users' devices. With the shift in paradigm from homogeneous cellular networks to the heterogeneous ones, the concept of data offloading to small cell base stations (sBS) has garnered significant attention. Caching at these small cell base stations has recently been proposed, where popular files are pre-fetched and stored locally in order to avoid bottlenecks in the limited capacity backhaul connection link to the core network. In this paper, we study distributed caching strategies in such a heterogeneous small cell wireless network from a reinforcement learning perspective. Using state of the art results, it can be shown that the optimal joint cache content placement in the sBSs turns out to be a NP-hard problem even when the sBS's are aware of the popularity profile of the files that are to be cached. To address this problem, we propose a coded caching framework, where the sBSs learn the popularity profile of the files (based on their demand history) via a combinatorial multi-armed bandit framework. The sBSs then pre-fetch segments of the Fountain-encoded versions of the popular files at regular intervals to serve users' requests. We show that the proposed coded caching framework can be modeled as a linear program that takes into account the network connectivity and thereby jointly designs the caching strategies. Numerical results are presented to show the benefits of the joint coded caching technique over naive decentralized cache placement strategies.	https://dx.doi.org/10.1109/ISWCS.2014.6933484	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Senthilkumar2009	Hybrid Genetic-fuzzy approach to Autonomous Mobile Robot	An Autonomous Mobile Robot (AMR) is a machine able to extract information from its environment and use knowledge about its world to move safely in a meaningful and purposeful manner. Robot Navigation and Obstacle Avoidance are from the most important problems in mobile robots, especially in unknown environments. It must be able to interact with other objects safely. Several techniques such as Fuzzy logic, Reinforcement learning, Neural Networks and Genetic Algorithms, have applied to AMR in order to improve their performance. During the past several years Hybrid Genetic-fuzzy method has emerged as one of the most active and fruitful areas for research in the application of intelligent system design. The objective of this work is to provide a Hybrid method by which an improved set of rules governing the actions and behavior of a simple navigating and obstacle avoiding AMR. Genes are in the form of distances and angles labels. The chromosomes are represented as a rule written in a Boolean algebraic form. The method used to enhance the performance employs a simulation model designed by using Visual Basic software.	https://dx.doi.org/10.1109/TEPRA.2009.5339649	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Serrano2020	QN-Docking: An innovative molecular docking methodology based on Q-Networks		https://doi.org/10.1016/j.asoc.2020.106678	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Servadei2020	Cost Optimization at Early Stages of Design Using Deep Reinforcement Learning	With the increase in the complexity of the modern System on Chips (SoCs) and the demand for a lower time-to-market, automation becomes essential in hardware design. This is particularly relevant in complex/time-consuming tasks, as the optimization of design cost for a hardware component. Design cost, in fact, may depend on several objectives, as for the hardware-software trade-off. Given the complexity of this task, the designer often has no means to perform a fast and effective optimization-in particular for larger and complex designs. In this paper, we introduce Deep Reinforcement Learning (DRL) for design cost optimization at the early stages of the design process. We first show that DRL is a perfectly suitable solution for the problem at hand. Afterwards, by means of a Pointer Network, a neural network specifically applied for combinatorial problems, we benchmark three DRL algorithms towards the selected problem. Results obtained in different settings show the improvements achieved by DRL algorithms compared to conventional optimization methods. Additionally, by using reward redistribution proposed in the recently introduced RUDDER method, we obtain significant improvements in complex designs.	https://dx.doi.org/10.1145/3380446.3430619	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sethumurugan2021	Designing a Cost-Effective Cache Replacement Policy using Machine Learning	Extensive research has been carried out to improve cache replacement policies, yet designing an efficient cache replacement policy that incurs low hardware overhead remains a challenging and time-consuming task. Given the surging interest in applying machine learning (ML) to challenging computer architecture design problems, we use ML as an offline tool to design a cost-effective cache replacement policy. We demonstrate that ML is capable of guiding and expediting the generation of a cache replacement policy that is competitive with state-of-the-art hand-crafted policies. In this work, we use Reinforcement Learning (RL) to learn a cache replacement policy. After analyzing the learned model, we are able to focus on a few critical features that might impact system performance. Using the insights provided by RL, we successfully derive a new cache replacement policy \endash Reinforcement Learned Replacement (RLR). Compared to the state-of-the-art policies, RLR has low hardware overhead, and it can be implemented without needing to modify the processor's control and data path to propagate information such as program counter. On average, RLR improves single-core and four-core system performance by 3.25\% and 4.86\% over LRU, with an overhead of 16.75KB for 2MB last-level cache (LLC) and 67KB for 8MB LLC.	https://dx.doi.org/10.1109/HPCA51647.2021.00033	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Piette2016_1	Patient-Centered Pain Care Using Artificial Intelligence and Mobile Health Tools: Protocol for a Randomized Study Funded by the US Department of Veterans Affairs Health Services Research and Developme	"BACKGROUND: Cognitive behavioral therapy (CBT) is one of the most effective treatments for chronic low back pain. However, only half of Department of Veterans Affairs (VA) patients have access to trained CBT therapists, and program expansion is costly. CBT typically consists of 10 weekly hour-long sessions. However, some patients improve after the first few sessions while others need more extensive contact. OBJECTIVE: We are applying principles from ""reinforcement learning"" (a field of artificial intelligence or AI) to develop an evidence-based, personalized CBT pain management service that automatically adapts to each patient's unique and changing needs (AI-CBT). AI-CBT uses feedback from patients about their progress in pain-related functioning measured daily via pedometer step counts to automatically personalize the intensity and type of patient support. The specific aims of the study are to (1) demonstrate that AI-CBT has pain-related outcomes equivalent to standard telephone CBT, (2) document that AI-CBT achieves these outcomes with more efficient use of clinician resources, and (3) demonstrate the intervention's impact on proximal outcomes associated with treatment response, including program engagement, pain management skill acquisition, and patients' likelihood of dropout. METHODS: In total, 320 patients with chronic low back pain will be recruited from 2 VA healthcare systems and randomized to a standard 10 sessions of telephone CBT versus AI-CBT. All patients will begin with weekly hour-long telephone counseling, but for patients in the AI-CBT group, those who demonstrate a significant treatment response will be stepped down through less resource-intensive alternatives including: (1) 15-minute contacts with a therapist, and (2) CBT clinician feedback provided via interactive voice response calls (IVR). The AI engine will learn what works best in terms of patients' personally tailored treatment plans based on daily feedback via IVR about their pedometer-measured step counts, CBT skill practice, and physical functioning. Outcomes will be measured at 3 and 6 months post recruitment and will include pain-related interference, treatment satisfaction, and treatment dropout. Our primary hypothesis is that AI-CBT will result in pain-related functional outcomes that are at least as good as the standard approach, and that by scaling back the intensity of contact that is not associated with additional gains in pain control, the AI-CBT approach will be significantly less costly in terms of therapy time. RESULTS: The trial is currently in the start-up phase. Patient enrollment will begin in the fall of 2016 and results of the trial will be available in the winter of 2019. CONCLUSIONS: This study will evaluate an intervention that increases patients' access to effective CBT pain management services while allowing health systems to maximize program expansion given constrained resources."	https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047730597&doi=10.2196\%2fresprot.4995&partnerID=40&md5=68fd2890d6960aa9ee6d6244de1766bd	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Piette2016_2	Patient-Centered Pain Care Using Artificial Intelligence and Mobile Health Tools: Protocol for a Randomized Study Funded by the US Department of Veterans Affairs Health Services Research and Developme	"BACKGROUND: Cognitive behavioral therapy (CBT) is one of the most effective treatments for chronic low back pain. However, only half of Department of Veterans Affairs (VA) patients have access to trained CBT therapists, and program expansion is costly. CBT typically consists of 10 weekly hour-long sessions. However, some patients improve after the first few sessions while others need more extensive contact. OBJECTIVE: We are applying principles from ""reinforcement learning"" (a field of artificial intelligence or AI) to develop an evidence-based, personalized CBT pain management service that automatically adapts to each patient's unique and changing needs (AI-CBT). AI-CBT uses feedback from patients about their progress in pain-related functioning measured daily via pedometer step counts to automatically personalize the intensity and type of patient support. The specific aims of the study are to (1) demonstrate that AI-CBT has pain-related outcomes equivalent to standard telephone CBT, (2) document that AI-CBT achieves these outcomes with more efficient use of clinician resources, and (3) demonstrate the intervention's impact on proximal outcomes associated with treatment response, including program engagement, pain management skill acquisition, and patients' likelihood of dropout. METHODS: In total, 320 patients with chronic low back pain will be recruited from 2 VA healthcare systems and randomized to a standard 10 sessions of telephone CBT versus AI-CBT. All patients will begin with weekly hour-long telephone counseling, but for patients in the AI-CBT group, those who demonstrate a significant treatment response will be stepped down through less resource-intensive alternatives including: (1) 15-minute contacts with a therapist, and (2) CBT clinician feedback provided via interactive voice response calls (IVR). The AI engine will learn what works best in terms of patients' personally tailored treatment plans based on daily feedback via IVR about their pedometer-measured step counts, CBT skill practice, and physical functioning. Outcomes will be measured at 3 and 6 months post recruitment and will include pain-related interference, treatment satisfaction, and treatment dropout. Our primary hypothesis is that AI-CBT will result in pain-related functional outcomes that are at least as good as the standard approach, and that by scaling back the intensity of contact that is not associated with additional gains in pain control, the AI-CBT approach will be significantly less costly in terms of therapy time. RESULTS: The trial is currently in the start-up phase. Patient enrollment will begin in the fall of 2016 and results of the trial will be available in the winter of 2019. CONCLUSIONS: This study will evaluate an intervention that increases patients' access to effective CBT pain management services while allowing health systems to maximize program expansion given constrained resources."	https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047730597&doi=10.2196\%2fresprot.4995&partnerID=40&md5=68fd2890d6960aa9ee6d6244de1766bd	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Piette2016_3	Patient-Centered Pain Care Using Artificial Intelligence and Mobile Health Tools: Protocol for a Randomized Study Funded by the US Department of Veterans Affairs Health Services Research and Developme	"BACKGROUND: Cognitive behavioral therapy (CBT) is one of the most effective treatments for chronic low back pain. However, only half of Department of Veterans Affairs (VA) patients have access to trained CBT therapists, and program expansion is costly. CBT typically consists of 10 weekly hour-long sessions. However, some patients improve after the first few sessions while others need more extensive contact. OBJECTIVE: We are applying principles from ""reinforcement learning"" (a field of artificial intelligence or AI) to develop an evidence-based, personalized CBT pain management service that automatically adapts to each patient's unique and changing needs (AI-CBT). AI-CBT uses feedback from patients about their progress in pain-related functioning measured daily via pedometer step counts to automatically personalize the intensity and type of patient support. The specific aims of the study are to (1) demonstrate that AI-CBT has pain-related outcomes equivalent to standard telephone CBT, (2) document that AI-CBT achieves these outcomes with more efficient use of clinician resources, and (3) demonstrate the intervention's impact on proximal outcomes associated with treatment response, including program engagement, pain management skill acquisition, and patients' likelihood of dropout. METHODS: In total, 320 patients with chronic low back pain will be recruited from 2 VA healthcare systems and randomized to a standard 10 sessions of telephone CBT versus AI-CBT. All patients will begin with weekly hour-long telephone counseling, but for patients in the AI-CBT group, those who demonstrate a significant treatment response will be stepped down through less resource-intensive alternatives including: (1) 15-minute contacts with a therapist, and (2) CBT clinician feedback provided via interactive voice response calls (IVR). The AI engine will learn what works best in terms of patients' personally tailored treatment plans based on daily feedback via IVR about their pedometer-measured step counts, CBT skill practice, and physical functioning. Outcomes will be measured at 3 and 6 months post recruitment and will include pain-related interference, treatment satisfaction, and treatment dropout. Our primary hypothesis is that AI-CBT will result in pain-related functional outcomes that are at least as good as the standard approach, and that by scaling back the intensity of contact that is not associated with additional gains in pain control, the AI-CBT approach will be significantly less costly in terms of therapy time. RESULTS: The trial is currently in the start-up phase. Patient enrollment will begin in the fall of 2016 and results of the trial will be available in the winter of 2019. CONCLUSIONS: This study will evaluate an intervention that increases patients' access to effective CBT pain management services while allowing health systems to maximize program expansion given constrained resources."	https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047730597&doi=10.2196\%2fresprot.4995&partnerID=40&md5=68fd2890d6960aa9ee6d6244de1766bd	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Piette2016_4	Patient-Centered Pain Care Using Artificial Intelligence and Mobile Health Tools: Protocol for a Randomized Study Funded by the US Department of Veterans Affairs Health Services Research and Developme	"BACKGROUND: Cognitive behavioral therapy (CBT) is one of the most effective treatments for chronic low back pain. However, only half of Department of Veterans Affairs (VA) patients have access to trained CBT therapists, and program expansion is costly. CBT typically consists of 10 weekly hour-long sessions. However, some patients improve after the first few sessions while others need more extensive contact. OBJECTIVE: We are applying principles from ""reinforcement learning"" (a field of artificial intelligence or AI) to develop an evidence-based, personalized CBT pain management service that automatically adapts to each patient's unique and changing needs (AI-CBT). AI-CBT uses feedback from patients about their progress in pain-related functioning measured daily via pedometer step counts to automatically personalize the intensity and type of patient support. The specific aims of the study are to (1) demonstrate that AI-CBT has pain-related outcomes equivalent to standard telephone CBT, (2) document that AI-CBT achieves these outcomes with more efficient use of clinician resources, and (3) demonstrate the intervention's impact on proximal outcomes associated with treatment response, including program engagement, pain management skill acquisition, and patients' likelihood of dropout. METHODS: In total, 320 patients with chronic low back pain will be recruited from 2 VA healthcare systems and randomized to a standard 10 sessions of telephone CBT versus AI-CBT. All patients will begin with weekly hour-long telephone counseling, but for patients in the AI-CBT group, those who demonstrate a significant treatment response will be stepped down through less resource-intensive alternatives including: (1) 15-minute contacts with a therapist, and (2) CBT clinician feedback provided via interactive voice response calls (IVR). The AI engine will learn what works best in terms of patients' personally tailored treatment plans based on daily feedback via IVR about their pedometer-measured step counts, CBT skill practice, and physical functioning. Outcomes will be measured at 3 and 6 months post recruitment and will include pain-related interference, treatment satisfaction, and treatment dropout. Our primary hypothesis is that AI-CBT will result in pain-related functional outcomes that are at least as good as the standard approach, and that by scaling back the intensity of contact that is not associated with additional gains in pain control, the AI-CBT approach will be significantly less costly in terms of therapy time. RESULTS: The trial is currently in the start-up phase. Patient enrollment will begin in the fall of 2016 and results of the trial will be available in the winter of 2019. CONCLUSIONS: This study will evaluate an intervention that increases patients' access to effective CBT pain management services while allowing health systems to maximize program expansion given constrained resources."	https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047730597&doi=10.2196\%2fresprot.4995&partnerID=40&md5=68fd2890d6960aa9ee6d6244de1766bd	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yoon2021	Game Theory and Machine Learning for Cyber Security	In this chapter, we propose a resource?aware active defense framework for software?defined networking (SDN)?based Internet?of?Battle?Things (IoBT) by leveraging the advanced features of deep reinforcement learning (DRL). The proposed framework aims to build a highly attack?resistant network against both physical and cyberspace epidemic attacks. Since typically not all nodes are fully utilized to accomplish a mission, nodes with low utilities can be discarded when they have failed or have been compromised, instead of repairing or replacing them, for a resource?constrained tactical network assigned with a time?sensitive mission. However, highly critical and capable nodes should be protected with high priority in order to maximize system security and mission completion rate in the given tactical network. Considering severe resource constraints in IoBT consisting of highly heterogeneous entities under high hostility and network dynamics, we proposed two resource?aware defense techniques: (1) a multilayer defense network architecture that can construct a network topology based on the importance levels of nodes to provide more security protection for more important nodes which can maximize both security and mission performance (e.g. service availability); and (2) a resource?aware intrusion response framework that can determine an optimal response action(destruction, repair, or replacement) in response to a detected failure/attack. We conduct a comparative analysis of multiple DRL algorithms against baseline schemes to demonstrate the superiority of our proposed DRL?based intrusion response strategies not only in system security (e.g. mean time to security failure, MTTSF) and performance (e.g. correct message delivery ratio for mission completion) but also in the accumulated reward obtainable by the system.	http://ieeexplore.ieee.org/document/9536321	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sha2022	Forwarding efficiency aware traffic scheduling algorithm based on deep reinforcement learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139156939&doi=10.11959\%2fj.issn.1000-436x.2022148&partnerID=40&md5=c63a4ae87cab6199e5ff7d95d22465a8	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Shaban2016	WaterMan: an on-farm water management game with heuristic capabilities	A modern computer-based simulation tool (WaterMan) in the form of a game for on-farm water management was developed for application in training events for farmers, students, and irrigators. The WaterMan game utilizes an interactive framework, thereby allowing the user to develop scenarios and test alternatives in a convenient, risk-free environment. It includes a comprehensive soil water and salt balance calculation algorithm. It also employs heuristic capabilities for modeling all of the important aspects of on-farm water management, and to provide quantitative performance evaluations and practical water management advice to the trainees. Random events (both favorable and unfavorable) and different strategic decisions are included in the game for more realism and to provide an appropriate level of challenge according to player performance. Thus, the ability to anticipate the player skill level, and to reply with random events appropriate to the anticipated level, is provided by the heuristic capabilities used in the software. These heuristic features were developed based on a combination of two artificial intelligence approaches: (1) a pattern recognition approach and (2) reinforcement learning based on a Markov decision processes approach, specifically the Q-learning method. These two approaches were combined in a new way to account for the difference in the effect of actions taken by the player and action taken by the system in the game world. The reward function for the Q-learning method was modified to reflect the suggested classification of the WaterMan game as what is referred to as a partially competitive and partially cooperative game.	https://dx.doi.org/10.1007/s00271-016-0516-6	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Shafi2017	Scenario-based multi-period program optimization for capability-based planning using evolutionary algorithms		https://doi.org/10.1016/j.asoc.2016.07.009	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Shah2018	Dynamic Optimization of the Level of Operational Effectiveness of a CSOC Under Adverse Conditions		https://doi.org/10.1145/3173457	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Shah2020	Neural Approximate Dynamic Programming for On-Demand Ride-Pooling	On-demand ride-pooling (e.g., UberPool, LyftLine, Grab-Share) has recently become popular because of its ability to lower costs for passengers while simultaneously increasing revenue for drivers and aggregation companies (e.g., Uber). Unlike in Taxi on Demand (ToD) services - where a vehicle is assigned one passenger at a time - in on-demand ride-pooling, each vehicle must simultaneously serve multiple passengers with heterogeneous origin and destination pairs without violating any quality constraints. To ensure near real-time response, existing solutions to the real-time ride-pooling problem are myopic in that they optimise the objective (e.g., maximise the number of passengers served) for the current time step without considering the effect such an assignment could have on assignments in future time steps. However, considering the future effects of an assignment that also has to consider what combinations of passenger requests can be assigned to vehicles adds a layer of combinatorial complexity to the already challenging problem of considering future effects in the ToD case. A popular approach that addresses the limitations of myopic assignments in ToD problems is Approximate Dynamic Programming (ADP). Existing ADP methods for ToD can only handle Linear Program (LP) based assignments, however, as the value update relies on dual values from the LP. The assignment problem in ride pooling requires an Integer Linear Program (ILP) that has bad LP relaxations. Therefore, our key technical contribution is in providing a general ADP method that can learn from the ILP based assignment found in ride-pooling. Additionally, we handle the extra combinatorial complexity from combinations of passenger requests by using a Neural Network based approximate value function and show a connection to Deep Reinforcement Learning that allows us to learn this value-function with increased stability and sample-efficiency. We show that our approach easily outperforms leading approaches for on-demand ride-pooling on a real-world dataset by up to 16\%, a significant improvement in city-scale transportation problems.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Shahbazi2020	Toward Social Media Content Recommendation Integrated with Data Science and Machine Learning Approach for E-Learners	Electronic Learning (e-learning) has made a great success and recently been estimated as a billion-dollar industry. The users of e-learning acquire knowledge of diversified content available in an application using innovative means. There is much e-learning software available-for example, LMS (Learning Management System) and Moodle. The functionalities of this software were reviewed and we recognized that learners have particular problems in getting relevant recommendations. For example, there might be essential discussions about a particular topic on social networks, such as Twitter, but that discussion is not linked up and recommended to the learners for getting the latest updates on technology-updated news related to their learning context. This has been set as the focus of the current project based on symmetry between user project specification. The developed project recommends relevant symmetric articles to e-learners from the social network of Twitter and the academic platform of DBLP. For recommendations, a Reinforcement learning model with optimization is employed, which utilizes the learners' local context, learners' profile available in the e-learning system, and the learners' historical views. The recommendations by the system are relevant tweets, popular relevant Twitter users, and research papers from DBLP. For matching the local context, profile, and history with the tweet text, we recognized that terms in the e-learning system need to be expanded to cover a wide range of concepts. However, this diversification should not include such terms which are irrelevant. To expand terms of the local context, profile and history, the software used the dataset of Grow-bag, which builds concept graphs of large-scale Computer Science topics based on the co-occurrence scores of Computer Science terms. This application demonstrated the need and success of e-learning software that is linked with social media and sends recommendations for the content being learned by the e-Learners in the e-learning environment. However, the current application only focuses on the Computer Science domain. There is a need for generalizing such applications to other domains in the future.	https://dx.doi.org/10.3390/sym12111798	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Shahmardan2020	Truck scheduling in a multi-door cross-docking center with partial unloading - Reinforcement learning-based simulated annealing approaches	In this paper, a truck scheduling problem at a cross-docking center is investigated where inbound trucks are also used as outbound. Moreover, inbound trucks do not need to unload and reload the demand of allocated destination, i.e. they can be partially unloaded. The problem is modeled as a mixed integer program to find the optimal dock-door and destination assignments as well as the scheduling of trucks to minimize makespan. Due to model complexity, a hybrid heuristic-simulated annealing is developed. A number of generic and tailor-made neighborhood search structures are also developed to efficiently search solution space. Moreover, some reinforcement learning methods are applied to intellectually learn more suitable neighborhood search structures in different situations. Finally, the numerical study shows that partial unloading of compound trucks has a crucial impact on makespan reduction.	https://dx.doi.org/10.1016/j.cie.2019.106134	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Shahmardan2020a	Truck scheduling in a multi-door cross-docking center with partial unloading endash Reinforcement learning-based simulated annealing approaches		https://doi.org/10.1016/j.cie.2019.106134	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Shahverdi2022	Irrigation canal control using enhanced fuzzy SARSA learning	Fuzzy SARSA learning (FSL) is a robust reinforcement learning (RL) technique that represents successful solutions in various industrial problems. Water management in irrigation canals is one of these problems where an FSL agent interacts with the canal environment to control the gates. FSL often requires a large number of interactive experiences and takes a long time in real-life problems. To reduce the iteration and speed up the learning process, an enhanced FSL (EFSL) was developed to accelerate the process of policy learning. A MATLAB program was written, and combined with the irrigation canal conveyance system simulation (ICSS) model. To evaluate the proposed idea, the E1R1 Dez canal, located in the south-west of Iran, was considered as a case study. Standard performance indicators were used for assessing the results based on considered water delivery scenarios, showing a shorter learning time with reasonable performance in controlling water depth changes within the canal.	https://dx.doi.org/10.1002/ird.2684	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Shakarami2020	A survey on the computation offloading approaches in mobile edge computing: A machine learning-based perspective	With the rapid developments in emerging mobile technologies, utilizing resource-hungry mobile applications such as media processing, online Gaming, Augmented Reality (AR), and Virtual Reality (VR) play an essential role in both businesses and entertainments. To soften the burden of such complexities incurred by fast developments of such serving technologies, distributed Mobile Edge Computing (MEC) has been developed, aimed at bringing the computation environments near the end-users, usually in one hop, to reach predefined requirements. In the literature, offloading approaches are developed to connect the computation environments to mobile devices by transferring resource-hungry tasks to the near servers. Because of some rising problems such as inherent software and hardware heterogeneity, restrictions, dynamism, and stochastic behavior of the ecosystem, the computation offloading issues consider as the essential challenging problems in the MEC environment. However, to the best of the author's knowledge, in spite of its significance, in machine learning-based (ML-based) computation offloading mechanisms, there is not any systematic, comprehensive, and detailed survey in the MEC environment. In this paper, we provide a review on the ML-based computation offloading mechanisms in the MEC environment in the form of a classical taxonomy to identify the contemporary mechanisms on this crucial topic and to offer open issues as well. The proposed taxonomy is classified into three main fields: Reinforcement learning-based mechanisms, supervised learning-based mechanisms, and unsupervised learning-based mechanisms. Next, these classes are compared with each other based on the essential features such as performance metrics, case studies, utilized techniques, and evaluation tools, and their advantages and weaknesses are discussed, as well. Finally, open issues and uncovered or inadequately covered future research challenges are argued, and the survey is concluded.	https://dx.doi.org/10.1016/j.comnet.2020.107496	Excluded	new_screen	E5: Other not a paper,E2: Software engineering is not the problem RL is used for	4
RL4SE	Shakshuki2006	RL-Agent That Learns in Collaborative Virtual Environment	Today, collaborative virtual environments consider intelligent software agents as essential part of the environment. In these environments, there is a need of agents that have the ability to learn user actions, which are gained from the experience of interactions with the users. This learning capability will predict the future actions of the user and provide better solutions to them. Collaborative virtual workspace (CVW) is a collaborative virtual environment where agents can be created and perform important tasks for the user. This paper presents an agent with the ability to monitor the user's actions and has the learning capability using reinforcement learning algorithm. To demonstrate the feasibility of this agent, it is implemented and demonstrated in FCVW an extension of CVW, an environment for collaboration and knowledge management	https://dx.doi.org/10.1109/ITNG.2006.116	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Shang2019	Environment Reconstruction with Hidden Confounders for Reinforcement Learning based Recommendation		https://doi.org/10.1145/3292500.3330933	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Shang2020	Reinforcement Learning-Based Solution to Power Grid Planning and Operation Under Uncertainties	With the ever-increasing stochastic and dynamic behavior observed in today's bulk power systems, securely and economically planning future operational scenarios that meet all reliability standards under uncertainties becomes a challenging computational task, which typically involves searching feasible and suboptimal solutions in a highly dimensional space via massive numerical simulations. This paper presents a novel approach to achieving this goal by adopting the state-of-the-art reinforcement learning algorithm, Soft Actor Critic (SAC). First, the optimization problem of finding feasible solutions under uncertainties is formulated as Markov Decision Process (MDP). Second, a general and flexible framework is developed to train SAC agent by adjusting generator active power outputs for searching feasible operating conditions. A software prototype is developed that verifies the effectiveness of the proposed approach via numerical studies conducted on the planning cases of the SGCC Zhejiang Electric Power Company.	https://dx.doi.org/10.1109/MLHPCAI4S51975.2020.00015	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Shang2020a	Occurrence Frequency and All Historical Failure Information Based Method for TCP in CI		https://doi.org/10.1145/3379177.3388903	Included	conflict_resolution		4
RL4SE	Shapiro2001	Using background knowledge to speed reinforcement learning in physical agents		https://doi.org/10.1145/375735.376305	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sharbaf2022	Automatic resolution of model merging conflicts using quality-based reinforcement learning	Modeling is an activity in the software development life cycle in which different experts and stakeholders collaborate as a team. In collaborative modeling, adhering to the optimistic versioning paradigm allows users to apply concurrent changes to the same model. In such a situation, conflicts may arise. To have an integrated yet consistent merged model, conflicts have to be resolved. To this end, automation is currently at its limit or is not supported at all, and user interaction is often required. To alleviate this flaw, there is an opportunity to apply Artificial Intelligence techniques in a collaborative modeling environment to empower the provisioning of automated and intelligent decision-making. In this paper, we propose the use of reinforcement learning algorithms to achieve merging conflict resolution with a high degree of automation. This enables the personalized and quality-based integration of model versions. To evaluate our idea, we demonstrate the resolution of UML class diagram conflicts using a learning process in an illustrative modeling scenario. We also show the applicability of our approach through a proof of concept implementation and assess its accuracy compared to the greedy and search-based algorithms. Moreover, we conducted an experience with five experts to evaluate the satisfaction of actual users with the selection of resolution actions for different conflicts. The result of the assessment validates our proposal with various syntactic and semantic conflicts.	https://dx.doi.org/10.1016/j.cola.2022.101123	Included	new_screen		4
RL4SE	Sharma2017	Literature survey of statistical, deep and reinforcement learning in natural language processing	This paper underlines the necessity to incorporate Deep learning and Neural networking in language models under scrutiny for Natural Language Processing. The paper describes various statistical models proposed and the limitations incurred in the same due to limited intelligence of a machine. We have discussed different neural networks highlighting the importance of Convolutional Neural Networking. We have discussed about open source software TensorFlow that works on Deep learning and the edge it has over the conventional models. Also we have recommended Reinforcement learning as an extension to neural networking which is widely used in gaming for the purpose of Natural Language Processing. We can utilize the reward-driven algorithm for better results.	https://dx.doi.org/10.1109/CCAA.2017.8229841	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sharma2005	A safe and consistent game-theoretic controller for nonlinear systems			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Shastha2020	Application of Reinforcement Learning to a Robotic Drinking Assistant	Meal assistant robots form a very important part of the assistive robotics sector since self-feeding is a priority activity of daily living (ADL) for people suffering from physical disabilities like tetraplegia. A quick survey of the current trends in this domain reveals that, while tremendous progress has been made in the development of assistive robots for the feeding of solid foods, the task of feeding liquids from a cup remains largely underdeveloped. Therefore, this paper describes an assistive robot that focuses specifically on the feeding of liquids from a cup using tactile feedback through force sensors with direct human-robot interaction (HRI). The main focus of this paper is the application of reinforcement learning (RL) to learn what the best robotic actions are, based on the force applied by the user. A model of the application environment is developed based on the Markov decision process and a software training procedure is designed for quick development and testing. Five of the commonly used RL algorithms are investigated, with the intention of finding the best fit for training, and the system is tested in an experimental study. The preliminary results show a high degree of acceptance by the participants. Feedback from the users indicates that the assistive robot functions intuitively and effectively.	https://dx.doi.org/10.3390/robotics9010001	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	She2022	BCLB: Blockchain-based Controller Load Balance for Safe and Reliable Resource Optimization	Controller in software-defined network (SDN) can realize the intelligent load balancing of switches. However, the controller is lacking in the ability of global load management and control methods for managing multi-domain controllers. With the benefits of realizing decentralized, traceable, and trusted data sharing, blockchain can provide controllers with information such as inter-domain links and the global view to assist achieving global controller load balancing. We propose one global controller load-balancing method based on blockchain in this paper. Our proposed method adopts in-band telemetry (INT) so as to sense the status of each network domain, and combine the load of the controller itself. This enables to package and upload the chain to form a decentralized data-sharing model. Based on the load and network domain status of the global controller, combined with deep reinforcement learning (DRL) algorithm analysis, a specific solution is obtained, and some edge switches in the network domain of the overloaded controller are selected for migration, thereby achieving global load balancing. The method in this paper can not only integrate global information and reduce migration costs, but also ensure that domain information is not leaked. Simulation results show its convergence and effectiveness.	https://dx.doi.org/10.1109/GCWkshps56602.2022.10008562	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sheikhi2016	Demand side management for a residential customer in multi-energy systems		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961801765&doi=10.1016\%2fj.scs.2016.01.010&partnerID=40&md5=b4bbaadeb9a08439c4d5e7832bffed47	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Shen2007	An Adaptive Markov Game Model for Threat Intent Inference	In an adversarial military environment, it is important to efficiently and promptly predict the enemy's tactical intent from lower level spatial and temporal information. In this paper, we propose a decentralized Markov game (MG) theoretic approach to estimate the belief of each possible enemy course of action (ECOA), which is utilized to model the adversary intents. It has the following advantages: (1) It is decentralized. Each cluster or team makes decisions mostly based on local information. We put more autonomies in each group allowing for more flexibilities; (2) A Markov decision process (MDP) can effectively model the uncertainties in the noisy military environment; (3) It is a game model with three players: red force (enemies), blue force (friendly forces), and white force (neutral objects); (4) Correlated-Q reinforcement learning is integrated. With the consideration that actual value functions are not normally known and they must be estimated, we integrate correlated-Q learning concept in our game approach to dynamically adjust the payoffs function of each player. A simulation software package has been developed to demonstrate the performance of our proposed algorithms. Simulations have verified that our proposed algorithms are scalable, stable, and satisfactory in performance.	https://dx.doi.org/10.1109/AERO.2007.352800	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Shen2016	A fast method to prevent traffic blockage by signal control based on reinforcement learning	In this paper, we present an efficient and fast way to prevent traffic blockage by controlling traffic signal. A new model is adopted to out program which is fused and developed by probabilistic model and cellular automatic model (CA model). Based on this model, we used wavelet neural network (WNN) for predicting traffic flow and use this data to improve the green or red light time sitting. Q-learning algorithm, as one of the reinforcement learning methods, also is applied to project the phase of traffic light from different intersections. Considering the requirement of model response rate, we also develop this algorithm for a fast respond to the complex and fickle traffic condition. Finally, a simulation study is carried out to evaluate out method and the result shows that this method can avoid traffic blockage efficiently.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Shen2021	Simulation-based Model Learning for Optimization of Building Energy Management	In this paper, we present a reinforcement learning based method for building energy management, which introduces a transition-model learning from building energy simulation software. The transition-model receives state from simulation model and returns cost-to-go to control strategy. The method is intended to combat two main gaps when integrating simulation program and optimization algorithm in building energy management, which are nonanalytical and nonlinear computation for simulation program and requirement of optimization algorithms for system dynamics. The method is realized on EnergyPlus, which interact with python program in functional mock-up interface (FMI) standard. The numerical test results show that the transition-model introduced can estimate cost-to-go to a certain extent and significantly improve efficiency of control.	https://dx.doi.org/10.1109/CASE49439.2021.9551522	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Shi2020	Reinforcement Learning Based Test Case Prioritization for Enhancing the Security of Software	In order to enhance the security of software, each system update needs to perform regression test. Regression testing in a continuous integration environment requires test cases to meet the needs of rapid feedback. Therefore, it is necessary to enable test cases to be effectively sorted within a certain time range so that more failure data could be discovered and the fault detection rate of testing could be improved. Reinforcement learning algorithms interact with the environment, so it is viable to optimize the sorting problem of test case in the process of continuous integration through a reward mechanism. In the development environment of continuous integration, it has been experimentally proven that the execution history of test cases in the last four cycles has a greater impact on the sorting of test cases in the current cycle. Therefore, a new RHE reward function was put forward by using part of weighted information obtained from historical execution result for enhancing the security of the system. Taking the influence of execution time into account, the multi-target sequencing technology for test case is employed with a view to improving the efficiency of defect discovery. It has been found by applying this sorting method to three industrial testing research that: (1) compared with weighted reward function based on the entire historical execution information, the function based on the four historical execution information had a higher capability of detecting faults; (2) The reward function obtained from the weighted historical results could effectively improve the fault detection rate and reduce the time consumed. (3) The multi-objective sorting methods taking execution time into consideration was able to maximize the number of testing cases that had already discovered faults within the available time.	https://dx.doi.org/10.1109/DSAA49011.2020.00076	Included	new_screen		4
RL4SE	Shi2021	Position Control and Production of Various Strategies for Game of Go Using Deep Learning Methods	Computer Go programs have surpassed top-level human players by using deep learning and reinforcement learning techniques. Other than the strength, entertaining Go AI and AI coaches are also interesting directions but have not been well investigated. Some researchers have worked on entertaining beginners or intermediate players. One topic is position control, aiming to make strong programs play close games against weak players. Under such a scenario, the naturalness of the moves is likely to influence weaker players' enjoyment. Another topic is producing various strategies (or preferences), which human players usually have. Some methods for the two topics have been proposed and evaluated for a traditional Monte-Carlo tree search (MCTS) program. However, there are some critical differences between traditional MCTS programs and recent programs based on AlphaGo Zero, such as LeelaZero and KataGo. For example, recent programs do not run random simulations to the ends of games in MCTS, making the existing method for producing various strategies not applicable. In this paper, we first summarize such differences and some resulted problems. We then adapt existing methods as well as propose new methods to solve the problems, where promising results are obtained. For position control, the modified LeelaZero can play gently against a weaker player (48\% of wins against a weaker program, Ray). A human subject experiment shows that the average number of unnatural moves per game is 1.22, while that by a simple method without considering naturalness is 2.29. We also propose a new position control method specifically for endgames. Finally, for producing various strategies, two methods are introduced. In our experiments, center- and edge/corner-oriented strategies are produced by both methods, and human players can successfully identify the strategies.	https://dx.doi.org/10.6688/JISE.202105_37(3).0004	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Shin2021	Deep Reinforcement Learning-Based Network Routing Technology for Data Recovery in Exa-Scale Cloud Distributed Clustering Systems	Research has been conducted to efficiently transfer blocks and reduce network costs when decoding and recovering data from an erasure coding-based distributed file system. Technologies using software-defined network (SDN) controllers can collect and more efficiently manage network data. However, the bandwidth depends dynamically on the number of data transmitted on the network, and the data transfer time is inefficient owing to the longer latency of existing routing paths when nodes and switches fail. We propose deep Q-network erasure coding (DQN-EC) to solve routing problems by converging erasure coding with DQN to learn dynamically changing network elements. Using the SDN controller, DQN-EC collects the status, number, and block size of nodes possessing stored blocks during erasure coding. The fat-tree network topology used for experimental evaluation collects elements of typical network packets, the bandwidth of the nodes and switches, and other information. The data collected undergo deep reinforcement learning to avoid node and switch failures and provide optimized routing paths by selecting switches that efficiently conduct block transfers. DQN-EC achieves a 2.5-times-faster block transmission time and 0.4-times-higher network throughput than open shortest path first (OSPF) routing algorithms. The bottleneck bandwidth and transmission link cost can be reduced, improving the recovery time approximately twofold.	https://dx.doi.org/10.3390/app11188727	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Shiraga2022	LibForce: C++ Library for Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139757035&doi=10.3233\%2fFAIA220294&partnerID=40&md5=2e454358042cb564bb057ae8e0816ce2	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Shiri2022	E2HRL: An Energy-efficient Hardware Accelerator for Hierarchical Deep Reinforcement Learning		https://doi.org/10.1145/3498327	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Shiri2022a	Efficient Language-Guided Reinforcement Learning for Resource-Constrained Autonomous Systems	In this article, we propose an energy-efficient architecture, which is designed to receive both images and text inputs as a step toward designing reinforcement learning agents that can understand human language and act in real-world environments. We evaluate our proposed method on three different software environments and a low power drone named Crazyflie to navigate toward specified goals and avoid obstacles successfully. To find the most efficient language-guided reinforcement learning model, we implemented the model with various configurations of image input sizes and text instruction sizes on the Crazyflie drone GAP8, which consists of eight reduced instruction set computer-V cores. The task completion success rate and onboard power consumption, latency, and memory usage of GAP8 are measured and compared with Jetson TX2 ARM central processing unit and Raspberry Pi 4. The results show that by decreasing 20\% of input image size we achieve up to 78\% energy improvement while achieving an 82\% task completion success rate.	https://dx.doi.org/10.1109/MM.2022.3199686	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Shobha2018	Machine Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051670389&doi=10.1016\%2fbs.host.2018.07.004&partnerID=40&md5=6f7f2011b8e71103d4333f344213645f	Excluded	new_screen	E5: Other not a paper,E5: Other not a paper	4
RL4SE	Shu2012	Transit signal priority strategy based on reinforcement learning algorithm			Excluded	new_screen	E5: Other not a paper,E2: Software engineering is not the problem RL is used for	4
RL4SE	Shukla2020	Flight test validation of a safety-critical neural network based longitudinal controller for a fixed-wing uas		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092604971&doi=10.2514\%2f6.2020-3093&partnerID=40&md5=8e1c406be32471b64874d6cd955e7ce1	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Shumsky2021	Reverse Engineering the Brain Based on Machine Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093095004&doi=10.1007\%2f978-3-030-60577-3_1&partnerID=40&md5=a3b863ec527ce266c8ad3e42df47fe0d	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Si2001	Learning programs for decision and control		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964501368&doi=10.1109\%2fICII.2001.983100&partnerID=40&md5=ad944de3eefdf6760792c077230b71d0	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Si2018	Learning loop invariants for program verification			Included	new_screen		4
RL4SE	Si2019	Learning a meta-solver for syntax-guided program synthesis			Included	new_screen		4
RL4SE	Sibmeunpiam2021	Q-Learning traffic-distributing approach to managing multiple-destination traffic		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112852772&doi=10.1109\%2fECTI-CON51831.2021.9454892&partnerID=40&md5=428b6a6c862944bdce850651ab0254f4	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Siegel2021	Morals, ethics, and the technology capabilities and limitations of automated and self-driving vehicles	"We motivate the desire for self-driving and explain its potential and limitations, and explore the need for-and potential implementation of-morals, ethics, and other value systems as complementary ""capabilities"" to the Deep Technologies behind self-driving. We consider how the incorporation of such systems may drive or slow adoption of high automation within vehicles. First, we explore the role for morals, ethics, and other value systems in self-driving through a representative hypothetical dilemma faced by a self-driving car. Through the lens of engineering, we explain in simple terms common moral and ethical frameworks including utilitarianism, deontology, and virtue ethics before characterizing their relationship to the fundamental algorithms enabling self-driving. The concepts of behavior cloning, state-based modeling, and reinforcement learning are introduced, with some algorithms being more suitable for the implementation of value systems than others. We touch upon the contemporary cross-disciplinary landscape of morals and ethics in self-driving systems from a joint philosophical and technical perspective, and close with considerations for practitioners and the public, particularly as individuals may not appreciate the nuance and complexity of using imperfect information to navigate diverse scenarios and tough-to-quantify value systems, while ""typical"" software development reduces complex problems to black and white decision-making."	https://dx.doi.org/10.1007/s00146-021-01277-y	Excluded	new_screen	E3: Only conceptual results are reported,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sihananto2022	Reinforcement Learning for Automatic Cryptocurrency Trading	Reinforcement learning as machine learning algorithms can construct software agents, and machines work automatically to determine the superior manners to maximize the algorithm. On the other hand, in recent years, cryptocurrencies are increasingly known because numerous people have used them for investment and trading. People have created automated cryptocurrency trading systems to save time in trading activities. Therefore, researchers are interested in inventing a computerized trading system by implementing a reinforcement learning algorithm. This implementation uses a stable baseline and OpenAi gym with three methods of RNN, such as A2C, ACER, and PPO. The result is that A2C is the best method for low-volume trade like BTC/USDT, while for higher-volume trade in ETH/USDT, the ACER method proved more beneficial. The best BTC/USDT trading method is A2C, with a reward in the testing phase of 0.332. Meanwhile, for ETH/USDT, the best approach is ACER, with the testing phase's reward is 0.257.	https://dx.doi.org/10.1109/ITIS57155.2022.10010206	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Silva2020	Q-Learning Applied to Soft-Kill Countermeasures For Unmanned Aerial Vehicles (UAVs)	This work presents a three-dimensional control algorithm using reinforcement learning to guide an attacking hunter drone capable of performing a global navigation satellite systems (GNSS) repeater attack on the GNSS receiver of a target invader drone. Considering the mission and movement requirements of the hunter drone, a Q-learning algorithm was developed, for which the table with the possible transitions of the states and actions is obtained by the actions that the vehicle can take considering directions and the respective consequences of each action. The learning capability of the proposed algorithm arises from the trial and error by an agent. The penalty calculation is based on the error of the invader position to the hunter's desired position of the attacked drone. The developed algorithm is tested using a software-in-the-loop (SITL) implementation, which is based on the Ardupilot platform. SITL simulations are performed in a developed testbed to emulate operational scenarios, where an unmanned aerial vehicle (UAV) is hijacked and then controlled by an attacking UAV until it reaches the final position desired by the hunter, usually a secure area where the vehicle can be captured without being destroyed. Results, including error metrics and action time, are discussed for different mission scenarios.	https://dx.doi.org/10.1109/PLANS46316.2020.9110222	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Silva2019	A survey of adaptive resonance theory neural network models for engineering applications	This survey samples from the ever-growing family of adaptive resonance theory (ART) neural network models used to perform the three primary machine learning modalities, namely, unsupervised, supervised and reinforcement learning. It comprises a representative list from classic to contemporary ART models, thereby painting a general picture of the architectures developed by researchers over the past 30 years. The learning dynamics of these ART models are briefly described, and their distinctive characteristics such as code representation, long-term memory, and corresponding geometric interpretation are discussed. Useful engineering properties of ART (speed, configurability, explainability, parallelization and hardware implementation) are examined along with current challenges. Finally, a compilation of online software libraries is provided. It is expected that this overview will be helpful to new and seasoned ART researchers.	https://doi.org/10.1016/j.neunet.2019.09.012	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Silvander2019	Business Process Optimization with Reinforcement Learning	We investigate the use of deep reinforcement learning to optimize business processes in a business support system. The focus of this paper is to investigate how a reinforcement learning algorithm named Q-Learning, using deep learning, can be configured in order to support optimization of business processes in an environment which includes some degree of uncertainty. We make the investigation possible by implementing a software agent with the help of a deep learning tool set. The study shows that reinforcement learning is a useful technique for business process optimization but more guidance regarding parameter setting is needed in this area.	https://dx.doi.org/10.1007/978-3-030-24854-3_13	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Silver2016	Mastering the game of Go with deep neural networks and tree search	The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.	https://www.ncbi.nlm.nih.gov/pubmed/26819042	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Silver2018	A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play	The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.	https://www.ncbi.nlm.nih.gov/pubmed/30523106	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Silver2017	Mastering the game of Go without human knowledge	A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo.	https://www.ncbi.nlm.nih.gov/pubmed/29052630	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Silver2007	Reinforcement Learning of Local Shape in the Game of Go	We explore an application to the game of Go of a reinforcement learning approach based on a linear evaluation function and large numbers of binary features. This strategy has proved effective in game playing programs and other reinforcement learning applications. We apply this strategy to Go by creating over a million features based on templates for small fragments of the board, and then use temporal difference learning and self-play. This method identifies hundreds of low level shapes with recognisable significance to expert Go players, and provides quantitive estimates of their values. We analyse the relative contributions to performance of templates of different types and sizes. Our results show that small, translation-invariant templates are surprisingly effective. We assess the performance of our program by playing against the Average Liberty Player and a variety of computer opponents on the 9x9 Computer Go Server. Our linear evaluation function appears to outperform all other static evaluation functions that do not incorporate substantial domain knowledge.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Silver2012	Temporal-difference search in computer Go	Temporal-difference learning is one of the most successful and broadly applied solutions to the reinforcement learning problem; it has been used to achieve master-level play in chess, checkers and backgammon. The key idea is to update a value function from episodes of real experience, by bootstrapping from future value estimates, and using value function approximation to generalise between related states. Monte-Carlo tree search is a recent algorithm for high-performance search, which has been used to achieve master-level play in Go. The key idea is to use the mean outcome of simulated episodes of experience to evaluate each state in a search tree. We introduce a new approach to high-performance search in Markov decision processes and two-player games. Our method, temporal-difference search, combines temporal-difference learning with simulation-based search. Like Monte-Carlo tree search, the value function is updated from simulated experience; but like temporal-difference learning, it uses value function approximation and bootstrapping to efficiently generalise between related states. We apply temporal-difference search to the game of 9x9 Go, using a million binary features matching simple patterns of stones. Without any explicit search tree, our approach outperformed an unenhanced Monte-Carlo tree search with the same number of simulations. When combined with a simple alpha-beta search, our program also outperformed all traditional (pre-Monte-Carlo) search and machine learning programs on the 9x9 Computer Go Server.	https://dx.doi.org/10.1007/s10994-012-5280-0	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Silver2008	Sample-based learning and search with permanent and transient memories		https://www.scopus.com/inward/record.uri?eid=2-s2.0-56449110907&doi=10.1145\%2f1390156.1390278&partnerID=40&md5=faff76cf3010047a658f9d354b692c4a	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Silvestro2022	Improving biodiversity protection through artificial intelligence	Over a million species face extinction, highlighting the urgent need for conservation policies that maximize the protection of biodiversity to sustain its manifold contributions to people's lives. Here we present a novel framework for spatial conservation prioritization based on reinforcement learning that consistently outperforms available state-of-the-art software using simulated and empirical data. Our methodology, conservation area prioritization through artificial intelligence (CAPTAIN), quantifies the trade-off between the costs and benefits of area and biodiversity protection, allowing the exploration of multiple biodiversity metrics. Under a limited budget, our model protects significantly more species from extinction than areas selected randomly or naively (such as based on species richness). CAPTAIN achieves substantially better solutions with empirical data than alternative software, meeting conservation targets more reliably and generating more interpretable prioritization maps. Regular biodiversity monitoring, even with a degree of inaccuracy characteristic of citizen science surveys, further improves biodiversity outcomes. Artificial intelligence holds great promise for improving the conservation and sustainable use of biological and ecosystem values in a rapidly changing and resource-limited world. Artificial intelligence methods can help biodiversity conservation planning in a rapidly evolving world. A framework based on reinforcement learning quantifies the trade-off between the costs and benefits of area and biodiversity protection and achieves better solutions with empirical data than alternative methods.	https://www.ncbi.nlm.nih.gov/pubmed/35614933	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Simoes2022	BahiaRT Setplays Collecting Toolkit and BahiaRT Gym	One of the challenges in the research using machine learning is the requirement to build realistic datasets using common sense knowledge from domain experts. The BahiaRT Setplays Collecting Toolkit is a software that supports research where soccer fans or experts can watch robots playing soccer and capture situations where they want to demonstrate a better setplay to that robot team. All demonstrations were gathered in a dataset used to feed a reinforcement learning mechanism that produced a control policy for setplays selection in a robot soccer team. This software impacts many research areas such as autonomous vehicles, unmanned aerial vehicles, dataset organization, reinforcement learning, and learning from demonstration. The BahiaRT Gym is an open-source tool designed to integrate the OpenAI Gym toolkit with the RoboCup Soccer Simulation 3D server(rcssserver3d), in order to make available an easier way to create training environments for the soccer teams while also facilitating the development of other RoboCup leagues's environments as well.	https://dx.doi.org/10.1016/j.simpa.2022.100401	Excluded	new_screen	E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for	4
RL4SE	Simsek2004	Reinforcement learning for procurement agents of factory of the future	Factory of the future is emerging with the existence of new modeling and application tools that can both simulate and manage the whole production process in an autonomous, intelligent and interactive manner. Holonic modeling and its software correspondence agent oriented technology provides us with these tools. Especially the use of learning algorithms trying to optimize the behaviors of software agents within a dynamic environment is the key factor in reaching the required properties. In this paper, we use the well known Q learning algorithm of reinforcement learning (RL) in evaluating production orders within a supply chain management (SCM) framework and making decisions with respect to these evaluations. We introduce our SCM model and show that RL performs better than traditional tools for dynamic problem solving in daily business. We also show cases where RL fails to perform efficiently.	https://dx.doi.org/10.1109/CEC.2004.1331051	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Simsek2004a	Reinforcement learning for procurement agents of the factory of the future	Factory of the future is emerging with the existence of new modeling and application tools that can both simulate and manage the whole production process in an autonomous, intelligent and interactive manner. Holonic modeling and its software correspondence agent oriented technology provides us with these tools. Especially the use of learning algorithms trying to optimize the behaviors of software agents within a dynamic environment is the key factor in reaching the required properties. In this paper, we use the well known Q learning algorithm of reinforcement learning (RL) in evaluating production orders within a supply chain management (SCM) framework and making decisions with respect to these evaluations. We introduce our SCM model and show that RL performs better than traditional tools for dynamic problem solving in daily business. We also show cases where RL fails to perform efficiently.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Singer2001	Co-evolving a neural-net evaluation function for Othello by combining genetic algorithms and reinforcement learning	The neural network has been used extensively as a vehicle for both genetic algorithms and reinforcement learning. This paper shows a natural way to combine the two methods and suggests that reinforcement learning may be superior to random mutation as an engine for the discovery of useful substructures. The paper also describes a software experiment that applies this technique to produce an Othello-playing computer program. The experiment subjects a pool of Othello-playing programs to a regime of successive adaptation cycles, where each cycle consists of an evolutionary phase, based on the genetic algorithm, followed by a learning phase, based on reinforcement learning. A key idea of the genetic implementation is the concept of feature-level crossover. The regime was run for three months through 900,000 individual matches of Othello. It ultimately yielded a program that is competitive with a human-designed Othello-program that plays at roughly intermediate level.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Singh2013	Learning to improve agent behaviours in GOAL		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883265063&doi=10.1007\%2f978-3-642-38700-5_10&partnerID=40&md5=e1d21fdc18f061f83134c44f2b1f7c2f	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Singh2018	Fast Numerical Program Analysis with Reinforcement Learning	We show how to leverage reinforcement learning (RL) in order to speed up static program analysis. The key insight is to establish a correspondence between concepts in RL and those in analysis: a state in RL maps to an abstract program state in analysis, an action maps to an abstract transformer, and at every state, we have a set of sound transformers (actions) that represent different trade-offs between precision and performance. At each iteration, the agent (analysis) uses a policy learned offline by RL to decide on the transformer which minimizes loss of precision at fixpoint while improving analysis performance. Our approach leverages the idea of online decomposition (applicable to popular numerical abstract domains) to define a space of new approximate transformers with varying degrees of precision and performance. Using a suitably designed set of features that capture key properties of abstract program states and available actions, we then apply Q-learning with linear function approximation to compute an optimized context-sensitive policy that chooses transformers during analysis. We implemented our approach for the notoriously expensive Polyhedra domain and evaluated it on a set of Linux device drivers that are expensive to analyze. The results show that our approach can yield massive speedups of up to two orders of magnitude while maintaining precision at fixpoint.	https://dx.doi.org/10.1007/978-3-319-96145-3_12	Included	new_screen		4
RL4SE	Sinha2021	DEMAND RESPONSE OPTIMIZATION FOR MIRCOGRID CLUSTERS WITH DEEP REINFORCEMENT LEARNING		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126230016&doi=10.1109\%2fICCCNT51525.2021.9579612&partnerID=40&md5=198e1cd24156c8d196c678525e639f03	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sirisha2023	Using a Software-Defined Air Interface Algorithm to Improve Service Quality	In the digital era, the Narrowband Internet of Things (Nb-IoT) influ-ences the massive Machine-Type-Communication (mMTC) features to establish secure routing among the 5G/6G mobile networks. It supports global coverage to the low-cost IoT devices distributed in terrestrial networks. Its key traffic char-acteristics include robust uplink, moderate data rate/device, extremely high energy efficiency, prolonging device lifetime, and Quality of Service (QoS). This paper proposes a Deep Reinforcement Learning (DRL) combined software-defined air interface algorithm applied on the switching system, satisfying the user require-ment and enabling them with the network resources to extend quality of service by choosing the most appropriate quality of service metric. In this framework, Non-Orthogonal Multiple Accesses (NOMA) and Rate-Splitting Multiple Access (RSMA) are combined to accommodate massive (Nb-IoT) devices that can be uti-lized the entire resource (frequency band) for tackling the unknown dynamics pro-hibitive. The proposed algorithm instantly assigns the network resources per user requirements and enhances selecting the best quality of service metric optimiza-tion. Therefore, it has potential benefits of high scalability, low latency, energy efficiency, and spectrum utility.	https://dx.doi.org/10.32604/iasc.2023.025980	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Skirzy?ski2021	Automatic discovery of interpretable planning strategies		https://doi.org/10.1007/s10994-021-05963-2	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Slimeni2019	Cooperative Q-learning based channel selection for cognitive radio networks		https://doi.org/10.1007/s11276-018-1737-9	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Smith2020	Hazard contribution modes of machine learning components			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Smith2017	Coevolving deep hierarchies of programs to solve complex tasks		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026415765&doi=10.1145\%2f3071178.3071316&partnerID=40&md5=56ab56ba2dc148e2e8b09422bff838c2	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Smith2018	Scaling Tangled Program Graphs to Visual Reinforcement Learning in ViZDoom	A tangled program graph framework (TPG) was recently proposed as an emergent process for decomposing tasks and simultaneously composing solutions by organizing code into graphs of teams of programs. The initial evaluation assessed the ability of TPG to discover agents capable of playing Atari game titles under the Arcade Learning Environment. This is an example of 'visual' reinforcement learning, i.e. agents are evolved directly from the frame buffer without recourse to hand designed features. TPG was able to evolve solutions competitive with state-of-the-art deep reinforcement learning solutions, but at a fraction of the complexity. One simplification assumed was that the visual input could be down sampled from a 210x 160 resolution to 42x 32. In this work, we consider the challenging 3D first person shooter environment of ViZDoom and require that agents be evolved at the original visual resolution of 320 x 240 pixels. In addition, we address issues for developing agents capable of operating in multi-task ViZDoom environments simultaneously. The resulting TPG solutions retain all the emergent properties of the original work as well as the computational efficiency. Moreover, solutions appear to generalize across multiple task scenarios, whereas equivalent solutions from deep reinforcement learning have focused on single task scenarios alone.	https://dx.doi.org/10.1007/978-3-319-77553-1_9	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Smith2019	A Model of External Memory for Navigation in Partially Observable Visual Reinforcement Learning Tasks		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064887950&doi=10.1007\%2f978-3-030-16670-0_11&partnerID=40&md5=c9ae92028d088a47b845bc403926fa3c	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Smith2016	Discovering Rubik's cube subgroups using coevolutionary GP - A five twist experiment		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986000804&doi=10.1145\%2f2908812.2908887&partnerID=40&md5=673519024828b4661d1b1356797f3375	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Soleyman2020	Multi-agent mission planning with reinforcement learning			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Solovyeva2020	Controlling system based on neural networks with reinforcement learning for robotic manipulator		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096066032&doi=10.31799\%2f1684-8853-2020-5-24-32&partnerID=40&md5=a71d8a763d2c5e02d6bb9a3f4913e303	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Solozabal2020	Virtual Network Function Placement Optimization With Deep Reinforcement Learning	Network Function Virtualization (NFV) introduces a new network architecture framework that evolves network functions, traditionally deployed over dedicated equipment, to software implementations that run on general-purpose hardware. One of the main challenges for deploying NFV is the optimal resource placement of demanded network services in the NFV infrastructure. The virtual network function placement and network embedding can be formulated as a mathematical optimization problem concerned with a set of feasibility constraints that express the restrictions of the network infrastructure and the services contracted. This problem has been reported to be NP-hard, as a result most of the optimization work carried out in the area has focused on designing heuristic and metaheuristic algorithms. Nevertheless, in highly constrained problems, as in this case, inferring a competitive heuristic can be a daunting task that requires expertise. Consequently, an interesting solution is the use of Reinforcement Learning to model an optimization policy. The work presented here extends the Neural Combinatorial Optimization theory by considering constraints in the definition of the problem. The resulting agent is able to learn placement decisions by exploring the NFV infrastructure with the aim of minimizing the overall power consumption. The experiments conducted demonstrate that when the proposed strategy is also combined with heuristics, highly competitive results are achieved using relatively simple algorithms.	https://dx.doi.org/10.1109/JSAC.2019.2959183	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sombolestan2019	Optimal path-planning for mobile robots to find a hidden target in an unknown environment based on machine learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049573978&doi=10.1007\%2fs12652-018-0777-4&partnerID=40&md5=936ebf3780ba76b54b521adab573310e	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Song2007	The Multi-Agent Data Collection in HLA-Based Simulation System	The high level architecture (HLA) for distributed simulation was proposed by the Defense Modeling and Simulation Office of the Department of Defense (DOD) in order to support interoperability among simulations as well as reuse of simulation models. One aspect of reusability is to collect and analyze data generated in simulation exercises, including a record of events that occur during the execution, and the states of simulation objects. In order to improve the performance of existing data collection mechanisms in the HLA simulation system, the paper proposes a multi-agent data collection system. The proposed approach adopts the hierarchical data management/organization mechanism to achieve fast data access which is indispensable to the analysis of simulation exercise. Furthermore, the multi-agent data collection system adopts a formalization expression method to describe the system behavioral characteristics, and implements the hierarchy language supports to the description by combing the XML and Petri net. In addition, we propose an independent reinforcement learning algorithm to generate optimized joint recording program which guarantees that the data collection and query tasks can be rationally distributed among logging agents as well as efficiently utilize computational resource. The testing results indicate that the proposed approach, under the premise of complete collection of simulation data, not only reduces the network load imposed by data collection components, but also provides effective supports to the analysis of simulation exercise.	https://dx.doi.org/10.1109/PADS.2007.30	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Song2020	A general large neighborhood search framework for solving integer linear programs			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Song2021	Deep reinforcement learning for modeling human locomotion control in neuromechanical simulation	"Modeling human motor control and predicting how humans will move in novel environments is a grand scientific challenge. Researchers in the fields of biomechanics and motor control have proposed and evaluated motor control models via neuromechanical simulations, which produce physically correct motions of a musculoskeletal model. Typically, researchers have developed control models that encode physiologically plausible motor control hypotheses and compared the resulting simulation behaviors to measurable human motion data. While such plausible control models were able to simulate and explain many basic locomotion behaviors (e.g. walking, running, and climbing stairs), modeling higher layer controls (e.g. processing environment cues, planning long-term motion strategies, and coordinating basic motor skills to navigate in dynamic and complex environments) remains a challenge. Recent advances in deep reinforcement learning lay a foundation for modeling these complex control processes and controlling a diverse repertoire of human movement; however, reinforcement learning has been rarely applied in neuromechanical simulation to model human control. In this paper, we review the current state of neuromechanical simulations, along with the fundamentals of reinforcement learning, as it applies to human locomotion. We also present a scientific competition and accompanying software platform, which we have organized to accelerate the use of reinforcement learning in neuromechanical simulations. This ""Learn to Move"" competition was an official competition at the NeurIPS conference from 2017 to 2019 and attracted over 1300 teams from around the world. Top teams adapted state-of-the-art deep reinforcement learning techniques and produced motions, such as quick turning and walk-to-stand transitions, that have not been demonstrated before in neuromechanical simulations without utilizing reference motion data. We close with a discussion of future opportunities at the intersection of human movement simulation and reinforcement learning and our plans to extend the Learn to Move competition to further facilitate interdisciplinary collaboration in modeling human motor control for biomechanics and rehabilitation research"	https://www.ncbi.nlm.nih.gov/pubmed/34399772	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Song2021a	Dancing along Battery: Enabling Transformer with Run-time Reconfigurability on Mobile Devices	A pruning-based AutoML framework for run-time reconfigurability, namely RT3, is proposed in this work. This enables Transformer-based large Natural Language Processing (NLP) models to be efficiently executed on resource-constrained mobile devices and reconfigured (i.e., switching models for dynamic hardware conditions) at run-time. Such reconfigurability is the key to save energy for battery-powered mobile devices, which widely use dynamic voltage and frequency scaling (DVFS) technique for hardware reconfiguration to prolong battery life. In this work, we creatively explore a hybrid block-structured pruning (BP) and pattern pruning (PP) for Transformer-based models and first attempt to combine hardware and software reconfiguration to maximally save energy for battery-powered mobile devices. Specifically, RT3 integrates two-level optimizations: First, it utilizes an efficient BP as the first-step compression for resource-constrained mobile devices; then, RT3 heuristically generates a shrunken search space based on the first level optimization and searches multiple pattern sets with diverse sparsity for PP via reinforcement learning to support lightweight software reconfiguration, which corresponds to available frequency levels of DVFS (i.e., hardware reconfiguration). At run-time, RT3 can switch the lightweight pattern sets within 45ms to guarantee the required real-time constraint at different frequency levels. Results further show that RT3 can prolong battery life over $ 4\times$ improvement with less than 1\% accuracy loss for Transformer and 1.5\% score decrease for DistilBERT.	https://dx.doi.org/10.1109/DAC18074.2021.9586295	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Song2021b	The optimization and improvement of bridge game system	This article from the background of the bridge, improving the system analysis, experimental test three perspective, based on the idea of reinforcement learning, from two aspects of contract and scoring ability, through a large number of calculations and the original program and every time a new system of winning IMP value level to achieve the agreed order, accumulate experience, design a set of good rewards and punishment mechanism, greatly improve the efficiency of the bridge to play CARDS and intelligence, thus the bridge game system was optimized and improved. In addition, the traditional methods need to manually extract the features of poor expansibility, this paper combined with reinforcement learning algorithm, the ideas of game devised a new system, under the condition of different effective play the computer, the program has also reached a higher level of the game structure design, for the incomplete information game theory provides a reasonable method, application creates opportunities to people living in the future.	https://dx.doi.org/10.1109/CCDC52312.2021.9602528	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Soni2019	Motion Planning using Reinforcement Learning for Electric Vehicle Battery optimization(EVBO)	The increasing demand for electric vehicle and autonomous vehicle as the alternate to the combustion-driven vehicle has motivated the research in the area of motion planning. Motion planmng is a complicated problem as it requires the consideration of multiple entities, mainly human behaviour. In this paper, reinforcement learning techniques are explored for the motion planning of an electnc vehicle(EV) while optimizing battery consumption. The EV travel time has also been evaluated under different reinforcement learning schemes. A traffic simulation network is developed for a high-traffic zone of Jaipur city using Simulation for Urban Mobility(SUMO) software. Model-based and model-free method like value-iteration and q-learning are applied to the developed traffic network. The results show that value iteration and q-learning have shown improved battery consumption. However, value iteration gives greater efficiency in terms of travel time as well as battery consumption.	https://dx.doi.org/10.1109/ICPECA47973.2019.8975684	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Soto2021	Towards Autonomous VNF Auto-scaling using Deep Reinforcement Learning	Network Function Virtualization (NFV) is one of the main enablers behind the promised improvements in the Fifth Generation (5G) networking era. Thanks to this concept, Network Functions (NFs) are evolving into software components (e.g., Vir-tual Network Functions (VNFs)) that can be deployed in general-purpose servers following a cloud-based approach. In this way, NFs can be deployed at scale, fulfilling a great variety of service requirements. Unfortunately, the complexity in the management and orchestration of NFV-based networks has increased due to the diverse demands from a growing number of network services. Such complexity calls for an automated and autonomous solution that self adapts to the needs of those network services. In this paper, we propose and compare a Deep Reinforcement Learning (DRL) agent, a classical Proportional-Integral-Derivative (PID) controller, and a Threshold (THD)-based algorithm for autonomously determining the amount of VNF instances to fulfill a service latency requirement without knowing or predicting the expected demand. Finally, we present a comparison of the three approaches in terms of created VNFs and peak latency performed in a discrete event simulator.	https://dx.doi.org/10.1109/SDS54264.2021.9731854	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sowmya2022	DAMIAN - data accrual machine intelligence with augmented networks for contextually coherent creative story generation		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144464061&doi=10.1504\%2fijbidm.2023.127314&partnerID=40&md5=5d2a4bdcbf43198a7c2fbed4ba20981a	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Spieker2020	Adaptive metamorphic testing with contextual bandits	Metamorphic Testing is a software testing paradigm which aims at using necessary properties of a system under test, called metamorphic relations, to either check its expected outputs, or to generate new test cases. Metamorphic Testing has been successful to test programs for which a full oracle is not available or to test programs for which there are uncertainties on expected outputs such as learning systems. In this article, we propose Adaptive Metamorphic Testing as a generalization of a simple yet powerful reinforcement learning technique, namely contextual bandits, to select one of the multiple metamorphic relations available for a program. By using contextual bandits, Adaptive Metamorphic Testing learns which metamorphic relations are likely to transform a source test case, such that it has higher chance to discover faults. We present experimental results over two major case studies in machine learning, namely image classification and object detection, and identify weaknesses and robustness boundaries. Adaptive Metamorphic Testing efficiently identifies weaknesses of the tested systems in context of the source test case. (C) 2020 Elsevier Inc. All rights reserved.	https://dx.doi.org/10.1016/j.jss.2020.110574	Included	new_screen		4
RL4SE	Spieker2020a	Learning to Generate Fault-revealing Test Cases in Metamorphic Testing		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126621869&doi=10.18420\%2fSE2021_37&partnerID=40&md5=72f4a5b61f17d8cf1033d0e70c7ce5f4	Excluded	new_screen	E5: Other not a paper,E5: Other not a paper	4
RL4SE	Sridhar2004	Discrete event modeling and simulation: V-Lab/spl reg/ - application to wireless sensor networks	The need for modeling and simulation (M&S) is seen in many diverse applications such as multi-agent systems, robotics, control systems, software engineering, complex adaptive systems, and homeland security. With the emerging applications of multi-agent systems, there is always a need for simulation to verify the results before the actual implementation. Multi-agent simulation provides a test bed for several soft computing algorithms like fuzzy logic, neural networks (NN), probabilistic reasoning (stochastic learning automata, reinforcement learning), and evolutionary algorithms (genetic algorithms). Fusion of soft computing methodology with existing simulation tools yields several advantages in simulating multi-agent systems. Such a fusion provides a novel and systematic way of handling tune-dependent parameters in the simulation without altering the essential functionality and problem solving capabilities of soft computing elements. The fusion here is the extension of the capabilities of simulation tools with intelligent tools from soft computing. This paper proposes a methodology for combining the agent-based architecture, discrete event system and the soft-computing methods in the simulation of multi-agent systems and defines a framework called virtual laboratory (V-Lab/spl reg/) for realizing such multi-agent system simulations. Detailed experimental results obtained from simulation of robotics agents and wireless sensor network is also discussed.	https://dx.doi.org/10.1109/ICSMC.2004.1399879	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Sridhar2004a	Discrete event modeling and simulation: V-Labtextregistered application to wireless sensor networks			Excluded	new_screen	E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sridhar2004b	A framework for multi-agent discrete event simulation: V-Lab (R)	The need for modeling and simulation (M&S) is seen in many diverse applications such as multi-agent systems, robotics, control systems, software engineering, complex adaptive systems, and homeland security. With the emerging applications of multi-agent systems, there is always a need for simulation to verify the results before the actual implementation. Multi-agent simulation provides a test bed for several soft computing algorithms like fuzzy logic, neural networks (NN), probabilistic reasoning (Stochastic Learning Automata, Reinforcement learning), and evolutionary algorithms (Genetic Algorithms). Fusion of soft computing methodology with existing simulation tools yields several advantages in simulating multi-agent systems. Such a fusion provides a novel and systematic way of handling time-dependent parameters in the simulation without altering the essential functionality and probiem-solving capabilities of soft computing elements. The fusion here is the extension of the capabilities of simulation tools with intelligent tools from soft computing. This paper proposes a methodology for combining the agent-based architecture, discrete event system and the soft-computing methods in the simulation of multi-agent systems and defines a framework called Virtual Laboratory (V-Lab (R)) for realizing such multi-agent system simulations.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sridhar2004c	A framework for multi-agent discrete event simulation: V-Labtextregistered			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sridhar2004d	Discrete event Modeling and simulation: V-Lab (R) Application to wireless sensor networks	The need for modeling and simulation (M&S) is seen in many diverse applications such as multi-agent systems, robotics, control systems, software engineering, complex adaptive systems, and homeland security. With the emerging applications of multi-agent systems, there is always a need for simulation to verify the results before the actual implementation. Multi-agent simulation provides a test bed for several soft computing algorithms like fuzzy logic, neural networks (NN), probabilistic reasoning (Stochastic Learning Automata, Reinforcement learning), and evolutionary algorithms (Genetic Algorithms). Fusion of soft computing methodology with existing simulation tools yields several advantages in simulating multi-agent systems. Such a fusion provides a novel and systematic way of handling time-dependent parameters in the simulation without altering the essential functionality and problem-solving capabilities of soft computing elements. The fusion here is the extension of the capabilities of simulation tools with intelligent tools from soft computing. This paper proposes a methodology for combining the agent-based architecture, discrete event system and the soft-computing methods in the simulation of multi-agent systems and defines a framework called Virtual Laboratory (V-Lab(R)) for realizing such multi-agent system simulations. Detailed experimental results obtained from simulation of robotics agents and wireless sensor network is also discussed.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sridharan2016	Towards an architecture for representation, reasoning, and learning in human-robot collaboration			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sridharan2016a	Can I Do That? Discovering Domain Axioms Using Declarative Programming and Relational Reinforcement Learning	Robots deployed to assist humans in complex, dynamic domains need the ability to represent, reason with, and learn from, different descriptions of incomplete domain knowledge and uncertainty. This paper presents an architecture that integrates declarative programming and relational reinforcement learning to support cumulative and interactive discovery of previously unknown axioms governing domain dynamics. Specifically, Answer Set Prolog (ASP), a declarative programming paradigm, is used to represent and reason with incomplete commonsense domain knowledge. For any given goal, unexplained failure of plans created by inference in the ASP program is taken to indicate the existence of unknown domain axioms. The task of learning these axioms is formulated as a Reinforcement Learning problem, and decision-tree regression with a relational representation is used to generalize from specific axioms identified over time. The new axioms are added to the ASP-based representation for subsequent inference. We demonstrate and evaluate the capabilities of our architecture in two simulated domains: Blocks World and Simple Mario.	https://dx.doi.org/10.1007/978-3-319-46840-2_3	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sridharan2017	Learning Affordances for Assistive Robots	This paper describes an architecture that enables a robot to represent, reason about, and learn affordances. Specifically, Answer Set Prolog is used to represent and reason with incomplete domain knowledge that includes affordances modeled as relations between attributes of the robot and the object(s) in the context of specific actions. The learning of affordance relations from observations obtained through reactive execution or active exploration is formulated as a reinforcement learning problem. A sampling-based approach and decision-tree regression with the underlying relational representation are used to obtain generic affordance relations that are added to the Answer Set Prolog program for subsequent reasoning. The capabilities of this architecture are illustrated and evaluated in the context of a simulated robot assisting humans in an indoor domain.	https://dx.doi.org/10.1007/978-3-319-70022-9_1	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sridharan2014	Integrating Reinforcement Learning and Declarative Programming to Learn Causal Laws in Dynamic Domains	Robots deployed to assist and collaborate with humans in complex domains need the ability to represent and reason with incomplete domain knowledge, and to learn from minimal feedback obtained from non-expert human participants. This paper presents an architecture that combines the complementary strengths of Reinforcement Learning (RL) and declarative programming to support such commonsense reasoning and incremental learning of the rules governing the domain dynamics. Answer Set Prolog (ASP), a declarative language, is used to represent domain knowledge. The robot's current beliefs, obtained by inference in the ASP program, are used to formulate the task of learning previously unknown domain rules as an RL problem. The learned rules are, in turn, encoded in the ASP program and used to plan action sequences for subsequent tasks. The architecture is illustrated and evaluated in the context of a simulated robot that plans action sequences to arrange tabletop objects in desired configurations.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sritharan2020	A Study on Deep Learning for Latency Constraint Applications in Beyond 5G Wireless Systems	The fifth generation (5G) of wireless communications has led to many advancements in technologies such as large and distributed antenna arrays, ultra-dense networks, software-based networks and network virtualization. However, the need for a higher level of automation to establish hyper-low latency, and hyper-high reliability for beyond 5G applications requires extensive research on machine learning with applications in wireless communications. Thereby, learning techniques will take a central stage in the sixth generation of wireless communications to cope up with the stringent application requirements. This paper studies the practical limitations of these learning methods in the context of resource management in a non-stationary radio environment. Based on the practical limitations we carefully design and propose supervised, unsupervised, and reinforcement learning models to support rate maximization objective under user mobility. We study the effects of practical systems such as latency and reliability on the rate maximization with deep learning models. For common testing in the non-stationary environment, we present a generic dataset generation method to benchmark across different learning models versus traditional optimal resource management solutions. Our results indicate that learning models have practical challenges related to training limiting their applications. These models need an environment-specific design to reach the accuracy of an optimal algorithm. Such an approach is practically not realistic due to the high resource requirement needed for frequent retraining.	https://dx.doi.org/10.1109/ACCESS.2020.3040133	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Staahl2019	Deep Reinforcement Learning for Multiparameter Optimization in de novo Drug Design	In medicinal chemistry programs it is key to design and make compounds that are efficacious and safe. This is a long, complex, and difficult multiparameter optimization process, often including several properties with orthogonal trends. New methods for the automated design of compounds against profiles of multiple properties are thus of great value. Here we present a fragment-based reinforcement learning approach based on an actor-critic model, for the generation of novel molecules with optimal properties. The actor and the critic are both modeled with bidirectional long short-term memory (LSTM) networks. The AI method learns how to generate new compounds with desired properties by starting from an initial set of lead molecules and then improving these by replacing some of their fragments. A balanced binary tree based on the similarity of fragments is used in the generative process to bias the output toward structurally similar molecules. The method is demonstrated by a case study showing that 93\% of the generated molecules are chemically valid and more than a third satisfy the targeted objectives, while there were none in the initial set.	https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070180995&doi=10.1021\%2facs.jcim.9b00325&partnerID=40&md5=67190c69b2673b65fe3c3dcbe197fa4d	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Stamenkovich1992	An application of artificial neural networks for autonomous ship navigation through a channel	A neural network model based on reinforcement learning is investigated for use as a shipboard autonomous channel navigator. The model used consists of two neuron-like elements. The basic learning scheme involves learning with a critic. The network consists of an adaptive critic element (ACE) and an adaptive search element (ASE). The ASE explores the channel region while the ACE criticizes the actions of the ASE and tries to predict failures of the ASE's attempt to navigate. The neural network model developed has been shown to be useful through software simulation with graphical feedback. A similar implementation could have applications in many electronic mapping systems utilizing vector information. The performance of such a system and its adaptability to new channels are investigated.<>	https://dx.doi.org/10.1109/PLANS.1992.185865	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	?tefan2021	Controlling hardware design behavior using Python based machine learning algorithms	Hardware design and software programming. Two powerful worlds, where functionality is described by coding complex functions which will operate either in silicon or in a processor logic. Hardware provides accurate implementation of application needs at highest speeds. Software provides the welcome flexibility when the system needs an update. And the combination of the two approaches, well represented in industry by System-on-chip[1], leads to the powerful devices which are more and more present in our lives. But the well co-operation between hardware and software bring its advantages even from design phase. Software helps a lot hardware development, through powerful tools used at design, verification, synthesis and all other design steps. Software development is accelerated by hardware platforms which make possible intensive testing and scenarios creation for validation of new program releases. This paper exploits the opportunity of using software in hardware development. It demonstrates, both graphically and numerically, that desired verification scenarios can be faster reached by integrating a powerful machine learning technique in design verification: reinforcement learning.	https://dx.doi.org/10.1109/EMES52337.2021.9484105	Excluded	new_screen	E5: Other not a paper,E2: Software engineering is not the problem RL is used for	4
RL4SE	Stefanova2018	Analysis of User Groups in Social Networks to Detect Socially Dangerous People	The article proposes a method for identifying socially dangerous people on the basis of metadata left by a user during registration in groups of social networks. This method will allow law enforcement agencies to identify socially dangerous elements of the society that actively use social networks in an unattended mode. Additionally, the method functions without violating the constitutional rights of users to privacy, with respect to personal correspondence. To increase the accuracy while identifying socially dangerous people, it was proposed to use a neural network that allows solving problems with reinforcement learning techniques. The article focuses on structuring the analytical system of social groups, on the scheme of an adaptive neural network, on the stages of the dictionary compilation for an adaptive neural network. Further, the article describes a software package with the help of which the idea of identifying socially dangerous people was realized. The authors provide the code for processing the request for text classification in the server of the neural network. The program complex is written implementing two programming languages: JavaScript and Java. To confirm the program functioning, the screen shot of the program-testing interface is given. It illustrates the process of identifying socially dangerous people on the basis of three selected criteria: normal, aggressive and suicidal. The results were verified by a psychologist on the basis of a special projective methods of personality assessment.	https://dx.doi.org/10.1109/INFOCOMMST.2018.8632086	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Stehr2008	Planning and learning algorithms for routing in Disruption-Tolerant Networks	We give an overview of algorithms that we have been developing in the DARPA disruption-tolerant networking program, which aims at improving communication in networks with intermittent and episodic connectivity. Thanks to the use of network caching, this can be accomplished without the need for a simultaneous end-to-end path that is required by traditional Internet and mobile ad-hoc network (MANET) protocols. We employ a disciplined two-level approach that clearly distinguishes the dissemination of application content from the dissemination of network-related knowledge, each of which can be supported by different algorithms. Specifically, we present probabilisitc reflection, a single-message protocol enabling the dissemination of knowledge in strongly disrupted networks. For content dissemination, we present two approaches, namely a symbolic planning algorithm that exploits partially predictable temporal behavior, and a distributed and disruption-tolerant reinforcement learning algorithm that takes into account feedback about past performance.	https://dx.doi.org/10.1109/MILCOM.2008.4753336	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Stein2016	Bid2Charge: Market User Interface Design for Electric Vehicle Charging			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Stein2017	Market interfaces for electric vehicle charging			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Steinbacher2022	Modelling Framework for Reinforcement Learning based Scheduling Applications	Over the last years, reinforcement learning has been extensively applied to schedule complex and dynamic systems. There are multitudes of simulation environments and algorithms, which hinder standardization and impede testing the suitability of reinforcement learning for specific scheduling applications and their easy implementation. This article proposes a framework to model production systems easily and transform them into standard industry simulation software to solve this issue. This framework contains major elements of classic production systems and references them adequately to allow effortless modelling. Furthermore, the domain models' adjacent systems and their respective functionalities are described to facilitate reinforcement learning based scheduling. This study demonstrates the framework's applicability using an existing dynamic scheduling problem. The experiences during modelling and training of the reinforcement learning subsequently are discussed. Copyright (C) 2022 The Authors.	https://dx.doi.org/10.1016/j.ifacol.2022.09.369	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Stenudd2010	Using machine learning in the adaptive control of a smart environment			Excluded	new_screen	E5: Other not a paper,E2: Software engineering is not the problem RL is used for	4
RL4SE	Stewart2014	Striatum and insula dysfunction during reinforcement learning differentiates abstinent and relapsed methamphetamine-dependent individuals	Background and aims Individuals with methamphetamine dependence (MD) exhibit dysfunction in brain regions involved in goal maintenance and reward processing when compared with healthy individuals. We examined whether these characteristics also reflect relapse vulnerability within a sample of MD patients. Design Longitudinal, with functional magnetic resonance imaging (fMRI) and clinical interview data collected at baseline and relapse status collected at 1-year follow-up interview. Setting Keck Imaging Center, University of California San Diego, USA. Participants MD patients (n = 60) enrolled into an in-patient drug treatment program at baseline. MD participants remaining abstinent at 1-year follow-up (abstinent MD group; n = 42) were compared with MD participants who relapsed within this period (relapsed MD group; n = 18). Measurements Behavioral and neural responses to a reinforcement learning (paper-scissors-rock) paradigm recorded during an fMRI session at time of treatment. Findings The relapsed MD group exhibited greater bilateral inferior frontal gyrus (IFG) and right striatal activation than the abstinent MD group during the learning of reward contingencies (Cohen's d range: 0.60-0.83). In contrast, the relapsed MD group displayed lower bilateral striatum, bilateral insula, left IFG and left anterior cingulate activation than the abstinent MD group (Cohen's d range: 0.90-1.23) in response to winning, tying and losing feedback. Conclusions Methamphetamine-dependent individuals who achieve abstinence and then relapse show greater inferior frontal gyrus activation during learning, and relatively attenuated striatal, insular and frontal activation in response to feedback, compared with methamphetamine-dependent people who remain abstinent.	https://www.ncbi.nlm.nih.gov/pubmed/24329936	Excluded	new_screen	E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for	4
RL4SE	Stewart2021	Developing Spatial Visualization Skills with Virtual Reality and Hand Tracking		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119855766&doi=10.1007\%2f978-3-030-90176-9_51&partnerID=40&md5=83571d3a9dc15dfec47849d5f2afda69	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Stocker2019	Reinforcement learning-based design of orienting devices for vibratory bowl feeders	The sorting of small parts is one of the common tasks in the field of industrial manufacturing. Vibratory bowl feeders (VBF) are commonly used to accomplish this task. Nowadays, the design process of the VBF is based on a manual and expensive trial-and-error approach, in which different traps are arranged and tuned. This paper outlines a method which modifies this conventional process using Reinforcement Learning to automate the VBF design. To enable this, a software agent is used to model the placement of traps on multiple positions and measure the subsequent configuration efficiency. A physics simulation provides the characteristics of the individual traps. During the training, Q-learning is applied to determine the environmental indicators under which a certain trap should be replaced. A 3D matrix is used to store information in a problem-related representation. Due to the trial-and-error principle of Reinforcement Learning, this training is comparable with the traditional proceedings. In addition, valuable action paths are stored in a memory and the agent frequently is trained on these paths in order to remember good solutions. Additionally, a knowledge base is used to exclude inefficient sets of traps. The rules for the knowledge base are built upon knowledge from the conventional design process. In first test cases, the trained agent is able to assemble traps achieving promising configurations. The results of the agent will be validated in the next step using physics simulation.	https://dx.doi.org/10.1007/s00170-019-03798-9	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Stocker2019a	Reinforcement learningendashbased design of orienting devices for vibratory bowl feeders		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065729520&doi=10.1007\%2fs00170-019-03798-9&partnerID=40&md5=db5b42660ea1f59805b850a625702ae0	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Stramiello2008	Aviation turbine engine diagnostic system (ATEDS) for the CH-47 helicopter			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Stringer2007	Learning movement sequences with a delayed reward signal in a hierarchical model of motor function	A key problem in reinforcement learning is how an animal is able to learn a sequence of movements when the reward signal only occurs at the end of the sequence. We describe how a hierarchical dynamical model of motor function is able to solve the problem of delayed reward in learning movement sequences using associative (Hebbian) learning. At the lowest level, the motor system encodes simple movements or primitives, while at higher levels the system encodes sequences of primitives. During training, the network is able to learn a high level motor program composed of a specific temporal sequence of motor primitives. The network is able to achieve this despite the fact that the reward signal, which indicates whether or not the desired motor program has been performed correctly, is received only at the end of each trial during learning. Use of a continuous attractor network in the architecture enables the network to generate the motor outputs required to produce the continuous movements necessary to implement the motor sequence.	https://doi.org/10.1016/j.neunet.2006.01.016	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Su2022	Effectively Generating Vulnerable Transaction Sequences in Smart Contracts with Reinforcement Learning-guided Fuzzing		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146949231&doi=10.1145\%2f3551349.3560429&partnerID=40&md5=10f56e6538696aaccfb0d555574f528f	Included	new_screen		4
RL4SE	Su2009	Research on task allocation of process planning based on reinforcement learning and neural network			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Subramanian2019	Approximate information state for partially observed systems	The standard approach for modeling partially observed systems is to model them as partially observable Markov decision processes (POMDPs) and obtain a dynamic program in terms of a belief state. The belief state formulation works well for planning but is not ideal for online reinforcement learning because the belief state depends on the model and, as such, is not observable when the model is unknown.In this paper, we present an alternative notion of an information state for obtaining a dynamic program in partially observed models. In particular, an information state is a sufficient statistic for the current reward which evolves in a controlled Markov manner. We show that such an information state leads to a dynamic programming decomposition. Then we present a notion of an approximate information state and present an approximate dynamic program based on the approximate information state. Approximate information state is defined in terms of properties that can be estimated using sampled trajectories. Therefore, they provide a constructive method for reinforcement learning in partially observed systems. We present one such construction and show that it performs better than the state of the art for three benchmark models.	https://dx.doi.org/10.1109/CDC40024.2019.9029898	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Such2019	An Atari model zoo for analyzing, visualizing, and comparing deep reinforcement learning agents			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sueldo2021	Integration of ROS and Tecnomatix for the development of digital twins based decision-making systems for smart factories	Digital twins employs simulation in conjunction with virtual environments and a variety of data coming from different plant equipment and physical systems to continuously update the digital models of the world in a feedback loop scheme to facilitate the decision-making processes. The heterogeneity of existing hardware and software requires the development of software architectures able to deal with the information exchange due to the integration and interaction of several system components and autonomous decision-making systems. In this work we propose the design and construction of a software architecture that integrates a manufacturing process simulator with the well-known robot operating system (ROS-Robot Operating System) to easily interchange information with an autonomous decision-making system. The proposal is tested with the simulator Tecnomatix and the free distribution ROS Melodic. We present an instance of software architecture for a typical complex case study of manufacturing plants and demonstrate its easy integration with an autonomous decision-making system based on the reinforcement- learning paradigm.	https://dx.doi.org/10.1109/TLA.2021.9468608	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sugiyama2010	Least absolute policy iteration - A robust approach to value function approximation		https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956819826&doi=10.1587\%2ftransinf.E93.D.2555&partnerID=40&md5=08b26f1bb0b4cd02f0cf7cfea9b40229	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Sun2018	A Q-Learning-Based Approach for Deploying Dynamic Service Function Chains	As the size and service requirements of today's networks gradually increase, large numbers of proprietary devices are deployed, which leads to network complexity, information security crises and makes network service and service provider management increasingly difficult. Network function virtualization (NFV) technology is one solution to this problem. NFV separates network functions from hardware and deploys them as software on a common server. NFV can be used to improve service flexibility and isolate the services provided for each user, thus guaranteeing the security of user data. Therefore, the use of NFV technology includes many problems worth studying. For example, when there is a free choice of network path, one problem is how to choose a service function chain (SFC) that both meets the requirements and offers the service provider maximum profit. Most existing solutions are heuristic algorithms with high time efficiency, or integer linear programming (ILP) algorithms with high accuracy. It's necessary to design an algorithm that symmetrically considers both time efficiency and accuracy. In this paper, we propose the Q-learning Framework Hybrid Module algorithm (QLFHM), which includes reinforcement learning to solve this SFC deployment problem in dynamic networks. The reinforcement learning module in QLFHM is responsible for the output of alternative paths, while the load balancing module in QLFHM is responsible for picking the optimal solution from them. The results of a comparison simulation experiment on a dynamic network topology show that the proposed algorithm can output the approximate optimal solution in a relatively short time while also considering the network load balance. Thus, it achieves the goal of maximizing the benefit to the service provider.	https://dx.doi.org/10.3390/sym10110646	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sun2020	Action Evaluation Hardware Accelerator for Next-Generation Real-Time Reinforcement Learning in Emerging IoT Systems	Internet of Things (IoT) sensors often operate in unknown dynamic environments comprising latency-sensitive data sources, dynamic processing loads, and communication channels of unknown statistics. Such settings represent a natural application domain of reinforcement learning (RL), which enables computing and learning decision policies online, with no a priori knowledge. In our previous work, we introduced a post-decision state (PDS) based RL framework, which considerably accelerates the rate of learning an optimal decision policy. The present paper formulates an efficient hardware architecture for the action evaluation step, which is the most computationally-intensive step in the PDS based learning framework. By leveraging the unique characteristics of PDS learning, we optimize its state value expectation and known cost computational blocks, to speed-up the overall computation. Our experiments show that the optimized circuit is 49 times faster than its software implementation counterpart, and six times faster than a Q-learning hardware accelerator.	https://dx.doi.org/10.1109/ISVLSI49217.2020.00084	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sun2020a	QOS-Aware Flow Control for Power-Efficient Data Center Networks with Deep Reinforcement Learning	Reducing the power consumption and maintaining the Flow Completion Time (FCT) for the Quality of Service (QoS) of applications in Data Center Networks (DCNs) are two major concerns for data center operators. However, existing works either fail in guaranteeing the QoS due to the neglect of the FCT constraints or achieve a less satisfying power efficiency. In this paper, we propose SmartFCT, which employs Software-Defined Networking (SDN) coupled with the Deep Reinforcement Learning (DRL) to improve the power efficiency of DCNs and guarantee the FCT. The DRL agent can generate a dynamic policy to consolidate traffic flows into fewer active switches in the DCN for power efficiency, and the policy also leaves different margins in different active links and switches to avoid FCT violation of unexpected short bursts of flows. Simulation results show that with similar FCT guarantee, SmartFCT can save 8\% more of the power consumption compared to the state-of-the-art solutions.	https://dx.doi.org/10.1109/ICASSP40776.2020.9054040	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sun2019	TIDE: Time-relevant deep reinforcement learning for routing optimization		https://doi.org/10.1016/j.future.2019.04.014	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sun2020b	Improving the Scalability of Deep Reinforcement Learning-Based Routing with Control on Partial Nodes	Machine Learning (ML)-based routing optimization has been proposed to optimize the performance of flow routing for future networks, such as Software-Defined Networks (SDNs). However, existing studies are either hard to converge for large networks or vulnerable to topology changes. In this paper, we propose SINET, a scalable and intelligent network control framework for routing optimization. To improve the robustness and scalability, SINET selects several critical routing nodes to be directly controlled by a Deep Reinforcement Learning (DRL) agent, which dynamically generates routing policy to optimize network performance. Simulation results show that SINET can reduce the average flow completion time by at least 32\% for a network with 82 nodes and exhibit better robustness against minor topology changes, compared to other DRL-based schemes.	https://dx.doi.org/10.1109/ICASSP40776.2020.9054483	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sun2021	Combining Deep Reinforcement Learning With Graph Neural Networks for Optimal VNF Placement	Network Function Virtualization (NFV) technology utilizes software to implement network function as virtual instances, which reduces the cost on various middlebox hardware. A Virtual Network Function (VNF) instance requires multiple resource types in the network (e.g., CPU, memory). Therefore, an efficient VNF placement policy should consider both the resource utilization problem and the Quality of Service (QoS) of flows, which is proved NP-hard. Recent studies employ Deep Reinforcement Learning (DRL) to solve the VNF placement problem, but existing DRL-based solutions cannot generalize well to different topologies. In this letter, we propose to combine the advantage of DRL and Graph Neural Network (GNN) to design our VNF placement scheme DeepOpt. Simulation results show that DeepOpt outperforms the state-of-the-art VNF placement schemes and shows a much better generalization ability in different network topologies.	https://dx.doi.org/10.1109/LCOMM.2020.3025298	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sun2020c	SmartFCT: Improving power-efficiency for data center networks with deep reinforcement learning	Reducing the power consumption of Data Center Networks (DCNs) and guaranteeing the Flow Completion Time (FCT) of applications in DCNs are two major concerns for data center operators. However, existing works cannot realize the two goals together because of two issues: (1) dynamic traffic pattern in DCNs is hard to accurately model; (2) an optimal flow scheduling scheme is computationally expensive. In this paper, we propose SmartFCT, which employs the Deep Reinforcement Learning (DRL) coupled with Software-Defined Networking (SDN) to improve the power efficiency of DCNs and guarantee FCT. SmartFCT dynamically collects traffic distribution from switches to train its DRL model. The well-trained DRL agent of SmartFCT can quickly analyze the complicated traffic characteristics using neural networks and adaptively generate a action for scheduling flows and deliberately configuring margins for different links. Following the generated action, flows are consolidated into a few of active links and switches for saving power, and fine-grained margin configuration for active links avoids FCT violation of unexpected flow bursts. Simulation results show that SmartFCT can guarantee FCT and save up to 12.2\% power consumption, compared with the state-of-the-art solutions.	https://dx.doi.org/10.1016/j.comnet.2020.107255	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sun2020d	DeepMigration: Flow Migration for NFV with Graph-based Deep Reinforcement Learning	Network Function Virtualization (NFV) enables flexible deployment of network services as applications. Network operators expect to use a limited number of Network Function (NF) instances to handle the fluctuating traffic load and provide network services. However, it is a big challenge to guarantee the Quality of Service (QoS) under the unpredictable network traffic while minimizing the processing resources. One typical solution is to realize NF scale-out, scale-in and load balancing by elastically migrating the related traffic flows with Software-Defined Networking (SDN). However, it is difficult to optimally migrate flows since many real-time statuses of NF instances should be considered to make accurate decisions. In this paper, we propose DeepMigration to solve the problem by efficiently and dynamically migrating traffic flows among different NF instances. DeepMigration is a Deep Reinforcement Learning (DRL)-based solution coupled with Graph Neural Network (GNN). By taking advantages of the graph-based relationship deduction ability from our customized GNN and the self-evolution ability from the experience training of DRL, DeepMigration can accurately model the cost (e.g., migration latency) and the benefit (e.g., reducing the number of NF instances) of flow migration among different NF instances and generate dynamic and effective flow migration policies to improve the QoS. Experiment results show that DeepMigration requires less migration cost and saves up to 71.6\% of the computation time than existing solutions.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sun2020e	An Intelligent Routing Technology Based on Deep Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097582791&doi=10.3969\%2fj.issn.0372-2112.2020.11.011&partnerID=40&md5=3c012cec55153fe60ee356434039f070	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sun2022	Deep Reinforcement-Learning-Guided Backup for Energy Harvesting Powered Systems		https://dx.doi.org/10.1109/TCAD.2021.3056328	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sun2022a	Research and Application of Integrated Data Fusion Terminal for Station Area Operation and Distribution Based on Edge Calculation and Software Definition	In this paper, we studied the integrated data fusion terminal for distribution area operation and distribution based on edge calculation and software definition. Firstly, the overall architecture of the integrated data fusion terminal for distribution area operation and distribution was constructed, and the software and hardware of the terminal were comprehensively designed. Then, an edge data stream hierarchical storage algorithm and an edge computing resource allocation method based on Deep Deterministic Policy Gradient (DDPG) reinforcement learning algorithm were proposed. In order to meet the capability of edge computing, the resource utilization efficiency, running time and throughput efficiency of CPU were compared and analyzed. The optimal CPU chip was selecited to complete the optimal design of terminal control system. Finally, combined with the field, the terminal was applied. The functions of data monitoring, fault early warning, topology identification and distribution area line loss were realized, which meets the professional needs of marketing and production, reduces the investment cost of power grid equipment and improves the on-site maintenance efficiency.	https://dx.doi.org/10.1109/CEEPE55110.2022.9783350	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sun2019a	Learning - Based adaptation framework for elastic software systems		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071371358&doi=10.18293\%2fSEKE2019-009&partnerID=40&md5=4614d3273ce88cef9117773361ef643d	Included	new_screen		4
RL4SE	Sun2022b	A Joint Learning and Game-Theoretic Approach to Multi-dimensional Resource Management in Fog Radio Access Networks		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139852020&doi=10.1109\%2fTVT.2022.3214075&partnerID=40&md5=2afca66069106f8f62310320cd4fca00	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sun2022c	Applying Reinforcement Learning for Shortest Path Problem	With the rapid development of high technology and artificial intelligence, there are plenty of novel software inventions have been created. Thus it is essential to consider how to make these innovations more practically to improve the convenience and efficiency of daily life. Therefore, this paper is concerned about implementing the machine learning method to address problems in daily life. Thus, a novel form of the reinforcement learning algorithm is applied to the shortest path problem abstracted from real life. The problem focuses on finding the most optimal route on a ten-note weighted graph from one point to a destination. The agent is trained four hundred times to accumulate values in the Q table, then the result is determined by the final Q value. Because of the success of resolving this problem, this algorithm should be implemented practically despite some limitations. With the help of this algorithm, the agent can determine the desired route extremely quickly to improve efficiency.	https://dx.doi.org/10.1109/BDICN55575.2022.00100	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sure2022	A Deep Reinforcement Learning Agent for General Video Game AI Framework Games	The General Video Game AI (GVGAI) software framework, which was originally created for General Game Playing (GGP), can be connected via the OpenAI Gym interface for the purpose of testing machine learning agents on a large number of 2-dimensional arcade-type games. In this paper, we present a Proximal Policy optimization (PPO) based deep reinforcement learning game-playing agent. Using the OpenAI Gym interface, we highlight how this agent performs on a number of randomly selected GVGAI games.	https://dx.doi.org/10.1109/ICAICA54878.2022.9844524	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Surhonne2022	GAE-LCT: A Run-Time GA-Based Classifier Evolution Method for Hardware LCT Controlled SoC Performance-Power Optimization		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144828406&doi=10.1007\%2f978-3-031-21867-5_18&partnerID=40&md5=9cf1374deb3d65eea41f29848b0cfdf6	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Suriyarachchi2022	Multi-agent Deep Reinforcement Learning for Shock Wave Detection and Dissipation using Vehicle-to-Vehicle Communication	Traffic shock waves are a commonly occurring phenomena caused by the delays in reaction times of Human Driven Vehicles (HDVs) resulting in unnecessary congestion in highway networks. Application of a suitable moving bottleneck control using Connected Autonomous Vehicles (CAVs) can result in shock wave mitigation and smoothing of the traffic flow. This traffic control scheme is dependent on accurately predicting shock wave conditions while choosing the best control to apply for the observation available to the CAV. In this work, we propose the use of a multi-agent shared policy reinforcement learning algorithm which leverages communication between CAVs for improved observability of downstream traffic conditions. A key feature of this method is the ability to perform shock wave dissipation control without the need for global information and the applicability of this method to multi-lane mixed traffic highways of arbitrary structure. We use the shared-parameter Proximal Policy Optimization (PPO) reinforcement learning strategy for obtaining the controls for each CAV in the simulation. We also built a custom SUMO-Gym wrapper for the multi-lane highway simulation with custom designed observation space, action space and rewards for each agent. The shock wave dissipation efficiency is evaluated on a three lane circular highway loop using realistic traffic simulation software and low CAV penetration levels.	https://dx.doi.org/10.1109/CDC51059.2022.9992948	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Sviatov2021	Detection of Scenes Features for Path Following on a Local Map of a Mobile Robot Using Neural Networks		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097265653&doi=10.1007\%2f978-3-030-65283-8_22&partnerID=40&md5=a7224810846e5d21ba2679dc101db129	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Syed2014	Channel selection in multi-hop cognitive radio network using reinforcement learning: An experimental study	Cognitive radio (CR) is a communication that enables better utilization of radio spectrum. Multi-hop CR network is an emerging area of interest for many researchers in recent years. The concept of multi-hop CR network is widely investigated with many wireless applications, such as video surveillance application. In this paper, we investigate a multi-hop CR network, which can serve as a video surveillance system. Reinforcement learning (RL), which is an artificial intelligence approach, is applied to select and switch to the best possible operating channel. In the CR network context, the unlicensed users (or secondary users, SUs) evade the licensed users' (or primary users', PUs') activities; while in the video surveillance system context, the honest users evade the malicious users' activities (or the denial of service attacks). In our investigation, the multi-hop network comprises of three SUs, namely source, relay and destination nodes. There is a single PU. The experimental setup consists of universal software radio peripheral (USRP) and GNU radio units. Our implementation results show that RL is an effective approach that enables SUs (or honest users) to evade PUs' (or malicious users') activities, and so it improves network performance.	https://dx.doi.org/10.1049/cp.2014.1402	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Syed2016	Route Selection for Multi-Hop Cognitive Radio Networks Using Reinforcement Learning: An Experimental Study	Cognitive radio (CR) enables unlicensed users to explore and exploit underutilized licensed channels (or white spaces). While multi-hop CR network has drawn significant research interest in recent years, majority work has been validated through simulation. A key challenge in multi-hop CR network is to select a route with high quality of service (QoS) and lesser number of route breakages. In this paper, we propose three route selection schemes to enhance the network performance of CR networks, and investigate them using a real testbed environment, which consists of universal software radio peripheral and GNU radio units. Two schemes are based on reinforcement learning (RL), while a scheme is based on spectrum leasing (SL). RL is an artificial intelligence technique, whereas SL is a new paradigm that allows communication between licensed and unlicensed users in CR networks. We compare the route selection schemes with an existing route selection scheme in the literature, called highest-channel (HC), in a multi-hop CR network. With respect to the QoS parameters (i.e., throughput, packet delivery ratio, and the number of route breakages), the experimental results show that RL approaches achieve a better performance in comparison with the HC approach, and also achieve close to the performance achieved by the SL approach.	https://dx.doi.org/10.1109/ACCESS.2016.2613122	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Szwarcfiter2022	Project scheduling in a lean environment to maximize value and minimize overruns		https://doi.org/10.1007/s10951-022-00727-9	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tadepalli1998	Model-based average reward reinforcement learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032050241&doi=10.1016\%2fs0004-3702\%2898\%2900002-2&partnerID=40&md5=22b0f183662c592173495c007d98ce58	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Takada2017	Reinforcement Learning for Creating Evaluation Function Using Convolutional Neural Network in Hex	An evaluation function in the board game decides the next move for computer AIs, and a high accurate evaluation function leads to a strong computer AI. Recently, the evaluation function using convolutional neural network(CNN) by supervised learning shows high evaluation accuracy. Supervised learning cannot exceed the teachers, but there must be a possibility to create a more accurate evaluation function by using reinforcement learning. In this paper, we proposed an evaluation function using CNN and reinforcement learning with games of self-play in Hex. The proposed evaluation function is tested with the previous evaluation function and world-champion program MoHex2.0. The results show that evaluation accuracy of the proposed evaluation function is higher than the previous evaluation functions, and proposed computer Hex algorithm EZO-CNN obtained a win rate of 60.0\% against MoHex2.0 even though the search time of EZO-CNN is shorter than MoHex2.0.	https://dx.doi.org/10.1109/TAAI.2017.16	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Takada2020	Reinforcement Learning to Create Value and Policy Functions Using Minimax Tree Search in Hex	Recently, the use of reinforcement-learning algorithms has been proposed to create value and policy functions, and their effectiveness has been demonstrated using Go, Chess, and Shogi. In previous studies, the policy function was trained to predict the search probabilities of each move output by Monte Carlo tree search; thus, a number of simulations were required to obtain the search probabilities. We propose a reinforcement-learning algorithm with game of self-play to create value and policy functions such that the policy function is trained directly from the game results without the search probabilities. In this study, we use Hex, a board game developed by Piet Hein, to evaluate the proposed method. We demonstrate the effectiveness of the proposed learning algorithm in terms of the policy function accuracy, and play a tournament with the proposed computer Hex algorithm DeepEZO and 2017 world-champion programs. The tournament results demonstrate that DeepEZO outperforms all programs. DeepEZO achieved a winning percentage of 79.3\% against the world-champion program MoHex2.0 under the same search conditions on 13 $\times$ 13 board. We also show that the highly accurate policy functions can be created by training the policy functions to increase the number of moves to be searched in the loser position.	https://dx.doi.org/10.1109/TG.2019.2893343	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Takama1998	A visual anthropomorphic agent with learning capability of cooperative answering strategy through speech dialog	As the opportunity of using computer systems spreads into everyday life, the importance of friendly human interfaces is increasing. As a form of next generation human interfaces, an anthropomorphic interface agent which mimics a face-to-face communication holds promise, and some early developments have started. Its multimodality including facial expression and speech dialog fits to human perception and can enhance the friendliness of the interface. We present an anthropomorphic interface agent called VSA (Visual Software Agent), which has a moving realistic facial image and a speech dialog function. Unlike other anthropomorphic interface systems, our VSA system has connection to a WWW browser (Netscape Navigator), so that it can serve as a new interface to a vast WWW information space and effectively use multimedia data written in a standardized HTML format. As immediate applications of the VSA, it is suitable for guidance systems which are used by various people with little knowledge of computers at, for example, department stores, company reception desks, university campuses, etc. We have implemented learning capability into our anthropomorphic agent. We use reinforcement learning, in particular, profit sharing method; but our research is unique in that the learning mechanism is implemented to acquire knowledge from speech dialogs. We show our implemented learning mechanism with reference to a task of campus guidance.	https://dx.doi.org/10.1109/APCHI.1998.704331	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tam2022	Graph Neural Networks for Intelligent Modelling in Network Management and Orchestration: A Survey on Communications	The advancing applications based on machine learning and deep learning in communication networks have been exponentially increasing in the system architectures of enabled software-defined networking, network functions virtualization, and other wired/wireless networks. With data exposure capabilities of graph-structured network topologies and underlying data plane information, the state-of-the-art deep learning approach, graph neural networks (GNN), has been applied to understand multi-scale deep correlations, offer generalization capability, improve the accuracy metrics of prediction modelling, and empower state representation for deep reinforcement learning (DRL) agents in future intelligent network management and orchestration. This paper contributes a taxonomy of recent studies using GNN-based approaches to optimize the control policies, including offloading strategies, routing optimization, virtual network function orchestration, and resource allocation. The algorithm designs of converged DRL and GNN are reviewed throughout the selected studies by presenting the state generalization, GNN-assisted action selection, and reward valuation cooperating with GNN outputs. We also survey the GNN-empowered application deployment in the autonomous control of optical networks, Internet of Healthcare Things, Internet of Vehicles, Industrial Internet of Things, and other smart city applications. Finally, we provide a potential discussion on research challenges and future directions.	https://dx.doi.org/10.3390/electronics11203371	Excluded	new_screen	E5: Other not a paper,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tambon2022	How to certify machine learning based safety-critical systems? A systematic literature review		https://doi.org/10.1007/s10515-022-00337-x	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E5: Other not a paper	4
RL4SE	Tan2022	Intelligent Handover Algorithm for Vehicle-to-Network Communications With Double-Deep Q-Learning	For vehicle-to-network communications, handover (HO) management enables vehicles to maintain the connection with the network while transiting through coverage areas of different base stations (BSs). However, the high mobility of vehicles means shorter connection periods with each BS that leads to frequent HOs, hence raises the necessity for optimal HO decision making for high quality infotainment services. Machine learning is capable of capturing underlying pattern via data driven methods to find optimal solutions to complex problems, and much learning-based HO optimization research has been conducted focusing on specific network setups. However, attention still needs to be paid to the actual deployment aspect and standardized datasets or simulation environments for evaluation. This paper proposes a deep reinforcement learning-based HO algorithm using the input parameters that are configurable in the existing measurement report of cellular networks. The performance of the proposed algorithm is evaluated using the well-known network simulator ns-3 with its official LTE module. A realistic network setup in the city center of Glasgow (U.K.) is configured with vehicle trajectories generated by the routes mobility model using Google Maps Directions API. Evaluation results reveal that the proposed algorithm significantly outperforms the A3 RSRP baseline with an average of 25.72\% packet loss reduction per HO, suggesting significant improvement in quality of service of phone call and video streaming, etc. The proposed algorithm also has a small implementation cost compared to some state-of-the-art and should be deployed by a software update to a local BS controller.	https://dx.doi.org/10.1109/TVT.2022.3169804	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tan2020	Optimized Deep Reinforcement Learning Approach for Dynamic System		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098700249&doi=10.1109\%2fISSE49799.2020.9272245&partnerID=40&md5=a364794874b5a3070719449fb4726900	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tanaka2016	"The Meaning of ""Understanding the Brain"": Peeking into the Brain of a Computational Neuroscientist"			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Tang2020	A zero-sum Markov defender-attacker game for modeling false pricing in smart grids and its solution by multi-agent reinforcement learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089172472&doi=10.3850\%2f978-981-11-2724-30743-cd&partnerID=40&md5=60fc3e5bad10a7725ed3873105b47ccd	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tang2021	Virtual Network Function Placement Optimization Algorithm Based on Improve Deep Reinforcement Learning	Considering the problem of Service Function Chain (SFC) placement optimization caused by the dynamic arrival of network service requests under the Network Function Virtualization/Software Defined Network (NFV/SDN) architecture, a Virtual Network Function (VNF) placement optimization algorithm based on improved deep reinforcement learning is proposed. Firstly, a stochastic optimization model of Markov Decision Process (MDP) is established to jointly optimizes SFC placement cost and delay cost, and is constrained by the delay of SFC, as well as the resources of common server Central Processing Unit (CPU) and physical link bandwidth. Secondly, in the process of VNF placement and resource allocation, there are problems such as too large state space, high dimension of action space, and unknown state transition probability. A VNF intelligent placement algorithm based on deep reinforcement learning is proposed to obtain an approximately optimal VNF placement strategy and resource allocation strategy. Finally, considering the problems of deep reinforcement learning agent's action exploration and utilization through epsilon greedy strategy, resulting in low learning efficiency and slow convergence speed, a method of action exploration and utilization based on the difference of value function is proposed, and further adopts dual experience playback pool to solve the problem of low utilization of empirical samples. Simulation results show that the algorithm can converge quickly, and it can optimize SFC placement cost and SFC end-to-end delay.	https://dx.doi.org/10.11999/JEIT200297	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tang2021a	Virtual Network Function Migration Optimization Algorithm Based on Deep Deterministic Policy Gradient	To solve the problem of Virtual Network Function (VNF) migration optimization, which is caused by the dynamic change of resource requirements of Service Function Chain (SFC) under Network Function Virtualization/Software Defined Network (NFV/SDN) architecture, a VNF migration optimization algorithm is proposed based on deep reinforcement learning. Firstly, based on the underlying CPU, bandwidth resources and SFC end-to-end delay constraints, a Markov Decision Process (MDP) based stochastic optimization model is established. This model is used to optimize jointly network energy consumption and SFC end-to-end delay by migrating VNF. Secondly, since the state space and action space of this paper are continuous value sets, a VNF intelligent migration algorithm based on Deep Deterministic Policy Gradient (DDPG) is proposed to obtain an approximate optimal VNF migration strategy. The simulation results show that the algorithm can achieve the compromise between network energy consumption and SFC end-to-end delay, and improve the resource utilization of the physical network.	https://dx.doi.org/10.11999/JEIT190921	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tang2015	Efficient Auto-Scaling Approach in the Telco Cloud Using Self-Learning Algorithm	Network Function Virtualization (NFV) and Software Defined Network (SDN) technologies makes it possible for the Telco Operators to assign resource for virtual network functions (VNF) on demand. Provision and orchestration of physical and virtual resource is crucial for both Quality of Service (QoS) guarantee and cost management in cloud computing environment. Auto-scaling mechanism is essential in the lifecycle management of those VNFs. Threshold based policy is always applied in classic IT cloud environments which can not satisfy carrier grade requirements such as reliability and stability. In this paper, we present a novel SLA-aware and Resource-efficient Self-learning Approach (SRSA) for auto-scaling policy decision. The scenarios of the service volatility is categorized into daily busy-and-idle scenario and burst-traffic scenario. First, we formulate the workload of the VNF as discrete-time series and treat procedure of policy-making in auto-scaling as a Markov Decision Process (MDP). Second, parameters in the Reinforcement Learning process are tuned cautiously. Finally the experiments show that our solution outperforms threshold based policy and voting policy adopted by RightScale in oscillation suppression, QoS guarantee, and energy saving.	https://dx.doi.org/10.1109/GLOCOM.2015.7417181	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tang2022	Malicious code dynamic traffic camouflage detection based on deep reinforcement learning in power system	In order to solve the problem that malicious code intrudes into software in various forms, which leads to its security performance degradation and cannot be used normally, this paper proposes a malicious code dynamic traffic camouflage detection method based on deep reinforcement learning in power system. The average mutual information between codes is calculated by deep reinforcement learning, and the weighted information gain of each code type feature is obtained. Different types of code feature set classifiers are generated, and an optimal classifier is output for each type of code feature set. The features are reduced by Linear Discriminant Analysis (LDA), and the network code is classified according to the extracted features. The potential malicious code is detected according to the explicit rules of deep reinforcement learning. Simulation results show that the detection method can improve the accuracy of malicious code classification, and the detection performance is increased to about 35\%. (C) 2022 The Authors. Published by Elsevier Ltd.	https://dx.doi.org/10.1016/j.egyr.2022.02.008	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tang2020a	Reinforcement learning for integer programming: Learning to cut			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tanimoto2004	Learning an evaluation function for shogi from data of games	This paper proposes a method for obtaining a reasonably accurate evaluation function of a shogi (Japanese chess) position through learning from data of games. An accurate evaluation function is indispensable for a strong shogi program. A shogi position is projected into a feature space which consists of feature variates charactering the position. Using such variates as input, we employ a multi-layer perceptron as a nonlinear evaluation function. Since it is not easy to obtain accurate evaluated values of positions, we employ reinforcement learning. Our experiments using hundreds of games show that the proposed method works well in obtaining a very accurate evaluation function for shogi, whose performance is comparable to that of a strong shogi program.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tanner2009	RL-Glue: Language-Independent Software for Reinforcement-Learning Experiments			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tao2021	Infrared camera assisted UAV autonomous control via deep reinforcement learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100299965&doi=10.2514\%2f6.2021-1121&partnerID=40&md5=90c51a3a25dba47c3297dafdd28e9642	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tao2022	A Hybrid Cloud and Edge Control Strategy for Demand Responses Using Deep Reinforcement Learning and Transfer Learning	A number of electric devices in buildings can be considered as important demand response (DR) resources, for instance, the battery energy storage system (BESS) and the heat, ventilation, and air conditioning (HVAC) systems. The conventional model-based DR methods rely on efficient on-demand computing resources. However, the current buildings suffer from the high cost of computing resources and lack a cost-effective automation system, which becomes the main obstacle to the popularization and implementation of the DR program. Therefore, in this paper, we present a hybrid cloud and edge control strategy for BESS and HVAC based on deep reinforcement learning (DRL). On the cloud infrastructure, the agent learns the control strategy online based on the proposed continuous dueling deep Q-learning (C-DDQN) algorithm, and the learned strategy is distributed to the edge devices for execution. Under this framework, the data-intensive application of cloud computing in real-time DR shows advantages in high processing speed, unlimited data aggregation, fault-tolerant, cost-saving, security, and confidentiality. However, if every controller is trained from the beginning, the cloud resources are wasted to a large extent. Therefore, we propose a transfer deep reinforcement learning methodology to transfer the control strategies between BESS and HVAC units. The transfer learning is realized based on fine-tuning and the proposed Evolving Domain Adaptation Network (EDAN). In case studies, it is verified that the proposed transfer deep reinforcement learning algorithm shows better convergence and learning capability compared with not applying transfer learning technologies. Compared with the conventional model-based method, the proposed methodology speeds up the decision-making time by 105 times.	https://dx.doi.org/10.1109/TCC.2021.3117580	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tateyama2008	Service flow simulation using reinforcement learning models and scene transition nets	Recently, a new academic field, ldquoservice engineeringrdquo has been very actively investigated. However, there are few effective software tools to simulate and evaluate services designed based on the concept of service engineering. In the past, the authors proposed a service flow simulation method using scene transition nets(STN) which is a graphic modeling and simulation method for discrete-continuous hybrid system. However, this method cannot simulate complex service flows including customerspsila decision-making. Nowadays, ldquoneuro economicsrdquo and ldquoneuro marketingrdquo have gotten a lot of attention as new study fields to understand customerspsila behaviors from a viewpoint of brain science. In these studies, it turned out that mechanism of reinforcement learning concerns behavioral selections of customers. In this paper, the authors propose to develop decision-making processes models of customers and to simulate customerspsila behaviors and service flows by using reinforcement learning models and STN.	https://dx.doi.org/10.1109/SICE.2008.4655000	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tateyama2009	A customer decision-making simulation method for design and evaluation of services using value function models		https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449187523&doi=10.1299\%2fkikaic.75.2120&partnerID=40&md5=2059023e51841eb445bf85faab45003d	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Taylor2021	Improving reinforcement learning with human assistance: an argument for human subject studies with HIPPO Gym	Reinforcement learning (RL) is a popular machine learning paradigm for game playing, robotics control, and other sequential decision tasks. However, RL agents often have long learning times with high data requirements because they begin by acting randomly. In order to better learn in complex tasks, we argue that an external teacher can often significantly help the RL agent learn. OpenAI Gym is a common framework for RL research, including a large number of standard environments and agents, making RL research significantly more accessible. This article introduces our new open-source RL framework, the Human Input Parsing Platform for Openai Gym (HIPPO Gym), and the design decisions that went into its creation. The goal of this platform is to facilitate human-RL research, making human-in-the-loop RL more accessible, including learning from demonstrations, learning from feedback, or curriculum learning. In addition, all experiments can be conducted over the internet without any additional software needed on the client's computer, making experiments at scale significantly easier.	https://dx.doi.org/10.1007/s00521-021-06375-y	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Taylor2012	An Autonomous Distal Reward Learning Architecture for Embodied Agents	Distal reward refers to a class of problems where reward is temporally distal from actions that lead to reward. The difficulty for any biological neural system is that the neural activations that caused an agent to achieve reward may no longer be present when the reward is experienced. Therefore in addition to the usual reward assignment problem, there is the additional complexity of rewarding through time based on neural activations that may no longer be present. Although this problem has been thoroughly studied over the years using methods such as reinforcement learning, we are interested in a more biologically motivated neural architectural approach. This paper introduces one such architecture that exhibits rudimentary distal reward learning based on associations of bottom-up visual sensory sequences with bottom-up proprioceptive motor sequences while an agent explores an environment. After sufficient learning, the agent is able to locate the reward through chaining together of top-down motor command sequences. This paper will briefly discuss the details of the neural architecture, the agent-based modeling system in which it is embodied, a virtual Morris water maze environment used for training and evaluation, and a sampling of numerical experiments characterizing its learning properties. (C) 2012 Published by Elsevier B. V. Selection and/or peer-review under responsibility of Program Committee of INNS-WC 2012	https://dx.doi.org/10.1016/j.procs.2012.09.129	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Taylor2022	App-Based Mindfulness Training Predicts Reductions in Smoking Behavior by Engaging Reinforcement Learning Mechanisms: A Preliminary Naturalistic Single-Arm Study	Mindfulness training (MT) has been shown to influence smoking behavior, yet the involvement of reinforcement learning processes as underlying mechanisms remains unclear. This naturalistic, single-arm study aimed to examine slope trajectories of smoking behavior across uses of our app-based MT craving tool for smoking cessation, and whether this relationship would be mediated by the attenuating impact of MT on expected reward values of smoking. Our craving tool embedded in our MT app-based smoking cessation program was used by 108 participants upon the experience of cigarette cravings in real-world contexts. Each use of the tool involved mindful awareness to the experience of cigarette craving, a decision as to whether the participant wanted to smoke or ride out their craving with a mindfulness exercise, and paying mindful attention to the choice behavior and its outcome (contentment levels felt from engaging in the behavior). Expected reward values were computed using contentment levels experienced from the choice behavior as the reward signal in a Rescorla-Wagner reinforcement learning model. Multi-level mediation analysis revealed a significant decreasing trajectory of smoking frequency across MT craving tool uses and that this relationship was mediated by the negative relationship between MT and expected reward values (all ps < 0.001). After controlling for the mediator, the predictive relationship between MT and smoking was no longer significant (p < 0.001 before and p = 0.357 after controlling for the mediator). Results indicate that the use of our app-based MT craving tool is associated with negative slope trajectories of smoking behavior across uses, mediated by reward learning mechanisms. This single-arm naturalistic study provides preliminary support for further RCT studies examining the involvement of reward learning mechanisms underlying app-based mindfulness training for smoking cessation.	https://www.ncbi.nlm.nih.gov/pubmed/35890811	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Teboul2013	Parsing Facades with Shape Grammars and Reinforcement Learning	In this paper, we use shape grammars (SGs) for facade parsing, which amounts to segmenting 2D building facades into balconies, walls, windows, and doors in an architecturally meaningful manner. The main thrust of our work is the introduction of reinforcement learning (RL) techniques to deal with the computational complexity of the problem. RL provides us with techniques such as Q-learning and state aggregation which we exploit to efficiently solve facade parsing. We initially phrase the 1D parsing problem in terms of a Markov Decision Process, paving the way for the application of RL-based tools. We then develop novel techniques for the 2D shape parsing problem that take into account the specificities of the facade parsing problem. Specifically, we use state aggregation to enforce the symmetry of facade floors and demonstrate how to use RL to exploit bottom-up, image-based guidance during optimization. We provide systematic results on the Paris building dataset and obtain state-of-the-art results in a fraction of the time required by previous methods. We validate our method under diverse imaging conditions and make our software and results available online.	https://www.ncbi.nlm.nih.gov/pubmed/23682000	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tekiner2013	Antnet routing algorithm with link evaporation and multiple ant colonies to overcome stagnation problem		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898165131&doi=10.4018\%2f978-1-4666-3652-1.ch012&partnerID=40&md5=af58ed6b92a3cdb5e6c5996da924a99e	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tenenbaum2021	Reverse-engineering core common sense with the tools of probabilistic programs, game-style simulation engines, and inductive program synthesis		https://doi.org/10.1145/3449639.3466000	Excluded	new_screen	E5: Other not a paper,E2: Software engineering is not the problem RL is used for	4
RL4SE	Teng2012	Self-Regulating Action Exploration in Reinforcement Learning	The basic tenet of a learning process is for an agent to learn for only as much and as long as it is necessary. With reinforcement learning, the learning process is divided between exploration and exploitation. Given the complexity of the problem domain and the randomness of the learning process, the exact duration of the reinforcement learning process can never be known with certainty. Using an inaccurate number of training iterations leads either to the non-convergence or the over-training of the learning agent. This work addresses such issues by proposing a technique to self-regulate the exploration rate and training duration leading to convergence efficiently. The idea originates from an intuitive understanding that exploration is only necessary when the success rate is low. This means the rate of exploration should be conducted in inverse proportion to the rate of success. In addition, the change in exploration-exploitation rates alters the duration of the learning process. Using this approach, the duration of the learning process becomes adaptive to the updated status of the learning process. Experimental results from the K-Armed Bandit and Air Combat Maneuver scenario prove that optimal action policies can be discovered using the right amount of training iterations. In essence, the proposed method eliminates the guesswork on the amount of exploration needed during reinforcement learning. (C) 2012 Published by Elsevier B. V. Selection and/or peer-review under responsibility of Program Committee of INNS-WC 2012	https://dx.doi.org/10.1016/j.procs.2012.09.110	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Terway2022	Fast Design Space Exploration of Nonlinear Systems: Part II	Nonlinear system design is often a multiobjective optimization problem involving the search for a design that satisfies a number of predefined constraints. The design space is typically very large since it includes all possible system architectures with different combinations of components composing each architecture. In this article, we address nonlinear system design space exploration through a two-step approach encapsulated in a framework called fast design space exploration of nonlinear systems (ASSENT). In the first step, we use a genetic algorithm to search for system architectures that allow discrete choices for component values or else only component values for a fixed architecture. This step yields a coarse design since the system may or may not meet the target specifications. In contrast to prior works on design space exploration that rely on forward design, we use an inverse design in step 2 to search over a continuous space and fine-tune the component values with the goal of improving the value of the objective function. We use a neural network (NN) to model the system response. The NN is converted into a mixed-integer linear program for active learning to sample component values efficiently. We illustrate the efficacy of ASSENT on problems ranging from nonlinear system design to the design of electrical circuits. Experimental results show that ASSENT achieves the same or better value of the objective function compared to various other optimization techniques for nonlinear system design by up to 53\%. We improve sample efficiency by 6\endash $12\times $ compared to reinforcement learning-based synthesis of electrical circuits.	https://dx.doi.org/10.1109/TCAD.2021.3119274	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Terzioglu2004	A network processor for a learning based routing protocol	Recently, cognitive packet network (CPN) is proposed as an alternative to the IP based network architectures and shows similarity with the discrete active networks. In CPN, there is no routing table, instead reinforcement learning (random neural networks) is used to route packets. CPN routes packets based on QoS, using measurements that are constantly collected by packets and deposited in mailboxes at routers. The applicability of the CPN concept has been demonstrated through several software implementations. However, higher data traffic and increasing packet processing demands require the implementation of this new network architecture in hardware. In this paper, we present a network processor architecture, which supports this learning based protocol.	https://dx.doi.org/10.1109/NEWCAS.2004.1359081	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tesauro2002	Programming backgammon using self-teaching neural nets	TD-Gammon is a neural network that is able to teach itself to play backgammon solely by playing against itself and learning from the results. Starting from random initial play, TD-Gammon's self-teaching methodology results in a surprisingly strong program: without lookahead, its positional judgement rivals that of human experts, and when combined with shallow lookahead, it reaches a level of play that surpasses even the best human players. The success of TD-Gammon has also been replicated by several other programmers; at least two other neural net programs also appear to be capable of superhuman play. Previous papers on TD-Gammon have focused on developing a scientific understanding of its reinforcement learning methodology. This paper views machine learning as a tool in a programmer's toolkit, and considers how it can be combined with other programming techniques to achieve and surpass world-class backgammon play. Particular emphasis is placed on programming shallow-depth search algorithms, and on TD-Gammon's doubling algorithm, which is described in print here for the first time. (C) 2002 Elsevier Science B.V. All rights reserved.	https://dx.doi.org/10.1016/S0004-3702(01)00110-2	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tesauro2002a	Pricing in Agent Economies Using Multi-Agent Q-Learning		https://doi.org/10.1023/A:1015504423309	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tessadori2012	Modular neuronal assemblies embodied in a closed-loop environment: toward future integration of brains and machines	"Behaviors, from simple to most complex, require a two-way interaction with the environment and the contribution of different brain areas depending on the orchestrated activation of neuronal assemblies. In this work we present a new hybrid neuro-robotic architecture based on a neural controller bi-directionally connected to a virtual robot implementing a Braitenberg vehicle aimed at avoiding obstacles. The robot is characterized by proximity sensors and wheels, allowing it to navigate into a circular arena with obstacles of different sizes. As neural controller, we used hippocampal cultures dissociated from embryonic rats and kept alive over Micro Electrode Arrays (MEAs) for 3-8 weeks. The developed software architecture guarantees a bidirectional exchange of information between the natural and the artificial part by means of simple linear coding/decoding schemes. We used two different kinds of experimental preparation: ""random"" and ""modular"" populations. In the second case, the confinement was assured by a polydimethylsiloxane (PDMS) mask placed over the surface of the MEA device, thus defining two populations interconnected via specific microchannels. The main results of our study are: (i) neuronal cultures can be successfully interfaced to an artificial agent; (ii) modular networks show a different dynamics with respect to random culture, both in terms of spontaneous and evoked electrophysiological patterns; (iii) the robot performs better if a reinforcement learning paradigm (i.e., a tetanic stimulation delivered to the network following each collision) is activated, regardless of the modularity of the culture; (iv) the robot controlled by the modular network further enhances its capabilities in avoiding obstacles during the short-term plasticity trial. The developed paradigm offers a new framework for studying, in simplified model systems, neuro-artificial bi-directional interfaces for the development of new strategies for brain machine interaction."	https://www.ncbi.nlm.nih.gov/pubmed/23248586	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Thiery2009	IMPROVEMENTS ON LEARNING TETRIS WITH CROSS ENTROPY	For playing the game of Tetris well, training a controller by the cross-entropy method seems to be a viable way (Szita and Lorincz, 2006; Thiery and Scherrer, 2009). We consider this method to tune an evaluation-based one-piece controller as suggested by Szita and Lorincz and we introduce some improvements. In this context, we discuss the influence of the noise, and we perform experiments with several sets of features such as those introduced by Bertsekas and Tsitsiklis (1996), by Dellacherie (Fahey, 2003), and some original features. This approach leads to a controller that outperforms the previous known results. On the original game of Tetris, we show that with probability 0.95 it achieves at least 91.0, 000 +/- 5\% lines per game on average. On a simplified version of Tetris considered by most research works, it achieves 35, 000; 000 +/- 20\% lines per game on average. We used this approach when we took part with the program BCTS in the 2008 Tetris domain Reinforcement Learning Competition and won the competition.	https://dx.doi.org/10.3233/ICG-2009-32104	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Thiyagarajan2021	Performance investigation of SVR for evaluating Voltage Stability Margin in a Power utility	Voltage stability margin is one of the key parameters to ensure the credibility of the power system in real time. Voltage stability margin indicates the maximum loading capacity of the system i.e, further increase in load will collapse the system voltage, and the system becomes unstable. Hence in real time, the evaluation of voltage stability margin is crucial and requires more significance. In recent times, reinforcement learning algorithms attracts a great deal of interest for determining the voltage stability margin. In this paper, voltage stability analysis on the power system has been accomplished using the Continuous power flow method with Spider SVM regression algorithm. The analysis was carried out in the IEEE 118 bus system, with the help of PSAT software in MATLAB. The result obtained from SVM regression model satisfies the expected model with the best accuracy for many cases.	https://dx.doi.org/10.1109/IPRECON52453.2021.9641018	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Thomaz2008	Teachable robots: Understanding human teaching behavior to build more effective robot learners		https://doi.org/10.1016/j.artint.2007.09.009	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Thomaz2006	Reinforcement Learning with Human Teachers: Understanding How People Want to Teach Robots	While reinforcement learning (RL) is not traditionally designed for interactive supervisory input from a human teacher, several works in both robot and software agents have adapted it for human input by letting a human trainer control the reward signal. In this work, we experimentally examine the assumption underlying these works, namely that the human-given reward is compatible with the traditional RL reward signal. We describe an experimental platform with a simulated RL robot and present an analysis of real-time human teaching behavior found in a study in which untrained subjects taught the robot to perform a new task. We report three main observations on how people administer feedback when teaching a robot a task through reinforcement learning: (a) they use the reward channel not only for feedback, but also for future-directed guidance; (b) they have a positive bias to their feedback -possibly using the signal as a motivational channel; and (c) they change their behavior as they develop a mental model of the robotic learner. In conclusion, we discuss future extensions to RL to accommodate these lessons	https://dx.doi.org/10.1109/ROMAN.2006.314459	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Thompson2020	A non-algorithmic approach to ``programming'' quantum computers via machine learning	Major obstacles remain to the implementation of macroscopic quantum computing: hardware problems of noise, decoherence, and scaling; software problems of error correction; and, most important, algorithm construction. Finding truly quantum algorithms is quite difficult, and many of these genuine quantum algorithms, like Shor's prime factoring or phase estimation, require extremely long circuit depth for any practical application, which necessitates error correction. In contrast, we show that machine learning can be used as a systematic method to construct algorithms, that is, to non-algorithmically ``program'' quantum computers. Quantum machine learning enables us to perform computations without breaking down an algorithm into its gate ``building blocks'', eliminating that difficult step and potentially increasing efficiency by simplifying and reducing unnecessary complexity. In addition, our non-algorithmic machine learning approach is robust to both noise and to decoherence, which is ideal for running on inherently noisy NISQ devices which are limited in the number of qubits available for error correction. We demonstrate this using a fundamentally nonclassical calculation: experimentally estimating the entanglement of an unknown quantum state. Results from this have been successfully ported to the IBM hardware and trained using a hybrid reinforcement learning method.	https://dx.doi.org/10.1109/QCE49297.2020.00019	Included	conflict_resolution		4
RL4SE	Thornton2020	Deep Reinforcement Learning Control for Radar Detection and Tracking in Congested Spectral Environments	This work addresses dynamic non-cooperative coexistence between a cognitive pulsed radar and nearby communications systems by applying nonlinear value function approximation via deep reinforcement learning (Deep RL) to develop a policy for optimal radar performance. The radar learns to vary the bandwidth and center frequency of its linear frequency modulated (LFM) waveforms to mitigate interference with other systems for improved target detection performance while also sufficiently utilizing available frequency bands to achieve a fine range resolution. We demonstrate that this approach, based on the Deep Q-Learning (DQL) algorithm, enhances several radar performance metrics more effectively than policy iteration or sense-and-avoid (SAA) approaches in several realistic coexistence environments. The DQL-based approach is also extended to incorporate Double Q-learning and a recurrent neural network to form a Double Deep Recurrent Q-Network (DDRQN), which yields favorable performance and stability compared to DQL and policy iteration. The practicality of the proposed scheme is demonstrated through experiments performed on a software defined radar (SDRadar) prototype system. Experimental results indicate that the proposed Deep RL approach significantly improves radar detection performance in congested spectral environments compared to policy iteration and SAA.	https://dx.doi.org/10.1109/TCCN.2020.3019605	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tian2022	Edge Intelligence Empowered Dynamic Offloading and Resource Management of MEC for Smart City Internet of Things	Internet of Things (IoT) has emerged as an enabling platform for smart cities. In this paper, the IoT devices' offloading decisions, CPU frequencies and transmit powers joint optimization problem is investigated for a multi-mobile edge computing (MEC) server and multi-IoT device cellular network. An optimization problem is formulated to minimize the weighted sum of the computing pressure on the primary MEC server (PMS), the sum of energy consumption of the network, and the task dropping cost. The formulated problem is a mixed integer nonlinear program (MINLP) problem, which is difficult to solve since it contains strongly coupled constraints and discrete integer variables. Taking the dynamic of the environment into account, a deep reinforcement learning (DRL)-based optimization algorithm is developed to solve the nonconvex problem. The simulation results demonstrate the correctness and the effectiveness of the proposed algorithm.	https://dx.doi.org/10.3390/electronics11060879	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tian2020	Traffic Engineering in Partially Deployed Segment Routing Over IPv6 Network With Deep Reinforcement Learning	Segment Routing (SR) is a source routing paradigm which is widely used in Traffic Engineering (TE). By using SR, a node steers a packet through an ordered list of instructions called segments. By some extensions of interior gateway protocol, SR can be applied to IP/MPLS or IPv6 network without signal protocol. SR over IPv6 (SRv6) is attracting wide attention because of its interoperation ability with IPv6. However, upgrading the existing IPv6 network directly to a full SRv6 one can be difficult, because large-scale equipment replacement or software upgrade may cause economic and technical problems. TE in partially deployed SR network is becoming a hot research topic. In this paper, we propose the TE algorithm Weight Adjustment-SRTE (WA-SRTE) in partially deployed SRv6 network, in which SRv6 capable nodes are dispersedly deployed. Our objective is to minimize the network's maximum link utilization. WA-SRTE converts the TE problem into a Deep Reinforcement Learning problem and optimizes the OSPF weight, SRv6 node deployment and traffic paths simultaneously. Besides, traffic variation is also considered and we use a representative Traffic Matrix (TM) to epitomize the traffic characteristics over a period of time. Experiments demonstrate that with 20\% to 40\% of the SRv6 nodes deployed, we can achieve TE performance as good as in a full SR network for the experiment topologies. The results with WA remarkably outperform the results without it. Our algorithm also gets near-optimal results with changing traffic.	https://dx.doi.org/10.1109/TNET.2020.2987866	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tian2019	Collaborative Power Management Through Knowledge Sharing Among Multiple Devices	Rapidly evolving embedded applications continuously demand more functionalities and better performance under tight energy and thermal budgets, and maintaining high energy efficiency has become a significant design challenge for mobile devices. Learning-based methods are adaptive to dynamic conditions and show great potential for runtime power management. However, with the ever-increasing complexity of both hardware and software, it is a challenging issue for a learning agent to explore the state-action space sufficiently and quickly find an efficient management policy. In this paper, we propose a reinforcement learning-based multi-device collaborative power management approach to address this issue. Multiple devices with different runtime conditions can acquire related knowledge during the learning process. Efficient knowledge sharing among these devices can potentially accelerate the learning process and improve the quality of the learned policies. We integrate the proposed method with dynamic voltage and frequency scaling on the multicore processors in mobile devices. Experimental results on realistic applications show that the collaborative power management can achieve up to a $7 \times$ speedup and 10\% energy reduction compared with state-of-the-art learning-based approaches.	https://dx.doi.org/10.1109/TCAD.2018.2837131	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tjahjadi2010	Robustness analysis of genetic network programming with reinforcement learning			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tlili2021	Risks Analyzing and Management in Software Project Management Using Fuzzy Cognitive Maps with Reinforcement Learning	Many projects fail each year simply because a risk has been misjudged, ignored or unidentified. An essential motivation for analyzing the risk of a project is to inform managers in order to reduce the risk, and therefore the loss of the project. Risk analysis can help identify the best actions that would reduce the risk and assess by how much. In the last decades, the Fuzzy Cognitive Map emerged as a powerful tool for modeling and supervising dynamic interactions in complex systems. There is two ways to construct them, the first way by experts of domain and the second way by learning method based on the historical of data. In this paper, we develop a new learning fuzzy cognitive maps based on a reinforcement learning algorithm so called Q-learning and we propose here a new formulation of kosko causality principle. This connection between fuzzy cognitive maps and reinforcement learning allows us to choose based on the historical of data learning process the best and the most important connections between concepts. In this work, we illustrate the effectiveness of the proposed approach by modeling and studying the analysis of project risk management as an economic decision support system.	https://dx.doi.org/10.31449/inf.v45i1.3104	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tlili2020	Software project risks management: Applying extended fuzzy cognitive maps with reinforcement learning			Included	new_screen		4
RL4SE	Tong2021	DDMTS: A novel dynamic load balancing scheduling scheme under SLA constraints in cloud computing	Cloud computing is a computing method based on the Internet designed to share resources through virtualization technology. For a large number of requests waiting to be processed, task scheduling is used to reasonably allocate computing resources to requests. With the rapid development of computer hardware and software, deep reinforcement learning (DRL) provides a new direction for better solving task scheduling problems. In this paper, we propose a novel DRL-based dynamic load balancing task scheduling algorithm under service-level agreement (SLA) constraints to reduce the load imbalance of virtual machines (VMs) and task rejection rate. First, we use the DRL method to select a suitable VM for the task and then determine whether to execute the task on the selected VM violates the SLA. If the SLA is violated, the task is refused and feedback a negative reward for DRL training; otherwise, the task is received and executed, and feedback a reward according to the balance of the VMs load after the task is executed. Compared with three other task scheduling algorithms applied to randomly generated benchmark and Google real user workload trace benchmark, the proposed algorithm exhibits the best performance in balancing VMs load and reducing the task rejection rate, improving the overall level of cloud computing services. (C) 2020 Elsevier Inc. All rights reserved.	https://dx.doi.org/10.1016/j.jpdc.2020.11.007	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tooranjipour2022	Constructing Safety Barrier Certificates for Unknown Linear Optimal Control Systems		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135823899&doi=10.1109\%2fICCA54724.2022.9831890&partnerID=40&md5=15f1af75174482805c5ecb7f8eb503ca	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Torabi2020	A collaborative agent-based traffic signal system for highly dynamic traffic conditions		https://doi.org/10.1007/s10458-019-09434-w	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Torrado2018	Deep Reinforcement Learning for General Video Game AI	The General Video Game AI (GVGAI) competition and its associated software framework provides a way of benchmarking AI algorithms on a large number of games written in a domain-specific description language. While the competition has seen plenty of interest, it has so far focused on online planning, providing a forward model that allows the use of algorithms such as Monte Carlo Tree Search. In this paper, we describe how we interface GVGAI to the OpenAI Gym environment, a widely used way of connecting agents to reinforcement learning problems. Using this interface, we characterize how widely used implementations of several deep reinforcement learning algorithms fare on a number of GVGAI games. We further analyze the results to provide a first indication of the relative difficulty of these games relative to each other, and relative to those in the Arcade Learning Environment under similar conditions.	https://dx.doi.org/10.1109/CIG.2018.8490422	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Touch2022	A Comparison of an Adaptive Self-Guarded Honeypot with Conventional Honeypots		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131191412&doi=10.3390\%2fapp12105224&partnerID=40&md5=ea0bde34013634c463adb4e28c9bd1d3	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Toumi2021	On using Deep Reinforcement Learning for Multi-Domain SFC placement	Service Function Chaining (SFC) has emerged as a promising technology for 5G and beyond. It leverages Network Function Virtualization (NFV) and Software Defined Networking (SDN) and allows the decomposition of a given service into a set of blocks that successively process data. The SFC placement issue has been extensively studied in the literature, and different solutions have been proposed using mathematical models and heuristics. More recently, Reinforcement Learning (RL) has emerged as a tool for decision-making that allows agents to elaborate policies based on the environment's feedback. In this paper, we study the benefits of using Deep Reinforcement Learning methods for the multi-domain SFC placement problem. We propose a Deep Deterministic Policy Gradient (DDPG) approach, where Linear Physical Programming is employed to generate rewards that reflect the solution's quality in terms of cost and latency. Through our experiments, we are able to demonstrate the efficiency of our approach with results that satisfy the SLA requirements.	https://dx.doi.org/10.1109/GLOBECOM46510.2021.9685367	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Treadgold2002	Proposals: a mechanism that combines reinforcement based learning and contextual information to push information to the user	Proposals are a context-based adaptive information push mechanism for natural language systems. The result of this mechanism is the presentation of relevant information to a user in response to a user interaction. Proposals have two main attributes: they are context-based, and they are adaptive. A proposal is context-based in that the natural language input entered by a user is matched to a corpus of proposals, and only those proposals that match to a certain degree of confidence are returned. Proposals are adaptive in that they incorporate a reinforcement learning mechanism that responds to those proposals selected by users. This proposal mechanism is applied to the Dejima Natural Interaction system, which employs the Adaptive Agent Oriented Software Architecture to extract interpretations from natural language user input. ne benefits of proposals for natural language systems and their possible application areas are discussed.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Triki2015	Hierarchical power management of a system with autonomously power-managed components using reinforcement learning		https://doi.org/10.1016/j.vlsi.2014.06.001	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Trivedi2021	Learning to Synthesize Programs as Interpretable and Generalizable Policies			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Troia2021	On Deep Reinforcement Learning for Traffic Engineering in SD-WAN	The demand for reliable and efficient Wide Area Networks (WANs) from business customers is continuously increasing. Companies and enterprises use WANs to exchange critical data between headquarters, far-off business branches and cloud data centers. Many WANs solutions have been proposed over the years, such as: leased lines, Frame Relay, Multi-Protocol Label Switching (MPLS), Virtual Private Networks (VPN). Each solution positions differently in the trade-off between reliability, Quality of Service (QoS) and cost. Today, the emerging technology for WAN is Software-Defined Wide Area Networking (SD-WAN) that introduces the Software-Defined Networking (SDN) paradigm into the enterprise-network market. SD-WAN can support differentiated services over public WAN by dynamically reconfiguring in real-time network devices at the edge of the network according to network measurements and service requirements. On the one hand, SD-WAN reduces the high costs of guaranteed QoS WAN solutions (as MPLS), without giving away reliability in practical scenarios. On the other, it brings numerous technical challenges, such as the implementation of Traffic Engineering (TE) methods. TE is critically important for enterprises not only to efficiently orchestrate network traffic among the edge devices, but also to keep their services always available. In this work, we develop different kind of TE algorithms with the aim of improving the performance of an SD-WAN based network in terms of service availability. We first evaluate the performance of baseline TE algorithms. Then, we implement different deep Reinforcement Learning (deep-RL) algorithms to overcome the limitations of the baseline approaches. Specifically, we implement three kinds of deep-RL algorithms, which are: policy gradient, TD- ? and deep Q-learning. Results show that a deep-RL algorithm with a well-designed reward function is capable of increasing the overall network availability and guaranteeing network protection and restoration in SD-WAN.	https://dx.doi.org/10.1109/JSAC.2020.3041385	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Trujillo2020	Does Neuron Coverage Matter for Deep Reinforcement Learning? A Preliminary Study		https://doi.org/10.1145/3387940.3391462	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tsay2011	Evolving Intelligent Mario Controller by Reinforcement Learning	Artificial Intelligence for computer games is an interesting topic which attracts intensive attention recently. In this context, Mario AI Competition modifies a Super Mario Bros game to be a benchmark software for people who program AI controller to direct Mario and make him overcome the different levels. This competition was handled in the IEEE Games Innovation Conference and the IEEE Symposium on Computational Intelligence and Games since 2009. In this paper, we study the application of Reinforcement Learning to construct a Mario AI controller that learns from the complex game environment. We train the controller to grow stronger for dealing with several difficulties and types of levels. In controller developing phase, we design the states and actions cautiously to reduce the search space, and make Reinforcement Learning suitable for the requirement of online learning.	https://dx.doi.org/10.1109/TAAI.2011.54	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tsingenopoulos2022	Adaptive Malware Control: Decision-Based Attacks in the Problem Space of Dynamic Analysis		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134542182&doi=10.1145\%2f3494110.3528243&partnerID=40&md5=c14bb920e21063da9f1b57a88182fc09	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tu2019	A Routing Optimization Method for Software-Defined SGIN Based on Deep Reinforcement Learning	As space networks become more and more important, the Space-ground Integration network (SGIN) has received unprecedented attention. However, dynamic changes of topology and link status of satellite networks bring many challenges to routing optimization in the SGIN. Traditional routing optimization methods do not perform well, as they do not consider changes of topology and link status, as well as the association between flows. Since the Machine Learning (ML) technologies have shown significant advantages in dynamic routing optimization, we proposed a Machine Learning-based Space-ground Integration Networking (ML-SSGIN) framework that combines Software-Defined Networking (SDN) technologies to solve this challenge. To evaluate the feasibility of the proposed framework, the Deep Deterministic Policy Gradient (DDPG), a Deep Reinforcement Learning (DRL) algorithm, is deployed to perform routing optimization, which can make routing decisions based on real-time link status. In particular, we utilize a neural network that integrates Long Short-Term Memory Network (LSTM) and Dense layers for its actor and critic part to improve perceptual capabilities of contextual correlations between flows. We compared the proposed DDPG neural network with the one only having the Dense layers. The results show that the proposed architecture is feasible and effective. What's more, compared to Open Shortest Path First (OSPF) algorithm, our proposed routing optimization method can adapt to continuously change flows, and link status, which improves end-to-end throughput and latency.	https://dx.doi.org/10.1109/GCWkshps45667.2019.9024680	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tufano2022	Using reinforcement learning for load testing of video games		https://doi.org/10.1145/3510003.3510625	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tunnermann2019	Deep reinforcement learning for coherent beam combining applications	Coherent beam combining is a method to scale the peak and average power levels of laser systems beyond the limit of a single emitter system. This is achieved by stabilizing the relative optical phase of multiple lasers and combining them. We investigated the use of reinforcement learning (RL) and neural networks (NN) in this domain. Starting from a randomly initialized neural network, the system converged to a phase stabilization policy, which was comparable to a software implemented proportional-integral-derivative (PID) controller. Furthermore, we demonstrate the capability of neural networks to predict relative phase noise, which is one potential advantage of this method. (C) 2019 Optical Society of America under the terms of the OSA Open Access Publishing Agreement	https://www.ncbi.nlm.nih.gov/pubmed/31510315	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tunyasuvunakool2020	dm_control: Software and tasks for continuous control	The dm_control software package is a collection of Python libraries and task suites for reinforcement learning agents in an articulated-body simulation. Infrastructure includes a wrapper for the MuJoCo physics engine and libraries for procedural model manipulation and task authoring. Task suites include the Control Suite, a set of standardized tasks intended to serve as performance benchmarks, a locomotion framework and task families, and a set of manipulation tasks with a robot arm and snap-together bricks. An adjunct tech report and interactive tutorial are also provided.	https://dx.doi.org/10.1016/j.simpa.2020.100022	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Turan2020	Dynamic pricing and fleet management for electric autonomous mobility on demand systems	The proliferation of ride sharing systems is a major drive in the advancement of autonomous and electric vehicle technologies. This paper considers the joint routing, battery charging, and pricing problem faced by a profit-maximizing transportation service provider that operates a fleet of autonomous electric vehicles. We first establish the static planning problem by considering time-invariant system parameters and determine the optimal static policy. While the static policy provides stability of customer queues waiting for rides even if consider the system dynamics, we see that it is inefficient to utilize a static policy as it can lead to long wait times for customers and low profits. To accommodate for the stochastic nature of trip demands, renewable energy availability, and electricity prices and to further optimally manage the autonomous fleet given the need to generate integer allocations, a real-time policy is required. The optimal real-time policy that executes actions based on full state information of the system is the solution of a complex dynamic program. However, we argue that it is intractable to exactly solve for the optimal policy using exact dynamic programming methods and therefore apply deep reinforcement learning to develop a near-optimal control policy. The two case studies we conducted in Manhattan and San Francisco demonstrate the efficacy of our real-time policy in terms of network stability and profits, while keeping the queue lengths up to 200 times less than the static policy.	https://dx.doi.org/10.1016/j.trc.2020.102829	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Turner2022	Analyzing Multi-Agent Reinforcement Learning and Coevolution in Cybersecurity	Cybersecurity simulations can offer deep insights into the behavior of agents in the battle to secure computer systems. We build on existing work modeling the competition between an attacker and defender on a network architecture in a zero-sum game using a graph database linking cybersecurity attack patterns, vulnerabilities, and software. We apply coevolution to this challenging environment, and in a novel modeling approach for this problem, interpret each population as a distribution over fixed strategies to form a mixed strategy Nash equilibrium. We compare the results to solutions generated by multi-agent reinforcement learning and show that evolutionary methods demonstrate a considerable degree of robustness to parameter misspecification in this environment.	https://dx.doi.org/10.1145/3512290.3528844	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tutsoy2015	CPG BASED RL ALGORITHM LEARNS TO CONTROL OF A HUMANOID ROBOT LEG	Autonomous humanoid robots equipped with learning capabilities are able to learn tasks such as sitting down, standing up, balancing, walking and running. In this paper, central pattern generator (CPG) based reinforcement learning (RL) algorithm is applied to a robot leg with 3-links to balance it at upright by reducing dimensionality of the learning problem from 6 to 2. MapleSim is used for the leg modelling and this model is combined with the CPG based RL algorithm by utilizing Modelica and Maple software properties. Maple multi-body analysis template and Modelica custom component template allow symbolic inverse kinematics solution for the leg to be obtained. Thus, time and information lost in case of using a numerical solution are eliminated. The learning results show that the value function is maximized, temporal difference error is significantly reduced to zero and the leg is balanced at upright.	https://dx.doi.org/10.2316/Journal.206.2015.2.206-4185	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tutsoy2017	Learning to balance an NAO robot using reinforcement learning with symbolic inverse kinematic	An autonomous humanoid robot (HR) with learning and control algorithms is able to balance itself during sitting down, standing up, walking and running operations, as humans do. In this study, reinforcement learning (RL) with a complete symbolic inverse kinematic (IK) solution is developed to balance the full lower body of a three-dimensional (3D) NAO HR which has 12 degrees of freedom. The IK solution converts the lower body trajectories, which are learned by RL, into reference positions for the joints of the NAO robot. This reduces the dimensionality of the learning and control problems since the IK integrated with the RL eliminates the need to use whole HR states. The IK solution in 3D space takes into account not only the legs but also the full lower body; hence, it is possible to incorporate the effect of the foot and hip lengths on the IK solution. The accuracy and capability of following real joint states are evaluated in the simulation environment. MapleSim is used to model the full lower body, and the developed RL is combined with this model by utilizing Modelica and Maple software properties. The results of the simulation show that the value function is maximized, temporal difference error is reduced to zero, the lower body is stabilized at the upright, and the convergence speed of the RL is improved with use of the symbolic IK solution.	https://dx.doi.org/10.1177/0142331216645176	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Tuyls2012	Multiagent learning: Basics, challenges, and prospects		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861480021&doi=10.1609\%2faimag.v33i3.2426&partnerID=40&md5=0eddda9e2d49b70476a726f3fae2d3f2	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Uchibe2008	2008 Special Issue: Finding intrinsic rewards by embodied evolution and constrained reinforcement learning	Understanding the design principle of reward functions is a substantial challenge both in artificial intelligence and neuroscience. Successful acquisition of a task usually requires not only rewards for goals, but also for intermediate states to promote effective exploration. This paper proposes a method for designing 'intrinsic' rewards of autonomous agents by combining constrained policy gradient reinforcement learning and embodied evolution. To validate the method, we use Cyber Rodent robots, in which collision avoidance, recharging from battery packs, and 'mating' by software reproduction are three major 'extrinsic' rewards. We show in hardware experiments that the robots can find appropriate 'intrinsic' rewards for the vision of battery packs and other robots to promote approach behaviors.	https://doi.org/10.1016/j.neunet.2008.09.013	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Uchibe2008a	Finding intrinsic rewards by embodied evolution and constrained reinforcement learning	Understanding the design principle of reward functions is a substantial challenge both in artificial intelligence and neuroscience. Successful acquisition of a task usually requires not only rewards for goals, but also for intermediate states to promote effective exploration. This paper proposes a method for designing 'intrinsic' rewards of autonomous agents by combining constrained policy gradient reinforcement learning and embodied evolution. To validate the method, we use Cyber Rodent robots, in which collision avoidance, recharging from battery packs, and 'mating' by software reproduction are three major 'extrinsic' rewards. We show in hardware experiments that the robots can find appropriate 'intrinsic' rewards for the vision of battery packs and other robots to promote approach behaviors. (C) 2008 Elsevier Ltd. All rights reserved.	https://www.ncbi.nlm.nih.gov/pubmed/19013054	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Uchibe2011	Evolution of rewards and learning mechanisms in Cyber Rodents		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931829034&doi=10.1017\%2fCBO9780511994838.007&partnerID=40&md5=e0edadde8476d158e490d9903b863c1e	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ugurlu2022	Sim-to-Real Deep Reinforcement Learning for Safe End-to-End Planning of Aerial Robots	In this study, a novel end-to-end path planning algorithm based on deep reinforcement learning is proposed for aerial robots deployed in dense environments. The learning agent finds an obstacle-free way around the provided rough, global path by only depending on the observations from a forward-facing depth camera. A novel deep reinforcement learning framework is proposed to train the end-to-end policy with the capability of safely avoiding obstacles. The Webots open-source robot simulator is utilized for training the policy, introducing highly randomized environmental configurations for better generalization. The training is performed without dynamics calculations through randomized position updates to minimize the amount of data processed. The trained policy is first comprehensively evaluated in simulations involving physical dynamics and software-in-the-loop flight control. The proposed method is proven to have a 38\% and 50\% higher success rate compared to both deep reinforcement learning-based and artificial potential field-based baselines, respectively. The generalization capability of the method is verified in simulation-to-real transfer without further training. Real-time experiments are conducted with several trials in two different scenarios, showing a 50\% higher success rate of the proposed method compared to the deep reinforcement learning-based baseline.	https://dx.doi.org/10.3390/robotics11050109	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ulam2008	Combining model-based meta-reasoning and reinforcement learning for adapting game-playing agents			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ullah2020	UAVs joint optimization problems and machine learning to improve the 5G and Beyond communication	Recently, unmanned aerial vehicles (UAVs) have gained notable interest in various applications such as wireless coverage, aerial surveillance, precision agriculture, construction, power lines monitoring and blood delivery, etc. The UAVs implicit attributes e.g., rapid deployment, quick mobility, increase in flight duration, improvements in payload capacities, etc. , place it as an effective candidate for many applications in 5G and Beyond communications. The UAVs-assisted next-generation communications are determined to be highly influenced by various techniques and technologies like artificial intelligence (AI), machine learning (ML), deep reinforcement learning (DRL), mobile edge computing (MEC), and software-defined networks (SDN). In this article, we develop a review to investigate the UAVs joint optimization problems to enhance system efficiency. We classify the joint optimization problems based on the number of parameters used in proposed optimization problems. Moreover, we explore the impact of AI, ML, DRL, MEC, and SDN over UAVs joint optimization problems and present future research challenges and directions.	https://dx.doi.org/10.1016/j.comnet.2020.107478	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ulurrasyadi2021	Walking Gait Learning for ``T-FLoW'' Humanoid Robot Using Rule-Based Learning	This work presents a fast and simple learning algorithm for humanoid robot walking gait cases. The standard method of reinforcement learning takes too much time to learn a stable walking gait. Thus, we propose a rule-based learning method that has never been used in this kind of walking gait learning case. We implement our method in a simplified TFLoW humanoid robot model in simulation software CoppeliaSim. The result shows by using our proposed method, T-FLoW humanoid robot can walk for 200 steps after taking the learning process for about 800 episodes and has a better walking performance than the classical pattern generation for planning a walking gait motion.	https://dx.doi.org/10.1109/IES53407.2021.9593960	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Upadhyaya2020	Cross-Layer Band Selection and Routing Design for Diverse Band-Aware DSA Networks	As several new spectrum bands are opening up for shared use, a new paradigm of Diverse Band-aware Dynamic Spectrum Access (d-DSA) has emerged. d-DSA equips a secondary device with software defined radios (SDRs) and utilize whitespaces (or idle channels) in multiple bands, including but not limited to TV, LTE, Citizen Broadband Radio Service (CBRS), unlicensed ISM. In this paper, we propose a decentralized, online multi-agent reinforcement learning based cross-layer BAnd selection and Routing Design (BARD) for such d-DSA networks. BARD not only harnesses whitespaces in multiple spectrum bands, but also accounts for unique electro-magnetic characteristics of those bands to maximize the desired quality of service (QoS) requirements of heterogeneous message packets; while also ensuring no harmful interference to the primary users in the utilized band. Our extensive experiments demonstrate that BARD outperforms the baseline dDSAaR algorithm in terms of message delivery ratio, however, at a relatively higher network latency, for varying number of primary and secondary users. Furthermore, BARD greatly outperforms its single-band DSA variants in terms of both the metrics in all considered scenarios.	https://dx.doi.org/10.1109/GLOBECOM42002.2020.9322500	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Vadim2018	Simultaneous use of imitation learning and reinforcement learning in artificial intelligence development for video games			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Vahidi2020	New demand response platform with machine learning and data analytics		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088733483&doi=10.1007\%2f978-3-030-31399-9_5&partnerID=40&md5=ccea1563e600f6ae0c3cbb6c86ce8b64	Excluded	new_screen	E3: Only conceptual results are reported,E2: Software engineering is not the problem RL is used for	4
RL4SE	Vahidinia2022	Mitigating Cold Start Problem in Serverless Computing: A Reinforcement Learning Approach	Serverless computing has revolutionized the world of cloud-based and event-driven applications with the introduction of Function-as-a-Service (FaaS) as the latest cloud computing model. This computational model increases the level of abstraction from the infrastructure and breaks the program into small units called functions. Thus, it brings benefits such as ease of development, saving resources, and reducing product launch time for enterprises and developers. Thanks to the scale-to-zero feature of this computational model, idle functions with no traffic will be depreciated from memory. However, this cost-saving approach adversely impacts delay leading to the cold start problem. Unfortunately, the existing solutions to alleviate the cold start delay are not resource-efficient as they follow a fixed policy over time. Thereby, this paper proposes a novel two-layer adaptive approach to tackle this issue. The first layer utilizes a holistic reinforcement learning algorithm to discover the function invocation patterns over time for determining the best time to keep the containers warm. The second layer is designed based on a Long Short-Term Memory (LSTM) to predict the function invocation times in the future to determine the required pre-warmed containers. The experimental results on the Openwhisk platform show that the proposed approach reduces the memory consumption by 12.73\% and improves the execution invocations on pre-warmed containers by 22.65\% compared to the Openwhisk platform.	https://dx.doi.org/10.1109/JIOT.2022.3165127	Excluded	new_screen	E3: Only conceptual results are reported,E2: Software engineering is not the problem RL is used for	4
RL4SE	deMeent2016	Black-box policy search with probabilistic programs			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	VanHeeswijk2020	Deep Reinforcement Learning in Linear Discrete Action Spaces		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103874633&doi=10.1109\%2fWSC48552.2020.9384078&partnerID=40&md5=17402a9a694a0942fbcf46bf3058fd9a	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Hooren2022	DRLNPS: A deep reinforcement learning network path switching solution	This paper proposes a solution to the problem of switching between different network paths. We choose to switch between multiprotocol label switching (MPLS) and software-defined wide area networking (SD-WAN) connections specifically as they are the mainstream currently. The solution should maintain a service license agreement (SLA) while choosing SD-WAN as long as possible to save cost. Therefore, a deep reinforcement learning solution is proposed that predicts when to switch based on bandwidth availability and quality of service (QoS) parameters like jitter and delay. Results show that double deep Q learning in combination with these parameters are suitable to make a sophisticated decision on link switching between MPLS and SD-WAN.	https://dx.doi.org/10.1002/dac.5192	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Vanrompay2008	Learning-based coordination of distributed component deployment		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042853045&doi=10.14279\%2ftuj.eceasst.11.131.129&partnerID=40&md5=5d0b3f83dbf3768059215acf6d388deb	Included	new_screen		4
RL4SE	Vasquez2014	Inverse Reinforcement Learning algorithms and features for robot navigation in crowds: An experimental comparison	For mobile robots which operate in human populated environments, modeling social interactions is key to understand and reproduce people's behavior. A promising approach to this end is Inverse Reinforcement Learning (IRL) as it allows to model the factors that motivate people's actions instead of the actions themselves. A crucial design choice in IRL is the selection of features that encode the agent's context. In related work, features are typically chosen ad hoc without systematic evaluation of the alternatives and their actual impact on the robot's task. In this paper, we introduce a new software framework to systematically investigate the effect features and learning algorithms used in the literature. We also present results for the task of socially compliant robot navigation in crowds, evaluating two different IRL approaches and several feature sets in large-scale simulations. The results are benchmarked according to a proposed set of objective and subjective performance metrics.	https://dx.doi.org/10.1109/IROS.2014.6942731	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Vazquez-Canteli2019	Reinforcement learning for demand response: A review of algorithms and modeling techniques		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056655519&doi=10.1016\%2fj.apenergy.2018.11.002&partnerID=40&md5=8d3e6e80d9ffce98f57fd5f365f104f3	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Veanes2006	Online testing with reinforcement learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-43049140624&doi=10.1007\%2f11940197_16&partnerID=40&md5=f9a8a243293c972d33d7a4851d03b72e	Included	conflict_resolution		4
RL4SE	Venkatesan2019	Predictive learning model for cognitive radio using reinforcement learning			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Verma2019	Verifiable and Interpretable Reinforcement Learning through Program Synthesis	We study the problem of generating interpretable and verifiable policies for Reinforcement Learning (RL). Unlike the popular Deep Reinforcement Learning (DRL) paradigm, in which the policy is represented by a neural network, the aim of this work is to find policies that can be represented in high-level programming languages. Such programmatic policies have several benefits, including being more easily interpreted than neural networks, and being amenable to verification by scalable symbolic methods. The generation methods for programmatic policies also provide a mechanism for systematically using domain knowledge for guiding the policy search. The interpretability and verifiability of these policies provides the opportunity to deploy RL based solutions in safety critical environments. This thesis draws on, and extends, work from both the machine learning and formal methods communities.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Verma2019a	Imitation-Projected Programmatic Reinforcement Learning	We study the problem of programmatic reinforcement learning, in which policies are represented as short programs in a symbolic language. Programmatic policies can be more interpretable, generalizable, and amenable to formal verification than neural policies; however, designing rigorous learning approaches for such policies remains a challenge. Our approach to this challenge-a meta-algorithm called PROPEL-is based on three insights. First, we view our learning task as optimization in policy space, modulo the constraint that the desired policy has a programmatic representation, and solve this optimization problem using a form of mirror descent that takes a gradient step into the unconstrained policy space and then projects back onto the constrained space. Second, we view the unconstrained policy space as mixing neural and programmatic representations, which enables employing state-of-the-art deep policy gradient approaches. Third, we cast the projection step as program synthesis via imitation learning, and exploit contemporary combinatorial methods for this task. We present theoretical convergence results for PROPEL and empirically evaluate the approach in three continuous control domains. The experiments show that PROPEL can significantly outperform state-of-the-art approaches for learning programmatic policies.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Verma2018	Programmatically Interpretable Reinforcement Learning	"We present a reinforcement learning framework, called Programmatically Interpretable Reinforcement Learning (PIRL), that is designed to generate interpretable and verifiable agent policies. Unlike the popular Deep Reinforcement Learning (DRL) paradigm, which represents policies by neural networks, PIRL represents policies using a high-level, domain-specific programming language. Such programmatic policies have the benefits of being more easily interpreted than neural networks, and being amenable to verification by symbolic methods. We propose a new method, called Neurally Directed Program Search (NDPs), for solving the challenging nonsmooth optimization problem of finding a programmatic policy with maximal reward. NDPS works by first learning a neural policy network using DRL, and then performing a local search over programmatic policies that seeks to minimize a distance from this neural ""oracle"". We evaluate NDPS on the task of learning to drive a simulated car in the TORCS car-racing environment. We demonstrate that NDPS is able to discover human-readable policies that pass some significant performance bars. We also show that PIRL policies can have smoother trajectories, and can be more easily transferred to environments not encountered during training, than corresponding policies discovered by DRL."		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Verner2019	Exposing Robot Learning to Students in Augmented Reality Experience	This paper considers a learning process in which the student teaches the robot new tasks, such as lifting unknown weights, via reinforcement learning procedure. Using CAD software, we ran virtual trials using the robot's digital twin in place of physical robot trials. When performing the task, the robot measures and sends the value of the weight to an IoT controller implemented on the ThingWorx platform and receives parameters of the optimal posture found through the virtual trials. When we presented the robot learning process to high school students they had difficulty fully understanding the robot's dynamics and selection of posture parameters. To address this difficulty, we developed an augmented reality interface which allows students to visualize robot postures on the digital twin and monitor the change in parameters (such as the center of gravity) measured by virtual sensors. The student can select a weightlifting posture and control the robot to implement it.	https://dx.doi.org/10.1007/978-3-319-95678-7_67	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Versaw2021	Modular Reinforcement Learning Framework for Learners and Educators		https://doi.org/10.1145/3472538.3472583	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Vieira2017	Dyna: toward a self-optimizing declarative language for machine learning applications		https://doi.org/10.1145/3088525.3088562	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Vigueras2017	Towards Automatic Learning of Heuristics for Mechanical Transformations of Procedural Code	The current trends in next-generation exascale systems go towards integrating a wide range of specialized (co-) processors into traditional supercomputers. Due to the efficiency of heterogeneous systems in terms of Watts and FLOPS per surface unit, opening the access of heterogeneous platforms to a wider range of users is an important problem to be tackled. However, heterogeneous platforms limit the portability of the applications and increase development complexity due to the programming skills required. Program transformation can help make programming heterogeneous systems easier by defining a step-wise transformation process that translates a given initial code into a semantically equivalent final code, but adapted to a specific platform. Program transformation systems require the definition of efficient transformation strategies to tackle the combinatorial problem that emerges due to the large set of transformations applicable at each step of the process. In this paper we propose a machine learning-based approach to learn heuristics to define program transformation strategies. Our approach proposes a novel combination of reinforcement learning and classification methods to efficiently tackle the problems inherent to this type of systems. Preliminary results demonstrate the suitability of this approach.	https://dx.doi.org/10.4204/EPTCS.237.4	Included	new_screen		4
RL4SE	Viswanadha2019	ATARI: Autonomous Testing and Real-Time Intelligence - A Framework for Autonomously Testing Modern Applications	The emergence of modern web frameworks, microservices, and cloud architectures requires testers to generate and maintain test cases, scripts, and data in short agile sprints, sometimes in as little as a few hours. It is impossible to produce these assets in a comprehensive manner using legacy DevOps approaches that largely depend on the subjective judgement of test engineers. In this paper, we demonstrate the implementation of ATARI, an autonomous testing framework that uses a combination of search algorithms, page-rank heuristics, and reinforcement learning techniques to radically improve the completeness and velocity of testing to meet the needs of modern software engineering practices.	https://dx.doi.org/10.1109/AITest.2019.000-8	Excluded	conflict_resolution	E5: Other not a paper,E5: Other not a paper	4
RL4SE	Vu2019	Joint Path Selection and Rate Allocation Framework for 5G Self-Backhauled mm-wave Networks	Owing to severe path loss and unreliable transmission over a long distance at higher frequency bands, this paper investigates the problem of path selection and rate allocation for multi-hop self-backhaul millimeter-wave (mm-wave) networks. Enabling multi-hop mm-wave transmissions raises a potential issue of increased latency, and thus, this paper aims at addressing the fundamental questions: how to select the best multi-hop paths and how to allocate rates over these paths subject to latency constraints? In this regard, a new system design, which exploits multiple antenna diversity, mm-wave bandwidth, and traffic splitting techniques, is proposed to improve the downlink transmission. The studied problem is cast to as a network utility maximization, subject to the upper delay bound constraint, network stability, and network dynamics. By leveraging stochastic optimization, the problem is decoupled into: 1) path selection and 2) rate allocation sub-problems, whereby a framework which selects the best paths is proposed using reinforcement learning techniques. Moreover, the rate allocation is a non-convex program, which is converted into a convex one by using the successive convex approximation method. Via mathematical analysis, the comprehensive performance analysis and convergence proof are provided for the proposed solution. The numerical results show that the proposed approach ensures reliable communication with a guaranteed probability of up to 99.9999\% and reduces latency by 50.64\% and 92.9\% as compared to baseline models. Furthermore, the results showcase the key tradeoff between latency and network arrival rate.	https://dx.doi.org/10.1109/TWC.2019.2904275	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Vyshnav2022	Reinforcement Learning Based Wind Farm Layout Optimization	Wind farm layout optimization is a the major decision factor for maximum utilisation of wind energy for large scale wind farms. As more methods are being researched to reduce losses in the wind power plants, none more effective in reducing the over all loss than the loss due to wake effect. The arrangement of location of the turbines influence the power generation as well as levelized cost of energy. In order to minimise over all loss of the power plant, effective positioning of the turbines is needed. In this study, a novel turbine layout optimization method utilizing reinforcement learning is implemented for a wind farm. Turbulence intensity and the deficit velocity due to wake loss from Gaussian wake effect is used as the input for the model. The simulated results from the wind resource assessment software, WAsP suggests that the proposed method is effective for the number of turbines used in the study.	https://dx.doi.org/10.1109/ICEARS53579.2022.9752054	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wachi2019	Failure-Scenario Maker for Rule-Based Agent using Multi-agent Adversarial Reinforcement Learning and its Application to Autonomous Driving	We examine the problem of adversarial reinforcement learning for multi-agent domains including a rule-based agent. Rule-based algorithms are required in safety-critical applications for them to work properly in a wide range of situations. Hence, every effort is made to find failure scenarios during the development phase. However, as the software becomes complicated, finding failure cases becomes difficult. Especially in multi-agent domains, such as autonomous driving environments, it is much harder to find useful failure scenarios that help us improve the algorithm. We propose a method for efficiently finding failure scenarios; this method trains the adversarial agents using multi-agent reinforcement learning such that the tested rule-based agent fails. We demonstrate the effectiveness of our proposed method using a simple environment and autonomous driving simulator.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Walker2022	Design of threshold-based energy storage control policy based on rule-constrained two-stage stochastic program	Assuming that a residential electricity consumer is equipped with solar photovoltaic panels integrated with energy storage while participating in a demand response program with time-varying price, this study focuses on developing a proper control policy for energy storage operations to minimize consumer electricity cost. In particular, this study intends to develop a threshold-based control policy that is designed to adjust the energy storage levels by charging and discharging energy storage to ensure that the energy storage levels are bounded from below by the thresholds across discrete time periods. In this case, the thresholds will be derived so that consumers are able to avoid the peak electricity rate and utilize more solar power generation while meeting the electricity demand. Specifically, the set of rule constraints is developed to enforce logical conditions to energy storage operations under the proposed control policy and integrated with the two-stage stochastic program to find proper thresholds using a real historical data set. Once the thresholds are obtained by solving the proposed rule-constrained two-stage stochastic program, the proposed control policy can be implemented to control energy storage operations. Compared to the existing approaches, the proposed control policy has merits in terms of practical application. Numerical experiments conducted with various residential house data show that the proposed threshold-based control policies result in 1\%-4\% gap compared to off-line optimal operations in terms of total energy cost for various residential house data. Also, compared to the reinforcement learning-based approaches, the proposed control policies show better performance with less computational time required to train the models.	https://dx.doi.org/10.1016/j.ijepes.2021.107798	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Walsh2010	Generalizing apprenticeship learning across hypothesis classes			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wan2017	A self-adaptation framework for dealing with the complexities of software changes	Software Self-adaption (SA) is a promising technology to reduce the cost of software maintenance. However, the complexities of software changes such as various and producing different effects, interrelated and occurring in an unpredictable context challenge the SA. The current methods may be insufficient to provide the required self-adaptation abilities to handle all the existent complexities of changes. Thus, this paper presents a self-adaptation framework which can provide a multi-agent system for self-adaptation control to equip software system with the required adaptation abilities. we employ the hybrid control mode and construct a two-layer MAPE control structure to deal with changes hierarchically. Multi-Objective Evolutionary Algorithm and Reinforcement Learning are applied to plan an adequate strategy for these changes. Finally, in order to validate the framework, we exemplify these ideas with a meta-Search system and confirm the required self-adaptive ability.	https://dx.doi.org/10.1109/ICSESS.2017.8342969	Included	new_screen		4
RL4SE	Wan2022	Interactive Reinforcement Learning-Based API Recommendation	API recommendation is an effective way to improve software productivity. It has become an important research topic in recent years, and therefore attracted lots of attention. Recently, researchers have proposed a great deal of API recommendation approaches to help developers find suitable APIs from a large number of candidates during the development process. However, most of these approaches do not effectively integrate dynamic human-machine interaction information into the recommendation session. In this article, we propose an API recommendation approach, ARENA (API Recommendation using intEractive reiNforcement leArning), which leverages reinforcement learning based on markov decision process (MDP) to improve the recommendation performance. By integrating the user interaction into the loop of MDP, ARENA enables users to continuously interact with the model. In this way, the user interaction information is used for optimizing the parameter of the model. In turn, the approach can provide personalized API recommendation result to end users.	https://dx.doi.org/10.1109/CTISC54888.2022.9849805	Included	new_screen		4
RL4SE	Wan2018	Improving Automatic Source Code Summarization via Deep Reinforcement Learning	Code summarization provides a high level natural language description of the function performed by code, as it can benefit the software maintenance, code categorization and retrieval. To the best of our knowledge, most state-of-the-art approaches follow an encoder-decoder framework which encodes the code into a hidden space and then decode it into natural language space, suffering from two major drawbacks: a) Their encoders only consider the sequential content of code, ignoring the tree structure which is also critical for the task of code summarization; b) Their decoders are typically trained to predict the next word by maximizing the likelihood of next ground-truth word with previous ground-truth word given. However, it is expected to generate the entire sequence from scratch at test time. This discrepancy can cause an exposure bias issue, making the learnt decoder suboptimal. In this paper, we incorporate an abstract syntax tree structure as well as sequential content of code snippets into a deep reinforcement learning framework (i.e., actor-critic network). The actor network provides the confidence of predicting the next word according to current state. On the other hand, the critic network evaluates the reward value of all possible extensions of the current state and can provide global guidance for explorations. We employ an advantage reward composed of BLEU metric to train both networks. Comprehensive experiments on a real-world dataset show the effectiveness of our proposed model when compared with some state-of-the-art methods.	https://dx.doi.org/10.1145/3238147.3238206	Included	new_screen		4
RL4SE	Wan2022a	Circuit and System Technologies for Energy-Efficient Edge Robotics: (Invited Paper)	As we march towards the age of ubiquitous intelligence, we note that AI and intelligence are progressively moving from the cloud to the edge. The success of Edge-AI is pivoted on innovative circuits and hardware that can enable inference and limited learning in resource-constrained edge autonomous systems. This paper introduces a series of ultra-low-power accelerator and system designs on enabling the intelligence in edge robotic platforms, including reinforcement learning neuro-morphic control, swarm intelligence, and simultaneous mapping and localization. We put an emphasis on the impact of the mixed-signal circuit, neuro-inspired computing system, benchmarking and software infrastructure, as well as algorithm-hardware co-design to realize the most energy-efficient Edge-AI ASICs for the next-generation intelligent and autonomous systems.	https://dx.doi.org/10.1109/ASP-DAC52403.2022.9712531	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wan2019	Model-Free Real-Time EV Charging Scheduling Based on Deep Reinforcement Learning	Driven by the recent advances in electric vehicle (EV) technologies, EVs have become important for smart grid economy. When EVs participate in demand response program which has real-time pricing signals, the charging cost can be greatly reduced by taking full advantage of these pricing signals. However, it is challenging to determine an optimal charging strategy due to the existence of randomness in traffic conditions, user's commuting behavior, and the pricing process of the utility. Conventional model-based approaches require a model of forecast on the uncertainty and optimization for the scheduling process. In this paper, we formulate this scheduling problem as a Markov Decision Process (MDP) with unknown transition probability. A model-free approach based on deep reinforcement learning is proposed to determine the optimal strategy for this problem. The proposed approach can adaptively learn the transition probability and does not require any system model information. The architecture of the proposed approach contains two networks: a representation network to extract discriminative features from the electricity prices and a Q network to approximate the optimal action-value function. Numerous experimental results demonstrate the effectiveness of the proposed approach.	https://dx.doi.org/10.1109/TSG.2018.2879572	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wan2021	Adversarial Attack for Deep Reinforcement Learning Based Demand Response		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124133083&doi=10.1109\%2fPESGM46819.2021.9637826&partnerID=40&md5=5da79c7f9faa848c98079be1ab12090b	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2022	Heterogeneous Defect Prediction Based on Federated Reinforcement Learning via Gradient Clustering	Heterogeneous defect prediction (HDP) refers to using heterogeneous data collected by other projects to build a defect prediction model to predict the software defects in a project. Traditional methods usually involve the measurement of the source project and the target project. However, due to the limitations of laws and regulations, these original data are not easy to obtain, which forms a data island. As a new machine learning paradigm, federated learning (FL) has great advantages in training heterogeneous data and data island. In order to solve the data island and data heterogeneity of HDP, we propose a novel Federated Reinforcement Learning via Gradient Clustering (FRLGC) method in this paper. Firstly, the parameters of the global model are transferred to each dueling deep Q network (dueling DQN) model and each client uses private data to train the dueling model which combines experience replay to increase data efficiency in limited datasets. Secondly, gaussian differential privacy is used to encrypt the model parameters to ensure the privacy and security of the model. Finally, we cluster the clients according to their locally encrypted model parameters and use weighted average to aggregate to create a new global model locally and globally. Experiments on nine projects in three public databases (Relink, NASA and AEEEM) show that FRLGC is superior to the relevant HDP solutions.	https://dx.doi.org/10.1109/ACCESS.2022.3195039	Included	new_screen		4
RL4SE	Wang2022a	Enriching query semantics for code search with reinforcement learning	Code search is a common practice for developers during software implementation. The challenges of accurate code search mainly lie in the knowledge gap between source code and natural language (i.e., queries). Due to the limited code-query pairs and large code-description pairs available, the prior studies based on deep learning techniques focus on learning the semantic matching relation between source code and corresponding description texts for the task, and hypothesize that the semantic gap between descriptions and user queries is marginal. In this work, we found that the code search models trained on code-description pairs may not perform well on user queries, which indicates the semantic distance between queries and code descriptions. To mitigate the semantic distance for more effective code search, we propose QueCos, a Query-enriched Code search model. QueCos learns to generate semantic enriched queries to capture the key semantics of given queries with reinforcement learning (RL). With RL, the code search performance is considered as a reward for producing accurate semantic enriched queries. The enriched queries are finally employed for code search. Experiments on the benchmark datasets show that QueCos can significantly outperform the state-of-the-art code search models.	https://doi.org/10.1016/j.neunet.2021.09.025	Included	new_screen		4
RL4SE	Wang2019	A New Solution for Freeway Congestion: Cooperative Speed Limit Control Using Distributed Reinforcement Learning	This paper presents a novel variable speed limit control system under the vehicle to infrastructure environment to optimize the freeway traffic mobility and safety. The control system is a multiagent system consists of several traffic control agents. The agents work cooperatively using the proposed distributed reinforcement learning approach to maximize the freeway traffic mobility and safety benefits. The traffic mobility objective is to maintain freeway traffic density slightly under the critical point to produce the maximum traffic volume, while the traffic safety objective is to reduce the speed difference between adjacent segments. The merits of distributed reinforcement learning are its model-free nature, and it can improve its performance continually as time goes on. The control system is developed on an open source traffic simulation software. Results revealed that compared with no control cases, the proposed system can noticeably decrease the total travel time and increase the bottleneck outflow. Moreover, the speed difference between freeway segments indicating the potential rear-end collision risk is significantly reduced. We also found that there could be more than one optimal traffic equilibrium according to different control objectives, which inspire us to design more optimal strategies in the future.	https://dx.doi.org/10.1109/ACCESS.2019.2904619	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2018	SDCoR: Software Defined Cognitive Routing for Internet of Vehicles	The Internet of Vehicles (IoV) is a subapplication of the Internet of Things in the automotive field. Large amounts of sensor data require to be transferred in real-time. Most of the routing protocols are specifically targeted to specific situations in IoV. But communication environment of IoV usually changes in the space-time dimension. Unfortunately, the traditional vehicular networks cannot select the optimal routing policy when facing the dynamic environment, due to the lack of abilities of sensing the environment and learning the best strategy. Sensing and learning constitute two key steps of the cognition procedure. Thus, in this paper, we present a software defined cognitive network for IoV (SDCIV), in which reinforcement learning and software defined network technology are considered for IoV to achieve cognitive capability. To the best of our knowledge, this paper is the first one that can give the optimal routing policy adaptively through sensing and learning from the environment of IoV. We perform experiments on a real vehicular dataset to validate the effectiveness and feasibility of the proposed algorithm. Results show that our algorithm achieves better performance than several typical protocols in IoV. We also show the feasibility and effectiveness of our proposed SDCIV.	https://dx.doi.org/10.1109/JIOT.2018.2812210	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2009	Creating human-like autonomous players in real-time first person shooter computer games			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2021	Resource Management for Secure Computation Offloading in Softwarized Cyber-Physical Systems	The evolution of the Internet of Things (IoT) makes an increased emphasis on extending their computing and storage capabilities by relying particularly on the cloud/edge computing (EC) for cyber-physical systems (CPSs). Especially, in software-defined CPS (SD-CPS), different software-defined networking (SDN) controllers share information and cooperate to make global decisions. To further enhance system security during the information sharing process, we introduce blockchain technology into SD-CPS. However, because many security-related decisions are sensitive to latency, it is vital to minimize the system latency in blockchain-empowered SD-CPS. In this article, a blockchain-empowered distributed SD-CPS framework is proposed to realize consensus and distributed resource management by offloading data in a hybrid network paradigm that combines cloud computing and EC. Moreover, to adaptively implement offloading and control strategies while guaranteeing data security, we design a resource management scheme for reducing system latency and provide the flexibility of cooperation. To foster intelligence, we formulate the joint communication, computation, and consensus problems as a Markov decision process and use deep reinforcement learning to balance resource allocation, reduce latency, and guarantee data security. Compared with other schemes, simulation results verify the effectiveness of the proposed scheme, which performs better on self-adaptation decision making and system delay reduction.	https://dx.doi.org/10.1109/JIOT.2021.3057594	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2021a	Resource Management for Secure Computation Offloading in Softwarized CyberendashPhysical Systems	The evolution of the Internet of Things (IoT) makes an increased emphasis on extending their computing and storage capabilities by relying particularly on the cloud/edge computing (EC) for cyber-physical systems (CPSs). Especially, in software-defined CPS (SD-CPS), different software-defined networking (SDN) controllers share information and cooperate to make global decisions. To further enhance system security during the information sharing process, we introduce blockchain technology into SD-CPS. However, because many security-related decisions are sensitive to latency, it is vital to minimize the system latency in blockchain-empowered SD-CPS. In this article, a blockchain-empowered distributed SD-CPS framework is proposed to realize consensus and distributed resource management by offloading data in a hybrid network paradigm that combines cloud computing and EC. Moreover, to adaptively implement offloading and control strategies while guaranteeing data security, we design a resource management scheme for reducing system latency and provide the flexibility of cooperation. To foster intelligence, we formulate the joint communication, computation, and consensus problems as a Markov decision process and use deep reinforcement learning to balance resource allocation, reduce latency, and guarantee data security. Compared with other schemes, simulation results verify the effectiveness of the proposed scheme, which performs better on self-adaptation decision making and system delay reduction.	https://dx.doi.org/10.1109/JIOT.2021.3057594	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2021b	Quantum Algorithms for Reinforcement Learning with a Generative Model	Reinforcement learning studies how an agent should interact with an environment to maximize its cumulative reward. A standard way to study this question abstractly is to ask how many samples an agent needs from the environment to learn an optimal policy for a gamma-discounted Markov decision process (MDP). For such an MDP, we design quantum algorithms that approximate an optimal policy (pi*), the optimal value function (v*), and the optimal Q-function (q*), assuming the algorithms can access samples from the environment in quantum superposition. This assumption is justified whenever there exists a simulator for the environment; for example, if the environment is a video game or some other program. Our quantum algorithms, inspired by value iteration, achieve quadratic speedups over the best-possible classical sample complexities in the approximation accuracy (epsilon) and two main parameters of the MDP: the effective time horizon (1/1-gamma) and the size of the action space (A). Moreover, we show that our quantum algorithm for computing q* is optimal by proving a matching quantum lower bound.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2022b	Research on the Intelligent Energy Governance of Parallel Hybrid Vehicle Based on Deep Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130190295&doi=10.13052\%2fspee1048-5236.4124&partnerID=40&md5=0201ce41bd580410454bd9bcf6a58160	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2002	Input profiling for injection molding by reinforcement learning			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2017	Integrating Reinforcement Learning with Multi-Agent Techniques for Adaptive Service Composition		https://doi.org/10.1145/3058592	Included	new_screen		4
RL4SE	Wang2020	Integrating recurrent neural networks and reinforcement learning for dynamic service composition		https://doi.org/10.1016/j.future.2020.02.030	Included	new_screen		4
RL4SE	Wang2022c	Automating Reinforcement Learning Architecture Design for Code Optimization		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127904168&doi=10.1145\%2f3497776.3517769&partnerID=40&md5=bd6a7921a0c045663e039af914a08518	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2022d	Generating Effective Software Obfuscation Sequences With Reinforcement Learning	Obfuscation is a prevalent security technique which transforms syntactic representation of a program to a complicated form, but still keeps program semantics unchanged. So far, developers heavily rely on obfuscation to harden their products and reduce the risk of adversarial reverse engineering. However, despite its spectacular progress, one crucial hurdle is that each of existing obfuscation method is designed specifically for obfuscating one program feature (e.g., identifier name, control flow), so an effective obfuscation scheme usually composes a considerable amount of different obfuscation methods. Therefore, one primary challenge lies in identifying effective combinations of obfuscation methods. In this research, we propose a principled technique for generating an optimal program obfuscation scheme by adopting a reinforcement learning approach. Given a program and a set of obfuscation transformations, a reinforcement learning model is progressively trained to select a sequence of obfuscation transformations, such that applying each transformation in order toward the program yields the optimal obfuscation result, making programs dissimilar while retaining reasonable instrumentation overhead. Our implementation can directly work on raw binary executables without source code, and our evaluation demonstrates that the trained models can effectively obfuscate executable files with low cost.	https://dx.doi.org/10.1109/TDSC.2020.3041655	Included	new_screen		4
RL4SE	Wang2016	A multi-agent reinforcement learning approach to dynamic service composition		https://doi.org/10.1016/j.ins.2016.05.002	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2015	Integrating Gaussian Process with Reinforcement Learning for Adaptive Service Composition	Service composition offers a powerful software paradigm to build complex and value-added applications by exploiting a service oriented architecture. However, the frequent changes in the internal and external environment demand adaptiveness of a composition solution. Meanwhile, the increasingly complex user requirements and the rapid growth of the composition space give rise to the scalability issue. To address these key challenges, we propose a new service composition scheme, integrating gaussian process with reinforcement learning for adaptive service composition. It uses kernel function approximation to predict the distribution of the objective function value with strong communication skills and generalization ability based on an off-policy Q-learning algorithm. The experimental results demonstrate that our method clearly outperforms the standard Q-learning solution for service composition.	https://dx.doi.org/10.1007/978-3-662-48616-0_13	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2021c	Spatio-Clock Synchronous Constraint Guided Safe Reinforcement Learning for Autonomous Driving		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121140394&doi=10.7544\%2fissn1000-1239.2021.20211023&partnerID=40&md5=89365577615c30a8f26abfbdbf2fec59	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2022e	Deep Learning for Securing Software-Defined Industrial Internet of Things: Attacks and Countermeasures	Software-defined networking (SDN) has become an attractive solution to carry out centralized and efficient control in Industrial Internet of Things (IIoT). However, its security has received little attention when applied to IIoT, and no comprehensive consideration has been given to attacks against forwarding nodes (FNs), the basic elements of the data plane. Therefore, in this article, we aim to investigate attacks against FNs from multiple perspectives in software-defined IIoT. To the best of our knowledge, we are the first to systematically consider this kind of attacks. Since it is difficult to predeploy all defense methods against various attacks, we propose a deep reinforcement learning (DRL)-based general attack tolerance scheme to guide the benign traffic flow bypass the attacked FNs. Furthermore, in view of the situation that the real data set is rare and the standard model-based data set is likely to be impractical, we use generative adversarial network (GAN), a representative deep generative model (DGM), to flexibly generate real-like network traffic for more sufficient and effective experimental verification on the attack tolerance scheme. Experimental results show that our proposed scheme can significantly improve the successful arrival rate of IIoT traffic and achieve near-optimal results.	https://dx.doi.org/10.1109/JIOT.2021.3126633	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2020a	Action Permissibility Prediction in Autonomous Driving through Deep Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083745459&doi=10.1088\%2f1757-899X\%2f782\%2f3\%2f032062&partnerID=40&md5=c5e7216e0a19c7c52abd9f91e3e20903	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2022f	Learning to Synthesize Relational Invariants		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146953454&doi=10.1145\%2f3551349.3556942&partnerID=40&md5=4e41dc2f82eacb63bd2ad9887d2f24c1	Included	conflict_resolution		4
RL4SE	Wang2019a	Application of Deep Reinforcement Learning in Beam Offset Calibration of MEBT at C-ADS Injector-II		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066024394&doi=10.1007\%2f978-981-13-3648-5_3&partnerID=40&md5=e1cef2e064794ddfc3a8e9bbfbfc60e0	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2019b	Deep reinforcement learning-based cooperative interactions among heterogeneous vehicular networks		https://doi.org/10.1016/j.asoc.2019.105557	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2022g	Security Enhancement Through Compiler-Assisted Software Diversity With Deep Reinforcement Learning	Traditional software defenses take corresponding actions after the attacks are discovered. The defenders in this situation are comparatively passive because the attackers may try many different ways to find vulnerability and bugs, but the software remains static. This leads to the imbalance between offense and defense. Software diversity alleviates the current threats by implementing a heterogeneous software system. The N-Variant eXecution (NVX) systems, effective and applicable runtime diversifying methods, apply multiple variants to imporove software security. Higher diversity can lead to less vulnerabilities that attacks can exploit. However, runtime diversifying methods such as address randomization and reverse stack can only provide limited diversity to the system. Thus, the authors enhance the diversity of variants with a compiler-assisted approach. They use a deep reinforcement learning-based algorithm to generate variants, ensuring the high diversity of the system. For different numbers of variants, they show the results of the Deep Q Network algorithm under different parameter settings.	https://dx.doi.org/10.4018/IJDCF.302878	Included	conflict_resolution		4
RL4SE	Wang2021d	Transfer reinforcement learning-based road object detection in next generation IoT domain		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104603461&doi=10.1016\%2fj.comnet.2021.108078&partnerID=40&md5=e13383124627632698010f42c9c84e40	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2019c	A Review of Microsoft Academic Services for Science of Science Studies	Since the relaunch of Microsoft Academic Services (MAS) 4 years ago, scholarly communications have undergone dramatic changes: more ideas are being exchanged online, more authors are sharing their data, and more software tools used to make discoveries and reproduce the results are being distributed openly. The sheer amount of information available is overwhelming for individual humans to keep up and digest. In the meantime, artificial intelligence (AI) technologies have made great strides and the cost of computing has plummeted to the extent that it has become practical to employ intelligent agents to comprehensively collect and analyze scholarly communications. MAS is one such effort and this paper describes its recent progresses since the last disclosure. As there are plenty of independent studies affirming the effectiveness of MAS, this paper focuses on the use of three key AI technologies that underlies its prowess in capturing scholarly communications with adequate quality and broad coverage: (1) natural language understanding in extracting factoids from individual articles at the web scale, (2) knowledge assisted inference and reasoning in assembling the factoids into a knowledge graph, and (3) a reinforcement learning approach to assessing scholarly importance for entities participating in scholarly communications, called the saliency, that serves both as an analytic and a predictive metric in MAS. These elements enhance the capabilities of MAS in supporting the studies of science of science based on the GOTO principle, i.e., good and open data with transparent and objective methodologies. The current direction of development and how to access the regularly updated data and tools from MAS, including the knowledge graph, a REST API and a website, are also described.	https://www.ncbi.nlm.nih.gov/pubmed/33693368	Excluded	new_screen	E5: Other not a paper,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2022h	A Flight Simulation Model Architecture for Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123315285&doi=10.1007\%2f978-981-16-8430-2_35&partnerID=40&md5=dae75c05f823159fc92ad276fa08eb55	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2012	Towards a General Supporting Framework for Self-Adaptive Software Systems	When confronted with their internal and environmental dynamics, various software systems increasingly require self-adaptation capabilities. The vision above requires the self-adaptation approach to tackle these challenges and some software systems have their own specific implementations. However, in this paper a general supporting framework is proposed for software systems to self-adapt with running environmental dynamics and meanwhile fulfill various user requirements. Three key issues are covered in the framework: 1) the overall control architecture, which adopts the double closed-loop style and respectively includes the self-adaptation loop and the self-learning loop; 2) a general descriptive language, which is a application-independent and unified language to represent self-adaptation knowledge about target systems; 3) three implementation mechanisms, including forward reasoning, planning and reinforcement learning using feedback, which are supported by the above descriptive language and executed at runtime in different modules. Finally, one scenario of on-demand services of massive data mining tasks is selected and the case study demonstrates how the framework is customized as required and how the approach works.	https://dx.doi.org/10.1109/COMPSACW.2012.38	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2012a	Pruning of redundant information to improve performance for agent control in a changing environment		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869458792&doi=10.1541\%2fieejeiss.132.1829&partnerID=40&md5=429b71110ea9fd72adfe720f44065565	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2022i	A multi-agent reinforcement learning-based collaborative jamming system: Algorithm design and software-defined radio implementation	In multi-agent confrontation scenarios, a jammer is constrained by the single limited performance and inefficiency of practical application. To cope with these issues, this paper aims to investigate the multi-agent jamming problem in a multi-user scenario, where the coordination between the jammers is considered. Firstly, a multi-agent Markov decision process (MDP) framework is used to model and analyze the multi-agent jamming problem. Secondly, a collaborative multi-agent jamming algorithm (CMJA) based on reinforcement learning is proposed. Finally, an actual intelligent jamming system is designed and built based on software-defined radio (SDR) platform for simulation and platform verification. The simulation and platform verification results show that the proposed CMJA algorithm outperforms the independent Q-learning method and provides a better jamming effect.	https://dx.doi.org/10.23919/JCC.2022.10.003	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2022j	Intelligent Jamming Against Dynamic Spectrum Access User: Algorithm Design and Verification System Implementation	In this letter, we investigate an intelligent jamming problem in a wireless confrontation scenario where both the user and the jammer can dynamically adjust spectrum access or the jamming pattern. To realize accurate jamming attacks, we first propose a deep learning-based pattern recognition method to recognize the user's spectrum access patterns. Then, based on the recognition results, we design targeted jamming methods including conventional jamming patterns and a deep reinforcement learning-based jamming pattern. Moreover, a practical intelligent jamming demonstration system is designed and built based on the software-defined radio (SDR) platform for verification. The simulation and platform verification results demonstrate the effectiveness of the proposed method in the dynamic spectrum confrontation scenario.	https://dx.doi.org/10.1109/LWC.2022.3204898	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2020b	Can You Trust AI-assisted Network Automation A DRL-based Approach to Mislead the Automation in SD-IPoEONs	We study the vulnerability of artificial intelligence assisted network automation (AIaNA), and design a deep reinforcement learning (DRL) model to mislead the AIaNA in software-defined IP over elastic optical networks (SD-IPoEONs) through crafting/injecting adversarial traffic samples.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2020c	Can You Trust AI-assisted Network Automation? A DRL-based Approach to Mislead the Automation in SD-IPoEONs	We study the vulnerability of artificial intelligence assisted network automation (AIaNA), and design a deep reinforcement learning (DRL) model to mislead the AIaNA in software-defined IP over elastic optical networks (SD-IPoEONs) through crafting/injecting adversarial traffic samples.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2020d	How to Mislead AI-Assisted Network Automation in SD-IPoEONs: A Comparison Study of DRL- and GAN-Based Approaches	Recently, the combination of artificial intelligence (AI) and software-defined networking (SDN) has attracted intensive research interests because it realizes and promotes AI-assisted network automation (AIaNA). Despite the initial successes of AIaNA, its vulnerabilities, i.e., the downside of the reduction of human involvement achieved by it, have not been carefully explored. In this work, we use software-defined IP over elastic optical networks (SD-IPoEONs) as the background, and study how to mislead the AIaNA system in them. Specifically, we target our attack on the deep neural network (DNN) based traffic predictor in the AIaNA system, and design an adversarial module (ADVM) that can craft and inject adversarial traffic samples adaptively to disturb its operation. We consider two approaches to design the ADVM, i.e., the deep reinforcement learning (DRL) based on deep deterministic policy gradient (DDPG), and the generative adversarial network (GAN) model. Our proposed ADVM can monitor and interact with a dynamic SD-IPoEON to train itself on-the-fly. This enables it to generate and inject adversarial samples in the most disturbing and hard-to-detect way and to severely affect the AIaNA's performance on multilayer service provisioning. Specifically, IP flows will be served incorrectly to result in unnecessary congestions/under-utilizations on lightpaths, and erroneous network reconfigurations will be invoked frequently. Simulation results confirm the effectiveness of our ADVM designs, and show that the GAN-based ADVM achieves better attack effects with smaller perturbation strength.	https://dx.doi.org/10.1109/JLT.2020.3003905	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2021e	Deep Reinforcement Learning and Docking Simulations for autonomous molecule generation in de novo Drug Design		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123046323&doi=10.1145\%2f3469877.3497694&partnerID=40&md5=cb9f2ad19bc01496cad79220a7acda4f	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2020e	Multi-objective Search for Model-based Testing	This paper presents a search-based approach relying on multi-objective reinforcement learning and optimization for test case generation in model-based software testing. Our approach considers test case generation as an exploration versus exploitation dilemma, and we address this dilemma by implementing a particular strategy of multi-objective multi-armed bandits with multiple rewards. After optimizing our strategy using the jMetal multi-objective optimization framework, the resulting parameter setting is then used by an extended version of the Modbat tool for model-based testing. We experimentally evaluate our search-based approach on a collection of examples, such as the ZooKeeper distributed service and PostgreSQL database system, by comparing it to the use of random search for test case generation. Our results show that test cases generated using our search-based approach can obtain more predictable and better state/transition coverage, find failures earlier, and provide improved path coverage.	https://dx.doi.org/10.1109/QRS51102.2020.00029	Included	new_screen		4
RL4SE	Wang2020f	A DRL-Aided Multi-Layer Stability Model Calibration Platform Considering Multiple Events	Maintaining accurate stability models for power system planning and operational analysis is of great importance. Calibrating problematic parameters using PMU measurements that work well for multiple events remains a challenging problem. To tackle the known issues, this paper presents a novel and generalized deep-reinforcement-learning (DRL)-aided platform for automated parameter calibration with an adaptive multilayer dueling Deep Q Network (D-DQN) algorithm that searches optimal parameter sets for multiple events simultaneously. This platform leverages state-of-the-art DRL algorithms and supports various types of stability models used in software vendors' transient stability programs. To help improve the efficiency of parameter calibration, a hierarchical structure with coarse-fine layers and adaptive steps is adopted when training effective DRL agents. It provides a systematic way to calibrate stability model parameters, which can save tremendous labor efforts for maintaining model accuracy and complying with industry standards. The effectiveness of the proposed approach is verified through numerical experiments on a realistic power plant model considering multiple system events.	https://dx.doi.org/10.1109/PESGM41954.2020.9282022	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2021f	Deep Reinforcement Scheduling of Energy Storage Systems for Real-Time Voltage Regulation in Unbalanced LV Networks With High PV Penetration	The ever-growing higher penetration of distributed energy resources (DERs) in low-voltage (LV) distribution systems brings both opportunities and challenges to voltage support and regulation. This paper proposes a deep reinforcement learning (DRL)-based scheduling scheme of energy storage systems (ESSs) to mitigate system voltage deviations in unbalanced LV distribution networks. The ESS-based voltage regulation problem is formulated as a multi-stage quadratic stochastic program, with the objective of minimizing the expected total daily voltage regulation cost while satisfying operational constraints. While existing voltage regulation methods are mostly focused on one-time-step control, this paper explores a day-horizon system-wide voltage regulation problem. In other words, the size of action and state spaces are extremely high-dimensional and need to be delicately handled. Furthermore, in order to overcome the difficulty of modeling uncertainties and develop a real-time solution, a learn-to-schedule feedback control framework is proposed by adapting the problem to a model-free DRL setting. The proposed algorithm is tested on a customized 6-bus system and a modified IEEE 34-bus system. Simulation results validate the effectiveness and near-optimality of voltage regulation by ESS in comparison with a deterministic quadratic program solution.	https://dx.doi.org/10.1109/TSTE.2021.3092961	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2009a	Study on Multi-agent Simulation System Based on Reinforcement Learning Algorithm	Multi-agent simulation system based on reinforcement learning algorithm is a micro-individual acts of modeling and simulation methods, which have wide applicability, distribution, intelligent and interactive features etc. Firstly, studying on reinforcement learning algorithm, and then analysis and design the multi-agent simulation system structure, multi-agent system main modules, the implementation of the definition and finally, carefully design the multi-agent simulation system software, and multi-agent simulation collective system simulation and surrounded the location gathered from the space simulation experiment, the results showed that: Construct a multi-agent simulation system based on reinforcement learning algorithm, achieve real-time simulation of multi-agene, and multi-agent to get effect quickly, and to quickly construct surrounded conduct by mobile groups, the conduct of the system to achieve the global optimum effect.	https://dx.doi.org/10.1109/CSIE.2009.234	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2007	Dual Representations for Dynamic Programming and Reinforcement Learning	We investigate the dual approach to dynamic programming and reinforcement learning, based on maintaining an explicit representation of stationary distributions as opposed to value functions. A significant advantage of the dual approach is that it allows one to exploit well developed techniques for representing, approximating and estimating probability distributions, without running the risks associated with divergent value function estimation. A second advantage is that some distinct algorithms for the average reward and discounted reward case in the primal become unified under the dual. In this paper, we present a modified dual of the standard linear program that guarantees a globally normalized state visit distribution is obtained. With this reformulation, we then derive novel dual forms of dynamic programming, including policy evaluation, policy iteration and value iteration. Moreover, we derive dual formulations of temporal difference learning to obtain new forms of Sarsa and Q-learning. Finally, we scale these techniques up to large domains by introducing approximation, and develop new approximate off-policy learning algorithms that avoid the divergence problems associated with the primal approach. We show that the dual view yields a viable alternative to standard value function based techniques and opens new avenues for solving dynamic programming and reinforcement learning problems	https://dx.doi.org/10.1109/ADPRL.2007.368168	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2019d	Efficient Robotic Task Generalization Using Deep Model Fusion Reinforcement Learning	Learning-based methods have been used to program robotic tasks in recent years. However, extensive training is usually required not only for the initial task learning but also for generalizing the learned model to the same task but in different environments. In this paper, we propose a novel Deep Reinforcement Learning algorithm for efficient task generalization and environment adaptation in the robotic task learning problem. The proposed method is able to efficiently generalize the previously learned task by model fusion to solve the environment adaptation problem. The proposed Deep Model Fusion (DMF) method reuses and combines the previously trained model to improve the learning efficiency and results. Besides, we also introduce a Multi-objective Guided Reward (MGR) shaping technique to further improve training efficiency. The proposed method was benchmarked with previous methods in various environments to validate its effectiveness.	https://dx.doi.org/10.1109/ROBIO49542.2019.8961391	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2021g	Accelerated Deep Reinforcement Learning for Fast Feedback of Beam Dynamics at KARA	Coherent synchrotron radiation (CSR) is generated when the electron bunch length is in the order of the magnitude of the wavelength of the emitted radiation. The self-interaction of short electron bunches with their own electromagnetic fields changes the longitudinal beam dynamics significantly. Above a certain current threshold, the micro-bunching instability develops, characterized by the appearance of distinguishable substructures in the longitudinal phase space of the bunch. To stabilize the CSR emission, a real-time feedback control loop based on reinforcement learning (RL) is proposed. Informed by the available THz diagnostics, the feedback is designed to act on the radio frequency (RF) system of the storage ring to mitigate the micro-bunching dynamics. To satisfy low-latency requirements given by the longitudinal beam dynamics, the RL controller has been implemented on hardware (FPGA). In this article, a real-time feedback loop architecture and its performance is presented and compared with a software implementation using Keras-RL on CPU/GPU. The results obtained with the CSR simulation Inovesa demonstrate that the functionality of both platforms is equivalent. The training performance of the hardware implementation is similar to software solution, while it outperforms the Keras-RL implementation by an order of magnitude. The presented RL hardware controller is considered as an essential platform for the development of intelligent CSR control systems.	https://dx.doi.org/10.1109/TNS.2021.3084515	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2019e	Facilitating HumanendashRobot Collaborative Tasks by Teaching-Learning-Collaboration From Human Demonstrations	Collaborative robots are widely employed in strict hybrid assembly tasks involved in intelligent manufacturing. In this paper, we develop a teaching-learning-collaboration (TLC) model for the collaborative robot to learn from human demonstrations and assist its human partner in shared working situations. The human could program the robot using natural language instructions according to his/her personal working preferences via this approach. Afterward, the robot learns from human assembly demonstrations by taking advantage of the maximum entropy inverse reinforcement learning algorithm and updates its task-based knowledge using the optimal assembly strategy. In the collaboration process, the robot is able to leverage its learned knowledge to actively assist the human in the collaborative assembly task. Experimental results and analysis demonstrate that the proposed approach presents considerable robustness and applicability in human-robot collaborative tasks. Note to Practitioners-This paper is motivated by the human-robot collaborative assembly problem in the context of advanced manufacturing. Collaborative robotics makes a huge shift from the traditional robot-in-a-cage model to robots interacting with people in an open working environment. When the human works with the robot in the shared workspace, it is significant to lessen human programming effort and improve the human-robot collaboration efficiency once the task is updated. We develop a TLC model for the robot to learn from human demonstrations and assist its human partner in collaborative tasks. Once the task is changed, the human may code the robot via natural language instructions according to his/her personal working preferences. The robot can learn from human assembly demonstrations to update its task-based knowledge, which can be leveraged by the robot to actively assist the human to accomplish the collaborative task. We demonstrate the advantages of the proposed approach via a set of experiments in realistic human-robot collaboration contexts.	https://dx.doi.org/10.1109/TASE.2018.2840345	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2022k	Reinforcement-Learning-Guided Source Code Summarization Using Hierarchical Attention	Code summarization (aka comment generation) provides a high-level natural language description of the function performed by code, which can benefit the software maintenance, code categorization and retrieval. To the best of our knowledge, the state-of-the-art approaches follow an encoder-decoder framework which encodes source code into a hidden space and later decodes it into a natural language space. Such approaches suffer from the following drawbacks: (a) they are mainly input by representing code as a sequence of tokens while ignoring code hierarchy; (b) most of the encoders only input simple features (e.g., tokens) while ignoring the features that can help capture the correlations between comments and code; (c) the decoders are typically trained to predict subsequent words by maximizing the likelihood of subsequent ground truth words, while in real world, they are excepted to generate the entire word sequence from scratch. As a result, such drawbacks lead to inferior and inconsistent comment generation accuracy. To address the above limitations, this paper presents a new code summarization approach using hierarchical attention network by incorporating multiple code features, including type-augmented abstract syntax trees and program control flows. Such features, along with plain code sequences, are injected into a deep reinforcement learning (DRL) framework (e.g., actor-critic network) for comment generation. Our approach assigns weights (pays ``attention'') to tokens and statements when constructing the code representation to reflect the hierarchical code structure under different contexts regarding code features (e.g., control flows and abstract syntax trees). Our reinforcement learning mechanism further strengthens the prediction results through the actor network and the critic network, where the actor network provides the confidence of predicting subsequent words based on the current state, and the critic network computes the reward values of all the possible extensions of the current state to provide global guidance for explorations. Eventually, we employ an advantage reward to train both networks and conduct a set of experiments on a real-world dataset. The experimental results demonstrate that our approach outperforms the baselines by around 22 to 45 percent in BLEU-1 and outperforms the state-of-the-art approaches by around 5 to 60 percent in terms of S-BLEU and C-BLEU.	https://dx.doi.org/10.1109/TSE.2020.2979701	Included	new_screen		4
RL4SE	Wang2019f	Facilitating Human-Robot Collaborative Tasks by Teaching-Learning-Collaboration From Human Demonstrations	Collaborative robots are widely employed in strict hybrid assembly tasks involved in intelligent manufacturing. In this paper, we develop a teaching-learning-collaboration (TLC) model for the collaborative robot to learn from human demonstrations and assist its human partner in shared working situations. The human could program the robot using natural language instructions according to his/her personal working preferences via this approach. Afterward, the robot learns from human assembly demonstrations by taking advantage of the maximum entropy inverse reinforcement learning algorithm and updates its task-based knowledge using the optimal assembly strategy. In the collaboration process, the robot is able to leverage its learned knowledge to actively assist the human in the collaborative assembly task. Experimental results and analysis demonstrate that the proposed approach presents considerable robustness and applicability in human-robot collaborative tasks. Note to Practitioners-This paper is motivated by the human-robot collaborative assembly problem in the context of advanced manufacturing. Collaborative robotics makes a huge shift from the traditional robot-in-a-cage model to robots interacting with people in an open working environment. When the human works with the robot in the shared workspace, it is significant to lessen human programming effort and improve the human-robot collaboration efficiency once the task is updated. We develop a TLC model for the robot to learn from human demonstrations and assist its human partner in collaborative tasks. Once the task is changed, the human may code the robot via natural language instructions according to his/her personal working preferences. The robot can learn from human assembly demonstrations to update its task-based knowledge, which can be leveraged by the robot to actively assist the human to accomplish the collaborative task. We demonstrate the advantages of the proposed approach via a set of experiments in realistic human-robot collaboration contexts.	https://dx.doi.org/10.1109/TASE.2018.2840345	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2021h	Contextual Shortest Path with Unknown Context Distributions	This paper proposes a class of stochastic decision problems, which we call Contextual Shortest Path (CSP) problems. The problem is motivated by dynamic path planning and obstacle avoidance for UAV and drone applications. More specifically, in path planning applications, there is a need to compute the path between two pre-determined locations in space while avoiding various spontaneous obstacles in the environment. In general, finding the optimal path in a stochastic environment with time-varying obstacles generalizes the problem of finding the shortest path over an undirected graph with stochastic edges, where random edge realizations are augmented with richer contexts that can change over time and require online inspection.Under perfect knowledge of context distributions, we provide an extended Dijkstra's algorithm to solve the associated dynamic program efficiently. When the context distributions are unknown and need to be learned online, we first adapt two algorithms as our baselines, one based on Thompson Sampling and the other based on ?-greedy exploration. We then propose a novel reinforcement learning algorithm, RL-CSP, which intelligently distributes exploration episodes over the time horizon and ensures the agent visits under-explored states. We bound the regret for RL-CSP, and augment the theoretical results with simulations over various network topologies. We further demonstrate the improved robustness of our RL-based algorithm in the stochastic shortest path setting.	https://dx.doi.org/10.1109/IEEECONF53345.2021.9723097	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2008	Crime simulation using GIS and artificial intelligent agents		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901550107&doi=10.4018\%2f978-1-59904-591-7.ch011&partnerID=40&md5=87ac73e0af19954212ecc3e31e2e82df	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2022l	Self-play learning strategies for resource assignment in Open-RAN networks		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123581219&doi=10.1016\%2fj.comnet.2021.108682&partnerID=40&md5=a45a44d7d3b67b45596d67e5d5f2d961	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2009b	Exploration of configural representation in landmark learning using working memory toolkit	The mechanisms by which humans and animals use visually-acquired landmarks to find their way around have proved fascinating. Considerable evidence suggests that animals navigate not only on the basis of the overall geometry of the space but also on the basis of a configural representation of the cues. In contrast to earlier linear models of elemental feature representation, configural representation requires individual stimulus be represented in the context of other Stimuli and is typified by non-linear learning tasks such as the transverse patterning problem. This paper explores the suitability of configural representation for automatic scene recognition in robot navigation by conducting experiments designed to infer semantic prediction of a scene from different configurations of its stimuli. The main contribution Of this work is that it provides a methodology for automatic landmark-based scene identification with the aid of a reinforcement learning based software package, called the working memory toolkit (WMtk), which allows reward associations between a target location and the conjunctive representations Of its Stimuli. Experimental results obtained with two different target locations are presented and compared with those of two other classification mechanisms, a Support vector machine approach and a simple linear two-class classifier, perceptron. (C) 2008 Elsevier B.V. All rights reserved.	https://dx.doi.org/10.1016/j.patrec.2008.09.002	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2021i	Hybrid Trajectory and Force Learning of Complex Assembly Tasks: A Combined Learning Framework	Complex assembly tasks involve nonlinear and low-clearance insertion trajectories with varying contact forces at different stages. For a robot to solve these tasks, it requires a precise and adaptive controller which conventional force control methods cannot provide. Imitation learning is a promising method for learning controllers that can solve the nonlinear trajectories from human demonstrations without needing to explicitly program them into the robot. However, the force profiles obtain from human demonstration via tele-operation tend to be sub-optimal for complex assembly tasks, thus it is undesirable to imitate such force profiles. Reinforcement learning learns adaptive control policies through interactions with the environment but struggles with low sample efficiency and equipment tear and wear in the physical world. To address these problems, we present a combined learning-based framework to solve complex robotic assembly tasks from human demonstrations via hybrid trajectory learning and force learning. The main contribution of this work is the development of a framework that combines imitation learning, to learn the nominal motion trajectory, with a reinforcement learning-based force control scheme to learn an optimal force control policy, which can satisfy the nominal trajectory while adapting to the force requirement of the assembly task. To further improve the imitation learning part, we develop a hierarchical architecture, following the idea of goal-conditioned imitation learning, to generate the trajectory learning policy on the skill level offline. Through experimental validations, we corroborate that the proposed learning-based framework can generate high-quality trajectories and find suitable force control policies which adapt to the tasks' force requirements more efficiently.	https://dx.doi.org/10.1109/ACCESS.2021.3073711	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2020g	Towards cost-effective service migration in mobile edge: A Q-learning approach	Service migration in mobile edge computing is a promising approach to improving the quality of service (QoS) for mobile users and reducing the network operational cost for service providers as well. However, these benefits are not free, coming at costs of bulk-data transfer, and likely service disruption, which could consequently increase the overall service costs. To gain the benefits of service migration while minimizing its cost across the edge nodes, in this paper, we leverage reinforcement learning (RL) method to design a cost-effective framework, called Mig-RL, for the service migration with a reduction of total service costs as a goal in a mobile edge environment. The Mig-RL leverages the infrastructure of edge network and deploys a migration agent through Q-leaming to learn the optimal policy with respect to the service migration status. We distinguish the Mig-RL from other existing works in several major aspects. First, we fully exploit the nature of this problem in a modest migration space, which allows us to constrain the number of service replicas whereby a defined state-action space could be effectively handled, as opposed to those methods that need to always approximate a huge state-action space for policy optimality. Second, we advocate a migration policy-base as a cache to save the learning process by retrieving the most effective policy whenever a similar migration pattern is encountered as time goes on. Finally, by exploiting the idea of software defined network, we also investigate the efficient implementation of Mig-RL in mobile edge network. Experimental results based on some real and synthesized access sequences show that Mig-RL, compared with the selected existing algorithms, can substantially minimize the service costs, and in the meantime, efficiently improve the QoS by adapting to the changes of mobile access patterns. (C) 2020 Elsevier Inc. All rights reserved.	https://dx.doi.org/10.1016/j.jpdc.2020.08.008	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2021j	Research on Autonomous Driving Perception Test Based on Adversarial Examples		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093095677&doi=10.1007\%2f978-981-15-8462-6_24&partnerID=40&md5=4d589a6f368f26cccd6512a9a21b9849	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2008a	A machine-learning approach to multi-robot coordination		https://doi.org/10.1016/j.engappai.2007.05.006	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2019g	Neural Malware Control with Deep Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082394489&doi=10.1109\%2fMILCOM47813.2019.9020862&partnerID=40&md5=425c1fd512ad43f0d711cc361919312f	Included	conflict_resolution		4
RL4SE	Wang2021k	Hybrid Electric Vehicle Energy Management With Computer Vision and Deep Reinforcement Learning	Modern automotive systems have been equipped with a highly increasing number of onboard computer vision hardware and software, which are considered to be beneficial for achieving eco-driving. This article combines computer vision and deep reinforcement learning (DRL) to improve the fuel economy of hybrid electric vehicles. The proposed method is capable of autonomously learning the optimal control policy from visual inputs. The state-of-the-art convolutional neural networks-based object detection method is utilized to extract available visual information from onboard cameras. The detected visual information is used as a state input for a continuous DRL model to output energy management strategies. To evaluate the proposed method, we construct 100 km real city and highway driving cycles, in which visual information is incorporated. The results show that the DRL-based system with visual information consumes 4.3-8.8\% less fuel compared with the one without visual information, and the proposed method achieves 96.5\% fuel economy of the global optimum-dynamic programming.	https://dx.doi.org/10.1109/TII.2020.3015748	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2019h	A survey of dynamic spectrum allocation based on reinforcement learning algorithms in cognitive radio networks		https://doi.org/10.1007/s10462-018-9639-x	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2022m	Online Power Management for Multi-Cores: A Reinforcement Learning Based Approach	Power and energy is the first-class design constraint for multi-core processors and is a limiting factor for future-generation supercomputers. While modern processor design provides a wide range of mechanisms for power and energy optimization, it remains unclear how software can make the best use of them. This article presents a novel approach for runtime power optimization on modern multi-core systems. Our policy combines power capping and uncore frequency scaling to match the hardware power profile to the dynamically changing program behavior at runtime. We achieve this by employing reinforcement learning (RL) to automatically explore the energy-performance optimization space from training programs, learning the subtle relationships between the hardware power profile, the program characteristics, power consumption and program running times. Our RL framework then uses the learned knowledge to adapt the chip's power budget and uncore frequency to match the changing program phases for any new, previously unseen program. We evaluate our approach on two computing clusters by applying our techniques to 11 parallel programs that were not seen by our RL framework at the training stage. Experimental results show that our approach can reduce the system-level energy consumption by 12 percent, on average, with less than 3 percent of slowdown on the application performance. By lowering the uncore frequency to leave more energy budget to allow the processor cores to run at a higher frequency, our approach can reduce the energy consumption by up to 17 percent while improving the application performance by 5 percent for specific workloads.	https://dx.doi.org/10.1109/TPDS.2021.3092270	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2021l	Reinforced learning from serial CT to improve the early diagnosis of lung cancer in screening		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103689710&doi=10.1117\%2f12.2582232&partnerID=40&md5=0a8bd1b4bdd73d5ae4bdbcc6e66d1dc2	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2020h	A Deep Reinforcement Learning Method for Solving Task Mapping Problems with Dynamic Traffic on Parallel Systems	Efficient mapping of application communication patterns to the network topology is a critical problem for optimizing the performance of communication bound applications on parallel computing systems. The problem has been extensively studied in the past, but they mostly formulate the problem as finding an isomorphic mapping between two static graphs with edges annotated by traffic volume and network bandwidth. But in practice, the network performance is difficult to be accurately estimated, and communication patterns are often changing over time and not easily obtained. Therefore, this work proposes a deep reinforcement learning (DRL) approach to explore better task mappings by utilizing the performance prediction and runtime communication behaviors provided from a simulator to learn an efficient task mapping algorithm. We extensively evaluated our approach using both synthetic and real applications with varied communication patterns on Torus and Dragonfly networks. Compared with several existing approaches from literature and software library, our proposed approach found task mappings that consistently achieved comparable or better application performance. Especially for a real application, the average improvement of our approach on Torus and Dragonfly networks are 11\% and 16\%, respectively. In comparison, the average improvements of other approaches are all less than 6\%.	https://dx.doi.org/10.1145/3432261.3432262	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2021m	Modeling Question Asking Using Neural Program Generation			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2020i	Improving Maneuver Strategy in Air Combat by Alternate Freeze Games with a Deep Reinforcement Learning Algorithm	In a one-on-one air combat game, the opponent's maneuver strategy is usually not deterministic, which leads us to consider a variety of opponent's strategies when designing our maneuver strategy. In this paper, an alternate freeze game framework based on deep reinforcement learning is proposed to generate the maneuver strategy in an air combat pursuit. The maneuver strategy agents for aircraft guidance of both sides are designed in a flight level with fixed velocity and the one-on-one air combat scenario. Middleware which connects the agents and air combat simulation software is developed to provide a reinforcement learning environment for agent training. A reward shaping approach is used, by which the training speed is increased, and the performance of the generated trajectory is improved. Agents are trained by alternate freeze games with a deep reinforcement algorithm to deal with nonstationarity. A league system is adopted to avoid thered queeneffect in the game where both sides implement adaptive strategies. Simulation results show that the proposed approach can be applied to maneuver guidance in air combat, and typical angle fight tactics can be learnt by the deep reinforcement learning agents. For the training of an opponent with the adaptive strategy, the winning rate can reach more than 50\%, and the losing rate can be reduced to less than 15\%. In a competition with all opponents, the winning rate of the strategic agent selected by the league system is more than 44\%, and the probability of not losing is about 75\%.	https://dx.doi.org/10.1155/2020/7180639	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wang2019i	Reinforcement learning with analogue memristor arrays	Reinforcement learning algorithms that use deep neural networks are a promising approach for the development of machines that can acquire knowledge and solve problems without human input or supervision. At present, however, these algorithms are implemented in software running on relatively standard complementary metal-oxide-semiconductor digital platforms, where performance will be constrained by the limits of Moore's law and von Neumann architecture. Here, we report an experimental demonstration of reinforcement learning on a three-layer 1-transistor 1-memristor (1T1R) network using a modified learning algorithm tailored for our hybrid analogue-digital platform. To illustrate the capabilities of our approach in robust in situ training without the need for a model, we performed two classic control problems: the cart-pole and mountain car simulations. We also show that, compared with conventional digital systems in real-world reinforcement learning tasks, our hybrid analogue-digital computing system has the potential to achieve a significant boost in speed and energy efficiency.	https://dx.doi.org/10.1038/s41928-019-0221-6	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Waqar2022	Test Suite Prioritization Based on Optimization Approach Using Reinforcement Learning	Regression testing ensures that modified software code changes have not adversely affected existing code modules. The test suite size increases with modification to the software based on the end-user requirements. Regression testing executes the complete test suite after updates in the software. Re-execution of new test cases along with existing test cases is costly. The scientific community has proposed test suite prioritization techniques for selecting and minimizing the test suite to minimize the cost of regression testing. The test suite prioritization goal is to maximize fault detection with minimum test cases. Test suite minimization reduces the test suite size by deleting less critical test cases. In this study, we present a four-fold methodology of test suite prioritization based on reinforcement learning. First, the testers' and users' log datasets are prepared using the proposed interaction recording systems for the android application. Second, the proposed reinforcement learning model is used to predict the highest future reward sequence list from the data collected in the first step. Third, the proposed prioritization algorithm signifies the prioritized test suite. Lastly, the fault seeding approach is used to validate the results from software engineering experts. The proposed reinforcement learning-based test suite optimization model is evaluated through five case study applications. The performance evaluation results show that the proposed mechanism performs better than baseline approaches based on random and t-SANT approaches, proving its importance for regression testing.	https://dx.doi.org/10.3390/app12136772	Included	new_screen		4
RL4SE	Wawrzynski2012	Autonomous Reinforcement Learning with Experience Replay for Humanoid Gait Optimization	This paper demonstrates application of Reinforcement Learning to optimization of control of a complex system in realistic setting that requires efficiency and autonomy of the learning algorithm. Namely, Actor-Critic with experience replay (which addresses efficiency), and the Fixed Point method for step-size estimation (which addresses autonomy) is applied here to approximately optimize humanoid robot gait. With complex dynamics and tens of continuous state and action variables, humanoid gait optimization represents a challenge for analytical synthesis of control. The presented algorithm learns a nimble gait within 80 minutes of training. (C) 2012 Published by Elsevier B. V. Selection and/or peer-review under responsibility of Program Committee of INNS-WC 2012	https://dx.doi.org/10.1016/j.procs.2012.09.130	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Weber2021	Safe Bayesian Optimization for Data-Driven Power Electronics Control Design in Microgrids: From Simulations to Real-World Experiments	Micro- and smart grids (MSG) play an important role both for integrating renewable energy sources in electricity grids and for providing power supply in remote areas. Modern MSGs are largely driven by power electronic converters due to their high efficiency and flexibility. Controlling MSGs is a challenging task due to requirements of power availability, safety and voltage quality within a wide range of different MSG topologies resulting in a demand for comprehensive testing of new control concepts during their development phase. This applies, in particular, to data-driven control approaches such as reinforcement learning, of which the stability and operating behavior can hardly be evaluated on an analytical basis. Therefore, the OpenModelica Microgrid Gym (OMG) package, an open-source software toolbox for the simulation and control optimization of MSGs, is proposed. It is capable of modeling and simulating arbitrary MSG topologies and offers a Python-based interface for plug & play controller testing. In particular, the standardized OpenAI Gym interface allows for easy data-driven control optimization. The usage and benefits of OMG for designing and testing data-driven controllers are demonstrated utilizing Bayesian optimization. Both the current and voltage control loops of a voltage source inverter operating in standalone, grid-forming mode for a remote MSG are automatically tuned given an uncertain application environment. Finally, the transfer to real-world laboratory experiments is successfully demonstrated.	https://dx.doi.org/10.1109/ACCESS.2021.3062144	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wedde2008	Distributed Learning Strategies for Collaborative Agents in Adaptive Decentralized Power Systems	For regenerative electric power the traditional top-down and long-term power management is obsolete, due to the wide dispersion and high unpredictability of wind and solar based power facilities. In the R&D DEZENT project we developed a multi-level bottom-up solution where autonomous software agents negotiate available energy quantities and needs on behalf of consumers and producer groups. We operate within very short time intervals of assumedly constant demand and supply, in our case 0.5 sec (switching delay for a light bulb). We prove security against a relevant variety of malicious attacks. In this paper the main contribution is to make the negotiation strategies themselves adaptive across periods. We adapted a Reinforcement Learning approach for defining and discussing learning strategies for collaborative autonomous agents that are clearly superior to previous (static) procedures. We report briefly on extensive comparative simulation.	https://dx.doi.org/10.1109/ECBS.2008.59	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wei2005	Absorptlydine: Typical unification of voice-over-ip and the lookaside buffer	"The ""fuzzy"" electrical engineering method to online algorithms is defined not only by the evaluation of XML, but also by the theoretical need for reinforcement learning [1,2]. In fact, few scholars would disagree with the development of Moore's Law, which embodies the structured principles of software engineering. We introduce new multimodal models, which we call AbsorptLydine."		Excluded	new_screen	E5: Other not a paper,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wei2022	Tracking and Aiming Adaptive Control for Unmanned Combat Ground Vehicle on the Move Based on Reinforcement Learning Compensation		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137819827&doi=10.12382\%2fbgxb.2021.0786&partnerID=40&md5=30b29e67d56711d7952fbc48254fdc55	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wei2017	SRC: RoboCup 2017 Small Size League Champion		https://doi.org/10.1007/978-3-030-00308-1_34	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wei2021	DRL-Deploy:Adaptive Service Function Chains Deployment with Deep Reinforcement Learning	With the development of software-defined networking (SDN) and network function virtualization (NFV), the service function chain (SFC) has become a popular paradigm for carrying and completing network services. In this new computing and networking paradigm, virtual network functions (VNFs) are deployed in software entities/virtual machines through physical device networks in a flexible manner to improve resource utilization and reduce management effectiveness. In this case, it is critical to effectively deploy SFCs within an acceptable time to improve quality of service, while meeting the constraints of the physical network. In this paper, we propose an adaptive deep reinforcement learning based method for the online deployment of SFC requests with different QoS requirements, called DRL-Deploy. DRL-Deploy integrates graph convolutional neural networks to effectively extract physical network features and then adopts a parallel method to improve training efficiency, which can converge to the best state. We compare with existing benchmarks, and extensive experiment results show that DRL-Deploy outperforms all others in terms of acceptance rate and long-term average revenue by 8.6\% and 22.9\%, respectively, while reducing long-term average cost by 36.4\%.	https://dx.doi.org/10.1109/ISPA-BDCloud-SocialCom-SustainCom52081.2021.00027	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wen2020	Modified deep learning and reinforcement learning for an incentive-based demand response model	Incentive-based demand response (DR) program can induce end users (EUs) to reduce electricity demand during peak period through rewards. In this study, an incentive-based DR program with modified deep learning and reinforcement learning is proposed. A modified deep learning model based on recurrent neural network (MDL-RNN) was first proposed to identify the future uncertainties of environment by forecasting day-ahead wholesale electricity price, photovoltaic (PV) power output, and power load. Then, reinforcement learning (RL) was utilized to explore the optimal incentive rates at each hour which can maximize the profits of both energy service providers (ESPs) and EUs. The results showed that the proposed modified deep learning model can achieve more accurate forecasting results compared with some other methods. It can support the development of incentive-based DR programs under uncertain environment. Meanwhile, the optimized incentive rate can increase the total profits of ESPs and EUs while reducing the peak electricity demand. A short-term DR program was developed for peak electricity demand period, and the experimental results show that peak electricity demand can be reduced by 17\%. This contributes to mitigating the supply-demand imbalance and enhancing power system security. (C) 2020 Elsevier Ltd. All rights reserved.	https://dx.doi.org/10.1016/j.energy.2020.118019	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wenrui2021	Path Planning for Mobile Robot in 3D Environment Based on Ant Colony Algorithm		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112741327&doi=10.1088\%2f1742-6596\%2f1982\%2f1\%2f012095&partnerID=40&md5=1f1823fe9af742d5318153dc73a820ee	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Weydmann2022	Switching to online: Testing the validity of supervised remote testing for online reinforcement learning experiments	Online experiments are an alternative for researchers interested in conducting behavioral research outside the laboratory. However, an online assessment might become a challenge when long and complex experiments need to be conducted in a specific order or with supervision from a researcher. The aim of this study was to test the computational validity and the feasibility of a remote and synchronous reinforcement learning (RL) experiment conducted during the social-distancing measures imposed by the pandemic. An additional feature of this study was to describe how a behavioral experiment originally created to be conducted in-person was transformed into an online supervised remote experiment. Open-source software was used to collect data, conduct statistical analysis, and do computational modeling. Python codes were created to replicate computational models that simulate the effect of working memory (WM) load over RL performance. Our behavioral results indicated that we were able to replicate remotely and with a modified behavioral task the effects of working memory (WM) load over RL performance observed in previous studies with in-person assessments. Our computational analyses using Python code also captured the effects of WM load over RL as expected, which suggests that the algorithms and optimization methods were reliable in their ability to reproduce behavior. The behavioral and computational validation shown in this study and the detailed description of the supervised remote testing may be useful for researchers interested in conducting long and complex experiments online.	https://www.ncbi.nlm.nih.gov/pubmed/36220950	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Whitbrook2010	Real-world transfer of evolved artificial immune system behaviours between small and large scale robotic platforms		https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650179668&doi=10.1007\%2fs12065-010-0039-7&partnerID=40&md5=65930a6e6ddb57b5bbc34c32cf9cd69b	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	White2010	Generating three binary addition algorithms using reinforcement programming		https://doi.org/10.1145/1900008.1900072	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Whiteson2010	The Reinforcement Learning Competitions	This article reports on the reinforcement learning competitions, which have been held annually since 2006. In these events, researchers from around the world developed reinforcement learning agents to compete in domains of various complexity and difficulty. We focus on the 2008 competition, which employed fundamentally redesigned evaluation frameworks that aimed systematically to encourage the submission of robust learning methods. We describe the unique challenges of empirical evaluation in reinforcement learning and briefly review the history of the previous competitions and the evaluation frameworks they employed. We describe the novel frameworks developed for the 2008 competition as well as the software infrastructure on which they rely. Furthermore, we describe the six competition domains, present selected competition results, and discuss the implications of these results. Finally, we summarize the 2009 competition, which used the same evaluation framework but different events, and outline ideas for the future of the competition.	https://dx.doi.org/10.1609/aimag.v31i2.2227	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wiering1999	Reinforcement learning soccer teams with incomplete world models	We use reinforcement learning (RL) to compute strategies for multiagent soccer teams. RL may profit significantly from world models (WMs) estimating state transition probabilities and rewards. In high-dimensional, continuous input spaces, however, learning accurate WMs is intractable. Here we show that incomplete WMs can help to quickly find good action selection policies. Our approach is based on a novel combination of CMACs and prioritized sweeping-like algorithms. Variants thereof outperform both Q(lambda)-learning with CMACs and the evolutionary method Probabilistic Incremental Program Evolution (PIPE) which performed best in previous comparisons.	https://dx.doi.org/10.1023/A:1008921914343	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wilke2021	Estimating the optimal treatment regime for student success programs		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111839705&doi=10.1007\%2fs41237-021-00140-0&partnerID=40&md5=af5e57c95d133db3b3c509608ec89699	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Williams2017	Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning	End-to-end learning of recurrent neural networks (RNNs) is an attractive solution for dialog systems; however, current techniques are data-intensive and require thousands of dialogs to learn simple behaviors. We introduce Hybrid Code Networks (HCNs), which combine an RNN with domain-specific knowledge encoded as software and system action templates. Compared to existing end-to-end approaches, HCNs considerably reduce the amount of training data required, while retaining the key benefit of inferring a latent representation of dialog state. In addition, HCNs can be optimized with supervised learning, reinforcement learning, or a mixture of both. HCNs attain state-of-the-art performance on the bAbI dialog dataset (Bordes and Weston, 2016), and outperform two commercially deployed customer-facing dialog systems.	https://dx.doi.org/10.18653/v1/P17-1062	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wilson2021	Enabling Intelligent Onboard Guidance, Navigation, and Control Using Near-Term Flight Hardware			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wilson2022	Enabling intelligent onboard guidance, navigation, and control using reinforcement learning on near-term flight hardware	Future space missions require technological advances to meet more stringent requirements. Next generation guidance, navigation, and control systems must safely operate autonomously in hazardous and uncertain environments. While these developments often focus on flight software, spacecraft hardware also creates computational limitations for onboard algorithms. Intelligent control methods combine theories from automatic control, artificial intelligence, and operations research to derive control systems capable of handling large uncertainties. While this can be beneficial for spacecraft control, such control systems often require substantial computational power. Recent improvements in single board computers have created physically lighter and less power-intensive processors that are suitable for spaceflight and purpose built for machine learning. In this study, we implement a reinforcement learning based controller on NVIDIA Jetson Nano hardware and apply this controller to a simulated Mars powered descent problem. The proposed approach uses optimal trajectories and guidance laws under nominal environment conditions to initialise a reinforcement learning agent. This agent learns a control policy to cope with environmental uncertainties and updates its control policy online using a novel update mechanism called Extreme Q-Learning Machine. We show that this control system performs well on flight suitable hardware, which demonstrates the potential for intelligent control onboard spacecraft.	https://dx.doi.org/10.1016/j.actaastro.2022.07.013	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Winkel2020	Validation of a fully automated liver segmentation algorithm using multi-scale deep reinforcement learning and comparison versus manual segmentation	Purpose: To evaluate the performance of an artificial intelligence (AI) based software solution tested on liver volumetric analyses and to compare the results to the manual contour segmentation. Materials and methods: We retrospectively obtained 462 multiphasic CT datasets with six series for each patient: three different contrast phases and two slice thickness reconstructions (1.5/5 mm), totaling 2772 series. AI-based liver volumes were determined using multi-scale deep-reinforcement learning for 3D body markers detection and 3D structure segmentation. The algorithm was trained for liver volumetry on approximately 5000 datasets. We computed the absolute error of each automatically- and manually-derived volume relative to the mean manual volume. The mean processing time/dataset and method was recorded. Variations of liver volumes were compared using univariate generalized linear model analyses. A subgroup of 60 datasets was manually segmented by three radiologists, with a further subgroup of 20 segmented three times by each, to compare the automatically-derived results with the ground-truth. Results: The mean absolute error of the automatically-derived measurement was 44.3 mL (representing 2.37 \% of the averaged liver volumes). The liver volume was neither dependent on the contrast phase (p = 0.697), nor on the slice thickness (p = 0.446). The mean processing time/dataset with the algorithm was 9.94 s (sec) compared to manual segmentation with 219.34 s. We found an excellent agreement between both approaches with an ICC value of 0.996. Conclusion: The results of our study demonstrate that AI-powered fully automated liver volumetric analyses can be done with excellent accuracy, reproducibility, robustness, speed and agreement with the manual segmentation.	https://www.ncbi.nlm.nih.gov/pubmed/32171914	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wolfe2018	Adaptive Link Optimization for 802.11 UAV Uplink Using a Reconfigurable Antenna	This paper presents a low-cost and flexible experimental testbed for aerial communication research along with an implementation and experimental evaluation of an aerial-to-ground 802.11g link with an adaptive beamsteering antenna system. The system consists of a software-defined radio (SDR) platform, and a pattern reconfigurable antenna mounted on a hexacopter unmanned aerial vehicle (UAV). First, the system design aspects of the tesbed are described. The performance of the reconfigurable antenna is characterized through radiation pattern measurements while the antenna is mounted on the underbelly of the UAV. A low complexity reinforcement learning based adaptive antenna selection algorithm is implemented on the aerial SDR testing platform to enhance the link quality. We present SNR measurements obtained during various indoor and outdoor flight scenarios. The results show that utilizing a reconfigurable antenna and intelligent antenna selection strategy onboard a UAV provides a higher mean SNR compared to an omni-directional antenna in both line of sight (LOS) and non-line of sight (NLOS) scenarios, and is more resilient to co-channel interference.	https://dx.doi.org/10.1109/MILCOM.2018.8599696	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wong2003	Integration of soft computing towards autonomous legged robots			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wong2011	Reinforcement Learning of Robotic Motion with Genetic Programming, Simulated Annealing and Self-Organizing Map	Reinforcement learning, a sub-area of machine learning, is a method of actively exploring feasible tactics and exploiting already known reward experiences in order to acquire a near-optimal policy. The Q-table of all state-action pairs forms the basis of policy of taking optimal action at each state. But an enormous amount of learning time is required for building the Q-table of considerable size. Moreover, Q-learning can only be applied to problems with discrete state and action spaces. This study proposes a method of genetic programming with simulated annealing to acquire a fairly good program for an agent as a basis for further improvement that adapts to the constraints of an environment. We also propose an implementation of Q-learning to solve problems with continuous state and action spaces using Self-Organizing Map (SOM). An experiment was done by simulating a robotic task with the Player/Stage/Gazebo (PSG) simulator. Experimental results showed the proposed approaches were both effective and efficient.	https://dx.doi.org/10.1109/TAAI.2011.57	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wong2009	Reinforcement learning for training a computer program of Chinese chess		https://doi.org/10.1504/IJIIDS.2009.027685	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Woo2018	A study on the optimal route design considering time of mobile robot using recurrent neural network and reinforcement learning	Recently, the robots market is growing rapidly, and robots are being applied in various industrial fields. In the future, robots will work in more complex and diverse environments. For example, a robot can perform one or more tasks and collaborate with people or other robots. In this situation, the path planning for the robots to perform their tasks efficiently is an important issue. In this study, we assume that the mobile robot performs one or more tasks, moves various places freely, and works with other robots. In this situation, if the path of the mobile robot is planned with the shortest path algorithm, waiting time may occur because the planned path is blocked by other robots. Sometimes it is possible to complete a task in a shorter time than returning or performing another task first. That is, the shortest path and the shortest path do not coincide with each other. The purpose of this study is to construct a network in which the mobile robot designs the shortest path planning considering shortest time by judging itself based on environment information and path planning information of other robots. For this purpose, a network is constructed using a recurrent neural network and reinforcement learning is used. We established the environment for network learning using the robot simulation program, V-Rep. We compare the effects of various network structures and select network models that meet the purpose. In the future work, we will try to prove the effect of network by comparing existing algorithm and network.	https://dx.doi.org/10.1007/s12206-018-0941-y	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wu2022	Deep reinforcement learning based multi-layered traffic scheduling scheme in data center networks	A web search, an online video, a connected Nest device, and hundreds of cloud services all give us a response in a fraction of a second. But what really happens when we click search or send a request. The request travels over the public internet and into fiber network. Millions of requests or packets of data travel through miles of cable over land and under sea, converging at one of the many data centers that operate all over the world. Data Center (DC) is the core site of data operation, storage and forwarding, which is also an important part of cloud platform. A large number of commercial switches and servers are usually deployed at the DC. The DC is a complex set of facilities. Data center networks (DCN) is a network applied in the DC, because the traffic in DC presents the typical characteristics of centralized exchange data and increased traffic, which puts forward further requirements for the DCN. DCN connects a large-scale server cluster and is a bridge for data transmission and storage. With the expansion of DC and the increasing number of service types, communication within data centers becomes more frequent. On the other hand, traffic between data centers has also increased dramatically. Considering the multi-layered transmission mode and traffic characteristics of DCN, this paper proposes a software defined networking (SDN) -based multi-layered traffic scheduling scheme for DCN, which is mainly focused on hop count, criticality and cost. Moreover, based on SDN architecture and Deep Q-Network of Reinforcement Learning (RL), the intelligent multi-layered traffic scheduling scheme is proposed to obtain the current optimal global routing strategy according to the real-time traffic demand in the network. The simulation results show that the proposed scheme outperforms benchmarks in terms of average throughput, normalized total throughput, link bandwidth utilization, average round-trip time and network traffic bandwidth loss rate.	https://dx.doi.org/10.1007/s11276-021-02883-w	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wu2020	Reliable Compilation Optimization Phase-ordering Exploration with Reinforcement Learning	Modern compilers provide a huge number of optional compilation optimization options. Not only the selection of compilation optimization options represents a hard problem to be solved, but also the ordering of the phases is adding further complexity, making it a long standing problem in compilation research. A large number of experiments have shown that different ordering of the phases has varying degrees of influence on the program. Currently, most research focuses on the traditional optimization goals, such as execution speedup and code size optimization. In this paper, we focus on the impact of the phase-ordering on program reliability. We propose a new model with reinforcement learning algorithm A3C for finding the phase order that can improve the reliability of the program. We performed our experiments with LLVM compiler framework, considering 130 LLVM optimization options. The experimental results show that when compared with LLVM standard options and the existing phase-ordering method with genetic algorithm, the phase order found by our model can bring higher reliability gain to the program.	https://dx.doi.org/10.1109/SMC42975.2020.9283132	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wu2022a	Balance Control of an Inverted Pendulum on a Quadruped Robot by Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126372738&doi=10.1088\%2f1742-6596\%2f2187\%2f1\%2f012024&partnerID=40&md5=ddda90709bf21c6598526a84bd18bb27	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wu2018	Using Reinforcement Learning to Handle the Runtime Uncertainties in Self-adaptive Software	The growth of scale and complexity of software as well as the complex environment with high dynamic lead to the uncertainties in self-adaptive software's decision making at run time. Self-adaptive software needs the ability to avoid negative effects of uncertainties to the quality attributes of the software. However, existing planning methods cannot handle the two types of runtime uncertainties caused by complexity of system and running environment. This paper proposes a planning method to handle these two types of runtime uncertainties based on reinforcement learning. To handle the uncertainty from the system, the proposed method can exchange ineffective self-adaptive strategies to effective ones according to the iterations of execution effects at run time. It can plan dynamically to handle uncertainty from environment by learning knowledge of relationship between system states and actions. This method can also generate new strategies to deal with unknown situations. Finally, we use a complex distributed e-commerce system, Bookstore, to validate the ability of proposed method.	https://dx.doi.org/10.1007/978-3-030-04771-9_28	Included	new_screen		4
RL4SE	Wu2020a	Regression Testing of Massively Multiplayer Online Role-Playing Games	Regression testing aims to check the functionality consistency during software evolution. Although general regression testing has been extensively studied, regression testing in the context of video games, especially Massively Multiplayer Online Role-Playing Games (MMORPGs), is largely untouched so far. One big challenge is that game testing requires a certain level of intelligence in generating suitable action sequences among the huge search space, to accomplish complex tasks in the MMORPG. Existing game testing mainly relies on either the manual playing or manual scripting, which are labor-intensive and time-consuming. Even worse, it is often unable to satisfy the frequent industrial game evolution. The recent process in machine learning brings new opportunities for automatic game playing and testing. In this paper, we propose a reinforcement learning-based regression testing technique that explores differential behaviors between multiple versions of an MMORPGs such that the potential regression bugs could be detected. The preliminary evaluation on real industrial MMORPGs demonstrates the promising of our technique.	https://dx.doi.org/10.1109/ICSME46990.2020.00074	Included	new_screen		4
RL4SE	Wu2020b	Graph-Based Heuristic Search for Module Selection Procedure in Neural Module Network		https://doi.org/10.1007/978-3-030-69535-4_34	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wu2019	REINAM: reinforcement learning for input-grammar inference		https://doi.org/10.1145/3338906.3338958	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Wust2005	QoS Control Strategies for High-Quality Video Processing		https://doi.org/10.1007/s11241-005-0502-1	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Xia2021	The Application of Internet of Things and Online Video Recognition in the Cultivation of Intelligent Secretarial Talents	In 2012, the Ministry of education listed secretarial science as a special undergraduate major, which is a qualitative leap in the development of higher Secretarial Education. The traditional training of secretarial talents is mainly oriented to enterprises, government organs and news media, so the training program is more inclined to Chinese major. In this paper, a frame sampling framework based on Multi-Agent Reinforcement Learning is proposed, which is used to sample key frames in long video and improve the accuracy of video recognition. The method proposed in this paper can reduce the cost of computation and reduce the number of queries of non-target attacks by more than 28\%.	https://dx.doi.org/10.1109/ICESC51422.2021.9532713	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E5: Other not a paper	4
RL4SE	Xia2019	Virtual comissioning of manufacturing system intelligent control			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Xia2022	Optimal Scheduling of Residential Heating, Ventilation and Air Conditioning Based on Deep Reinforcement Learning	Residential heating, ventilation and air conditioning (HVAC) provides important demand response resources for the new power system with high proportion of renewable energy. Residential HAVC scheduling strategies that adapt to realtime electricity price signals formulated by demand response program and ambient temperature can significantly reduce electricity costs while ensuring occupants' comfort. However, since the pricing process and weather conditions are affected by many factors, conventional model-based method is difficult to meet the scheduling requirements in complex environments. To solve this problem, we propose an adaptive scheduling strategy for residential HVAC based on deep reinforcement learning (DRL) method. The scheduling problem can be regarded as a Markov decision process (MDP). The proposed method can adaptively learn the state transition probability to make economical decision under the tolerance violations. Specifically, the residential thermal parameters obtained by the least-squares parameter estimation (LSPE) can provide a basis for the state transition probability of MDP. Daily simulations are verified under the electricity prices and temperature data sets, and numerous experimental results demonstrate the effectiveness of the proposed method.	https://dx.doi.org/10.35833/MPCE.2022.000249	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Xiang2022	A Mobile Robot Experiment System with Lightweight Simulator Generator for Deep Reinforcement Learning Algorithm	More and more researchers are trying to use deep reinforcement learning (DRL) for mobile robot tasks due to its powerful inference capability. However, deep reinforcement learning requires a large amount of data for DRL training in the pre-experimental stage, which hinders the application of the algorithm. On the other hand, the inconsistency between the ROS data interface and DRL GYM-Like data interface leads to a high cost of migration of the algorithm verification. This paper proposes a fast simulator generation method using linear approximate kinematics model and bake-based lidar rendering methods to generate a fast approximate simulator used in the pre-experiment stage to solve the problem of data cost. At the same time, an experimental system design scheme that converts the ROS interface into a GYM-like interface is also proposed to simplify the deployment process of deep reinforcement learning. We evaluate our proposed method on collision avoidance tasks in a variety of kinematics models and lidar scenarios. Our Method achieves about 14.2 times kinematics simulation speedup and 2.56 times lidar rendering speedup. We open-sourced our simulation environment and robot system software at https://github.com/efc-robot/MultiVehicleEnv and https://github.com/efc-robot/NICS_MultiRobot_Platform	https://dx.doi.org/10.1109/ROBIO55434.2022.10011961	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Xiao2018	Improving the universality and learnability of neural programmer-interpreters with combinator abstraction			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Xiao2017	Game Theoretic Study on Channel-Based Authentication in MIMO Systems	In this paper, we investigate the authentication based on radio channel information in multiple-input multiple-output (MIMO) systems and formulate the interactions between a receiver with multiple antennas and a spoofing node as a zero-sum physical (PHY)-layer authentication game. In this game, the receiver chooses the test threshold of the hypothesis test to maximize its Bayesian risk-based utility in the spoofing detection, while the adversary chooses its attack rate, i.e., how often a spoofing signal is sent. We derive the Nash equilibrium (NE) of the static PHY-layer authentication game and present the condition that the NE exists, showing that both the spoofing detection error rates and the spoofing rate decrease with the number of transmit and receive antennas. We propose a PHY-layer spoofing detection algorithm for MIMO systems based on Q-learning, in which the receiver applies the reinforcement learning technique to achieve the optimal test threshold via trials in a dynamic game without knowing the system parameters, such as the channel time variation and spoofing cost. We also use Dyna architecture and prioritized sweeping (Dyna-PS) to improve the spoofing detection in time-variant radio environments. The proposed authentication algorithms are implemented over universal software radio peripherals and evaluated via experiments in an indoor environment. Experimental results show that the Dyna-PS-based spoofing detection algorithm further reduces the spoofing detection error rates and increases the utility of the receiver compared with the Q-learning-based algorithm, and both performances improve with more number of transmit or receive antennas.	https://dx.doi.org/10.1109/TVT.2017.2652484	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Xiao2018a	Reinforcement Learning-Based NOMA Power Allocation in the Presence of Smart Jamming	Nonorthogonal multiple access (NOMA) systems are vulnerable to jamming attacks, especially smart jammers who apply programmable and smart radio devices such as software-defined radios to flexibly control their jamming strategy according to the ongoing NOMA transmission and radio environment. In this paper, the power allocation of a base station in a NOMA system equipped with multiple antennas contending with a smart jammer is formulated as a zero-sum game, in which the base station as the leader first chooses the transmit power on multiple antennas, while a jammer as the follower selects the jamming power to interrupt the transmission of the users. A Stackelberg equilibrium of the antijamming NOMA transmission game is derived and conditions assuring its existence are provided to disclose the impact of multiple antennas and radio channel states. A reinforcement learning-based power control scheme is proposed for the downlink NOMA transmission without being aware of the jamming and radio channel parameters. The Dyna architecture that formulates a learned world model from the real antijamming transmission experience and the hotbooting technique that exploits experiences in similar scenarios to initialize the quality values are used to accelerate the learning speed of the Q-learning-based power allocation, and thus, improve the communication efficiency of the NOMA transmission in the presence of smart jammers. Simulation results show that the proposed scheme can significantly increase the sum data rates of users, and thus, the utilities compared with the standard Q-learning-based strategy.	https://dx.doi.org/10.1109/TVT.2017.2782726	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Xiao2016	PHY-Layer Spoofing Detection With Reinforcement Learning in Wireless Networks	In this paper, we investigate the PHY-layer authentication that exploits radio channel information (such as received signal strength indicators) to detect spoofing attacks in wireless networks. The interactions between a legitimate receiver and spoofers are formulated as a zero-sum authentication game. The receiver chooses the test threshold in the hypothesis test to maximize its utility based on the Bayesian risk in the spoofing detection, whereas the spoofers determine their attack frequencies to minimize the utility of the receiver. The Nash equilibrium of the static authentication game is derived, and its uniqueness is discussed. We also investigate a repeated PHY-layer authentication game for a dynamic radio environment. As it is challenging for the radio nodes to obtain the exact channel parameters in advance, we propose spoofing detection schemes based on Q-learning and Dyna-Q, which achieve the optimal test threshold in the spoofing detection via reinforcement learning. We implement the PHY-layer spoofing detectors over universal software radio peripherals and evaluate their performance via experiments in indoor environments. Both simulation and experimental results have validated the efficiency of the proposed strategies.	https://dx.doi.org/10.1109/TVT.2016.2524258	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Xiao2015	Spoofing Detection with Reinforcement Learning in Wireless Networks	In this paper, we investigate the PHY-layer authentication in wireless networks, which exploits PHY-layer channel information such as the received signal strength indicators to detect spoofing attacks. The interactions between a legitimate receiver node and a spoofer are formulated as a PHY- authentication game. More specifically, the receiver chooses the test threshold in the hypothesis test of the spoofing detection to maximize its expected utility based on Bayesian risk to detect the spoofer. On the other hand, the spoofing node decides its attack strength, i.e., the frequency to send a spoofing packet that claims to use another node's MAC address, based on its individual utility in the zero-sum game. As it is challenging for most radio nodes to obtain the exact channel models in advance in a dynamic radio environment, we propose a spoofing detection scheme based on reinforcement learning techniques, which achieves the optimal test threshold in the spoofing detection via Q-learning and implement it over universal software radio peripherals (USRP). Experimental results are presented to validate its efficiency in spoofing detection.	https://dx.doi.org/10.1109/GLOCOM.2015.7417078	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Xiao2021	Plasticity-on-Chip Design: Exploiting Self-Similarity for Data Communications	With the increasing demand for distributed big data analytics and data-intensive programs which contribute to large volumes of packets among processing elements (PEs) and memory banks, we witness a pressing need for new mathematical models and algorithms that can engineer a brain-inspired plasticity into the computing platforms by mining the topological complexity of high-level programs (HLPs) and exploiting their self-similar and fractal characteristics for designing reconfigurable domain-specific computing architectures. In this article, we present Plasticity-on-Chip (PoC) by engineering plasticity into ''artificial brains'' to mine and exploit the self-similarity of HLPs. First, we present a communication modeling of HLPs (e.g., C/C++ implementations of various applications) that relies on static and dynamic compiler analysis of programs with varying input seeds, performing comprehensive program analysis of all traces, and representing the HLPs as weighted directed acyclic graphs while capturing the intrinsic timing constraints and data/control flow requirements. Second, we propose a rigorous mathematical framework for determining the optimal parallel degree of executing a set of interacting HLPs (by partitioning them into clusters of densely interconnected supernodes - tasks) which helps us decide the number of available heterogeneous PEs, the amount of required memory and the structure of the synthesized deadlock-free irregular NoC topology that offers an efficient communication medium. These clusters serve as abstract models of computation for the synthesized PEs within the parallel execution model. Finally, exploiting the fractal and complex networks concepts, we extract in-depth features from graphs that serve as inputs for distributed reinforcement learning. Our experimental results on synthesized PEs and NoCs show performance improvements as high as 7.61x when compared to the traditional NoC and 2.6x compared to gem5-Aladdin.	https://dx.doi.org/10.1109/TC.2021.3071507	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Xie2022	Hierarchical Reinforcement Learning Based Video Semantic Coding for Segmentation	The rapid development of intelligent tasks, e.g., segmentation, detection, and classification, etc, has brought an urgent need for semantic compression, which aims to reduce the compression cost while maintaining the original semantic information. However, it is impractical to directly integrate the semantic metric into the traditional codecs since they cannot be optimized in an end-to-end manner. To solve this problem, some pioneering works have applied reinforcement learning to implement image-wise semantic compression. Nevertheless, the video semantic compression has not been explored since its complex reference architectures and compression modes. In this paper, we take a step forward to video semantic compression and propose the Hierarchical Reinforcement Learning based task-driven Video Semantic Coding, named as HRLVSC. Specifically, to simplify the complex mode decision of video semantic coding, we divided the action space into frame-level and CTU-level spaces in a hierarchical manner, and then explore the best mode selection for them progressively with the cooperation of frame-level and CTU-level agents. Moreover, since the modes of video semantic coding will exponentially increase with the number of frames in a Group of Pictures (GOP), we carefully investigate the effects of different mode selections for video semantic coding, and design a simple but effective mode simplification strategy for it. We have validated our HRLVSC on video segmentation task with HEVC reference software HM16.19. Extensive experimental results demonstrated that our HRLVSC can achieve over 39\% BD-rate saving for video semantic coding under the Low Delay P configuration.	https://dx.doi.org/10.1109/VCIP56404.2022.10008806	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Xie2020	Data driven hybrid edge computing-based hierarchical task guidance for efficient maritime escorting with multiple unmanned surface vehicles	The advancement of hardware and software technology makes multiple cooperative unmanned surface vehicles (USVs) utilized in maritime escorting with low cost and high efficiency. USVs can work as edge computing devices to locally and cooperatively perform heavy computational tasks without dependence of remote cloud servers. As such, we organize a team of USVs to escort a high value ship (e.g., mother ship) in a complex maritime environment with hostile intruder ships, where the significant challenge is to learn cooperation of USVs and assign each USV tasks to achieve optimal performance. To this end, in this paper, a hierarchical scheme is proposed for the USV team to guard a valuable ship, which belongs to problems of sparse rewards and long-time horizons in multi-agent reinforcement learning. The core idea utilized in the proposed scheme is centralized training with decentralized execution, where USVs learn policies to guard a high-value ship with extra shared environmental data from other USVs through communication. Specifically, the ships are divided into two groups, i.e., high-level ship and low-level USVs. The high-level ship optimizes decision-level policy to predict intercept points, while each low-level USV utilizes multi-agent reinforcement learning to learn task-level policy to intercept intruders. The hierarchical task guidance is exploited in maritime escorting, whereby high-level ship's decision-level policy guides the training and execution of task-level policies of USVs. Simulation results and experiment analysis show that the proposed hierarchical scheme can efficiently execute the escort mission.	https://dx.doi.org/10.1007/s12083-019-00857-6	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Xie2022a	Motion control for laser machining via reinforcement learning	Laser processing techniques such as laser machining, marking, cutting, welding, polishing and sintering have become important tools in modern manufacturing. A key step in these processes is to take the intended design and convert it into coordinates or toolpaths that are useable by the motion control hardware and result in efficient processing with a sufficiently high quality of finish. Toolpath design can require considerable amounts of skilled manual labor even when assisted by proprietary software. In addition, blind execution of predetermined toolpaths is unforgiving, in the sense that there is no compensation for machining errors that may compromise the quality of the final product. In this work, a novel laser machining approach is demonstrated, utilizing reinforcement learning (RL) to control and supervise the laser machining process. This autonomous RL-controlled system can laser machine arbitrary pre-defined patterns whilst simultaneously detecting and compensating for incorrectly executed actions, in real time. Published by Optica Publishing Group under the terms of the Creative Commons Attribution 4.0 License.	https://www.ncbi.nlm.nih.gov/pubmed/36224829	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Xu2019	Analysis and Comparison of Monte Carlo Tree Search versus ACO Algorithms in Distribution of Resources and Environment		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074442815&doi=10.1088\%2f1757-899X\%2f612\%2f5\%2f052005&partnerID=40&md5=1ff9618f0d02fe08c70638ca1d6f070f	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E5: Other not a paper	4
RL4SE	Xu2022	Online Adaptive Optimal Control Algorithm of Partial Unknown System with Adding Experience Replay and Safety Check		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140489078&doi=10.1007\%2f978-981-19-6226-4_66&partnerID=40&md5=29d90ed96543cec761bd18e912b518b6	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Xu2022a	Autonomous and cooperative control of UAV cluster with multi-agent reinforcement learning	In this paper, we expolore Multi-Agent Reinforcement Learning (MARL) methods for unmanned aerial vehicle (UAV) cluster. Considering that the current UAV cluster is still in the program control stage, the fully autonomous and intelligent cooperative combat has not been realised. In order to realise the autonomous planning of the UAV cluster according to the changing environment and cooperate with each other to complete the combat goal, we propose a new MARL framework. It adopts the policy of centralised training with decentralised execution, and uses Actor-Critic network to select the execution action and then to make the corresponding evaluation. The new algorithm makes three key improvements on the basis of Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm. The first is to improve learning framework; it makes the calculated Q value more accurate. The second is to add collision avoidance setting, which can increase the operational safety factor. And the third is to adjust reward mechanism; it can effectively improve the cluster's cooperative ability. Then the improved MADDPG algorithm is tested by performing two conventional combat missions. The simulation results show that the learning efficiency is obviously improved, and the operational safety factor is further increased compared with the previous algorithm.	https://dx.doi.org/10.1017/aer.2021.112	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Xu2022b	The research on intelligent cooperative combat of UAV cluster with multi-agent reinforcement learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116027706&doi=10.1007\%2fs42401-021-00105-x&partnerID=40&md5=2b7707790c9d256693b6b37877f9396c	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Xu2013	Tactile identification of objects using Bayesian exploration	In order to endow robots with human-like tactile sensory abilities, they must be provided with tactile sensors and intelligent algorithms to select and control useful exploratory movements and interpret data from all available sensors. Current robotic systems do not possess such sensors or algorithms. In this study we integrate multimodal tactile sensing (force, vibration and temperature) from the BioTac\textregistered with a Shadow Dexterous Hand and program the robot to make exploratory movements similar to those humans make when identifying objects by their compliance, texture, and thermal properties. Signal processing strategies were developed to provide measures of these perceptual properties. When identifying an object, exploratory movements are intelligently selected using a process we have previously developed called Bayesian exploration [1], whereby exploratory movements that provide the most disambiguation between likely candidates of objects are automatically selected. The exploration algorithm was augmented with reinforcement learning whereby its internal representations of objects evolved according to its cumulative experience with them. This allowed the algorithm to compensate for drift in the performance of the anthropomorphic robot hand and the ambient conditions of testing, improving accuracy while reducing the number of exploratory movements required to identify an object. The robot correctly identified 10 different objects on 99 out of 100 presentations.	https://dx.doi.org/10.1109/ICRA.2013.6631001	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Xu2022c	Fuzzing with Multi-dimensional Control of Mutation Strategy		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111419315&doi=10.1007\%2f978-3-030-79728-7_27&partnerID=40&md5=2980cbbae5a9f2a6a293ce860297c9f2	Included	new_screen		4
RL4SE	Xu2020	A Modified Incentive-based Demand Response Model using Deep Reinforcement Learning	In recent years, significant challenges emerge in power grid, such as peak load reducing and renewable incorporating, which can be relieved through demand response program. This paper proposes a demand reduction modified function to better describe the fluctuation of customers' energy demand, aiming to help the service provider designing an optimal incentive rate more close to the real situation of demand response. In particular, to overcome the complexity of this high dimensional and continuous decision making problem, an algorithm is designed based on reinforcement learning and deep neural network. Simulation results show that this proposed algorithm for the modified incentive-based demand response model encourages the participation of customers, and also enhances the overall profits of both service provider and customers to achieve a win-win result.	https://dx.doi.org/10.1109/APPEEC48164.2020.9220364	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Xu2019a	Deep Reinforcement Learning-Based Tie-Line Power Adjustment Method for Power System Operation State Calculation	Operation state calculation (OSC) provides safe operating boundaries for power systems. The operators rely on the software-aid OSC results to dispatch the generators for grid control. Currently, the OSC workload has increased dramatically, as the power grid structure expands rapidly to mitigate renewable source integration. However, the OSC is processed with a lot of manual interventions in most dispatching centers, which makes the OSC error-prone and personnel-experience oriented. Therefore, it is crucial to upgrade the current OSC in an automatic mode for efficiency and quality improvements. An essential process in the OSC is the tie-line power (TP) adjustment. In this paper, a new TP adjustment method is proposed using an adaptive mapping strategy and a Markov Decision Process (MDP) formulation. Then, a model-free deep reinforcement learning (DRL) algorithm is proposed to solve the formulated MDP and learn an optimal adjustment strategy. The improvement techniques of ``stepwise training'' and ``prioritized target replay'' are included to decompose the large-scale complex problems and improve the training efficiency. Finally, five experiments are conducted on the IEEE 39-bus system and an actual 2725-bus power grid of China for the effectiveness demonstration.	https://dx.doi.org/10.1109/ACCESS.2019.2949480	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Xu2005	Artificial cognitive BP-CT ant routing algorithm	This paper analyses the primary features of computing intelligence in the circumstance of multi-agents modeling, and the artificial cognitive methods with computing intelligent agents, and the artificial cognitive features in reinforcement learning and the Q-routing algorithm which is a kind of reinforcement learning in the domain of intelligent network. At the same time, aiming at the problem in AntNet routing algorithm, this paper introduces BP-CT ant routing algorithm and simulates the algorithm on OMNeT++ software platform, then proves the availability of the algorithm. Considering the global planning and optimal control theory, BP-CT ant routing algorithm has some potential aspects of intelligent control, and shows good QoS performance.	https://dx.doi.org/10.1109/IJCNN.2005.1556006	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Xu2022d	Reinforcement Learning for Load-balanced Parallel Particle Tracing	We explore an online reinforcement learning (RL) paradigm to dynamically optimize parallel particle tracing performance in distributed-memory systems. Our method combines three novel components: (1) a work donation algorithm, (2) a high-order workload estimation model, and (3) a communication cost model. First, we design an RL-based work donation algorithm. Our algorithm monitors workloads of processes and creates RL agents to donate data blocks and particles from high-workload processes to low-workload processes to minimize program execution time. The agents learn the donation strategy on the fly based on reward and cost functions designed to consider processes' workload changes and data transfer costs of donation actions. Second, we propose a workload estimation model, helping RL agents estimate the workload distribution of processes in future computations. Third, we design a communication cost model that considers both block and particle data exchange costs, helping RL agents make effective decisions with minimized communication costs. We demonstrate that our algorithm adapts to different flow behaviors in large-scale fluid dynamics, ocean, and weather simulation data. Our algorithm improves parallel particle tracing performance in terms of parallel efficiency, load balance, and costs of I/O and communication for evaluations with up to 16,384 processors.	https://www.ncbi.nlm.nih.gov/pubmed/35130159	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Xu2018	Software defined satellite attitude control algorithm based on deep reinforcement learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060247833&doi=10.13700\%2fj.bh.1001-5965.2018.0357&partnerID=40&md5=d76b364ed21ac62aba7f6d24b71792d9	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Xu2021	A Reinforcement Learning Based Approach to Identify Resource Bottlenecks for Multiple Services Interactions in Cloud Computing Environments		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101555861&doi=10.1007\%2f978-3-030-67540-0_4&partnerID=40&md5=5eeccc18636b859baaf1fb2874f16883	Included	conflict_resolution		4
RL4SE	Xu2019b	Agent-based modeling and simulation of the electricity market with residential demand response		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101561201&doi=10.17775\%2fCSEEJPES.2019.01750&partnerID=40&md5=444076c9de2c68899dc52eaaacc9bc76	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Xu2021a	Agent-based modeling and simulation for the electricity market with residential demand response		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103271355&doi=10.17775\%2fCSEEJPES.2019.01750&partnerID=40&md5=ec7e44742b2e22e2e01ba472b61bf6a3	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Xu2020a	RJCC: Reinforcement-Learning-Based Joint Communicational-and-Computational Resource Allocation Mechanism for Smart City IoT	With the fast development of smart cities and 5G, the amount of mobile data is growing exponentially. The centralized cloud computing mode is hard to support the continuous exchanging and processing of information generated by millions of the Internet-of-Things (IoT) devices. Therefore, mobile-edge computing (MEC) and software-defined networking (SDN) are introduced to form a cloud-edge-terminal collaboration network (CETCN) architecture to jointly utilize the communicational and computational resources. Although the CETCN brings many benefits, there still exist some challenges, such as the unclear operation mode, low utilization of edge resources, as well as the limited energy of terminals. To address these problems, a reinforcement learning-based joint communicational-and-computational resource allocation mechanism (RJCC) is proposed to optimize overall processing delay under energy limits. In RJCC, a Q -learning-based online offloading algorithm and a Lagrange-based migration algorithm are designed to jointly optimize computation offloading across multisegments and on edge platform, respectively. The simulation results show that the proposed RJCC outperforms the delay-optimal, energy-optimal, and edge-to-terminal offloading algorithm by 42\%-74\% in long-term average energy consumption while maintaining relatively low delay.	https://dx.doi.org/10.1109/JIOT.2020.3002427	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Xu2023	Towards effective semantic annotation for mobile and edge services for Internet-of-Things ecosystems	[2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] L. Kuang, J. Zheng, K. Li, H. Gao, Intelligent traffic signal control based on reinforcement learning with state reduction for smart cities, ACM Trans. Internet Technol. 21 (4) (2021) 102:1-102:24. S.A. Adrianna Gregory, Edward. Cone, How Digital Business Ecosystems Drive Efficiency and Innovation in a New Era, Tech. rep., Oxford Economic, 2021. B.A. Myers, J. Stylos, Improving api usability, Commun. ACM 59 (6) (2016) 62-69. H. Gao, K. Xu, M. Cao, J. Xiao, Q. Xu, Y. Yin, The deep features and attention mechanism-based method to dish healthcare under social iot systems: An empirical study with a hand-deep local-global net, IEEE Trans. Comput. Soc. Syst. (TCSS) 9 (1) (2022) 336-347. J. Gerken, H.-C. Jetter, M. Zollner, M. Mader, H. Reiterer, The concept maps method as a tool to evaluate the usability of apis, in: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 2011, pp. 373-3382. X. Gu, H. Zhang, D. Zhang, S. Kim, Deep api learning, in: Proceedings of the 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering, 2016, pp. 631-642. J. Bloch, How to design a good api and why it matters, in: Proceedings of the 21st ACM SIGPLAN Symposium on Object-Oriented Programming Systems, Languages, and Applications, 2006, pp. 506-507. M. Shi, J. Liu, D. Zhou, M. Tang, F. Xie, T. Zhang, A probabilistic topic model for mashup tag recommendation, in: Proceedings of IEEE International Conference on Web Services, ICWS, 2016, pp. 444-451. A.N. Tarekegn, M. Giacobini, K. Michalak, A review of methods for imbalanced multi-label classification, Pattern Recognit. 118 (2021) 1-11. M.R. Boutell, J. Luo, X. Shen, C.M. Brown, Learning multi-label scene classification, Pattern Recognit. 37 (9) (2004) 1757-1771. W. Liu, I. Tsang, On the optimality of classifier chain for multi-label classi-fication, in: Proceedings of Conference on Advances in Neural Information Processing Systems, 2015, pp. 1-15. X. Sun, J. Wang, J. Feng, S.-S. Chen, F. He, Classifying biomedical knowledge in pubmed using multi-label vector machines with weaker optimization constraints, Neural Comput. Appl. 28 (2017) 1233-1243. G. Chatzigeorgakidis, S. Karagiorgou, S. Athanasiou, S. Skiadopoulos, Fml-knn: scalable machine learning on big data using k-nearest neighbor joins, J. Big Data 5 (2018) 1-10. S. Liu, J. Chen, A multi-label classification based approach for sentiment classification, Expert Syst. Appl. 42 (3) (2018) 1083-1093. J. Mojoo, Y. Zhao, M. Kavitha, J. Miyao, T. Kurita, Learning with incomplete labels for multi-label image annotation using cnn and restricted boltz-mann machines, in: Proceedings of International Conference on Neural Information Processing (NeurIPS), 2019, pp. 286-298. L. Li, M. Wang, L. Zhang, H. Wang, Learning semantic similarity for multi -label text categorization, in: Proceedings of Workshop on Chinese Lexical Semantics, 2014, pp. 260-269. H. Dong, V. Suarez-Paniagua, W. Whiteley, H. Wu, Explainable automated coding of clinical notes using hierarchical label-wise attention networks and label embedding initialisation, J. Biomed. Inform. 116 (2021) 1-10. L. Qi, H. Song, X. Zhang, G. Srivastava, X. Xu, S. Yu, Compatibility-aware web API recommendation for mashup creation via textual description mining, ACM Trans. Multimed. Comput. Commun. Appl. 17 (1s) (2021) 1-19. H. Gao, X. Qin, R.J.D. Barroso, W. Hussain, Y. Xu, Y. Yin, Collaborative learning-based industrial IoT API recommendation for software-defined devices: The implicit knowledge discovery perspective, IEEE Trans. Emerg. Top. Comput. Intell. 6 (1) (2022) 66-76. M. Shi, J. Liu, D. Zhou, M. Tang, F. Xie, T. Zhang, A probabilistic topic model for mashup tag recommendation, in: Proceedings of IEEE International Conference on Web Services, ICWS, 2016, pp. 444-451. M. Shi, J. Liu, D. Zhou, Y. Tang, A topic-sensitive method for mashup tag recommendation utilizing multi-relational service data, IEEE Trans. Serv. Comput. (TSC) 14 (2) (2021) 342-355. B. Kwapong, R. Anarfi, K.K. Fletcher, A knowledge graph approach to mashup tag recommendation, in: Proceedings of IEEE International Conference on Services Computing, SCC, 2020, pp. 92-99. M. Shi, Y. Tang, Y. Huang, M. Lin, Mashup tag completion with attention-based topic model, Serv. Orient. Comput. Appl. 15 (1) (2021) 43-54. K.K. Fletcher, An attention model for mashup tag recommendation, in: Proceedings of IEEE International Conference on Services Computing, SCC, 2020, pp. 50-64. M.-L. Zhang, Z.-H. Zhou, A review on multi-label learning algorithms, IEEE Trans. Knowl. Data Eng. 26 (8) (2014) 1819-1837.	https://dx.doi.org/10.1016/j.future.2022.09.021	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Xu2022e	Codabench: Flexible, easy-to-use, and reproducible meta-benchmark platform	Obtaining a standardized benchmark of computational methods is a major issue in data-science communities. Dedicated frameworks enabling fair benchmarking in a unified environment are yet to be developed. Here, we introduce Codabench, a meta-benchmark platform that is open sourced and community driven for benchmarking algorithms or software agents versus datasets or tasks. A public instance of Codabench is open to everyone free of charge and allows benchmark organizers to fairly compare submissions under the same setting (software, hardware, data, algorithms), with custom protocols and data formats. Codabench has unique features facilitating easy organization of flexible and reproducible benchmarks, such as the possibility of reusing templates of benchmarks and supplying compute resources on demand. Codabench has been used internally and externally on various applications, receiving more than 130 users and 2,500 submissions. As illustrative use cases, we introduce four diverse benchmarks covering graph machine learning, cancer heterogeneity, clinical diagnosis, and reinforcement learning.	https://www.ncbi.nlm.nih.gov/pubmed/35845844	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Yadava2018	Attitude control of a nanosatellite system using reinforcement learning and neural networks	This paper describes a robust and efficient attitude determination and control system on-board a nanosatellite that makes use of the concepts of neural networks and reinforcement learning to develop an attitude control algorithm which can provide the required torque for stabilization of the satellite body along all three axes. The control system under consideration takes data from six sun sensors (one on each of the panels of the satellite body), a magnetometer and a gyroscope, placed inside the satellite, as input. It also requires the input from an on-board GPS module, which is run once per orbit (ideally) due to constraints of electric power in a nanosatellite system. The system consists of multiple stages, the first of which is running the orbit propagator. This will make use of the latest position and velocity vectors obtained from the GPS module for estimating the current position of the satellite, since the GPS module cannot be run on each iteration of the algorithm. The proposed system makes use of a neural network to perform the task of the orbit propagator, by forming a non-linear function for position estimation. Next, using the position vector of the satellite, the ideal orientation of the satellite is estimated in terms of the ideal magnetic field vector (using the IGRF model) and the ideal sun vector (in the orbit frame). These are then fed into the controller along with the measured magnetic field vector and the sun vector (in the body frame) to get the required torque. The controller mainly consists of two neural networks to give the torques which will help in the stabilization of the satellite. The neural networks in the controller are trained using reinforcement learning and temporal difference learning, using a modification of the actor - critic algorithm in reinforcement learning. The controller will be trained before the launch of the satellite using Software in the Loop (SIL) simulations of the desired orbit of the satellite to tune the parameters of the neural networks. Further, once the satellite is in orbit, the controller will be tuned after fixed intervals of time to adjust to any changes in the environment in the orbit.	https://dx.doi.org/10.1109/AERO.2018.8396409	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yala2022	Optimizing risk-based breast cancer screening policies with reinforcement learning	Screening programs must balance the benefit of early detection with the cost of overscreening. Here, we introduce a novel reinforcement learning-based framework for personalized screening, Tempo, and demonstrate its efficacy in the context of breast cancer. We trained our risk-based screening policies on a large screening mammography dataset from Massachusetts General Hospital (MGH; USA) and validated this dataset in held-out patients from MGH and external datasets from Emory University (Emory; USA), Karolinska Institute (Karolinska; Sweden) and Chang Gung Memorial Hospital (CGMH; Taiwan). Across all test sets, we find that the Tempo policy combined with an image-based artificial intelligence (AI) risk model is significantly more efficient than current regimens used in clinical practice in terms of simulated early detection per screen frequency. Moreover, we show that the same Tempo policy can be easily adapted to a wide range of possible screening preferences, allowing clinicians to select their desired trade-off between early detection and screening costs without training new policies. Finally, we demonstrate that Tempo policies based on AI-based risk models outperform Tempo policies based on less accurate clinical risk models. Altogether, our results show that pairing AI-based risk models with agile AI-designed screening policies has the potential to improve screening programs by advancing early detection while reducing overscreening.	https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122869623&doi=10.1038\%2fs41591-021-01599-w&partnerID=40&md5=d91b315692bc860e06d72bffdcdc6e20	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yamaba2010	A Design Support System for Discrete Production Systems with AGVs -Object-Oriented Simulator Generator and Acquisition of AGV Control Rules Using Reinforcement Learning	A simulation-based approach is one of the most effective ways of solving design problems of production systems under uncertain conditions. However, a simulator generating system is indispensable to employ this approach. Also, such a generator is required to devise an appropriate operational rule set for each simulator. In this work, a design support system was developed using object-oriented technique, which composes simulators with various configurations of software components. Reinforcement learning was adopted in order to obtain a rule set for operating each of composed simulators. Verification and evaluation of effectiveness of the method was confirmed through a series of experiments (design of lanes and locations of machines, acquisition of proper control rules of vehicles) using an automated guarded vehicle system (AGVS) as a model system.	https://dx.doi.org/10.1252/kakoronbunshu.36.127	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yamagata2021	Falsification of Cyber-Physical Systems Using Deep Reinforcement Learning	A Cyber-Physical System (CPS) is a system which consists of software components and physical components. Traditional system verification techniques such as model checking or theorem proving are difficult to apply to CPS because the physical components have infinite number of states. To solve this problem, robustness guided falsification of CPS is introduced. Robustness measures how robustly the given specification is satisfied. Robustness guided falsification tries to minimize the robustness by changing inputs and parameters of the system. The input with a minimal robustness (counterexample) is a good candidate to violate the specification. Existing methods use several optimization techniques to minimize robustness. However, those methods do not use temporal structures in a system input and often require a large number of simulation runs to minimize the robustness. In this paper, we explore state-of-the-art Deep Reinforcement Learning (DRL) techniques, i.e., Asynchronous Advantage Actor-Critic (A3C) and Double Deep Q Network (DDQN), to reduce the number of simulation runs required to find such counterexamples. We theoretically show how robustness guided falsification of a safety property is formatted as a reinforcement learning problem. Then, we experimentally compare the effectiveness of our methods with three baseline methods, i.e., random sampling, cross entropy and simulated annealing, on three well known CPS systems. We thoroughly analyse the experiment results and identify two factors of CPS which make DRL based methods better than existing methods. The most important factor is the availability of the system internal dynamics to the reinforcement learning algorithm. The other factor is the existence of learnable structure in the counterexample.	https://dx.doi.org/10.1109/TSE.2020.2969178	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yamagishi2021	Hardware-oriented deep reinforcement learning for edge computing	A new deep reinforcement learning enhancement is proposed for edge computing. This work focuses on deep Q-networks (DQNs), which are used in deep reinforcement learning. Although DQNs are typically improved through a software-based approach, hardware-specific knowledge such as that on data paths and pipelines is used for improving a DQN. The DQN performance is improved and the number of resources are reduced through an efficient hardware design that considers the learning flow and parameter search. As the scale of the problem increases, the amount of reduction in the use of resources also increases. For example, when the size of the block catch game is 5x10, the memory requirement is reduced by approximately 50\% compared to a previous DQN. The proposed hardware-oriented approach can be applied to any software technology. This study facilitates the development of novel technologies that can be realized through edge computing.	https://dx.doi.org/10.1587/nolta.12.526	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yamaguchi2010	SkyAI: Highly modularized reinforcement learning library	This paper introduces a software library of reinforcement learning (RL) methods, named SkyAI. SkyAI is a highly modularized RL library for real/simulated robots to learn behaviors. Our ultimate goal is to develop an artificial intelligence (AI) program with which the robots can learn to behave as their users' wish. In this paper, we describe the concepts, the requirements, and the current implementation of SkyAI. SkyAI provides two conflicting features: high execution-speed enough for real robot systems and high flexibility to design learning systems. We also demonstrate the applications to crawling tasks of both a humanoid robot in simulation and a real spider robot.	https://dx.doi.org/10.1109/ICHR.2010.5686285	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yamaguchi2010a	SkyAI: Highly modularized reinforcement learning library - Concepts, requirements, and implementation		https://www.scopus.com/inward/record.uri?eid=2-s2.0-79851475664&doi=10.1109\%2fICHR.2010.5686285&partnerID=40&md5=b3bdc2647ed2f625b86b8e701ea7a3f3	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yamamoto2022	Performance Evaluation of Reinforcement Learning Based Distributed Channel Selection Algorithm in Massive IoT Networks	In recent years, the demand for new applications using various Internet of Things (IoT) devices has led to an increase in the number of devices connected to wireless networks. However, owing to the limitation of available frequency resources for IoT devices, the degradation of the communication quality caused by channel congestion is a practical problem in developing IoT technology. Many IoT devices have hardware and software limitations that prevent centralized channel allocation, and congestion is even more severe in massive IoT networks without a central controller. Therefore, developing a distributed and sophisticated channel selection algorithm is necessary. In previous studies, the channel selection of each IoT device was modeled as a multi-armed bandit (MAB) problem, and a wireless channel selection method based on the MAB algorithm, which is a simple reinforcement learning, was proposed. In particular, it has been shown that the MAB algorithm of tug-of-war (TOW) dynamics can efficiently select channels with much lower computational complexity and power compared with other reinforcement learning-based channel-selection methods. This paper proposes a distributed channel selection method based on TOW dynamics in fully decentralized networks. We evaluate the effectiveness of the proposed method and other distributed channel-selection methods on the communication success rate in massive IoT networks by experiments and simulations. The results show that the proposed method improves the communication success rate more than other distributed channel selection methods even in a dense and dynamic network environment.	https://dx.doi.org/10.1109/ACCESS.2022.3186703	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yamamoto2001	An Acquisition of Evaluation Function for Shogi by Learning Self-Play		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040425414&doi=10.1111\%2f1475-3995.00267&partnerID=40&md5=4d0f982ac3e0842d021301097da291b1	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yan2020	Towards Real-Time Path Planning through Deep Reinforcement Learning for a UAV in Dynamic Environments	Path planning remains a challenge for Unmanned Aerial Vehicles (UAVs) in dynamic environments with potential threats. In this paper, we have proposed a Deep Reinforcement Learning (DRL) approach for UAV path planning based on the global situation information. We have chosen the STAGE Scenario software to provide the simulation environment where a situation assessment model is developed with consideration of the UAV survival probability under enemy radar detection and missile attack. We have employed the dueling double deep Q-networks (D3QN) algorithm that takes a set of situation maps as input to approximate the Q-values corresponding to all candidate actions. In addition, the epsilon-greedy strategy is combined with heuristic search rules to select an action. We have demonstrated the performance of the proposed method under both static and dynamic task settings.	https://dx.doi.org/10.1007/s10846-019-01073-3	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yan2019	Employee ridesharing: Reinforcement learning and choice modeling			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yang2019	On Sampling Time Maximization in Wireless Powered Internet of Things	Sensing devices operating in the upcoming Internet of Things (IoT) are likely to rely on the radio frequency (RF) transmissions of a hybrid access point (HAP) for energy. The HAP is also responsible for setting the sampling or monitoring time of these devices according to their harvested energy. This task, however, is made challenging when the HAP has imprecise knowledge of the channel gains to each device. Consequently, the HAP does not know exactly the amount of energy harvested by each device. As a result, the HAP may program a sensing device with an incorrect sampling time. To address this problem, we employ stochastic programming, and use it to determine the time used for charging, and also the sampling time of each device. Its objective is to maximize the minimum sampling time of devices. The formulated stochastic program, however, requires a model or the probability distribution of channel gains. To this end, we propose a reinforcement learning (RL) approach to solve the same problem. In addition, as the state-space contains continuous quantities, we use linear function approximation and a set of novel features to represent the large state-space. Our experiment results show that the RL approach is able to achieve 93\% of the minimum sampling time computed by the stochastic program.	https://dx.doi.org/10.1109/TGCN.2019.2907913	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yang2019a	Research on Autonomous Navigation Control of Unmanned Ship Based on Unity3D	In order to study the state of autonomous piloting of unmanned ship in different terrains, based on the Unity3D game engine software, the ship piloting simulation of the sea surface and the inland river scene is designed respectively. The autonomous navigation control of unmanned ships is implemented using a deep reinforcement learning algorithm, so as to enable the unmanned ship to improve the adaptability under the complicated and variable real-time simulation conditions, and constantly improve and optimize through machine reinforcement learning. Finally, the simulation results show that the unmanned ship can successfully avoid obstacles and reach the destination in a complex environment on the simulation platform based on Unity3D.	https://dx.doi.org/10.1109/ICCAR.2019.8813722	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yang2022	Hardware-aware Automated Architecture Search for Brain-inspired Hyperdimensional Computing	Brain-inspired neural network, a.k.a., hyperdimensional computing (HDC), has been becoming a promising candidate for resource-limited edge computing, due to its small size and robustness. However, the previous HDC architecture exploration only considers the software aspects, like HDC operations. This paper presents a hardware-aware automated architecture search framework, namely HwAwHDC, for HDC, which can consider both hardware and software characters in a uniform reinforcement learning based optimization loop. It fills the gap in the automatic design of HDC architectures for given applications and hardware constraints. We do a thorough analysis to formulate the search spaces for HwAwHDC. On top of this, we design a hardware-friendly reward and employ reinforcement learning (RL) to explore the HDC architectures. The encouraging experimental evidence shows the effectiveness of our framework. The identified model achieves a leading score on each task with lower power consumption and higher energy efficiency compared with the deep learning approach and HDC approach.	https://dx.doi.org/10.1109/ISVLSI54635.2022.00078	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yang2021	Urban Traffic Control in Software Defined Internet of Things via a Multi-Agent Deep Reinforcement Learning Approach	As the growth of vehicles and the acceleration of urbanization, the urban traffic congestion problem becomes a burning issue in our society. Constructing a software defined Internet of things(SD-IoT) with a proper traffic control scheme is a promising solution for this issue. However, existing traffic control schemes do not make the best of the advances of the multi-agent deep reinforcement learning area. Furthermore, existing traffic congestion solutions based on deep reinforcement learning(DRL) only focus on controlling the signal of traffic lights, while ignore controlling vehicles to cooperate traffic lights. So the effect of urban traffic control is not comprehensive enough. In this article, we propose Modified Proximal Policy Optimization (Modified PPO) algorithm. This algorithm is ideally suited as the traffic control scheme of SD-IoT. We adaptively adjust the clip hyperparameter to limit the bound of the distance between the next policy and the current policy. What's more, based on the collected data of SD-IoT, the proposed algorithm controls traffic lights and vehicles in a global view to advance the performance of urban traffic control. Experimental results under different vehicle numbers show that the proposed method is more competitive and stable than the original algorithm. Our proposed method improves the performance of SD-IoT to relieve traffic congestion.	https://dx.doi.org/10.1109/TITS.2020.3023788	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yang2021a	Integrating Vehicle Positioning and Path Tracking Practices for an Autonomous Vehicle Prototype in Campus Environment	This paper presents the implementation of an autonomous electric vehicle (EV) project in the National Taiwan University of Science and Technology (NTUST) campus in Taiwan. The aim of this work was to integrate two important practices of realizing an autonomous vehicle in a campus environment, including vehicle positioning and path tracking. Such a project is helpful to the students to learn and practice key technologies of autonomous vehicles conveniently. Therefore, a laboratory-made EV was equipped with real-time kinematic GPS (RTK-GPS) to provide centimeter position accuracy. Furthermore, the model predictive control (MPC) was proposed to perform the path tracking capability. Nevertheless, the RTK-GPS exhibited some robust positioning concerns in practical application, such as a low update rate, signal obstruction, signal drift, and network instability. To solve this problem, a multisensory fusion approach using an unscented Kalman filter (UKF) was utilized to improve the vehicle positioning performance by further considering an inertial measurement unit (IMU) and wheel odometry. On the other hand, the model predictive control (MPC) is usually used to control autonomous EVs. However, the determination of MPC parameters is a challenging task. Hence, reinforcement learning (RL) was utilized to generalize the pre-trained datum value for the determination of MPC parameters in practice. To evaluate the performance of the RL-based MPC, software simulations using MATLAB and a laboratory-made, full-scale electric vehicle were arranged for experiments and validation. In a 199.27 m campus loop path, the estimated travel distance error was 0.82\% in terms of UKF. The MPC parameters generated by RL also achieved a better tracking performance with 0.227 m RMSE in path tracking experiments, and they also achieved a better tracking performance when compared to that of human-tuned MPC parameters.	https://dx.doi.org/10.3390/electronics10212703	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yang2021b	A Learning Control Method of Automated Vehicle Platoon at Straight Path with DDPG-Based PID	Cooperative adaptive cruise control (CACC) has important significance for the development of the connected and automated vehicle (CAV) industry. The traditional proportional integral derivative (PID) platoon controller adjustment is not only time-consuming and laborious, but also unable to adapt to different working conditions. This paper proposes a learning control method for a vehicle platooning system using a deep deterministic policy gradient (DDPG)-based PID. The main contribution of this study is automating the PID weight tuning process by formulating this objective as a deep reinforcement learning (DRL) problem. The longitudinal control of the vehicle platooning is divided into upper and lower control structures. The upper-level controller based on the DDPG algorithm can adjust the current PID controller parameters. Through offline training and learning in a SUMO simulation software environment, the PID controller can adapt to different road and vehicular platooning acceleration and deceleration conditions. The lower-level controller controls the gas/brake pedal to accurately track the desired acceleration and speed. Based on the hardware-in-the-loop (HIL) simulation platform, the results show that in terms of the maximum speed error, for the DDPG-based PID controller this is 0.02-0.08 m/s less than for the conventional PID controller, with a maximum reduction of 5.48\%. In addition, the maximum distance error of the DDPG-based PID controller is 0.77 m, which is 14.44\% less than that of the conventional PID controller.	https://dx.doi.org/10.3390/electronics10212580	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yang2021c	Deep Reinforcement Agent for Failure-aware Job scheduling in High-Performance Computing	Job scheduling is crucial in high-performance computing (HPC), which is dedicated to deciding when and which jobs are allocated to the system and placing the jobs on which resources, by considering multiple scheduling goals. Along with the incremental of various resources and dazzling deep learning training (DLT) workloads, job failure becomes a quite common issue in HPC, which will affect user satisfaction and cluster utilization. To alleviate the influence of hardware and software errors as much as possible, in this paper, we aim to tackle the problem of failure-aware job scheduling in HPC clusters. Inspired by the success of previous studies of deep reinforcement learning-driven job scheduling, we propose a novel HPC scheduling agent named FARS (Failure-aware RL-based scheduler) by considering the effects of job failures. On the one hand, a neural network is applied to map the information of raw cluster and job states to job placement decisions. On the other hand, to consider the influence of job failure for user satisfaction and cluster utilization, FARS leverages make-span of the entire workload as the training objective. Additionally, effective exploration and experience replay techniques are applied to obtain effectively converged agent. To evaluate the capability of FARS, we design extensive trace-based simulation experiments with the popular DLT workloads. The experimental results show that, compared with the best baseline model, FARS obtains 5.69\% improvement of average make-span under different device error rates. Together, our FARS is an ideal candidate for failure-aware job scheduler in HPC clusters.	https://dx.doi.org/10.1109/ICPADS53394.2021.00061	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yang2020	Artificial Neural Networks Applied as Molecular Wave Function Solvers	We use artificial neural networks (ANNs) based on the Boltzmann machine (BM) architectures as an encoder of ab initio molecular many-electron wave functions represented with the complete active space configuration interaction (CAS-CI) model. As first introduced by the work of Carleo and Troyer for physical systems, the coefficients of the electronic configurations in the CI expansion are parametrized with the BMs as a function of their occupancies that act as descriptors. This ANN-based wave function ansatz is referred to as the neural-network quantum state (NQS). The machine learning is used for training the BMs in terms of finding a variationally optimal form of the ground-state wave function on the basis of the energy minimization. It is relevant to reinforcement learning and does not use any reference data nor prior knowledge of the wave function, while the Hamiltonian is given based on a user-specified chemical structure in the first-principles manner. Carleo and Troyer used the restricted Boltzmann machine (RBM), which has hidden units, for the neural network architecture of NQS, while, in this study, we further introduce its replacement with the BM that has only visible units but with different orders of connectivity. For this hidden-node free BM, the second- and third-order BMs based on quadratic and cubic energy functions, respectively, were implemented. We denote these second- and third-order BMs as BM2 and BM3, respectively. The pilot implementation of the NQS solver into an exact diagonalization module of the quantum chemistry program was made to assess the capability of variants of the BM-based NQS. The test calculations were performed by determining the CAS-CI wave functions of illustrative molecular systems, indocyanine green, and dinitrogen dissociation. The simulated energies have been shown to converge to CAS-CI energy in most cases by improving RBM with an increasing number of hidden nodes. BM3 systematically yields lower energies than BM2, reproducing the CAS-CI energies of dinitrogen across potential energy curves within an error of 50 mu E-h.	https://www.ncbi.nlm.nih.gov/pubmed/32320233	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yang2021d	Automatic Hierarchical Reinforcement Learning for Reusing Service Process Fragments	Prevailing research trend is to use Web services for data publishing and sharing among organizations, but existing works often fall short of service reuse. Developing efficient solutions to achieve composite services has drawn significant attention in services computing. Services and service process fragments reuse is critical to improve the efficiency of software development and economize on human and material resources, meanwhile Reinforcement Learning (RL) is one commonly used approach in services computing. However, in service composition and service process fragments (SPFs) reusing scenarios, traditional RL methods cannot guarantee good efficiency for large-scale service processes construction problems. In this paper, we present a novel SPF reusing framework that combines automatic Hierarchical Reinforcement Learning (HRL) and extended Cocke-Kasami-Younger (CKY) algorithm. This framework has the ability to reuse any granularity of SPFs. We firstly get action models and trajectories by means of analysis on historical service process fragments. Furthermore, the ``Causal Analysis'' identifies the causal relationships among the actions in a trajectory, i.e. returning a causally annotated trajectory (CAT). Then, we utilize the SPF-Hierarchy algorithm to discover a coherent task hierarchy for each service process fragment. Finally, we map the hierarchy obtained from the previous stage to the HRL-CKY algorithm, which can fulfill the reuse and retrieval of any granularity of SPFs. The effectiveness and robustness of our approach are evaluated through a set of experiments.	https://dx.doi.org/10.1109/ACCESS.2021.3054852	Included	new_screen		4
RL4SE	Yang2021e	Hit and Lead Discovery with Explorative RL and Fragment-based Molecule Generation			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yang2022a	Verification of intelligent scheduling based on deep reinforcement learning for distributed workshops via discrete event simulation		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146839706&doi=10.14743\%2fapem2022.4.444&partnerID=40&md5=8c49690985ee8a4c1e62a3c75e7919c2	Excluded	new_screen	E3: Only conceptual results are reported,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yang2020a	Designing freeform imaging systems based on reinforcement learning	"The design of complex freeform imaging systems with advanced system specification is often a tedious task that requires extensive human effort. In addition, the lack of design experience or expertise that result from the complex and uncertain nature of freeform optics, in addition to the limited history of usage, also contributes to the design difficulty. In this paper, we propose a design framework of freeform imaging systems using reinforcement learning. A trial-and-error method employing different design routes that use a successive optimization process is applied in different episodes under an e-greedy policy. An ""exploitation-exploration, evaluation and back-up"" approach is used to interact with the environment and discover optimal policies. Design results with good imaging performance and related design routes can be found automatically. The design experience can be further summarized using the obtained data directly or through other methods such as clustering-based machine learning. The experience offers valuable insight for completing other related design tasks. Human effort can be significantly reduced in both the design process and the tedious process of summarizing experience. This design framework can be integrated into optical design software and runs nonstop in the background or on servers to complete design tasks and acquire experience automatically for various types of systems. (C) 2020 Optical Society of America under the terms of the OSA Open Access Publishing Agreement"	https://www.ncbi.nlm.nih.gov/pubmed/33114913	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yang2010	Video repeat recognition and mining by visual features		https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951726473&doi=10.1007\%2f978-3-642-12900-1_12&partnerID=40&md5=2ff5ac0f7cd14b73157352ee480e3032	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yang2018	The Effect of Scalable Information on Artificial Intelligence	Many researchers would agree that it had not been for reinforcement learning, the visualization of lambda calculus might never have occurred. In fact, few scholars would disagree with the evaluation of wide-area networks, which embodies the unproven principles of concurrent software engineering [1]. We disconfirm that while the famous large-scale algorithm for the visualization of expert systems by Moore et al. is recursively enumerable, kernels[2] and Internet QoS are rarely incompatible.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E5: Other not a paper	4
RL4SE	Yang2019b	Green-Oriented Offloading and Resource Allocation by Reinforcement Learning in MEC	Mobile Edge Computing (MEC) is a promising approach to satisfy the increasing demand of computation-intensive applications in fifth-generation (5G) networks. In this MEC networks system, Smart Mobile Devices (SMDs) can implement tasks migration in close proximity to a MEC server via wireless channels. In this paper, we formulate a Green-Oriented Problem (GOP) in MEC networks to minimize the cost of energy consumption for all SMDs in MEC system. In order to address the problem, we jointly optimize the offloading decision, wireless communication resource allocation and computational resource allocation while meeting the latency constraints. Apart from this, noting that finding an optimal policy of GOP in this MEC system is a Mixed Integer Linear Program (MINLP) problem, we use the Reinforcement Learning (RL) method which takes a long-term goal into consideration. Our numerical experiments demonstrate that the proposed algorithm improves energy efficiency of the computation offloading in MEC system.	https://dx.doi.org/10.1109/SmartIoT.2019.00066	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yang2020b	A systematic study of reward for reinforcement learning based continuous integration testing		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089907018&doi=10.1016\%2fj.jss.2020.110787&partnerID=40&md5=0c6a319a56d70dcdeabc0d40d6f6f597	Excluded	new_screen	E5: Other not a paper,E5: Other not a paper	4
RL4SE	Yang2011	Genetic network programming-sarsa with subroutines for trading rules on stock markets		https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960540773&doi=10.20965\%2fjaciii.2011.p0488&partnerID=40&md5=98a804fab329684ab7e91d5d7eac62c7	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yang2020c	Data-Driven Solutions to Mixed H-2/H-infty Control: A Hamilton-Inequality-Driven Reinforcement Learning Approach, booktitle = CCTA 2020 - 4th IEEE Conference on Control Technology and Applications, pa		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094116832&doi=10.1109\%2fCCTA41146.2020.9206320&partnerID=40&md5=8b11dbbce7540795402c042b6aa3a945	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yang2021f	Adaptive Reward Computation in Reinforcement Learning based Continuous Integration Testing		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102269411&doi=10.1109\%2fACCESS.2021.3063232&partnerID=40&md5=76d69f032757103bd68cb9581c5806d5	Included	conflict_resolution		4
RL4SE	Yang2021g	Adaptive Reward Computation in Reinforcement Learning-Based Continuous Integration Testing		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102280980&doi=10.1109\%2fACCESS.2021.3063232&partnerID=40&md5=e74a52f937e7ac407150467e95369549	Included	new_screen		4
RL4SE	Yang2021h	Program Synthesis Guided Reinforcement Learning for Partially Observed Environments			Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yang2019c	MIRAS: Model-based Reinforcement Learning for Microservice Resource Allocation over Scientific Workflows	Microservice, an architectural design that decomposes applications into loosely coupled services, is adopted in modern software design, including cloud-based scientific workflow processing. The microservice design makes scientific workflow systems more modular, more flexible, and easier to develop. However, cloud deployment of microservice workflow execution systems doesn't come for free, and proper resource management decisions have to be made in order to achieve certain performance objective (e.g., response time) within constraint operation cost. Nevertheless, effective online resource allocation decisions are hard to achieve due to dynamic workloads and the complicated interactions of microservices in each workflow. In this paper, we propose an adaptive resource allocation approach for microservice workflow system based on recent advances in reinforcement learning. Our approach (1) assumes little prior knowledge of the microservice workflow system and does not require any elaborately designed model or crafted representative simulator of the underlying system, and (2) avoids high sample complexity which is a common drawback of model-free reinforcement learning when applied to real-world scenarios. We show that our proposed approach automatically achieves effective policy for resource allocation with limited number of time-consuming interactions with the microservice workflow system. We perform extensive evaluations to validate the effectiveness of our approach and demonstrate that it outperforms existing resource allocation approaches with read-world emulated workflows.	https://dx.doi.org/10.1109/ICDCS.2019.00021	Included	new_screen		4
RL4SE	Yao2019	Intelligence-Driven Networking Architecture		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107062899&doi=10.1007\%2f978-3-030-15028-0_2&partnerID=40&md5=b1f965853859781fe017cffad5ad03be	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yao2019a	Interactive semantic parsing for if-then recipes via hierarchical reinforcement learning		https://doi.org/10.1609/aaai.v33i01.33012547	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yao2019b	CoaCor: Code Annotation for Code Retrieval with Reinforcement Learning		https://doi.org/10.1145/3308558.3313632	Included	new_screen		4
RL4SE	Piette2016_5	Patient-Centered Pain Care Using Artificial Intelligence and Mobile Health Tools: Protocol for a Randomized Study Funded by the US Department of Veterans Affairs Health Services Research and Developme	"BACKGROUND: Cognitive behavioral therapy (CBT) is one of the most effective treatments for chronic low back pain. However, only half of Department of Veterans Affairs (VA) patients have access to trained CBT therapists, and program expansion is costly. CBT typically consists of 10 weekly hour-long sessions. However, some patients improve after the first few sessions while others need more extensive contact. OBJECTIVE: We are applying principles from ""reinforcement learning"" (a field of artificial intelligence or AI) to develop an evidence-based, personalized CBT pain management service that automatically adapts to each patient's unique and changing needs (AI-CBT). AI-CBT uses feedback from patients about their progress in pain-related functioning measured daily via pedometer step counts to automatically personalize the intensity and type of patient support. The specific aims of the study are to (1) demonstrate that AI-CBT has pain-related outcomes equivalent to standard telephone CBT, (2) document that AI-CBT achieves these outcomes with more efficient use of clinician resources, and (3) demonstrate the intervention's impact on proximal outcomes associated with treatment response, including program engagement, pain management skill acquisition, and patients' likelihood of dropout. METHODS: In total, 320 patients with chronic low back pain will be recruited from 2 VA healthcare systems and randomized to a standard 10 sessions of telephone CBT versus AI-CBT. All patients will begin with weekly hour-long telephone counseling, but for patients in the AI-CBT group, those who demonstrate a significant treatment response will be stepped down through less resource-intensive alternatives including: (1) 15-minute contacts with a therapist, and (2) CBT clinician feedback provided via interactive voice response calls (IVR). The AI engine will learn what works best in terms of patients' personally tailored treatment plans based on daily feedback via IVR about their pedometer-measured step counts, CBT skill practice, and physical functioning. Outcomes will be measured at 3 and 6 months post recruitment and will include pain-related interference, treatment satisfaction, and treatment dropout. Our primary hypothesis is that AI-CBT will result in pain-related functional outcomes that are at least as good as the standard approach, and that by scaling back the intensity of contact that is not associated with additional gains in pain control, the AI-CBT approach will be significantly less costly in terms of therapy time. RESULTS: The trial is currently in the start-up phase. Patient enrollment will begin in the fall of 2016 and results of the trial will be available in the winter of 2019. CONCLUSIONS: This study will evaluate an intervention that increases patients' access to effective CBT pain management services while allowing health systems to maximize program expansion given constrained resources."	https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047730597&doi=10.2196\%2fresprot.4995&partnerID=40&md5=68fd2890d6960aa9ee6d6244de1766bd	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yao2009	Dynamic mission decision-making method of the uninhabited air vehicle group based on reinforcement learning			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yarahmadi2021	A bankruptcy based approach to solving multi-agent credit assignment problem	Multi-agent systems (MAS) are one of the prominent symbols of artificial intelligence (AI) that, in spite of having smaller entities as agents, have many applications in software development, complex system modeling, intelligent traffic control, etc. Learning of MAS, which is commonly based on Reinforcement Learning (RL), is one of the problems that play an essential role in the performance of such systems in an unknown environment. A major challenge in Multi-Agent Reinforcement Learning (MARL) is the problem of credit assignment in them. In this paper, in order to solve Multi-agent Credit Assignment (MCA) problem, we present a bottom-up method based on the bankruptcy concept for the effective distribution of the credits received from the environment in a MAS so that its performance is increased. In this work, considering the Task Start Threshold (TST) of the agents as a new constraint and a multi-score environment, as well as giving priority to agents of lower TST, three methods PTST, T-MAS and T-KAg are presented, which are based on the bankruptcy concept as a sub branch of game theory. In order to evaluate these methods, seven criteria were used among which density was a new one. The simulation results of the proposed methods indicated that the performance of the proposed methods was enhanced in comparison with those of the existing methods in six parameters while it proved a weaker performance in only one parameter.	https://dx.doi.org/10.22075/ijnaa.2021.24783.2825	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ye2019	Automated vehicle's behavior decision making using deep reinforcement learning and high-fidelity simulation environment	Automated vehicles (AVs) are deemed to be the key element for the intelligent transportation system in the future. Many studies have been made to improve AVs' ability of environment recognition and vehicle control, while the attention paid to decision making is not enough and the existing decision algorithms are very preliminary. Therefore, a framework of the decisionmaking training and learning is put forward in this paper. It consists of two parts: the deep reinforcement learning (DRL) training program and the high-fidelity virtual simulation environment. Then the basic microscopic behavior, car-following (CF), is trained within this framework. In addition, theoretical analysis and experiments were conducted to evaluate the proposed reward functions for accelerating training using DAL. The results show that on the premise of driving comfort, the efficiency of the trained AV increases 7.9\% and 3.8\% respectively compared to the classical adaptive cruise control models, intelligent driver model and constant-time headway policy. Moreover, on a more complex three-lane section, we trained an integrated model combining both CF and lane-changing behavior, with the average speed further growing 2.4\%. It indicates that our framework is effective for AV's decision-making learning.	https://dx.doi.org/10.1016/j.trc.2019.08.011	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yedidsion2021	A Scavenger Hunt for Service Robots	Creating robots that can perform general-purpose service tasks in a human-populated environment has been a longstanding grand challenge for AI and Robotics research. One particularly valuable skill that is relevant to a wide variety of tasks is the ability to locate and retrieve objects upon request. This paper models this skill as a Scavenger Hunt (SH) game, which we formulate as a variation of the NP-hard stochastic traveling purchaser problem. In this problem, the goal is to find a set of objects as quickly as possible, given probability distributions of where they may be found. We investigate the performance of several solution algorithms for the SH problem, both in simulation and on a real mobile robot. We use Reinforcement Learning (RL) to train an agent to plan a minimal cost path, and show that the RL agent can outperform a range of heuristic algorithms, achieving near optimal performance. In order to stimulate research on this problem, we introduce a publicly available software stack and associated website that enable users to upload scavenger hunts which robots can download, perform, and learn from to continually improve their performance on future hunts.	https://dx.doi.org/10.1109/ICRA48506.2021.9561722	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yang2020c_1	Data-Driven Solutions to Mixed H-2/H-infty Control: A Hamilton-Inequality-Driven Reinforcement Learning Approach, booktitle = CCTA 2020 - 4th IEEE Conference on Control Technology and Applications, pa		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094116832&doi=10.1109\%2fCCTA41146.2020.9206320&partnerID=40&md5=8b11dbbce7540795402c042b6aa3a945	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yeh2018	Automatic Bridge Bidding Using Deep Reinforcement Learning	Bridge is among the zero-sum games for which artificial intelligence has not yet outperformed expert human players. The main difficulty lies in the bidding phase of bridge, which requires cooperative decision making with partial information. Existing artificial intelligence systems for bridge bidding rely on, and are thus restricted by, human-designed bidding systems or features. In this work, we propose a flexible and pioneering bridge-bidding system, which can learn either with or without the aid of human domain knowledge. The system is based on a novel deep reinforcement learning model, which extracts sophisticated features and learns to bid automatically based on raw card data. The model includes an upper-confidence-bound algorithm and additional techniques to achieve a balance between exploration and exploitation. We further study how different pieces of human knowledge can be exploited to assist the model. Our experiments demonstrate the promising performance of our proposed model. In particular, the model can advance from having no knowledge on bidding to achieving a superior performance compared with a champion-winning computer bridge program that implements a human-designed bidding system. In addition, further synergies can be extracted by incorporating expert knowledge into the proposed model.	https://dx.doi.org/10.1109/TG.2018.2866036	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yeh2017	Multistage Temporal Difference Learning for 2048-Like Games	Szubert and Ja?kowski successfully used temporal difference (TD) learning together with n -tuple networks for playing the game 2048. However, we observed a phenomenon that the programs based on TD learning still hardly reach large tiles. In this paper, we propose multistage TD (MS-TD) learning, a kind of hierarchical reinforcement learning method, to effectively improve the performance for the rates of reaching large tiles, which are good metrics to analyze the strength of 2048 programs. Our experiments showed significant improvements over the one without using MS-TD learning. Namely, using 3-ply expectimax search, the program with MS-TD learning reached 32768-tiles with a rate of 18.31\%, while the one with TD learning did not reach any. After further tuned, our 2048 program reached 32768-tiles with a rate of 31.75\% in 10,000 games, and one among these games even reached a 65536-tiles, which is the first ever reaching a 65536-tiles to our knowledge. In addition, MS-TD learning method can be easily applied to other 2048-like games, such as Threes. Based on MS-TD learning, our experiments for Threes also demonstrated similar performance improvement, where the program with MS-TD learning reached 6144-tiles with a rate of 7.83\%, while the one with TD learning only reached 0.45\%.	https://dx.doi.org/10.1109/TCIAIG.2016.2593710	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yen2019	Scaffolding Learning for the Novice Players of Go		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076786489&doi=10.1007\%2f978-3-030-35343-8_15&partnerID=40&md5=119ec6ee72a309f0f6249bf96397b7e0	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yesilevskyi2019	ISTM networks for anaerobic digester control		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076252801&doi=10.29202\%2fnvngu\%2f2019-5\%2f21&partnerID=40&md5=a68cfdc9a1e2fdda25514f43673eedf8	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yin2012	Applying temporal difference learning to acquire a high-performance position evaluation function	Chinese-Chess is more complex board game than International-Chess, it is usually played on a square grid containing 10 $\times$ 9 intersections. In Chinese-Chess Computer Game (CCCG), the most time-consuming aspect of building a high performance game playing program is the design, implementation and tuning of the position evaluation function. In this paper, a three-layer fully-connected feed forward neural network is designed as a position evaluation function. Temporal difference learning (TDL) is a reinforcement learning algorithm, which uses the difference of a pair of successive position-values to incrementally update the weights. Based on the three-layer neural network with single output, we derive a new weight updating rule for applying TD(?) in CCCG. Starting with random initial weights between -0.5 and 0.5, the neural network is trained through the new rule on the grand-master database games. In the training process, each grand-master game is learned iteratively by the neural network until the evaluation value of the position in grand-master game becomes stable ultimately. In the experiments, we validate that our learned evaluation function is feasible and effective.	https://dx.doi.org/10.1109/ICCSE.2012.6295031	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yin2012a	Reinforcement Learning for Improving Gene Identification Accuracy by Combination of Gene-Finding Programs		https://doi.org/10.4018/jamc.2012010104	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yin2021	An reinforcement learning approach for allocating software resources	Software resource allocation is an significant factor of system configuration which plays a critical role in guaranteeing the performance of multitier web service systems. Computing the optimal allocation of different software resources in order to meet performance requirements under dynamic workloads conditions is in highly challenging. Existing approaches mostly rely on translating domain knowledge from experts into computational solutions through heuristics-based optimization techniques. While such techniques are useful, they cannot leverage actual usage data generated by system users which may contain allocation strategies that are not captured by domain experts' knowledge. In this paper, we propose an iterative feedback mechanism which solves the problem to some extent by optimizing software resource allocation of multitier web systems through imitating system users who have achieved excellent performance. Specifically, we propose a deep Q-learning network-based approach for performance prediction to deal with the dynamic changes of complex workloads. The performance prediction method involves the reinforcement learning method for capturing the dynamics of online software resource allocation, and then computing the current optimal policy. We implement the approach in the multitier web benchmark system, and the experimental results demonstrated significant improvement compared to models built based on domain knowledge.	https://dx.doi.org/10.1002/cpe.6349	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Ying2003	Microscopic urban traffic simulation with multi-agent system	Computer traffic simulation is important for making new traffic control and traffic guidance strategies. Microscopic traffic simulators can model traffic flow in a realistic manner. In this paper, a framework of a microscopic urban traffic simulator based on multi-agent system is introduced. The traffic light control agent and the vehicle-driver agent, which are the most important agents, are described in detail. The agents are implemented in a prototype simulator program written in MS C++, which consists of multi-lane roads, intersections, traffic lights and vehicles. Under the environment of the simulator, preliminary experiments have verified the efficiency of a reinforcement learning traffic light control method designed by us.	https://dx.doi.org/10.1109/ICICS.2003.1292784	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yingzi2009	Multi-agent Co-evolutionary Scheduling Approach Based on Genetic Reinforcement Learning	The paper presents an adaptive iterative distributed scheduling algorithm that operates dynamically to schedule the job in the dynamic job-shop. The manufacturing system is scheduled by the multi-agent system where every machine and job is associated with its own software agent. Each agent learns how to select presumably good schedules, by this way the size of the search space can be reduced. In order to get adaptive behavior, genetic algorithm is incorporated to drive parallel search and the evolution direction. Meanwhile, the reinforcement learning system is done with the phased Q-learning by defining the intermediate state pattern. The paper suggests a cooperation technique for the agents, as well. We also analyze the time and the solution and present some experimental results.	https://dx.doi.org/10.1109/ICNC.2009.475	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yom-Tov2008	A self-optimized job scheduler for heterogeneous server clusters	Heterogeneous clusters and grid infrastructures are becoming increasingly popular. In these computing infrastructures, machines have different resources, including memory sizes, disk space, and installed software packages. These differences give rise to a problem of over-provisioning, that is, sub-optimal utilization of a cluster due to users requesting resource capacities greater than what their jobs actually need. Our analysis of a real workload file (LANL CM5) revealed differences of up to two orders of magnitude between requested memory capacity and actual memory usage. This paper presents an algorithm to estimate actual resource capacities used by batch jobs. Such an algorithm reduces the need for users to correctly predict the resources required by their jobs, while at the same time managing the scheduling system to obtain superior utilization of available hardware. The algorithm is based on the Reinforcement Learning paradigm; it learns its estimation policy on-line and dynamically modifies it according to the overall cluster load. The paper includes simulation results which indicate that our algorithm can yield an improvement of over 30\% in utilization (overall throughput) of heterogeneous clusters.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E3: Only conceptual results are reported	4
RL4SE	Yoo2020	Reinforcement learning-based SLC cache technique for enhancing SSD write performance			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Younes2020	Agent-mediated application emergence through reinforcement learning from user feedback	Cyber-physical and ambient systems surround the human user with applications that should be tailored as possible to her/his preferences and the current situation. We propose to build them automatically and on the fly by composition of software components present at the time in the environment, but without prior expression of the user's needs or process specification or composition model. In order to produce knowledge useful for automatic composition in the absence of an initial guideline, we have developed a generic solution based on lifelong online reinforcement learning. It is decentralized within a multi-agent system where agents learn incrementally from user feedback to satisfy her/him. Different use cases have been experimented in which applications, adapted to the user and the situation, are composed and emerge automatically and continuously.	https://dx.doi.org/10.1109/WETICE49692.2020.00009	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Young2022	Enabling Artificial Intelligence Studies in Off-Road Mobility Through Physics-Based Simulation of Multiagent Scenarios	"We describe a simulation environment that enables the design and testing of control policies for off-road mobility of autonomous agents. The environment is demonstrated in conjunction with the training and assessment of a reinforcement learning policy that uses sensor fusion and interagent communication to enable the movement of mixed convoys of human-driven and autonomous vehicles. Policies learned on rigid terrain are shown to transfer to hard (silt-like) and soft (snow-like) deformable terrains. The environment described performs the following: multivehicle multibody dynamics cosimulation in a time/space-coherent infrastructure that relies on the Message Passing Interface standard for low-latency parallel computing; sensor simulation (e.g., camera, GPU, IMU); simulation of a virtual world that can be altered by the agents present in the simulation; training that uses reinforcement learning to ""teach"" the autonomous vehicles to drive in an obstacle-riddled course. The software stack described is open source. Relevant movies: Project Chrono. Off-road AV simulations, 2020(2)."	https://dx.doi.org/10.1115/1.4053321	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Young2021	Enabling artificial intelligence studies in off-road mobility through physics-based simulation of multi-agent scenarios		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120489953&doi=10.1115\%2fDETC2021-67070&partnerID=40&md5=8dcb1ad1e1598220dad8378c7f57a4be	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yu2020	When QoE meets learning: A distributed traffic-processing framework for elastic resource provisioning in HetNets		https://doi.org/10.1016/j.comnet.2019.106904	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yu2022	Research on Reactive Power Optimization Strategy under the Intelligent Improvement Model of the Distribution Network		https://doi.org/10.1155/2022/9310507	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yu2020a	Application of Reinforcement Learning in the Design of Microwave Circuit Matching Structure	In the context of the rapid development of machine learning and intelligent manufacturing, we try to introduce intelligent mechanism into the design and production of microwave components. In this paper, an automatic design method for matching structure of microwave components based on reinforcement learning (Q-learning) is proposed, by using the return value extracted from the Smith Chart and optimization of algorithm parameters with microwave theory. Its feasibility is verified by the combined use of the software Ansoft HFSS and Matlab. Compared with random search, Q-learning algorithm can help us to analyze each simulation result and accumulate experience, shorten the simulation time and reduce the burden of designers greatly. Besides, we introduce an empirical factor epsilon to improve the convergence efficiency. The algorithm can be further optimized with the empirical factor, especially in the first-round learning, in the absence of simulation experience, the prior experience helps us reduce the time of simulation obviously.	https://dx.doi.org/10.1007/978-981-32-9441-7_17	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yu2018	Ship trajectory tracking using improved simulated annealing and reinforcement learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072314310&doi=10.1109\%2fICInfA.2018.8812464&partnerID=40&md5=8fe25cd531700bf08e63ca8bf7cd8d86	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yu2018a	Dynamic Control Flow in Large-Scale Machine Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050890739&doi=10.1145\%2f3190508.3190551&partnerID=40&md5=df44df6f0ee5a17c4de311dcebcfcdeb	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yuan2021	Artificial Intelligence Empowered QoS-Oriented Network Association for Next-Generation Mobile Networks	The increasing complexity and dynamics of 5G mobile networks have brought revolutionary changes in its modeling and control, where efficient routing and resource allocation strategies become beneficial. Software-Defined Network (SDN) makes it possible to achieve the automatic management of network resources. Relying on the powerful decision-making capability of SDNs, network association can be flexibly implemented for adapting to the dynamic of the real-time network status. In this paper, we first construct a jitter graph-based network model as well as a Poisson process-based traffic model in the context of 5G mobile networks. Second, we solve the problem of QoS routing with resource allocation based on queueing theory using a low computational complexity greedy algorithm, which takes finding a feasible path set as the main task and resource allocation as the auxiliary task. Finally, we design a QoS-oriented adaptive routing scheme based on Deep Reinforcement Learning (DRL) SPACE, which is a DRL architecture with parameterized action space, in order to find an optimal path from the source to the destination. To validate the feasibility of the greedy QoS routing strategy with resource allocation, we make a numerical packet-level simulation to model a M/M/C/N queuing system. Moreover, extensive simulation results demonstrate that our proposed routing strategy is able to improve the traffic's QoS metrics, such as the packet loss ratio and queueing delay.	https://dx.doi.org/10.1109/TCCN.2021.3065463	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yuan2021a	Application of Deep Reinforcement Learning Algorithm in Uncertain Logistics Transportation Scheduling	Nowadays, finding the optimal route for vehicles through online vehicle path planning is one of the main problems that the logistics industry needs to solve. Due to the uncertainty of the transportation system, especially the last-mile delivery problem of small packages in uncertain logistics transportation, the calculation of logistics vehicle routing planning becomes more complex than before. Most of the existing solutions are less applied to new technologies such as machine learning, and most of them use a heuristic algorithm. This kind of solution not only needs to set a lot of constraints but also requires much calculation time in the logistics network with high demand density. To design the uncertain logistics transportation path with minimum time, this paper proposes a new optimization strategy based on deep reinforcement learning that converts the uncertain online logistics routing problems into vehicle path planning problems and designs an embedded pointer network for obtaining the optimal solution. Considering the long time to solve the neural network, it is unrealistic to train parameters through supervised data. This article uses an unsupervised method to train the parameters. Because the process of parameter training is offline, this strategy can avoid the high delay. Through the simulation part, it is not difficult to see that the strategy proposed in this paper will effectively solve the uncertain logistics scheduling problem under the limited computing time, and it is significantly better than other strategies. Compared with traditional mathematical procedures, the algorithm proposed in this paper can reduce the driving distance by 60.71\%. In addition, this paper also studies the impact of some key parameters on the effect of the program.	https://doi.org/10.1155/2021/5672227	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yun2019	Reinforcement Learning as a Pre-Diagnostic Tool for TCP/IP Protocols on In-Car Networks	In-car networks such as Automotive Ethernet will enable new services in smart vehicles, but they are an untrodden territory for many network protocols that should support the applications. For instance, TCP has been newly incorporated as an integral part of AUTOSAR, the standard software framework for electronic control units (ECUs). However, the in- vehicle network environment can be starkly different from the Internet where TCP has been optimized. The disparity between the two environments warrants re-evaluation of the Internet protocols such as TCP in the forecasted in-car network environments. In this paper, we propose a novel approach where we employ reinforcement learning as a pre-diagnostic tool to predict potential problems and to present possible remedies.	https://dx.doi.org/10.1109/VTCFall.2019.8891225	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yusof2015	Simulation of mobile robot navigation utilizing reinforcement and unsupervised weightless neural network learning algorithm	The approach of transforming human expert knowledge into computer program only allow a system to solve foreseen and tested outcomes compared to a system having self-learning capabilities. This paper will summarize and discuss the research, design and implementation of a novel self-learning algorithm which combines: (a) Q-Learning - A reinforcement learning algorithm; and (b) AutoWiSARD - An unsupervised weightless neural network learning algorithm. The self-learning algorithm was implemented in an autonomous mobile robot navigation and obstacle avoidance system in a simulated environment. The AutoWiSARD algorithm identifies, differentiates and classifies the obstacles and the Q-learning algorithm learns and tries to maneuver through these obstacles. This novel hybrid technique allows the autonomous system to acquire knowledge, learn and record experience thus attaining self-learning state. The final result shows the simulated mobile robot was able to differentiate various shapes of obstacles such as corners and walls; and create complex control sequences of movements to maneuver through these obstacles.	https://dx.doi.org/10.1109/SCORED.2015.7449308	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yuvaraj2021	An Improved Task Allocation Scheme in Serverless Computing Using Gray Wolf Optimization (GWO) Based Reinforcement Learning (RIL) Approach		https://doi.org/10.1007/s11277-020-07981-0	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zahedi2022	DSPVR: dynamic SFC placement with VNF reuse in Fog-Cloud Computing using Deep Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142936229&doi=10.1007\%2fs12652-022-04465-w&partnerID=40&md5=af6eee5c271185cb997574af0b98d810	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zaitceva2022	Real-time Reinforcement Learning of Vibration Machine PI-controller	Controller tuning is a standard engineering task. To quickly adjust the controller settings in real-time, it becomes necessary to use intelligent control algorithms. In this paper, we propose an approach to tuning the speed controller of a vibration machine, which will ensure its maximum performance, using the reinforcement learning method. In this context of problem solving, the policy is presented in a parametric family of controller gains. In this case, the agent interacts with the virtual environment and the PI controller is implemented the software. The effectiveness of the proposed approach has been verified by real-time simulation and experiments on the two-rotor vibration unit. The advantage of the described learning algorithm is that the complex system is considered a black box. Thus, it is required to know the reference drive speed and measure the output speed.	https://dx.doi.org/10.1109/DCNA56428.2022.9923235	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zakharov2015	Students Interdisciplinary Knowledge Estimation with Analysis of his (her) Behavior in Social Network: Ontological Approach	The paper considers task of quantitative estimation of the student's interdisciplinary knowledge. A set of estimation methods based on subject ontology usage formalized as a semantic network is been proposed. The following machine learning types were used: supervised learning, unsupervised learning, semi-supervised learning, reinforcement learning, active learning, multi-level learning, multitasking learning. The prototype of a software system that extracts information about the activity of students in social networks and evaluates their interdisciplinary knowledge with the use of these methods is being presented.	https://dx.doi.org/10.1007/978-3-319-23766-4_47	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zanette2021	Provable Benefits of Actor-Critic Methods for Offline Reinforcement Learning			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zeadally2020	Securing Internet of Things (IoT) with machine learning	Advances in hardware, software, communication, embedding computing technologies along with their decreasing costs and increasing performance have led to the emergence of the Internet of Things (IoT) paradigm. Today, several billions of Internet-connected devices are part of the IoT ecosystem. IoT devices have become an integral part of the information and communication technology (ICT) infrastructure that supports many of our daily activities. The security of these IoT devices has been receiving a lot of attention in recent years. Another major recent trend is the amount of data that is being produced every day which has reignited interest in technologies such as machine learning and artificial intelligence. We investigate the potential of machine learning techniques in enhancing the security of IoT devices. We focus on the deployment of supervised, unsupervised learning techniques, and reinforcement learning for both host-based and network-based security solutions in the IoT environment. Finally, we discuss some of the challenges of machine learning techniques that need to be addressed in order to effectively implement and deploy them so that they can better protect IoT devices.	https://dx.doi.org/10.1002/dac.4169	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zeppenfeld2010	Autonomic workload management for multi-core processor systems		https://doi.org/10.1007/978-3-642-11950-7_6	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhan2022	Reinforcement learning-based register renaming policy for simultaneous multithreading CPUs?		https://doi.org/10.1016/j.eswa.2021.115717	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhan2022_1	Reinforcement learning-based register renaming policy for simultaneous multithreading CPUs?		https://doi.org/10.1016/j.eswa.2021.115717	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhan2021	Reinforcement learning-based register renaming policy for simultaneous multithreading CPUs[Formula presented]		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112332236&doi=10.1016\%2fj.eswa.2021.115717&partnerID=40&md5=d04c0b5dbedba21b170f814ecbfca583	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhan2021a	Reinforcement learning-based register renaming policy for simultaneous multithreading CPUs	Simultaneous multithreading (SMT) improves the performance of superscalar CPUs by exploiting thread level parallelism with shared entries for better utilization of resources. A key issue for this out-of-order execution is that the occupancy latency of a physical rename register can be undesirably long due to many program execution-dependent factors that result in performance degradation. Such an issue becomes even more problematic in an SMT environment in which these registers are shared among concurrently running threads. Smartly managing this critical shared resource to ensure that slower threads do not block faster threads' execution is essential to the advancement of SMT performance. In this paper, an actor-critic style reinforcement learning (RL) algorithm is proposed to dynamically assigning an upper-bound (cap) of the rename registers any thread is allowed to use according to the threads' real-time demand. In particular, a critic network projects the current Issue Queues (IQ) usage, register file usage, and the cap value to a reward; an actor network is trained to project the current IQ usage and register file usage to the optimal real-time cap value via ascending the instructions per cycle (IPC) gradient within the trajectory distribution. The proposed method differs from the state-of-the-art (Wang and Lin, 2018) as the cap for the rename registers for each thread is adjusted in real-time according to the policy and state transition from self-play. The proposed method shows an improvement in IPC up to 162.8\% in a 4-threaded system, 154.8\% in a 6-threaded system and up to 101.7\% in an 8-threaded system. The code is now available open source at https://github.com/98k-bot/RL-based-SMT-Register-Renaming-Policy.	https://dx.doi.org/10.1016/j.eswa.2021.115717	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhan2022a	Optimization of Sensing Locations of Autonomous Underwater Vehicles for Optimal Environmental Prediction and Acoustic Target Tracking	A distributed, mobile, cooperative, and autonomous sensor network formed by a collection of autonomous underwater vehicles equipped with various types of environmental and acoustic sensors is an important type of tool in many underwater monitoring and surveillance applications. optimizing the spatial-temporal sensing locations of the networked autonomous underwater vehicles to enable the vehicles to collect informative spatial-temporal data streams for accurate predictions of the states of the dynamic underwater environments and maneuvering targets is a key issue for the efficient operation of such robotic sensor networks. This paper presents a developed software tool to implement the optimization. In the developed tool, a deep reinforcement learning-based strategy is developed to optimize the sensing locations of a swarm of autonomous underwater gliders for the accurate prediction of three-dimensional states of ocean environments, where the prediction is implemented with a data-driven ocean model based on the dynamic mode decomposition method with the sparse glider sensing data. And a Monte Carlo tree search-based strategy is developed to optimize the motion path of an autonomous underwater vehicle for detecting and tracking a target with sonar sensor, where the ensemble forecasts of environments are taken into account in the prediction of sonar performance of probability of detection and the path optimization. This paper presents the two developed strategies of optimal sensor placement and path planning and the simulation demonstrations showing their performance.	https://dx.doi.org/10.1109/OCEANS47191.2022.9977025	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2022	Online Updating Energy Management Strategy Based on Deep Reinforcement Learning With Accelerated Training for Hybrid Electric Tracked Vehicles	An online updating energy management strategy (EMS) based on deep reinforcement learning (DRL) with accelerated training is proposed to further reduce fuel consumption and improve the adaptability of the algorithm. The online frame continuously updates neural network parameters every predetermined time. First, the mathematical model of a series hybrid electric tracked vehicle (SHETV) is established, and the accuracy of the model is verified by collecting data from the real vehicle. Second, an intelligent EMS is developed by combining deep deterministic policy gradient (DDPG) and prioritized experience replay (PER). DDPG can improve the control effect and accelerate the training process by eliminating the discretization of variables. The addition of PER can further improve fuel economy and SOC performance and shorten the training time. Third, an online updating framework based on DDPG-PER is proposed to make EMS better adapt to complex driving conditions. Finally, a software-in-the-loop simulation is built, and the effectiveness of the proposed online updating EMS is verified by comparison with other state-of-the-art algorithms. Simulation results show that the training process of the off-line EMS is greatly accelerated, which provides a potential for online application. Meanwhile, the fuel economy of the online updating EMS reached 93.9\% of the benchmark DP.	https://dx.doi.org/10.1109/TTE.2022.3156590	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2021	An Efficient Dynamic Optimization Algorithm for Path-Constrained Switched Systems	Dynamic optimization is one of the model-based adaptive reinforcement learning methods, which has been widely used in industrial systems with switching mechanisms. This article presents an efficient dynamic optimization strategy to locate an optimal input and switch times for switched systems with guaranteed satisfaction for path constraints during the whole time period. In this article, we propose a single-level algorithm where, at each iteration, gradients of the objective function with respect to switch times and the system input are evaluated by solving adjoint systems and sensitivity equations, respectively. Then the optimization of the input is performed at the same iteration with that of the switch time vector, which greatly reduces the number of nonlinear programs (NLPs) and computational burden compared with multistage algorithms. The feasibility of the optimal solution is guaranteed by adapting a new policy iteration method proposed to switched systems. It is proven that the proposed algorithm terminates finitely, and converges to a solution which satisfies the Karush-Kuhn-Tucker (KKT) conditions to specified tolerances. Numerical case studies are provided to illustrate that the proposed algorithm has less expensive computational time than the bi-level algorithm.	https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119613075&doi=10.1109\%2fTNNLS.2021.3113345&partnerID=40&md5=e82fb495a2b023065c868886e082594b	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2019	A cooperative multi-agent deep reinforcement learning framework for real-time residential load scheduling		https://doi.org/10.1145/3302505.3310069	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2022a	Deep Reinforcement Learning in a Dynamic Environment: A Case Study in the Telecommunication Industry	Reinforcement learning, particularly deep reinforcement learning, has made remarkable progress in recent years and is now used not only in simulators and games but is also making its way into embedded systems as another software-intensive domain. However, when implemented in a real-world context, reinforcement learning is typically shown to be fragile and incapable of adapting to dynamic environments. In this paper, we provide a novel dynamic reinforcement learning algorithm for adapting to complex industrial situations. We apply and validate our approach using a telecommunications use case. The proposed algorithm can dynamically adjust the position and antenna tilt of a drone-based base station to maintain reliable wireless connectivity for mission-critical users. When compared to traditional reinforcement learning approaches, the dynamic reinforcement learning algorithm improves the overall service performance of a drone-based base station by roughly 20\%. Our results demonstrate that the algorithm can quickly evolve and continuously adapt to the complex dynamic industrial environment.	https://dx.doi.org/10.1109/SEAA56994.2022.00019	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2022b	Offline reinforcement learning for eco-driving control at signalized intersections		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137670261&doi=10.3969\%2fj.issn.1001-0505.2022.04.018&partnerID=40&md5=e3d1a39da057130e9c3a3db00fac5645	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2022c	Online routing and spectrum allocation in elastic optical networks based on dueling Deep Q-network	The study of online routing and spectrum allocation (RSA) problem has assumed increasing importance due to the exponential growth of dynamic traffic with uncertainty in elastic optical networks (EONs). This paper first formulates offline RSA as a mixed-integer linear program (MILP) with the consideration of the time attribute, and then models online RSA by a carefully designed Markov decision process (MDP), including the state, action and reward. To deal with such a complex dynamic optimization problem, a novel algorithm based on the classic deep reinforcement learning (DRL) framework, Deep Q-network (DQN), is developed. For further promoting the training efficiency, a dueling network architecture, an improved E-greedy strategy and a series of parameter adjustments are applied. Simulation results demonstrate the effectiveness of the proposed algorithm and show its superiority in reducing the blocking probability compared with the state of the art, which further exhibits the potential of applying DRL to solve such complex real-time decision-making problems.	https://dx.doi.org/10.1016/j.cie.2022.108663	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2022d	A Distributed Multi-Robot Path Planning Algorithm for Searching Multiple Hidden Targets	This paper proposes a new distributed multi-robot path planning algorithm based on fuzzy logic control and reinforcement learning, which can navigate for robots searching for hidden targets in unknown environment. The algorithm is composed of two controllers, a fuzzy logic controller based on multiple behavior coordination strategy and a policy controller based on deep reinforcement learning. The first controller is used for roaming search to find the hidden targets, and the role of the second controller is to navigate from the current position to the targets. To check the performance of the algorithm, we simulate it in simulation environment built in the CoppeliaSim simulation software and implemented by e-Puck robots. The simulation results demonstrate the effectiveness of the proposed method.	https://dx.doi.org/10.1109/ICICML57342.2022.10009821	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2022e	Real-Time Generation Method of Oil Painting Style Brushstrokes Based on Inverse Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133207305&doi=10.1155\%2f2022\%2f2996960&partnerID=40&md5=b9096fb8a231bbc02d5f9423c0a7838b	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2018	An AI based High-speed Railway Automatic Train Operation System Analysis and Design	Recent years, the research and application of High-Speed Railway (HSR) automatic train operation (ATO) system are under fast development, while the safety, energy efficiency and passenger comfort of ATO systems still need improvement. On the other hand, Artificial Intelligence (AI) technology, for example, Deep Learning, has been widely applied in automata industry such as robot control and driverless vehicle. In this paper, we propose a new idea of improving train control system performance with AI technologies such as Deep Reinforcement Learning and Imitation learning, and describe the system objective, structure and development process. The details of key processes such as establishment of Train Running Condition Evaluation Index, acquisition and processing of relevant big data, construction of AI based automatic train operation model and the program of simulation and experiment are presented in this paper, which provides a brand new and practical idea to the development of High-Speed Railway automatic train operation systems.	https://dx.doi.org/10.1109/ICIRT.2018.8641650	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2022f	GA-SCS: Graph-Augmented Source Code Summarization		https://doi.org/10.1145/3554820	Included	new_screen		4
RL4SE	Zhang2020	Survey of Machine Learning Enabled Software Self-adaptation		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089897087&doi=10.13328\%2fj.cnki.jos.006076&partnerID=40&md5=7d6c7ebc2ac2f5c5ef17b3ea9ba8a4df	Excluded	conflict_resolution	E5: Other not a paper,E5: Other not a paper	4
RL4SE	Zhang2022g	Evaluation of Reinforcement-Learning Queue Management Algorithm for Undersea Acoustic Networks Using ns-3		https://doi.org/10.1145/3532577.3532601	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2021a	ReLeDP: Reinforcement-Learning-Assisted Dynamic Pricing for Wireless Smart Grid	The smart grid must ensure that power providers can obtain substantial benefits by selling energy, while at the same time, they need to consider the cost of consumers. To realize this win-win situation, the smart grid relies on dynamic pricing mechanisms. However, most of the existing dynamic pricing schemes are based on artificial objective rules or conventional models, which cannot ensure the desired effectiveness. Thus, we apply reinforcement learning to model the supply-demand relationship between power providers and consumers in a smart grid. The dynamic pricing problem of the smart grid is modeled as a discrete Markov decision process, and the decision process is solved by Q-learning. Now, the success of any intelligent dynamic pricing scheme relies on timely data transmission. However, the scale and speed of data generation can create several network bottlenecks that can further reduce the performance of any dynamic pricing scheme. Hence, to overcome this challenge, we have proposed an artificial-intelligence-based adaptive network architecture that adopts software-defined networking. In this architecture, we have used a self-organized map-based traffic classification approach followed by a dynamic virtual network embedding mechanism. We demonstrate the effectiveness of the dynamic pricing strategy supported through adaptive network architecture based on various performance indicators. The outcomes suggest that the proposed strategy is of great significance to realize the sustainability of power energy in the future. Lastly, we discuss various implementation challenges and future directions before concluding the article.	https://dx.doi.org/10.1109/MWC.011.2000431	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2019a	Two-level task scheduling with multi-objectives in geo-distributed and large-scale SaaS cloud	With the exploding of data-intensive web applications and requests (tasks), geo-distributed and large-scale data centers (DCs) are widely deployed in Software as a Service (SaaS) cloud, but server failures continue to grow at the same time. In this context, task scheduling problems become more intricate and both scheduling quality and scheduling speed raise further concerns. In this paper, we first propose a virtualized & monitoring SaaS model with predictive maintenance to minimize the costs of fault tolerance. Then with the monitored and predicted available states of servers, we focus on dynamic real-time task scheduling in geo-distributed and large-scale DCs with heterogeneous servers. Multiple objectives, including the long-term performance benefits, energy and communication costs, are taken into consideration in order to improve scheduling quality. For inter-DC and intra-DC task scheduling, two dynamic programming problems are formulated respectively, but there exists the problem that both state and action spaces are too large to be solved by simple iterations. To address this issue, we introduce the idea of reinforcement learning theory into solving traditional stochastic dynamic programming problems in the large-scale SaaS cloud, and put forward a cascaded two-level (inter-DC and intra-DC level) approximate dynamic programming (ADP) task-scheduling algorithm. The computation complexity can be significantly reduced and scheduling speed can be greatly improved. Finally, we conduct experiments with both random simulation data and Google cloud trace-logs. QoS evaluations and comparisons demonstrate that two ADP algorithms can work cooperatively, and our two-level ADP algorithm is more effective under large quantity of bursty requests.	https://dx.doi.org/10.1007/s11280-019-00680-2	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2021b	DREVAN: Deep Reinforcement Learning-based Vulnerability-Aware Network Adaptations for Resilient Networks	In this work, we proposed a vulnerability-aware network adaptation framework that can generate a robust network topology against epidemic attacks by leveraging deep reinforcement learning (DRL) to build a resilient network. We call our proposed framework DREVAN, representing Deep REinforcement Learning-based Vulnerability-Aware Network Adaptations. The goal of the proposed DREVAN is to minimize security vulnerability caused by epidemic attacks exploiting the software monoculture for node compromise while maximizing network connectivity in terms of the size of the giant component. To be specific, the DREVAN aims to autonomously identify a pair of network adaptation budgets for adding or removing edges (i.e., how many edges to add or remove) by leveraging DRL. In this work, we tackle the inherent challenge of using DRL in reducing the learning curve of a DRL agent by proposing two algorithms. First, we proposed a vulnerability ranking algorithm of edges and nodes, namely VREN, for the DRL agent to select which edges to add or remove based on the lowest expected vulnerability between two nodes or the highest vulnerability of edges, respectively. Second, we also developed a Fractal-based Solution Search algorithm (FSS) to effectively direct the DRL agent towards the effective samples to visit and quickly identify and converge to an optimal solution (i.e., budget sizes of edge additions and removals). Via our extensive comparative performance analysis of the six different schemes, we demonstrated the outperformance of our proposed DREVAN-based schemes over counterpart and baseline schemes.	https://dx.doi.org/10.1109/CNS53000.2021.9705041	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2012	Reinforcement learning in robot path optimization		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859154510&doi=10.4304\%2fjsw.7.3.657-662&partnerID=40&md5=208dc2df6471aa0b70319b04be4231dd	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2021c	Robustness against adversary models on MNIST by Deep-Q Reinforcement Learning based Parallel-GANs	This paper presents a method of enhancing robustness of classification machine learning model. Robustness of computer programming is an important topic, not only affects the security of user information but also stability and performance of program itself. That is, a technique aims to misclassify the machine learning model is called Adversarial Attack. We present an experiment to increase the robustness of machine learning models. The dataset MNIST, which includes hand-writing digits, is used as the experiment subject. Several techniques are applied such as Generative Adversarial Networks in Parallel form, Reinforcement Learning in Deep-Q formation, Dynamic sampling is used as prediction of unknown attack. Black-box is set to be experimental scenario. The experimental results show that the average robustness of the system under several attack conditions is as high as 90\%.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2019b	Experience-Driven Wireless D2D Network Link Scheduling: A Deep Learning Approach	The protocol design of device-to-device (D2D) networks have regained research interest in recent years, due to the increasing number of networking devices and the diverse deployment settings. Most of the network optimization tasks are fundamentally difficult NP-hard problems in wireless settings, because managing interference introduces combinatorial complexity. Existing approaches use general heuristic algorithms for the underlying graph problems. While efficient and simple, they are not adaptive to the changing requirement and priorities of the service providers, and make no use of the past data to recognize and exploit the information within. In this paper, we study a representative network optimization task of maximizing the throughput-based system utility through link scheduling in a single-radio, single-channel D2D networks, and propose a learning-based method to leverage past experience to generate a good scheduling policy. We combine the pattern matching capabilities provided from recurrent neural networks (RNN) and the flexibility in changing environment from reinforcement learning (RL). The algorithm is implemented with existing software frameworks and tested with numerical experiments. We find that its overall solution quality is comparable to existing heuristics with various network scales, and report an improved system throughput with significant lower computation time.	https://dx.doi.org/10.1109/ICC.2019.8761818	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2020a	Design and implementation of reinforcement learning-based intelligent jamming system	Here the intelligent jammer issue is studied. With the rapid development of cognitive radio technology, current cognitive terminals can adaptively or intelligently switch channel by spectrum sensing and decision-making. Most of the traditional jamming methods, such as swept jamming and comb jamming, generally work in a relatively fixed pattern, which are not able to effectively jam the terminals empowered with cognition and spectrum decision-making capability. In view of this problem, the authors propose an intelligent jamming decision-making system based on reinforcement learning. First, in order to jam a pair of transmitter and receiver with adaptive frequency hopping capability, a jammer with spectrum sensing, offline training and learning scheme is proposed. Second, a reinforcement learning-based algorithm for jamming decision-making is proposed and simulated. A special feature of the proposed scheme is that considering the reward is difficult to obtain in the actual communication system, a virtual jamming decision-making method is used to enable the jammer to learn and jam efficiently without the user's prior information. Finally, the proposed jamming model and algorithm are implemented and verified on Universal software radio peripheral testbed.	https://dx.doi.org/10.1049/iet-com.2020.0410	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2022h	Efficient Hierarchical Storage Management Empowered by Reinforcement Learning	With the rapid development of big data and cloud computing, data management has become increasingly challenging. Over the years, a number of frameworks for data management have become available. Most of them are highly efficient, but ultimately create data silos. It becomes difficult to move and work coherently with data as new requirements emerge. A possible solution is to use an intelligent hierarchical (multi-tier) storage system (HSS). A HSS is a meta solution that consists of different storage frameworks organized as a jointly constructed storage pool. A built-in data migration policy that determines the optimal placement of the datasets in the hierarchy is essential. Placement decisions is a non-trivial task since it should be made according to the characteristics of the dataset, the tier status in a hierarchy, and access patterns. This paper presents an open-source hierarchical storage framework with a dynamic migration policy based on reinforcement learning (RL). We present a mathematical model, a software architecture, and implementations based on both simulations and a live cloud-based environment. We compare the proposed RL-based strategy to a baseline of three rule-based policies, showing that the RL-based policy achieves significantly higher efficiency and optimal data distribution in different scenarios.	https://dx.doi.org/10.1109/TKDE.2022.3176753	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2023	How to Mitigate DDoS Intelligently in SD-IoV: A Moving Target Defense Approach	Software defined Internet of Vehicles (SD-IoV) is an emerging paradigm for accomplishing Industrial Internet of Things (IIoT). Unfortunately, SD-IoV still faces security challenges. Traditional solutions respond after attacks happening, which is low-effective. To cope with this problem, moving target defense (MTD) was proposed to modify network configurations dynamically. However, current MTD for IIoT has several drawbacks: 1) it cannot handle highly dynamic environments; 2) MTD strategy lacks intelligence because it needs attack-defense models; 3) they are difficult to trace sources. In this article, we propose an intelligent MTD scheme to defend against distributed denial-of-service in SD-IoV. Firstly, we model the configuration mutation of roadside units as a Markov decision process (MDP), and adopt deep reinforcement learning to solve the optimal configuration. Next, we evaluate the trust of vehicles after shuffling, which can distinguish spy vehicles. Finally, extensive simulation results confirm the effectiveness of our solution compared with representative methods.	https://dx.doi.org/10.1109/TII.2022.3190556	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2022i	Integrated Modular Avionics System Reconstruction Method Based on Sequential Game Multi-agent Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130045134&doi=10.12263\%2fDZXB.20211268&partnerID=40&md5=482bceba81be819971ccd9e71a27a114	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang1995	High-performance job-shop scheduling with a time-delay TD(?) network			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang1995_1	High-performance job-shop scheduling with a time-delay TD(?) network			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2021d	An Edge-Cloud Integrated Solution for Buildings Demand Response Using Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098320403&doi=10.1109\%2fTSG.2020.3014055&partnerID=40&md5=3c880fb19df8c81cc15cf1bc7cc76b33	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2012a	A learning strategy for software testing optimization based on dynamic programming		https://doi.org/10.1145/2430475.2430483	Included	new_screen		4
RL4SE	Zhang2021e	Testbed implementation of reinforcement learning-based demand response energy management system		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108718015&doi=10.1016\%2fj.apenergy.2021.117131&partnerID=40&md5=2db4961b1cc50ce5f13a92485cd0c605	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2021f	Testbed implementation of reinforcement learning-based demand response	Demand response (DR) has been acknowledged as an effective method to improve the stability, and financial efficiency of power grids. During operation of a DR program, there are usually multiple interactions among different grid entities, which complicates decision-making processes with respect to grid operations. Recently, reinforcement learning (RL) has attracted increasing attention for managing complex decision-making problems, owing to its self-learning capacity. Several theoretical RL-based approaches have been proposed for addressing various DR issues, but the practical feasibility of these theoretical approaches remains to be proven. In this paper, a conceptual architecture is firstly proposed to support DR management of a diversified facility in the context of a price-based DR environment. Secondly, exhaustive guidelines are provided to illustrate how to implement a multi-agent RL-based algorithm in the constructed DR management system. Afterwards, a laboratory-level testbed was set up to evaluate the effectiveness of the deployed DR algorithm. The experimental evaluation results show that the RL-based DR algorithm takes about 20s and 50 episodes to achieve optimal load control policy. By executing the optimal operation policy, the overall energy consumption during the highest price period (i.e., 15:00-18:00) is significantly reduced by 133.6\% compared with the lowest price period (i.e., 02:00-05:00).	https://dx.doi.org/10.1016/j.apenergy.2021.117131	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2022j	Query and Attention Augmentation for Knowledge-Based Explainable Reasoning	Explainable visual question answering (VQA) models have been developed with neural modules and query-based knowledge incorporation to answer knowledge-requiring questions. Yet, most reasoning methods cannot effectively generate queries or incorporate external knowledge during the reasoning process, which may lead to suboptimal results. To bridge this research gap, we present Query and Attention Augmentation, a general approach that augments neural module networks to jointly reason about visual and external knowledge. To take both knowledge sources into account during reasoning, it parses the input question into a functional program with queries augmented through a novel reinforcement learning method, and jointly directs augmented attention to visual and external knowledge based on intermediate reasoning results. With extensive experiments on multiple VQA datasets, our method demonstrates significant performance, explainability, and generalizability over state-of-the-art models in answering questions requiring different extents of knowledge. Our source code is available at https://github.com/SuperJohnZhang/QAA.	https://dx.doi.org/10.1109/CVPR52688.2022.01513	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2021g	Open-Loop Motion Control of a Hydraulic Soft Robotic Arm Using Deep Reinforcement Learning		https://doi.org/10.1007/978-3-030-89095-7_30	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2017	Intelligent Cloud Resource Management with Deep Reinforcement Learning	The cloud provides low-cost and flexible IT resources (hardware and software) across the Internet. As more cloud providers seek to drive greater business outcomes and the environments of the cloud become more complicated, it is evident that the era of the intelligent cloud has arrived. The intelligent cloud faces several challenges, including optimizing the economic cloud service configuration and adaptively allocating resources. In particular, there is a growing trend toward using machine learning to improve the intelligence of cloud management. This article discusses an architecture of intelligent cloud resource management with deep reinforcement learning. The deep reinforcement learning makes clouds automatically and efficiently negotiate the most appropriate configuration, directly from complicated cloud environments. Finally, we give an example to evaluate and conclude the remarkable ability of the intelligent cloud with deep reinforcement learning.	https://dx.doi.org/10.1109/MCC.2018.1081063	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2022k	A dynamic planning model for deploying service functions chain in fog-cloud computing		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135161938&doi=10.1016\%2fj.jksuci.2022.07.012&partnerID=40&md5=c228c9dccefeedd76f8e9a7ba5c5fd41	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2019c	Photo Cropping via Deep Reinforcement Learning	Automatic image cropping aims at changing the composition of images to improve the aesthetic quality of images. It can provide professional advice for image editors and save time. Most of the existing automatic image cropping methods are based on specific features such as aesthetic features or salient features. These methods adopt sliding window mechanism to generate numerous cropping candidates, and then select the final results based on these specific features. It is very time-consuming and can only produce cropping results of a limited aspect ratio. In the face of these situations, a DLRL (deep learning framework combined with reinforcement learning) framework is proposed for image cropping, which only uses the basic features of the image for cropping without producing numerous candidate windows. Moreover, cropping step by step is more in line with the process of image cropping by people using Photoshop or other software. Experiments show that the proposed method can save a lot of time and improve cropping efficiency. The method proposed achieves the state-of-art performance in the open Flickr Cropping Dataset and CUHK Image Cropping Dataset.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2020b	Research on Agent Control Algorithm Based on Reinforcement Learning	The control algorithm of the agent is an important research area of artificial intelligence, which reflects the way the agent completes tasks in a dynamic environment. Reinforcement learning is an advanced agent learning algorithm, which makes decision learning through the interaction between the agent and the dynamic environment, improves the strategy after repeated experiments, and finally gets the optimal action plan. In this paper, the PUMA560 robotic arm is selected as the agent body for simulation experiment research. In order to design a high-precision, low-error, fast-control robotic arm control method, we use Webots simulation software as a platform, use image-based visual servoing control algorithms, and use reinforcement learning algorithm Q-learning as the basis to improve and optimize control method.	https://dx.doi.org/10.1109/ICCEA50009.2020.00127	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2021h	Residential Demand Response Considered Strategic Bidding for Load Aggregators with Soft Actor-Critic Algorithm		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124708110&doi=10.1109\%2fIAS48185.2021.9677133&partnerID=40&md5=e5243894d2d49cadf03cf1fddd543687	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2022l	Soft Actor-Critic Algorithm Featured Residential Demand Response Strategic Bidding for Load Aggregators		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129617955&doi=10.1109\%2fTIA.2022.3172068&partnerID=40&md5=afbe2d98321af6291b4c120fa73b7ecf	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2021i	Reinforcement learning-based fuzzing technology		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087001692&doi=10.1007\%2f978-3-030-50399-4_24&partnerID=40&md5=a84c794934c57e5d1dd8f585d7093b70	Included	new_screen		4
RL4SE	Zhang2022m	Federated Reinforcement Learning for Real-Time Electric Vehicle Charging and Discharging Control	With the recent advances in mobile energy storage technologies, electric vehicles (EVs) have become a crucial part of smart grids. When EVs participate in the demand response program, the charging cost can be significantly reduced by taking full advantage of the real-time pricing signals. However, many stochastic factors exist in the dynamic environment, bringing significant challenges to design an optimal charging/discharging control strategy. This paper develops an optimal EV charging/discharging control strategy for different EV users under dynamic environments to maximize EV users' benefits. We first formulate this problem as a Markov decision process (MDP). Then we consider EV users with different behaviors as agents in different environments. Furthermore, a horizontal federated reinforcement learning (HFRL)-based method is proposed to fit various users' behaviors and dynamic environments. This approach can learn an optimal charging/discharging control strategy without sharing users' profiles. Simulation results illustrate that the proposed real-time EV charging/discharging control strategy can perform well among various stochastic factors.	https://dx.doi.org/10.1109/GCWkshps56602.2022.10008598	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2022n	UniRLTest: Universal platform-independent testing with reinforcement learning via image understanding		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136821763&doi=10.1145\%2f3533767.3543292&partnerID=40&md5=5aec148a2629c348684ea1752ce31976	Included	conflict_resolution		4
RL4SE	Zhang2019d	Reinforcement learning in clinical medicine: a method to optimize dynamic treatment regime over time	Precision medicine requires individualized treatment regime for subjects with different clinical characteristics. Machine learning methods have witnessed rapid progress in recent years, which can be employed to make individualized treatment regime in clinical practice. The idea of reinforcement learning method is to take action in response to the changing environment. In clinical medicine, this idea can be used to assign optimal regime to patients with distinct characteristics. In the field of statistics, reinforcement learning has been widely investigated, aiming to identify an optimal dynamic treatment regime (DTR). Q-learning is among the earliest methods to identify optimal DTR, which fits linear outcome models in a recursive manner. The advantage is its easy interpretation and can be performed in most statistical software. However, it suffers from the risk of misspecification of the linear model. More recently, some other methods not so heavily depend on model specification have been developed such as inverse probability weighted estimator and augmented inverse probability weighted estimator. This review introduces the basic ideas of these methods and shows how to perform the learning algorithm within R environment.	https://www.ncbi.nlm.nih.gov/pubmed/31475215	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang2021j	Reinforcement and Transfer Learning for Distributed Analytics in Fragmented Software Defined Coalitions	To support network agility and dynamism, the Distributed Analytics and Information Sciences (DAIS) International Technology Alliance (ITA) https://dais-ita.org/pub has introduced a new architecture called Software Defined Coalitions (SDC), which significantly extends the Software Defined Networking (SDN) to include communication, computation, storage, database and sensor resources. Reinforcement Learning (RL) has been shown to be very effective for managing SDC. However, due to link failure or operational requirements, SDC may become fragmented and reconnected again over time. This paper aims to investigate how RL can be made robust and efficient by transfer learning (TL) in presence of SDC fragmentation. For illustration, we consider an SDC with two domains, which allocate analytic tasks to data servers for processing. Each domain has a local RL agent to distribute tasks to servers within the domain. When the SDC is formed, a global RL agent is established to interact with the two local agents so that tasks can now be allocated to servers anywhere across the SDC for efficiency. Our objective here is two-folded. First, training the local RL agent is challenging due to the space-action explosion. We adopt and show how a newly developed method that separates state from action space can improve training. Second, we develop a TL technique to train the global RL agent, which can significantly reduce the amount of time required for achieving close to the optimal performance after the SDC domains are reconnected following fragmentation. As a result, the combined RL-TL technique enables efficient and robust management and control of SDC despite fragmentation.	https://dx.doi.org/10.1117/12.2587874	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhao2021	A cooperative water wave optimization algorithm with reinforcement learning for the distributed assembly no-idle flowshop scheduling problem	The distributed assembly no-idle flow-shop scheduling problem (DANIFSP) is a novel and considerable model, which is suitable for the modern supply chains and manufacturing systems. In this study, a cooperative water wave optimization algorithm, named CWWO, is proposed to address the DANIFSP with the goal of minimizing the maximum assembly completion time. In the propagation phase, a reinforcement learning mechanism based on the framework of the VNS is designed to balance the exploration and exploitation of the CWWO algorithm. Afterwards, the path-relinking combined with the VNS method as the modified breaking operator is introduced to enhance the capability of local search. Furthermore, a multi-neighborhood perturbation strategy in the refraction phase is applied to extract knowledge information to increase the probability of escaping the local optimal. Moreover, the comprehensive experimental program is executed to calibrate the control parameters of the CWWO algorithm and illustrate the cooperative effect of the three modified operations. The performance of the CWWO algorithm is verified on the benchmark set, and the experimental results demonstrated the stability and validity of the CWWO algorithm.	https://dx.doi.org/10.1016/j.cie.2020.107082	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhao2020	Airport gate assignment problem with deep reinforcement learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083191858&doi=10.3772\%2fj.issn.1006-6748.2020.01.014&partnerID=40&md5=91ba726e5a0c66922dd5d89e4039713e	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhao2019	Routing for Crowd Management in Smart Cities: A Deep Reinforcement Learning Perspective	The concept of smart city has been flourishing based on the prosperous development of various advanced technologies: mobile edge computing (MEC), ultra-dense networking, and software defined networking. However, it becomes increasingly complicated to design routing strategies to meet the stringent and ever changing network requirements due to the dynamic distribution of the crowd in different sectors of smart cities. To alleviate the network congestion and balance the network load for supporting smart city services with dramatic disparities, we design a deep-reinforcement- learning-based smart routing algorithm to make the distributed computing and communication infrastructure thoroughly viable while simultaneously satisfying the latency constraints of service requests from the crowd. Besides the proposed algorithm, extensive numerical results are also presented to validate its efficacy.	https://dx.doi.org/10.1109/MCOM.2019.1800603	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhao2017	A Reinforcement Learning-Based Framework for the Generation and Evolution of Adaptation Rules	One of the challenges in self-adaptive systems concerns how to make adaptation to themselves at runtime in response to possible and even unexpected changes from the environment and/or user goals. A feasible solution to this challenge is rule-based adaptation, in which, adaptation decisions are made according to predefined rules that specify what particular actions should be performed to react to different changing events from the environment. Although it has the characteristic of highly- efficient decision making for adaptation, rule-based adaptation has two limitations: 1. no guarantee that those predefined rules will lead to optimal or nearly-optimal adaptation results; 2. weak support to evolve these rules to cope with non-stationary environment and changeable user goals at runtime. In this paper, we propose a reinforcement learning-based framework to the generation and evolution of software adaptation rules. This framework manifests two key capabilities for self-adaptation: 1. the capability of automatically learning adaptation rules from different goal settings at the offline phase; 2. the capability of automatically evolving adaptation rules from real-time information about the environment and user goals at the online phase. The two capabilities are built on the combination of reinforcement learning and case-based reasoning techniques. This framework improves the existing rule-based adaptation from two points: the flexibility of adaptation logic, and the quality of adaptation rules. We evaluate this framework through a case study of an E-commerce web application, which shows that this framework improves both the efficiency and effectiveness of self-adaptation.	https://dx.doi.org/10.1109/ICAC.2017.47	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhao2007	An optimal control method for hybrid systems based on Q-learning for an intersection traffic signal control			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhao2008	A method to design reinforcement function based on fuzzy rules in Q-learning			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhao2022	Automatic Parameter Tuning via Reinforcement Learning for Crowd Simulation with Social Distancing	Reinforcement learning (RL) has been applied to a variety of fields such as gaming and robot navigation. We study the application of RL in crowd simulation by proposing an automatic parameter tuning system based on Proximal Policy Optimization (PPO). The system can be used with any crowd simulation software to improve the quality of the simulation by automatically assigning parameters to each agent during the simulation. Our experiments indicate that the automatic parameter tuning system can reduce unexpected congestions in counterflow scenarios. In addition, by utilizing the improved commonly used crowd simulation algorithms and our parame-ter tunning system, we can represent social distancing behavior of pedestrians under COVID-19, where pedestrians comply to the suggested social distance when they have enough space to move while they reduce their social distances to others when there is limited space.	https://dx.doi.org/10.1109/MMAR55195.2022.9874284	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhao2021a	Design of Friends-Making Recommendation System Based on Reinforcement Learning and Generative Adversarial Network		https://doi.org/10.1145/3452940.3453040	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhao2022a	A Lightweight Approach of Human-Like Playtest for Android Apps	A play test is the process in which testers play video games for software quality assurance. Manual testing is expensive and time-consuming, especially when there are many mobile games to test and every game version requires extensive testing. Current testing frameworks (e.g., Android Monkey) are limited as they adopt no domain knowledge to play games. Learning-based tools (e.g., Wuji) require tremendous manual effort and ML expertise of developers. This paper presents LIT-a lightweight approach to generalize play test tactics from manual testing, and to adopt the tactics for automatic testing. Lit has two phases: tactic generalization and tactic concretization. In Phase I, when a human tester plays an Android game $G$ for a while (e.g., eight minutes), Lit records the tester's inputs and related scenes. Based on the collected data, Lit infers a set of context-aware, abstract play test tactics that describe under what circumstances, what actions can be taken. In Phase II, LIttests $G$ based on the generalized tactics. Namely, given a randomly generated game scene, Lit tentatively matches that scene with the abstract context of any inferred tactic; if the match succeeds, Lit customizes the tactic to generate an action for playtest. Our evaluation with nine games shows Lit to outperform two state-of-the-art tools and a reinforcement learning (RL)-based tool, by covering more code and triggering more errors. Lit complements existing tools and helps developers test various casual games (e.g., match3, shooting, and puzzles).	https://dx.doi.org/10.1109/SANER53432.2022.00047	Included	new_screen		4
RL4SE	Zhao2022b	DouZero+: Improving DouDizhu AI by Opponent Modeling and Coach-guided Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139109732&doi=10.1109\%2fCoG51982.2022.9893710&partnerID=40&md5=db20c87611f56af6bb8788e8f1b02a9d	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhao2022c	A brain-inspired intention prediction model and its applications to humanoid robot	With the development of artificial intelligence and robotic technology in recent years, robots are gradually integrated into human daily life. Most of the human-robot interaction technologies currently applied to home service robots are programmed by the manufacturer first, and then instruct the user to trigger the implementation through voice commands or gesture commands. Although these methods are simple and effective, they lack some flexibility, especially when the programming program is contrary to user habits, which will lead to a significant decline in user experience satisfaction. To make that robots can better serve human beings, adaptable, simple, and flexible human-robot interaction technology is essential. Based on the neural mechanism of reinforcement learning, we propose a brain-inspired intention prediction model to enable the robot to perform actions according to the user's intention. With the spike-timing-dependent plasticity (STDP) mechanisms and the simple feedback of right or wrong, the humanoid robot NAO could successfully predict the user's intentions in Human Intention Prediction Experiment and Trajectory Tracking Experiment. Compared with the traditional Q-learning method, the training times required by the proposed model are reduced by (N-2 - N)/4, where N is the number of intentions.	https://www.ncbi.nlm.nih.gov/pubmed/36340762	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zheng2016	OWLS: Observational Wireless Life-enhancing System (Extended Abstract)			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E5: Other not a paper	4
RL4SE	Zheng2012	An utility-based job scheduling algorithm for current computing Cloud considering reliability factor	The analysis and research of power system necessitates the current computing. However, the bottleneck of current computing lies in the limited computing capacity in power system. Cloud computing's service-oriented characteristics advance a new way of service provisioning called utility based computing, which could provide powerful computing capability for current computing. However, toward the deployment of practical current computing Cloud, we encounter one challenge that the existing job scheduling algorithms under utility based computing do not take hardware/software failure and recovery in the Cloud into account. In an attempt to address this challenge, we introduce the failure and recovery scenario in the current Cloud computing entities and propose a Reinforcement Learning (RL) based algorithm to make job scheduling in the current computing Cloud fault tolerant. We carry out experimental comparison with Resource-constrained Utility Accrual algorithm (RUA), Utility Accrual Packet scheduling algorithm (UPA) and LBESA to demonstrate the feasibility of our proposed approach.	https://dx.doi.org/10.1109/ICSESS.2012.6269464	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zheng2016a	OWLS: Observational Wireless Life-enhancing System	Socially assistive robotics technologies for individuals, who have been affected by age-related disabilities and similar types of disorders, have become popular options for facilitating natural independence and uninterrupted mobility. Wireless wearable sensor systems enable proactive personal health management and the ubiquitous monitoring of vital signs to keep an active watch on immediate health conditions. In this paper, we develop a system, called OWLS, where multiple wearable sensors, software agents, robots and health analysis technology, have been integrated into a single personal therapy solution (SPTS). Our system uses a reinforcement learning algorithm to make decisions about the user's current health conditions, and to take appropriate actions, as necessary (i.e, contacting outside parties). We show that the approach of non-invasive monitoring, when combined with an alert system, makes this a desirable SPTS in future health care.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zheng2019	StrokeNet: A neural painting environment			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zheng2021	Blockchain-Based Secure Computation Offloading in Vehicular Networks	Vehicular ad hoc networks (VANETs) has become an important part of modern intelligent transportation systems (ITS). However, under the influence of malicious mobile vehicles, offloading vehicle tasks to the cloud server is threatened by security attacks. Edge cloud offloading (ECCO) has considered a promising approach to enable latency-sensitive VANET. How to solve the complex computation offloading of vehicles while ensuring the high security of the cloud server is an issue that needs urgent research. In this paper, we studied the safety and offloading of multi-vehicle ECCO system based on cloud blockchain. First, to achieve consensus in the vehicular environment, we propose a distributed hierarchical software-defined VANET (SDVs) framework to establish a security architecture. Secondly, to improve the security of offloading, we propose to use blockchain-based access control, which protects the cloud from illegal offloading actions. Finally, to solve the intensive computing problem of authorized vehicles, we determine task offloading via jointly optimizing offloading decisions, consensus mechanism decisions, allocation of computation resources and channel bandwidth. The optimization method is designed to minimize long-term system of delays, energy consumption, and flow costs for all vehicles. To better resolve the proposed offloading method, we develop a new deep reinforcement learning (DRL) algorithm via utilizing extended deep Q-networks. We evaluate the performance of our framework on access control and offloading through numerical simulations, which have significant advantages over existing solutions.	https://dx.doi.org/10.1109/TITS.2020.3014229	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zheng2022	Research on Generalized Intelligent Routing Technology Based on Graph Neural Network	Aiming at the problems of poor load balancing ability and weak generalization of the existing routing algorithms, this paper proposes an intelligent routing algorithm, GNN-DRL, in the Software Defined Networking (SDN) environment. The GNN-DRL algorithm uses a graph neural network (GNN) to perceive the dynamically changing network topology, generalizes the state of nodes and edges, and combines the self-learning ability of Deep Reinforcement Learning (DRL) to find the optimal routing strategy, which makes GNN-DRL minimize the maximum link utilization and reduces average end-to-end delay under high network load. In this paper, the GNN-DRL intelligent routing algorithm is compared with the Open Shortest Path First (OSPF), Equal-Cost Multi-Path (ECMP), and intelligence-driven experiential network architecture for automatic routing (EARS). The experimental results show that GNN-DRL reduces the maximum link utilization by 13.92\% and end-to-end delay by 9.48\% compared with the superior intelligent routing algorithm EARS under high traffic load, and can be effectively extended to different network topologies, making possible better load balancing capability and generalizability.	https://dx.doi.org/10.3390/electronics11182952	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zheng2022a	Research on Energy-Saving Routing Technology Based on Deep Reinforcement Learning	With the vigorous development of the Internet, the network traffic of data centers has exploded, and at the same time, the network energy consumption of data centers has also increased rapidly. Existing routing algorithms only realize routing optimization through Quality of Service (QoS) and Quality of Experience (QoE), which ignores the energy consumption of data center networks. Aiming at this problem, this paper proposes an Ee-Routing algorithm, which is an energy-saving routing algorithm based on deep reinforcement learning. First, our method takes the energy consumption and network performance of the data plane in the software-defined network as the joint optimization goal and establishes an energy-efficient traffic scheduling scheme for the elephant flows and the mice flows. Then, we use Deep Deterministic Policy Gradient (DDPG), which is a deep learning framework, to achieve continuous and energy-efficient traffic scheduling for joint optimization goals. The training process of our method is based on a Convolutional Neural Network (CNN), which can effectively improve the convergence efficiency of the algorithm. After the algorithm training converges, the energy-efficient path weights of the elephant flows and the mice flows are output, and the balanced scheduling of routing energy-saving and network performance is completed. Finally, the results show that our algorithm has good convergence and stability. Compared with the DQN-EER routing algorithm, Ee-Routing improves the energy saving percentage by 13.93\%, and compared with the EARS routing algorithm, Ee-Routing reduces the delay by 13.73\%, increases the throughput by 10.91\%, and reduces the packet loss rate by 13.51\%.	https://dx.doi.org/10.3390/electronics11132035	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhi-Xuan2020	Online Bayesian goal inference for boundedly-rational planning agents			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhi-Yong2007	On-line Reinforcement Learning Control for Urban Traffic Signals	It is quit difficult to archive perfect effects by applying the traditional modeling and control methods to the urban traffic signal control system because of non-linearity, fuzzyness, self-organization and uncertainty in the system. The artificial intelligence technologies may offer a new way to resolve this problem. In allusion to characteristics of the traffic signal control system, this paper proposes an on-line control algorithm based on Dyna-Q reinforcement learning, and utilizes the experiential knowledge gained by the traffic signal control agent in the trial-error process to estimate the model, and then plans the actions in the estimated model, accordingly it can accelerate the iterative process of the Q-learning. This paper adapts TSIS(a microscopic traffic analysis software) to implement the simulation on two traffic trunk roads which consist of 10 intersections. Comparing with fixed-time control, genetic algorithm and Q-learning control algorithm, simulation results indicate that Dyna-Q reinforcement learning algorithm has an obvious superiority.	https://dx.doi.org/10.1109/CHICC.2006.4347023	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhong2012	Guidance Compliance Behavior on VMS Based on SOAR Cognitive Architecture	SOAR is a cognitive architecture named from state, operator and result, which is adopted to portray the drivers' guidance compliance behavior on variable message sign (VMS) in this paper. VMS represents traffic conditions to drivers by three colors: red, yellow, and green. Based on the multiagent platform, SOAR is introduced to design the agent with the detailed description of the working memory, long-term memory, decision cycle, and learning mechanism. With the fixed decision cycle, agent transforms state through four kinds of operators, including choosing route directly, changing the driving goal, changing the temper of driver, and changing the road condition of prediction. The agent learns from the process of state transformation by chunking and reinforcement learning. Finally, computerized simulation program is used to study the guidance compliance behavior. Experiments are simulated many times under given simulation network and conditions. The result, including the comparison between guidance and no guidance, the state transition times, and average chunking times are analyzed to further study the laws of guidance compliance and learning mechanism.	https://dx.doi.org/10.1155/2012/530561	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhou2021	Unsupervised Program Synthesis for Images by Sampling without Replacement			Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhou2020	Adaptive interventions for optimizing malaria control: An implementation study protocol for a block-cluster randomized, sequential multiple assignment trial	"BACKGROUND: In the past two decades, the massive scale-up of long-lasting insecticidal nets (LLINs) and indoor residual spraying (IRS) has led to significant reductions in malaria mortality and morbidity. Nonetheless, the malaria burden remains high, and a dozen countries in Africa show a trend of increasing malaria incidence over the past several years. This underscores the need to improve the effectiveness of interventions by optimizing first-line intervention tools and integrating newly approved products into control programs. Because transmission settings and vector ecologies vary from place to place, malaria interventions should be adapted and readapted over time in response to evolving malaria risks. An adaptive approach based on local malaria epidemiology and vector ecology may lead to significant reductions in malaria incidence and transmission risk. METHODS/DESIGN: This study will use a longitudinal block-cluster sequential multiple assignment randomized trial (SMART) design with longitudinal outcome measures for a period of 3 years to develop an adaptive intervention for malaria control in western Kenya, the first adaptive trial for malaria control. The primary outcome is clinical malaria incidence rate. This will be a two-stage trial with 36 clusters for the initial trial. At the beginning of stage 1, all clusters will be randomized with equal probability to either LLIN, piperonyl butoxide-treated LLIN (PBO Nets), or LLIN + IRS by block randomization based on their respective malaria risks. Intervention effectiveness will be evaluated with 12 months of follow-up monitoring. At the end of the 12-month follow-up, clusters will be assessed for ""response"" versus ""non-response"" to PBO Nets or LLIN + IRS based on the change in clinical malaria incidence rate and a pre-defined threshold value of cost-effectiveness set by the Ministry of Health. At the beginning of stage 2, if an intervention was effective in stage 1, then the intervention will be continued. Non-responders to stage 1 PBO Net treatment will be randomized equally to either PBO Nets + LSM (larval source management) or an intervention determined by an enhanced reinforcement learning method. Similarly, non-responders to stage 1 LLIN + IRS treatment will be randomized equally to either LLIN + IRS + LSM or PBO Nets + IRS. There will be an 18-month evaluation follow-up period for stage 2 interventions. We will monitor indoor and outdoor vector abundance using light traps. Clinical malaria will be monitored through active case surveillance. Cost-effectiveness of the interventions will be assessed using Q-learning. DISCUSSION: This novel adaptive intervention strategy will optimize existing malaria vector control tools while allowing for the integration of new control products and approaches in the future to find the most cost-effective malaria control strategies in different settings. Given the urgent global need for optimization of malaria control tools, this study can have far-reaching implications for malaria control and elimination. TRIAL REGISTRATION: US National Institutes of Health, study ID NCT04182126 . Registered on 26 November 2019."	https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088351886&doi=10.1186\%2fs13063-020-04573-y&partnerID=40&md5=048c42b8405cd84ae667f74888af8aba	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhou2023	Early childhood learning patterns for a home visiting program in rural China	This paper uses novel experimental data from a prototypical early childhood home visiting program in China with high-frequency measurements to investigate the growth of multiple skills. After identifying the presence of child skill development on multiple skills during the intervention, we further study the features of child learning patterns. We find that individual heterogeneity and previous task performance (state dependence) are key properties of the child's task performance during the intervention, consistent with models of reinforcement learning.	https://www.ncbi.nlm.nih.gov/pubmed/35567396	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhou2021a	Research on intelligent scheduling and monitoring method of workshop logistics system		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117530595&doi=10.1088\%2f1742-6596\%2f2033\%2f1\%2f012172&partnerID=40&md5=22241cc33cd00fc36af737ff53152f83	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhou2021b	Rate Control Method Based on Deep Reinforcement Learning for Dynamic Video Sequences in HEVC	Rate control (RC) plays a critical role in the transmission of high-quality video data under certain bandwidth restrictions in High Efficiency Video Coding (HEVC). Most current HEVC RC algorithms based on spatio-temporal information for rate-distortion (R-D) model parameters cannot effectively handle the cases with dynamic video sequences that contain fast moving objects, significant object occlusion or scene changes. In this paper, we propose an RC method based on deep reinforcement learning (DRL) for dynamic video sequences in HEVC to improve the coding efficiency. First, the rate control problem is formulated as a Markov decision process (MDP) problem. Second, with the MDP model, we develop a DRL-based algorithm to find the optimal quantization parameters (QPs) by training a deep neural network. The resulting intelligent agent selects the optimal RC strategy to reduce distortion, buffer and quality fluctuations by observing the current state of the encoder. The asynchronous advantage actor-critic (A3C) method is used to solve the MDP problem. Finally, the proposed DRL-based RC method is implemented in the newest video coding standard. Experimental results show that the proposed method offers substantially enhanced RC accuracy and consistently outperforms HEVC reference software and other state-of-the-art algorithms.	https://dx.doi.org/10.1109/TMM.2020.2992968	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhou2002	Computational models of the amygdala and the orbitofrontal cortex: A hierarchical reinforcement learning system for robotic control	This paper presents biologically plausible computational models of brain areas involved in emotion processing and the decision-making process. In the models, the amygdala, the orbotofrontal cortex (OFC) and the basal ganglia work together as a multiple-level hierarchical reinforcement learning system. The amygdala decodes sensory cues into reward-related variables providing a reward-related abstract representation for the decision making process in the OFC, while the basal ganglia learn and execute subtask policies. Here we hypothesize how the amygdala may learn these representations. The models have been implemented in software to control a Khepera robot in a physical environment designed for comparison with animal behaviours. We show that the representation of principal emotion components in the reward function may lead to a more efficient learning algorithm than general Q learning.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhou2022	A Reinforcement Learning Method for Multiasset Roadway Improvement Scheduling Considering Traffic Impacts	Maintaining roadway pavements and bridge decks is key to providing high levels of service for road users. However, improvement actions incur downtime. These actions are typically scheduled by asset class, yet implemented on any asset type, they have network-wide impacts on traffic performance. This paper presents a bilevel program wherein the upper level involves a Markov decision process (MDP) through which potential roadway improvement actions across asset classes are prioritized and scheduled. The MDP approach considers uncertainty in component deterioration effects, while incorporating the benefits of implemented improvement actions. The upper level takes as input traffic flow estimates obtained from a lower-level user equilibrium traffic formulation that recognizes changes in capacities determined by decisions taken in the upper level. Because an exact solution of this bilevel, stochastic, dynamic program is formidable, a deep reinforcement learning (DRL) method is developed. The model and solution methodology were tested on a hypothetical problem from the literature. The importance of obtaining optimal activity plans that account for downtime effects, traffic congestion impacts, uncertainty in deterioration processes, and multiasset classes is demonstrated.	https://dx.doi.org/10.1061/(ASCE)IS.1943-555X.0000702	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhou2022a	Learning to Optimize DAG Scheduling in Heterogeneous Environment	Scheduling job flows efficiently and rapidly on distributed computing clusters is one of huge challenges for daily operation of data centers. In a practical scenario, a single job consists of numerous stages with complex dependency relation represented as a Directed Acyclic Graph (DAG) structure. Nowa-days a data center usually equips with a cluster of heterogeneous computing servers which are different in the hardware/software configuration. From both the cost saving and environmental friendliness, the data centers could benefit a lot from optimizing the job scheduling problems in the heterogeneous environment. Thus the problem has attracted more and more attention from both the industry and academy. In this paper, we propose a task-duplication based learning algorithm, namely LACHESIS22The second of the Three Fates in ancient Greek mythology, who deter-mines destiny., aiming to optimize the problem. In the proposed approach, it first perceives the topological dependencies between jobs using a reinforcement learning framework and a specially designed graph neural network (GNN) to select the most promising task to be executed. Then the task is assigned to a specific executor with the consideration of duplicating all its precedent tasks according to an expert-designed rules. We have conducted extensive experiments over standard workloads to evaluate the proposed solution. The experimental results suggest that LACHESIS can achieve at most 26.7\% reduction of makespan and 35.2\% improvement of speedup ratio over seven strong baseline algorithms, including the state-of-the-art heuristics methods and a variety of deep reinforcement learning based algorithms.	https://dx.doi.org/10.1109/MDM55031.2022.00040	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhou2022b	An End-to-End Automatic Cache Replacement Policy Using Deep Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142619209&doi=10.1609\%2ficaps.v32i1.19840&partnerID=40&md5=1822d7931fca017b7d298ff08f99c24a	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhou2018	Infinite Time Horizon Maximum Causal Entropy Inverse Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035769716&doi=10.1109\%2fTAC.2017.2775960&partnerID=40&md5=9ff127d1b5cd0f94f452dd030dbc991c	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhu2019	An inductive synthesis framework for verifiable reinforcement learning		https://doi.org/10.1145/3314221.3314638	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhu2022	SAAS parallel task scheduling based on cloud service flow load algorithm		https://doi.org/10.1016/j.comcom.2021.10.037	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhu2022a	Intelligent System Application in Clinical Management of Medical Teaching Based on Deep Reinforcement Learning		https://doi.org/10.1155/2022/4881321	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhu2012	Achieving Quality of Service with Adaptation-based Programming for medium access protocols	Designing network protocols that work well under a variety of network conditions typically involves a large amount of manual tuning and guesswork, particularly when choosing dynamic update strategies for numeric parameters. The situation is made more complex by adding the Quality of Service (QoS) requirements to a network protocol. A fundamentally different approach for designing protocols is via Reinforcement Learning (RL) algorithms which allow protocols to be automatically optimized through network simulation. However, getting RL to work well in practice requires considerable expertise and carries a significant implementation overhead. To help overcome this challenge, recent work has developed the programming paradigm of Adaptation-Based Programming (ABP), which allows programmers who are not RL-experts to write self-optimizing ``adaptive programs''. In this work, we study the potential of applying ABP to the problem of designing network protocols via simulation. We demonstrate the flexibility of our design method via a number of case studies, each of which investigates the performance of an adaptive program written for the backoff mechanism of the MAC layer in the 802.11 standard. Our results show that the learned protocols typically outperform 802.11 on a number of evaluation metrics and network conditions.	https://dx.doi.org/10.1109/GLOCOM.2012.6503398	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhu2021	Distantly supervised biomedical relation extraction using piecewise attentive convolutional neural network and reinforcement learning	OBJECTIVE: There have been various methods to deal with the erroneous training data in distantly supervised relation extraction (RE), however, their performance is still far from satisfaction. We aimed to deal with the insufficient modeling problem on instance-label correlations for predicting biomedical relations using deep learning and reinforcement learning. MATERIALS AND METHODS: In this study, a new computational model called piecewise attentive convolutional neural network and reinforcement learning (PACNN+RL) was proposed to perform RE on distantly supervised data generated from Unified Medical Language System with MEDLINE abstracts and benchmark datasets. In PACNN+RL, PACNN was introduced to encode semantic information of biomedical text, and the RL method with memory backtracking mechanism was leveraged to alleviate the erroneous data issue. Extensive experiments were conducted on 4 biomedical RE tasks. RESULTS: The proposed PACNN+RL model achieved competitive performance on 8 biomedical corpora, outperforming most baseline systems. Specifically, PACNN+RL outperformed all baseline methods with the F1-score of 0.5592 on the may-prevent dataset, 0.6666 on the may-treat dataset, and 0.3838 on the DDI corpus, 2011. For the protein-protein interaction RE task, we obtained new state-of-the-art performance on 4 out of 5 benchmark datasets. CONCLUSIONS: The performance on many distantly supervised biomedical RE tasks was substantially improved, primarily owing to the denoising effect of the proposed model. It is anticipated that PACNN+RL will become a useful tool for large-scale RE and other downstream tasks to facilitate biomedical knowledge acquisition. We also made the demonstration program and source code publicly available at http://112.74.48.115:9000/.	https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121224401&doi=10.1093\%2fjamia\%2focab176&partnerID=40&md5=4a3d2aa339e0b8ed7dda68ebc1f255b5	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhuang2021	A Reinforcement-Learning-Based Deployment Strategy for GPP Components		https://doi.org/10.1145/3479162.3479188	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhuo2022	Rearchitecting in-memory object stores for low latency		https://doi.org/10.14778/3494124.3494138	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zolotukhin2021	The Methods for the Prediction of Climate Control Indicators in the Internet of Things Systems			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zou2020	Deploying tactical communication node vehicles with AlphaZero algorithm	The construction of mobile ad-hoc networks on the battlefield is mainly planned by staff or automatically planned with the help of network topology planning models in the network planning software. Most of these algorithms are actually more or less based on human knowledge or thinking ways to model network entities, environments, and rules, and the accuracy and rationality of models are not strong enough to match the changed battlefield. The AlphaZero algorithm generalised in chess and other games provides a new intelligent method to solve complex problems in the military field. Based on the AlphaZero algorithm, this study proposes a method for intelligent deployment of mobile ad-hoc networks with tactical communication node vehicles. Making an analogy between deploying tactical communication node vehicles and playing Go, the authors construct a deep reinforcement learning model for deployment of communication node vehicles. Starting from random play, and giving no domain knowledge, only setting the judgment of the network structure, with training the designing strategy value deep neural network by self-play reinforcement learning, they successfully deployed communication node vehicles on tabula rasa map and constructed battlefield mobile ad-hoc networks with deep reinforcement learning and Monte-Carlo tree search.	https://dx.doi.org/10.1049/iet-com.2019.0349	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zubow2021	GrGym: When GNU Radio goes to (AI) Gym		https://doi.org/10.1145/3446382.3448358	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zuckerman2021	Cohmeleon: Learning-Based Orchestration of Accelerator Coherence in Heterogeneous SoCs		https://doi.org/10.1145/3466752.3480065	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Piette2016_6	Patient-Centered Pain Care Using Artificial Intelligence and Mobile Health Tools: Protocol for a Randomized Study Funded by the US Department of Veterans Affairs Health Services Research and Developme	"BACKGROUND: Cognitive behavioral therapy (CBT) is one of the most effective treatments for chronic low back pain. However, only half of Department of Veterans Affairs (VA) patients have access to trained CBT therapists, and program expansion is costly. CBT typically consists of 10 weekly hour-long sessions. However, some patients improve after the first few sessions while others need more extensive contact. OBJECTIVE: We are applying principles from ""reinforcement learning"" (a field of artificial intelligence or AI) to develop an evidence-based, personalized CBT pain management service that automatically adapts to each patient's unique and changing needs (AI-CBT). AI-CBT uses feedback from patients about their progress in pain-related functioning measured daily via pedometer step counts to automatically personalize the intensity and type of patient support. The specific aims of the study are to (1) demonstrate that AI-CBT has pain-related outcomes equivalent to standard telephone CBT, (2) document that AI-CBT achieves these outcomes with more efficient use of clinician resources, and (3) demonstrate the intervention's impact on proximal outcomes associated with treatment response, including program engagement, pain management skill acquisition, and patients' likelihood of dropout. METHODS: In total, 320 patients with chronic low back pain will be recruited from 2 VA healthcare systems and randomized to a standard 10 sessions of telephone CBT versus AI-CBT. All patients will begin with weekly hour-long telephone counseling, but for patients in the AI-CBT group, those who demonstrate a significant treatment response will be stepped down through less resource-intensive alternatives including: (1) 15-minute contacts with a therapist, and (2) CBT clinician feedback provided via interactive voice response calls (IVR). The AI engine will learn what works best in terms of patients' personally tailored treatment plans based on daily feedback via IVR about their pedometer-measured step counts, CBT skill practice, and physical functioning. Outcomes will be measured at 3 and 6 months post recruitment and will include pain-related interference, treatment satisfaction, and treatment dropout. Our primary hypothesis is that AI-CBT will result in pain-related functional outcomes that are at least as good as the standard approach, and that by scaling back the intensity of contact that is not associated with additional gains in pain control, the AI-CBT approach will be significantly less costly in terms of therapy time. RESULTS: The trial is currently in the start-up phase. Patient enrollment will begin in the fall of 2016 and results of the trial will be available in the winter of 2019. CONCLUSIONS: This study will evaluate an intervention that increases patients' access to effective CBT pain management services while allowing health systems to maximize program expansion given constrained resources."	https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047730597&doi=10.2196\%2fresprot.4995&partnerID=40&md5=68fd2890d6960aa9ee6d6244de1766bd	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Yang2020c_2	Data-Driven Solutions to Mixed H-2/H-infty Control: A Hamilton-Inequality-Driven Reinforcement Learning Approach, booktitle = CCTA 2020 - 4th IEEE Conference on Control Technology and Applications, pa		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094116832&doi=10.1109\%2fCCTA41146.2020.9206320&partnerID=40&md5=8b11dbbce7540795402c042b6aa3a945	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhan2022_2	Reinforcement learning-based register renaming policy for simultaneous multithreading CPUs?		https://doi.org/10.1016/j.eswa.2021.115717	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Zhang1995_2	High-performance job-shop scheduling with a time-delay TD(?) network			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
